{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/smoyarodrigo/03MAIR-Algoritmos-de-Optimizacion/blob/main/Untitled3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5Lr59O0NTlm",
        "outputId": "cfb970a8-c407-4f90-e7b9-8fc56595667c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ejecutando en Colab: True\n"
          ]
        }
      ],
      "source": [
        "# 1.2 Localizar entorno de trabajo\n",
        "mount = '/content/gdrive'\n",
        "drive_root = mount + \"/My Drive/08_MIAR/actividades/proyecto practico\"\n",
        "\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    IN_COLAB = True\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "print(\"Ejecutando en Colab:\", IN_COLAB)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1.3 Montar carpeta de datos local (solo Colab)\n",
        "import os\n",
        "\n",
        "if IN_COLAB:\n",
        "    print(\"Colab: montando Google Drive en\", mount)\n",
        "    drive.mount(mount)\n",
        "\n",
        "    # Asegurarse de que la carpeta de la práctica existe\n",
        "    os.makedirs(drive_root, exist_ok=True)\n",
        "    print(\"Carpeta verificada:\", drive_root)\n",
        "\n",
        "    # Cambiar al directorio de la práctica\n",
        "    %cd $drive_root\n",
        "\n",
        "# Verificamos que estamos en la ruta correcta\n",
        "print(\"Directorio actual:\", os.getcwd())\n",
        "print(\"Archivos disponibles:\", os.listdir())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sSmcj_jBNy3A",
        "outputId": "7c4a481e-4ee6-4d81-b983-d7dbff51cb6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab: montando Google Drive en /content/gdrive\n",
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Carpeta verificada: /content/gdrive/My Drive/08_MIAR/actividades/proyecto practico\n",
            "/content/gdrive/My Drive/08_MIAR/actividades/proyecto practico\n",
            "Directorio actual: /content/gdrive/My Drive/08_MIAR/actividades/proyecto practico\n",
            "Archivos disponibles: ['baseline_Acrobot-v1_weights.h5f.index', 'baseline_Acrobot-v1_weights.h5f.data-00000-of-00001', 'peso_dqn_mejorado_Acrobot-v1_paso_5000.h5f.index', 'peso_dqn_mejorado_Acrobot-v1_paso_5000.h5f.data-00000-of-00001', 'peso_dqn_mejorado_Acrobot-v1_paso_10000.h5f.index', 'peso_dqn_mejorado_Acrobot-v1_paso_10000.h5f.data-00000-of-00001', 'peso_dqn_mejorado_Acrobot-v1_paso_15000.h5f.index', 'peso_dqn_mejorado_Acrobot-v1_paso_15000.h5f.data-00000-of-00001', 'peso_dqn_mejorado_Acrobot-v1_paso_20000.h5f.index', 'peso_dqn_mejorado_Acrobot-v1_paso_20000.h5f.data-00000-of-00001', 'peso_dqn_mejorado_Acrobot-v1_paso_25000.h5f.index', 'peso_dqn_mejorado_Acrobot-v1_paso_25000.h5f.data-00000-of-00001', 'peso_dqn_mejorado_Acrobot-v1_paso_30000.h5f.index', 'peso_dqn_mejorado_Acrobot-v1_paso_30000.h5f.data-00000-of-00001', 'peso_dqn_mejorado_Acrobot-v1_paso_35000.h5f.index', 'peso_dqn_mejorado_Acrobot-v1_paso_35000.h5f.data-00000-of-00001', 'peso_dqn_mejorado_Acrobot-v1_paso_40000.h5f.index', 'peso_dqn_mejorado_Acrobot-v1_paso_40000.h5f.data-00000-of-00001', 'peso_dqn_mejorado_Acrobot-v1_paso_45000.h5f.index', 'peso_dqn_mejorado_Acrobot-v1_paso_45000.h5f.data-00000-of-00001', 'peso_dqn_mejorado_Acrobot-v1_paso_50000.h5f.index', 'peso_dqn_mejorado_Acrobot-v1_paso_50000.h5f.data-00000-of-00001', 'peso_dqn_mejorado_Acrobot-v1_paso_55000.h5f.index', 'peso_dqn_mejorado_Acrobot-v1_paso_55000.h5f.data-00000-of-00001', 'peso_dqn_mejorado_Acrobot-v1_paso_60000.h5f.index', 'peso_dqn_mejorado_Acrobot-v1_paso_60000.h5f.data-00000-of-00001', 'peso_dqn_mejorado_Acrobot-v1_paso_65000.h5f.index', 'peso_dqn_mejorado_Acrobot-v1_paso_65000.h5f.data-00000-of-00001', 'peso_dqn_mejorado_Acrobot-v1_paso_70000.h5f.index', 'peso_dqn_mejorado_Acrobot-v1_paso_70000.h5f.data-00000-of-00001', 'peso_dqn_mejorado_Acrobot-v1_paso_75000.h5f.index', 'peso_dqn_mejorado_Acrobot-v1_paso_75000.h5f.data-00000-of-00001', 'peso_dqn_mejorado_Acrobot-v1_paso_80000.h5f.index', 'peso_dqn_mejorado_Acrobot-v1_paso_80000.h5f.data-00000-of-00001', 'peso_dqn_mejorado_Acrobot-v1_paso_85000.h5f.index', 'peso_dqn_mejorado_Acrobot-v1_paso_85000.h5f.data-00000-of-00001', 'peso_dqn_mejorado_Acrobot-v1_paso_90000.h5f.index', 'peso_dqn_mejorado_Acrobot-v1_paso_90000.h5f.data-00000-of-00001', 'peso_dqn_mejorado_Acrobot-v1_paso_95000.h5f.index', 'peso_dqn_mejorado_Acrobot-v1_paso_95000.h5f.data-00000-of-00001', 'peso_dqn_mejorado_Acrobot-v1_paso_100000.h5f.index', 'dqn_mejorado_Acrobot-v1_final.h5f.index', 'dqn_mejorado_Acrobot-v1_log.json', 'peso_dqn_mejorado_Acrobot-v1_paso_100000.h5f.data-00000-of-00001', 'dqn_mejorado_Acrobot-v1_final.h5f.data-00000-of-00001', 'modelo_keras_completo.h5', 'dqn_weights.h5f.data-00000-of-00001', 'dqn_weights.h5f.index', 'ddqn_weights.h5f.data-00000-of-00001', 'ddqn_weights.h5f.index', 'dueling_dqn_weights.h5f.data-00000-of-00001', 'dueling_dqn_weights.h5f.index', 'checkpoint', 'dqn_full_model.h5', 'ddqn_full_model.h5', 'dueling_dqn_full_model.h5']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install gym==0.17.3\n",
        "%pip install git+https://github.com/Kojoley/atari-py.git\n",
        "%pip install keras-rl2==1.0.5\n",
        "%pip install tensorflow==2.12.0\n",
        "%pip install protobuf==3.20.*\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GPFKSfewOFiP",
        "outputId": "30c1fd47-1ca7-443b-83c4-90ab3070cb55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Ignoring invalid distribution ~rotobuf (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~rotobuf (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: gym==0.17.3 in /usr/local/lib/python3.11/dist-packages (0.17.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from gym==0.17.3) (1.15.3)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.11/dist-packages (from gym==0.17.3) (1.23.5)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from gym==0.17.3) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gym==0.17.3) (1.6.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.11/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym==0.17.3) (1.0.0)\n",
            "\u001b[33mWARNING: Ignoring invalid distribution ~rotobuf (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~rotobuf (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~rotobuf (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting git+https://github.com/Kojoley/atari-py.git\n",
            "  Cloning https://github.com/Kojoley/atari-py.git to /tmp/pip-req-build-ikefyvcp\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/Kojoley/atari-py.git /tmp/pip-req-build-ikefyvcp\n",
            "  Resolved https://github.com/Kojoley/atari-py.git to commit 86a1e05c0a95e9e6233c3a413521fdb34ca8a089\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from atari-py==1.2.2) (1.23.5)\n",
            "\u001b[33mWARNING: Ignoring invalid distribution ~rotobuf (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~rotobuf (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~rotobuf (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: keras-rl2==1.0.5 in /usr/local/lib/python3.11/dist-packages (1.0.5)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (from keras-rl2==1.0.5) (2.12.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (25.2.10)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (1.72.1)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (3.13.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (0.4.30)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (2.12.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (18.1.1)\n",
            "Requirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (24.2)\n",
            "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow->keras-rl2==1.0.5)\n",
            "  Using cached protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (1.17.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (2.12.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (4.14.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow->keras-rl2==1.0.5) (0.45.1)\n",
            "Requirement already satisfied: jaxlib<=0.4.30,>=0.4.27 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow->keras-rl2==1.0.5) (0.4.30)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow->keras-rl2==1.0.5) (0.4.1)\n",
            "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow->keras-rl2==1.0.5) (1.15.3)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (2.38.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (3.8)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (3.1.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (4.9.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (2.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (2025.4.26)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (3.2.2)\n",
            "Using cached protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[33mWARNING: Ignoring invalid distribution ~rotobuf (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mInstalling collected packages: protobuf\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "grpcio-status 1.71.0 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.8 which is incompatible.\n",
            "tf-keras 2.18.0 requires tensorflow<2.19,>=2.18, but you have tensorflow 2.12.0 which is incompatible.\n",
            "tensorflow-decision-forests 1.11.0 requires tensorflow==2.18.0, but you have tensorflow 2.12.0 which is incompatible.\n",
            "bigframes 2.5.0 requires cloudpickle>=2.0.0, but you have cloudpickle 1.6.0 which is incompatible.\n",
            "bigframes 2.5.0 requires numpy>=1.24.0, but you have numpy 1.23.5 which is incompatible.\n",
            "tensorflow-text 2.18.1 requires tensorflow<2.19,>=2.18.0, but you have tensorflow 2.12.0 which is incompatible.\n",
            "ydf 0.12.0 requires protobuf<6.0.0,>=5.29.1, but you have protobuf 4.25.8 which is incompatible.\n",
            "orbax-checkpoint 0.11.13 requires jax>=0.5.0, but you have jax 0.4.30 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed protobuf-4.25.8\n",
            "Requirement already satisfied: tensorflow==2.12.0 in /usr/local/lib/python3.11/dist-packages (2.12.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (25.2.10)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (1.72.1)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (3.13.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (0.4.30)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (2.12.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (18.1.1)\n",
            "Requirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (4.25.8)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (1.17.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (2.12.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (4.14.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow==2.12.0) (0.45.1)\n",
            "Requirement already satisfied: jaxlib<=0.4.30,>=0.4.27 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow==2.12.0) (0.4.30)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow==2.12.0) (0.4.1)\n",
            "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow==2.12.0) (1.15.3)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.38.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.8)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.1.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (4.9.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2025.4.26)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.2.2)\n",
            "Collecting protobuf==3.20.*\n",
            "  Using cached protobuf-3.20.3-py2.py3-none-any.whl.metadata (720 bytes)\n",
            "Using cached protobuf-3.20.3-py2.py3-none-any.whl (162 kB)\n",
            "Installing collected packages: protobuf\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 4.25.8\n",
            "    Uninstalling protobuf-4.25.8:\n",
            "      Successfully uninstalled protobuf-4.25.8\n",
            "\u001b[33mWARNING: Ignoring invalid distribution ~rotobuf (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "grpcio-status 1.71.0 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 3.20.3 which is incompatible.\n",
            "tf-keras 2.18.0 requires tensorflow<2.19,>=2.18, but you have tensorflow 2.12.0 which is incompatible.\n",
            "tensorflow-decision-forests 1.11.0 requires tensorflow==2.18.0, but you have tensorflow 2.12.0 which is incompatible.\n",
            "bigframes 2.5.0 requires cloudpickle>=2.0.0, but you have cloudpickle 1.6.0 which is incompatible.\n",
            "bigframes 2.5.0 requires numpy>=1.24.0, but you have numpy 1.23.5 which is incompatible.\n",
            "tensorflow-text 2.18.1 requires tensorflow<2.19,>=2.18.0, but you have tensorflow 2.12.0 which is incompatible.\n",
            "ydf 0.12.0 requires protobuf<6.0.0,>=5.29.1, but you have protobuf 3.20.3 which is incompatible.\n",
            "orbax-checkpoint 0.11.13 requires jax>=0.5.0, but you have jax 0.4.30 which is incompatible.\n",
            "tensorflow-metadata 1.17.1 requires protobuf<6.0.0,>=4.25.2; python_version >= \"3.11\", but you have protobuf 3.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed protobuf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Imports generales\n",
        "from __future__ import division\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "# Entorno Gym\n",
        "import gym\n",
        "\n",
        "# Deep Learning\n",
        "import tensorflow as tf\n",
        "\n",
        "#Keras-RL\n",
        "from rl.agents.dqn import DQNAgent\n",
        "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy\n",
        "from rl.memory import SequentialMemory\n",
        "from rl.callbacks import FileLogger\n",
        "\n",
        "# capa de compatibilidad con los optimizadores viejo:\n",
        "from tensorflow.keras.optimizers.legacy import Adam"
      ],
      "metadata": {
        "id": "DZE803sbP7J6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env_name = 'Acrobot-v1'\n",
        "env = gym.make(env_name)\n",
        "\n",
        "# Semillas para reproducibilidad\n",
        "np.random.seed(123)\n",
        "env.seed(123)\n",
        "\n",
        "n_obs = env.observation_space.shape\n",
        "nb_actions = env.action_space.n\n"
      ],
      "metadata": {
        "id": "dRnU8gXJQP_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MODELO BASE"
      ],
      "metadata": {
        "id": "M5pjD5t1S3Q8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#\tHyper-params\n",
        "memory_limit\t=\t10000\n",
        "window_length\t=\t4\n",
        "max_eps,\tmin_eps,\ttest_eps\t=\t1.0,\t0.1,\t0.05\n",
        "steps_exploration\t=\t25000\n",
        "steps_warmup\t=\t100\n",
        "gamma\t=\t0.99\n",
        "target_model_update\t=\t1000\n",
        "train_interval\t=\t4\n",
        "learning_rate\t=\t1e-3\n",
        "nb_steps\t=\t50000\n",
        "batch_size\t=\t128\n",
        "#\tModelo\tsimple\n",
        "input_shape\t=\t(n_obs)\n",
        "model\t=\ttf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.Flatten(input_shape=(window_length,)\t+\tenv.observation_space.shape))\n",
        "model.add(tf.keras.layers.Dense(16))\n",
        "model.add(tf.keras.layers.Activation('relu'))\n",
        "model.add(tf.keras.layers.Dense(16))\n",
        "model.add(tf.keras.layers.Activation('relu'))\n",
        "model.add(tf.keras.layers.Dense(16))\n",
        "model.add(tf.keras.layers.Activation('relu'))\n",
        "model.add(tf.keras.layers.Dense(nb_actions))\n",
        "model.add(tf.keras.layers.Activation('linear'))\n",
        "print(model.summary())\n",
        "#\tMemory\n",
        "memory\t=\tSequentialMemory(limit=memory_limit,\twindow_length=window_length)\n",
        "#\tPolicy\n",
        "policy\t=\tLinearAnnealedPolicy(EpsGreedyQPolicy(),\tattr='eps',\tvalue_max=max_eps,\tvalue_min=min_eps,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tvalue_test=test_eps,\tnb_steps=steps_exploration)\n",
        "#\tAgente\n",
        "egreed_dqn\t=\tDQNAgent(model=model,\tnb_actions=nb_actions,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tpolicy=policy,\tmemory=memory,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnb_steps_warmup=steps_warmup,\tgamma=gamma,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\ttarget_model_update=target_model_update,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\ttrain_interval=train_interval,\tbatch_size=batch_size)\n",
        "#\tCompilar\n",
        "egreed_dqn.compile(tf.keras.optimizers.Adam(learning_rate=learning_rate),\tmetrics=['mae'])\n",
        "#\tEntrenar\n",
        "log_filename\t=\t'baseline_{}_log.json'.format(env_name)\n",
        "callbacks\t=\t[FileLogger(log_filename,\tinterval=100)]\n",
        "#egreed_dqn.fit(env,\tcallbacks=callbacks,\tnb_steps=nb_steps,\tvisualize=False,\tverbose=2)\n",
        "egreed_dqn.compile(Adam(learning_rate=learning_rate), metrics=['mae'])\n",
        "#\tSalvar\tpesos\n",
        "egreed_dqn.save_weights('baseline_{}_weights.h5f'.format(env_name),\toverwrite=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oytJL81tSJYI",
        "outputId": "f519f764-2a63-4407-ed4d-da50049776bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten (Flatten)           (None, 24)                0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 16)                400       \n",
            "                                                                 \n",
            " activation (Activation)     (None, 16)                0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 16)                272       \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 16)                0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 16)                272       \n",
            "                                                                 \n",
            " activation_2 (Activation)   (None, 16)                0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 3)                 51        \n",
            "                                                                 \n",
            " activation_3 (Activation)   (None, 3)                 0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 995\n",
            "Trainable params: 995\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#\tTestear\n",
        "egreed_dqn.load_weights('baseline_{}_weights.h5f'.format(env_name))\n",
        "egreed_dqn.test(env,\tnb_episodes=10,\tvisualize=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QpT4cSS4WNPZ",
        "outputId": "9a109b26-3d9e-49ee-8edf-b5c460155e07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing for 10 episodes ...\n",
            "Episode 1: reward: -500.000, steps: 500\n",
            "Episode 2: reward: -500.000, steps: 500\n",
            "Episode 3: reward: -500.000, steps: 500\n",
            "Episode 4: reward: -500.000, steps: 500\n",
            "Episode 5: reward: -500.000, steps: 500\n",
            "Episode 6: reward: -500.000, steps: 500\n",
            "Episode 7: reward: -500.000, steps: 500\n",
            "Episode 8: reward: -500.000, steps: 500\n",
            "Episode 9: reward: -500.000, steps: 500\n",
            "Episode 10: reward: -500.000, steps: 500\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7d0ef0457d50>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importación de bibliotecas estándar para manejo numérico, reproducibilidad y visualización\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import random\n",
        "import gym\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Importación de módulos necesarios de Keras y Keras-RL\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation, Flatten, BatchNormalization, Dropout\n",
        "from tensorflow.keras.optimizers.legacy import Adam\n",
        "from rl.agents.dqn import DQNAgent\n",
        "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy\n",
        "from rl.memory import SequentialMemory\n",
        "\n",
        "# -------------------- CONFIGURACIÓN GLOBAL --------------------\n",
        "\n",
        "# Establecimiento de semillas para garantizar la reproducibilidad de los experimentos\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "# Inicialización del entorno Acrobot-v1 de OpenAI Gym\n",
        "env = gym.make('Acrobot-v1')\n",
        "env.seed(SEED)\n",
        "env.action_space.seed(SEED)\n",
        "env.observation_space.seed(SEED)\n",
        "\n",
        "# Dimensiones del espacio de acción y observación\n",
        "nb_actions = env.action_space.n\n",
        "obs_shape = env.observation_space.shape\n",
        "\n",
        "# -------------------- DEFINICIÓN DE ARQUITECTURAS --------------------\n",
        "\n",
        "# Mejora 1: Arquitectura con normalización por lotes y tres capas densas\n",
        "def build_model_mejora1():\n",
        "    model = Sequential()\n",
        "    model.add(Flatten(input_shape=(1,) + obs_shape))\n",
        "    model.add(Dense(64))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dense(64))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dense(32, activation='relu'))\n",
        "    model.add(Dense(nb_actions, activation='linear'))\n",
        "    return model\n",
        "\n",
        "# Mejora 2: Red más profunda con Dropout para regularización\n",
        "def build_model_mejora2():\n",
        "    model = Sequential()\n",
        "    model.add(Flatten(input_shape=(1,) + obs_shape))\n",
        "    model.add(Dense(256, activation='relu'))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Dense(256, activation='relu'))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dense(nb_actions, activation='linear'))\n",
        "    return model\n",
        "\n",
        "# Mejora 3: Arquitectura intermedia con activación y normalización combinadas\n",
        "def build_model_mejora3():\n",
        "    model = Sequential()\n",
        "    model.add(Flatten(input_shape=(1,) + obs_shape))\n",
        "    model.add(Dense(128))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(128))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dense(nb_actions))\n",
        "    model.add(Activation('linear'))\n",
        "    return model\n",
        "\n",
        "# Mejora 4 (Baseline): Red simple con capas densas pequeñas\n",
        "def build_model_mejora4():\n",
        "    model = Sequential()\n",
        "    model.add(Flatten(input_shape=(1,) + obs_shape))\n",
        "    model.add(Dense(16, activation='relu'))\n",
        "    model.add(Dense(16, activation='relu'))\n",
        "    model.add(Dense(16, activation='relu'))\n",
        "    model.add(Dense(nb_actions, activation='linear'))\n",
        "    return model\n",
        "\n",
        "# -------------------- CONFIGURACIÓN DE EXPERIMENTOS --------------------\n",
        "\n",
        "# Lista de configuraciones con arquitectura y parámetros específicos por mejora\n",
        "parametros = [\n",
        "    {\n",
        "        'modelo': build_model_mejora1,\n",
        "        'nombre': 'Mejora 1',\n",
        "        'params': dict(memory_limit=50000, eps_min=0.01, eps_test=0.01, steps_exploration=100000,\n",
        "                       warmup_steps=1000, gamma=0.98, target_model_update=500,\n",
        "                       batch_size=64, learning_rate=5e-4, nb_steps=100000)\n",
        "    },\n",
        "    {\n",
        "        'modelo': build_model_mejora2,\n",
        "        'nombre': 'Mejora 2',\n",
        "        'params': dict(memory_limit=50000, eps_min=0.02, eps_test=0.02, steps_exploration=200000,\n",
        "                       warmup_steps=5000, gamma=0.995, target_model_update=250,\n",
        "                       batch_size=256, learning_rate=3e-4, nb_steps=200000)\n",
        "    },\n",
        "    {\n",
        "        'modelo': build_model_mejora3,\n",
        "        'nombre': 'Mejora 3',\n",
        "        'params': dict(memory_limit=100000, eps_min=0.05, eps_test=0.01, steps_exploration=150000,\n",
        "                       warmup_steps=5000, gamma=0.99, target_model_update=1000,\n",
        "                       batch_size=128, learning_rate=1e-4, nb_steps=200000)\n",
        "    },\n",
        "    {\n",
        "        'modelo': build_model_mejora4,\n",
        "        'nombre': 'Base',\n",
        "        'params': dict(memory_limit=10000, eps_min=0.1, eps_test=0.05, steps_exploration=25000,\n",
        "                       warmup_steps=100, gamma=0.99, target_model_update=1000,\n",
        "                       batch_size=128, learning_rate=1e-3, nb_steps=50000)\n",
        "    }\n",
        "]\n",
        "\n",
        "# -------------------- FUNCIÓN DE ENTRENAMIENTO --------------------\n",
        "\n",
        "def entrenar_evaluar(model_func, nombre, p):\n",
        "    \"\"\"\n",
        "    Entrena y evalúa un agente DQN con arquitectura y configuración específicas.\n",
        "\n",
        "    Args:\n",
        "        model_func: función constructora del modelo.\n",
        "        nombre: identificador del experimento.\n",
        "        p: diccionario de hiperparámetros.\n",
        "    Returns:\n",
        "        recompensas: arreglo NumPy con las recompensas por episodio durante la evaluación.\n",
        "    \"\"\"\n",
        "    model = model_func()\n",
        "    memory = SequentialMemory(limit=p['memory_limit'], window_length=1)\n",
        "    policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps',\n",
        "                                  value_max=1.0, value_min=p['eps_min'],\n",
        "                                  value_test=p['eps_test'], nb_steps=p['steps_exploration'])\n",
        "\n",
        "    agente = DQNAgent(model=model, nb_actions=nb_actions, memory=memory,\n",
        "                      policy=policy, nb_steps_warmup=p['warmup_steps'],\n",
        "                      gamma=p['gamma'], target_model_update=p['target_model_update'],\n",
        "                      train_interval=1, batch_size=p['batch_size'])\n",
        "\n",
        "    agente.compile(Adam(learning_rate=p['learning_rate']), metrics=['mae'])\n",
        "\n",
        "    print(f\"\\nEntrenando {nombre}...\")\n",
        "    agente.fit(env, nb_steps=p['nb_steps'], visualize=False, verbose=2)\n",
        "\n",
        "    print(f\"\\nEvaluando {nombre}...\")\n",
        "    resultados = agente.test(env, nb_episodes=20, visualize=False, verbose=0)\n",
        "    recompensas = np.array(resultados.history['episode_reward'])\n",
        "    print(f\"{nombre} → Recompensa media: {recompensas.mean():.2f} ± {recompensas.std():.2f}\")\n",
        "    return recompensas\n",
        "\n",
        "# -------------------- EJECUCIÓN DE EXPERIMENTOS --------------------\n",
        "\n",
        "# Diccionario para almacenar las recompensas de cada agente\n",
        "resultados = {}\n",
        "for item in parametros:\n",
        "    recompensas = entrenar_evaluar(item['modelo'], item['nombre'], item['params'])\n",
        "    resultados[item['nombre']] = recompensas\n",
        "\n",
        "# -------------------- VISUALIZACIÓN DE RESULTADOS --------------------\n",
        "\n",
        "# Histograma comparativo de las distribuciones de recompensa\n",
        "plt.figure(figsize=(10, 6))\n",
        "for nombre, recompensas in resultados.items():\n",
        "    plt.hist(recompensas, bins=10, alpha=0.5, label=nombre)\n",
        "plt.title('Comparación de Recompensas - Todas las Mejoras')\n",
        "plt.xlabel('Recompensa por Episodio')\n",
        "plt.ylabel('Frecuencia')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Tabla resumen de estadísticos descriptivos\n",
        "df_comparacion = pd.DataFrame(resultados)\n",
        "resumen = df_comparacion.describe().loc[['mean', 'std']].T\n",
        "resumen.columns = ['Media', 'Desviación estándar']\n",
        "print(\"\\nResumen comparativo de todas las mejoras:\\n\")\n",
        "print(resumen)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "mpeV8kLO90A8",
        "outputId": "21be0793-d4e7-4590-9839-4af63e40c964"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'rl'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-f56fabda7951>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mActivation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchNormalization\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAdam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mrl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdqn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDQNAgent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mrl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLinearAnnealedPolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEpsGreedyQPolicy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mrl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequentialMemory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'rl'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importación de bibliotecas necesarias para el entorno, procesamiento y visualización\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import random\n",
        "import gym\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Componentes para la construcción de modelos de redes neuronales con Keras\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense, Activation, Flatten, BatchNormalization, Dropout, Input, Lambda\n",
        "from tensorflow.keras.optimizers.legacy import Adam\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "# Importación de módulos de Keras-RL necesarios para agentes DQN y gestión de políticas y memoria\n",
        "from rl.agents.dqn import DQNAgent\n",
        "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy, BoltzmannQPolicy\n",
        "from rl.memory import SequentialMemory\n",
        "\n",
        "# Establecimiento de semillas para asegurar la reproducibilidad de los resultados\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "# Inicialización del entorno Acrobot-v1 y fijación de las semillas para consistencia experimental\n",
        "env = gym.make('Acrobot-v1')\n",
        "env.seed(SEED)\n",
        "if hasattr(env.action_space, 'seed'):\n",
        "    env.action_space.seed(SEED)\n",
        "if hasattr(env.observation_space, 'seed'):\n",
        "    env.observation_space.seed(SEED)\n",
        "\n",
        "# Extracción de dimensiones del espacio de observación y número de acciones disponibles\n",
        "nb_actions = env.action_space.n\n",
        "obs_shape = env.observation_space.shape\n",
        "\n",
        "# Definición de modelo base para DQN: red neuronal multicapa densa con técnicas de regularización\n",
        "def build_base_model():\n",
        "    model = Sequential()\n",
        "    model.add(Flatten(input_shape=(1,) + obs_shape))\n",
        "    model.add(Dense(128))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(128))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(64))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dense(nb_actions))\n",
        "    model.add(Activation('linear'))  # Salida lineal para estimar Q-valores\n",
        "    return model\n",
        "\n",
        "# Definición del modelo Dueling DQN que separa el valor del estado y la ventaja de cada acción\n",
        "def build_dueling_model():\n",
        "    input_layer = Input(shape=(1,) + obs_shape)\n",
        "    x = Flatten()(input_layer)\n",
        "    x = Dense(128, activation='relu')(x)\n",
        "    x = Dense(128, activation='relu')(x)\n",
        "\n",
        "    value = Dense(1, activation='linear')(x)  # V(s)\n",
        "    advantage = Dense(nb_actions, activation='linear')(x)  # A(s,a)\n",
        "\n",
        "    # Combinación de V y A para calcular Q(s,a) de forma dueling\n",
        "    def dueling_output(inputs):\n",
        "        v, a = inputs\n",
        "        return v + (a - K.mean(a, axis=1, keepdims=True))\n",
        "\n",
        "    q_values = Lambda(dueling_output)([value, advantage])\n",
        "    return Model(inputs=input_layer, outputs=q_values)\n",
        "\n",
        "# Función generalizada para entrenar y evaluar un agente DQN configurable\n",
        "def train_agent(model, double_dqn=False, name='dqn_model', policy_type='epsgreedy'):\n",
        "    \"\"\"\n",
        "    Entrena un agente DQN con configuración flexible, incluyendo opción de política ε-greedy o Boltzmann,\n",
        "    así como activación de Double DQN.\n",
        "\n",
        "    Args:\n",
        "        model: modelo Keras de red neuronal.\n",
        "        double_dqn: activa Double DQN si es True.\n",
        "        name: nombre base para guardar los resultados.\n",
        "        policy_type: tipo de política de exploración (ε-greedy o Boltzmann).\n",
        "    Returns:\n",
        "        Array de recompensas obtenidas durante la evaluación.\n",
        "    \"\"\"\n",
        "    memory = SequentialMemory(limit=100000, window_length=1)\n",
        "\n",
        "    # Definición de política de acción\n",
        "    if policy_type == 'boltzmann':\n",
        "        policy = BoltzmannQPolicy()  # Política suave basada en temperatura\n",
        "    else:\n",
        "        policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps',\n",
        "                                      value_max=1.0, value_min=0.05,\n",
        "                                      value_test=0.01, nb_steps=150000)\n",
        "\n",
        "    # Construcción del agente DQN\n",
        "    agent = DQNAgent(model=model,\n",
        "                     nb_actions=nb_actions,\n",
        "                     memory=memory,\n",
        "                     policy=policy,\n",
        "                     nb_steps_warmup=5000,\n",
        "                     gamma=0.99,\n",
        "                     target_model_update=1000,\n",
        "                     train_interval=4,\n",
        "                     batch_size=128,\n",
        "                     enable_double_dqn=double_dqn)\n",
        "\n",
        "    # Compilación del agente con optimizador Adam y métrica de error medio absoluto\n",
        "    agent.compile(Adam(learning_rate=1e-4), metrics=['mae'])\n",
        "\n",
        "    print(f\"\\nEntrenando {name.upper()}...\")\n",
        "    agent.fit(env, nb_steps=200000, visualize=False, verbose=2)\n",
        "    agent.save_weights(f'{name}_weights.h5f', overwrite=True)\n",
        "\n",
        "    print(f\"\\nEvaluando {name.upper()}...\")\n",
        "    agent.load_weights(f'{name}_weights.h5f')\n",
        "    history = agent.test(env, nb_episodes=20, visualize=False, verbose=0)\n",
        "\n",
        "    rewards = np.array(history.history['episode_reward'])\n",
        "    print(f\"{name.upper()} → Recompensa media: {rewards.mean():.2f} ± {rewards.std():.2f}\")\n",
        "    return rewards\n",
        "\n",
        "# Entrenamiento y evaluación de los distintos agentes con diferentes configuraciones\n",
        "rewards_dqn = train_agent(build_base_model(), double_dqn=False, name='dqn', policy_type='epsgreedy')\n",
        "rewards_ddqn = train_agent(build_base_model(), double_dqn=True, name='ddqn', policy_type='epsgreedy')\n",
        "rewards_dueling = train_agent(build_dueling_model(), double_dqn=True, name='dueling_dqn', policy_type='epsgreedy')\n",
        "rewards_boltzmann = train_agent(build_base_model(), double_dqn=False, name='boltzmann_dqn', policy_type='boltzmann')\n",
        "\n",
        "# ---- VISUALIZACIÓN DE RESULTADOS ----\n",
        "\n",
        "# Histograma comparativo de recompensas obtenidas\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.hist(rewards_dqn, bins=10, alpha=0.5, label='DQN')\n",
        "plt.hist(rewards_ddqn, bins=10, alpha=0.5, label='Double DQN')\n",
        "plt.hist(rewards_dueling, bins=10, alpha=0.5, label='Dueling DQN')\n",
        "plt.hist(rewards_boltzmann, bins=10, alpha=0.5, label='Boltzmann DQN')\n",
        "plt.title('Distribución de Recompensas - Evaluación en 20 Episodios')\n",
        "plt.xlabel('Recompensa total por episodio')\n",
        "plt.ylabel('Frecuencia')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Boxplot para observar variabilidad, medianas y posibles valores atípicos\n",
        "df_results = pd.DataFrame({\n",
        "    'DQN': rewards_dqn,\n",
        "    'Double DQN': rewards_ddqn,\n",
        "    'Dueling DQN': rewards_dueling,\n",
        "    'Boltzmann DQN': rewards_boltzmann\n",
        "})\n",
        "plt.figure(figsize=(10, 6))\n",
        "df_results.boxplot()\n",
        "plt.title('Boxplot de Recompensas por Algoritmo')\n",
        "plt.ylabel('Recompensa total por episodio')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Representación de la evolución de las recompensas a lo largo de los episodios\n",
        "plt.figure(figsize=(10, 6))\n",
        "for col in df_results.columns:\n",
        "    plt.plot(df_results.index, df_results[col], label=col)\n",
        "plt.title('Evolución de Recompensas por Episodio')\n",
        "plt.xlabel('Episodio')\n",
        "plt.ylabel('Recompensa')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Generación de tabla resumen con estadísticos descriptivos por agente\n",
        "summary_table = df_results.describe().loc[['mean', 'std']].T\n",
        "summary_table.columns = ['Media', 'Desviación estándar']\n",
        "print(\"\\nResumen estadístico por algoritmo:\\n\")\n",
        "print(summary_table)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "WbES8C6V9ZHn",
        "outputId": "7e5729bb-7f2c-47ac-9838-acf6a373b3bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'rl'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-aca8b83f8f72>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Importación de módulos de Keras-RL necesarios para agentes DQN y gestión de políticas y memoria\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mrl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdqn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDQNAgent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mrl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLinearAnnealedPolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEpsGreedyQPolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBoltzmannQPolicy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mrl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequentialMemory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'rl'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se incrementó la capacidad de representación del modelo mediante el aumento del número de capas y neuronas, pasando de una arquitectura de 64/64/32 a una de 128/128/64. Esta mayor profundidad permite una abstracción más efectiva de las características del entorno, crucial para problemas de control complejos como Acrobot.\n",
        "\n",
        "Se incorporó regularización con Dropout, lo cual ayuda a prevenir el sobreajuste y favorece la generalización del modelo en nuevas situaciones.\n",
        "\n",
        "Se duplicó el número total de pasos de entrenamiento (nb_steps) de 100000 a 200000 para permitir una exploración y consolidación del aprendizaje más prolongadas.\n",
        "\n",
        "El factor de descuento gamma se incrementó de 0.98 a 0.99, dando mayor importancia a las recompensas futuras, lo que es coherente con el objetivo a largo plazo de equilibrar el péndulo.\n",
        "\n",
        "El tamaño del batch se amplió de 64 a 128 para obtener gradientes más estables durante el entrenamiento, y la tasa de aprendizaje learning_rate se redujo de 5e-4 a 1e-4 para que el ajuste de los pesos sea más fino y estable.\n",
        "\n",
        "La memoria de experiencias aumentó de 50000 a 100000 elementos, permitiendo al agente tener un mayor repertorio de experiencias para aprender.\n",
        "\n",
        "El periodo de exploración se extendió a 150000 pasos para permitir una exploración más rica antes de pasar a la explotación. Asimismo, el intervalo entre entrenamientos pasó de 1 a 4 para reducir la correlación temporal entre muestras y mejorar la eficiencia computacional.\n",
        "\n",
        "El número de pasos de calentamiento aumentó a 5000 para llenar adecuadamente la memoria antes de entrenar. Finalmente, el intervalo de actualización del modelo objetivo se duplicó (de 500 a 1000), lo que contribuye a una mayor estabilidad en la convergencia del aprendizaje."
      ],
      "metadata": {
        "id": "MyUoiGGWRVrk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Investigación y detalle teórico de variantes de DQN\n",
        "\n",
        "En esta sección analizamos y comparamos, a nivel teórico, las diferencias entre el agente DQN con política ε-greedy visto en baseline y tres políticas propuestas: política de Boltzmann, Double DQN y Dueling DQN.\n",
        "\n",
        "---\n",
        "\n",
        "### 2.1 Política de Boltzmann vs. ε-greedy\n",
        "\n",
        "- **ε-greedy**  \n",
        "  Explora con probabilidad ε escogiendo una acción al azar, y explota con probabilidad 1 – ε seleccionando la acción con mayor valor Q.  \n",
        "  - **Ventaja**: muy simple de implementar y entender.  \n",
        "  - **Desventaja**: la exploración es a ciegas (todas las acciones no óptimas tienen igual probabilidad).\n",
        "\n",
        "- **Boltzmann (softmax) policy**  \n",
        "  Define la probabilidad de elegir la acción \\(a\\) en el estado \\(s\\) como  \n",
        "  $$\n",
        "    P(a \\mid s)\n",
        "    = \\frac{\\exp\\bigl(Q(s,a)/\\tau\\bigr)}\n",
        "           {\\sum_{a'} \\exp\\bigl(Q(s,a')/\\tau\\bigr)},\n",
        "  $$\n",
        "  donde \\(\\tau\\) (“temperatura”) controla la exploración:  \n",
        "  - \\(\\tau\\) alto → distribución casi uniforme (más exploración).  \n",
        "  - \\(\\tau\\) bajo → mayor concentración en la acción de mayor Q (más explotación).  \n",
        "  - **Ventajas**:  \n",
        "    1. Exploración dirigida: las acciones se muestrean proporcionalmente a sus valores Q.  \n",
        "    2. Transición más suave exploración–explotación.  \n",
        "  - **Desventajas**:  \n",
        "    1. Selección de \\(\\tau\\) crítica.  \n",
        "    2. Cálculo de la softmax costoso si \\(\\lvert\\mathcal{A}\\rvert\\) es grande.\n",
        "\n",
        "| Política   | Exploración dirigida            | Parámetro | Complejidad  |\n",
        "|------------|---------------------------------|-----------|--------------|\n",
        "| ε-greedy   | No (equiprobable)               | ε         | Muy baja     |\n",
        "| Boltzmann  | Sí (proporcional a Q)           | τ         | Baja-media   |\n",
        "\n",
        "---\n",
        "\n",
        "### 2.2 Double DQN\n",
        "\n",
        "**Problema**  \n",
        "DQN estándar usa la misma red para seleccionar la acción \\(\\arg\\max\\) y evaluarla, lo que introduce **overestimation**.\n",
        "\n",
        "**Double DQN** (van Hasselt et al., 2016) propone:\n",
        "\n",
        "1. Acción seleccionada por la red principal \\(\\theta\\):  \n",
        "   \\[\n",
        "     a^* = \\arg\\max_{a'} Q(s',a';\\,\\theta).\n",
        "   \\]\n",
        "2. Valor estimado por la red objetivo \\(\\theta^-\\):  \n",
        "   \\[\n",
        "     Q(s',\\,a^*;\\,\\theta^-).\n",
        "   \\]\n",
        "3. TD target:\n",
        "   $$\n",
        "     y = r + \\gamma\\,Q\\bigl(s',\\,\\arg\\max_{a'}Q(s',a';\\theta)\\;;\\;\\theta^-\\bigr).\n",
        "   $$\n",
        "\n",
        "- **Ventaja**: reduce el sesgo de sobreestimación, mejora estabilidad y políticas finales.  \n",
        "- **Desventaja**: ligera complejidad extra (dos redes).\n",
        "\n",
        "---\n",
        "\n",
        "### 2.3 Dueling DQN\n",
        "\n",
        "**Motivación**  \n",
        "En muchos estados, el valor del estado \\(V(s)\\) domina, y la diferencia entre acciones es pequeña. Separar ambas estimaciones puede acelerar el aprendizaje.\n",
        "\n",
        "**Arquitectura** (Wang et al., 2016):\n",
        "\n",
        "1. Capas compartidas → dos “streams”:\n",
        "   - **Valor** \\(V(s)\\)\n",
        "   - **Ventaja** \\(A(s,a)\\)\n",
        "2. Combinación:\n",
        "   $$\n",
        "     Q(s,a)\n",
        "     = V(s)\n",
        "       + \\Bigl(A(s,a)\n",
        "         - \\tfrac{1}{|\\mathcal{A}|}\\sum_{a'}A(s,a')\\Bigr).\n",
        "   $$\n",
        "\n",
        "- **Ventaja**: aprendizaje más rápido en entornos con bajo impacto de la acción.  \n",
        "- **Desventaja**: arquitectura y ajuste de hiperparámetros algo más complejos.\n",
        "\n",
        "---\n",
        "\n",
        "### 2.4 Comparativa de variantes\n",
        "\n",
        "| Característica                    | ε-greedy DQN     | Boltzmann DQN      | Double DQN         | Dueling DQN        |\n",
        "|-----------------------------------|------------------|--------------------|--------------------|--------------------|\n",
        "| **Exploración**                   | ε-aleatoria      | softmax (τ)        | ε-aleatoria        | ε-aleatoria        |\n",
        "| **Overestimation bias**           | elevado          | elevado            | reducido           | reducido           |\n",
        "| **Estimación valor**              | directa \\(Q\\)    | directa \\(Q\\)      | directa \\(Q\\)      | separada \\(V/A\\)   |\n",
        "| **Complejidad arquitectural**     | baja             | baja-media         | media              | media-alta         |\n",
        "| **Estabilidad y convergencia**    | moderada         | moderada           | alta               | alta               |\n",
        "\n",
        "> **Nota**: es posible combinar Double y Dueling con exploración softmax en lugar de ε-greedy.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "rSDgjv9jYq6V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Una vez hayas entrenado y guardado los pesos (.h5f), puedes guardar todo el modelo en formato .h5 completo así:\n",
        "\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# 1) Reconstruye la arquitectura (o usa build_base_model / build_dueling_model)\n",
        "dqn_model = build_base_model()\n",
        "ddqn_model = build_base_model()\n",
        "dueling_model = build_dueling_model()\n",
        "\n",
        "# 2) Carga los pesos entrenados\n",
        "dqn_model.load_weights('dqn_weights.h5f')\n",
        "ddqn_model.load_weights('ddqn_weights.h5f')\n",
        "dueling_model.load_weights('dueling_dqn_weights.h5f')\n",
        "\n",
        "# 3) Guarda el modelo completo en .h5\n",
        "dqn_model.save('dqn_full_model.h5')\n",
        "ddqn_model.save('ddqn_full_model.h5')\n",
        "dueling_model.save('dueling_dqn_full_model.h5')\n",
        "\n",
        "print(\"Modelos guardados como .h5 en tu directorio de trabajo.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJVJNqQFALt5",
        "outputId": "3bbe66f2-4419-4df3-e473-1b6cc2af9b49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modelos guardados como .h5 en tu directorio de trabajo.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Descargarlo\n",
        "files.download('dqn_full_model.h5')\n",
        "files.download('ddqn_full_model.h5')\n",
        "files.download('dueling_dqn_full_model.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "t3DeZO1hAy9M",
        "outputId": "a94a4b8d-00be-4ebc-8b99-1eb6c0ab5134"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_d408b8d2-e7bc-477b-86c0-0aaf3fa2508a\", \"dqn_full_model.h5\", 145728)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_c96dcdb9-9db7-471a-bc5a-17634152a001\", \"ddqn_full_model.h5\", 145728)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_73b6c507-3f57-4a83-bde7-602004162723\", \"dueling_dqn_full_model.h5\", 93952)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Investigación teórica y desarrollo conceptual/comparativo sobre:\n",
        "- **Boltzmann Policy**\n",
        "- **Double DQN**\n",
        "- **Dueling DQN**\n"
      ],
      "metadata": {
        "id": "VzBCY083BoP6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Boltzman Policy**\n",
        "\n",
        "  La política de Boltzman ó Softmax Policy, es una técnica de exploración en aprendizaje por refuerzo basada en la asignación probabilística de acciones según sus valores Q estimados. Considera todas las acciones, ponderadas por su valor Q, a diferencia de ε-greedy que escogen la mejor acción o una aleatoria.\n"
      ],
      "metadata": {
        "id": "eDIrti3IE36L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como vimos en el apartado anterior la fórmula que representa la probabilidad de seleccionar una acción\n",
        "𝑎\n",
        "a en un estado\n",
        "𝑠\n",
        "s viene dada por:\n",
        "\n",
        "$$\n",
        "P(a|s) = \\frac{e^{Q(s,a)/\\tau}}{\\sum_{a'} e^{Q(s,a')/\\tau}}\n",
        "$$"
      ],
      "metadata": {
        "id": "hryJUnGEHFII"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Donde:\n",
        "\n",
        "𝑄\n",
        "(\n",
        "𝑠\n",
        ",\n",
        "𝑎\n",
        ")\n",
        "es el valor de acción estimado y τ es el parámetro de temperatura que controla el grado de aleatoriedad. Valores bajos son más determinista (parecida a greedy), mientras que valores altos promueven más exploración.\n"
      ],
      "metadata": {
        "id": "AiX7TXYeHp-o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "_qcyXCFPIpj_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Ventajas\n",
        "- Explora de forma más inteligente que ε-greedy.\n",
        "- Mejora la estabilidad del aprendizaje.\n",
        "\n",
        "###  Desventajas\n",
        "- Requiere sintonización de τ.\n",
        "- Puede ser más costosa computacionalmente.\n",
        "\n",
        "> *Referencia:* Sutton, R. S., & Barto, A. G. (2018). _Reinforcement Learning: An Introduction_.\n"
      ],
      "metadata": {
        "id": "Ht2p8wYSIgnv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Double DQN**\n",
        "\n",
        "El algoritmo Double DQN fue propuesto para resolver un problema específico del DQN original: la sobreestimación de los valores Q. En DQN, cuando se actualiza el valor Q se selecciona y evalúa la mejor acción con la misma red neuronal, lo que puede llevar a estimaciones optimistas.\n",
        "\n",
        "Double DQN desacopla estas dos funciones, por lo que la selección de la acción óptima se realiza con la red principal, y La evaluación del valor Q de dicha acción se realiza con la red objetivo.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "X_trPSfBM2Hb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Por tanto la actualización del target se ve modificada de la siguiente manera:\n",
        "$$\n",
        "y = r + \\gamma \\, Q_{\\text{target}}(s', \\arg\\max_a Q_{\\text{main}}(s', a))\n",
        "$$"
      ],
      "metadata": {
        "id": "KNkTsbn6NuO6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Ventajas\n",
        "- Estimaciones Q más precisas.\n",
        "- Mejora la estabilidad y el rendimiento del aprendizaje.\n",
        "\n",
        "###  Desventajas\n",
        "- Aumenta ligeramente la complejidad del código.\n",
        "- No siempre mejora el rendimiento si el entorno no sufre de sobreestimaciones.\n",
        "\n",
        "*Referencia*:\n",
        "Hasselt, H. van. (2010). Double Q-learning. In Advances in Neural Information Processing Systems (NeurIPS)."
      ],
      "metadata": {
        "id": "J_5vS0xnO0kJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Dueling DQN**\n",
        "\n",
        "Dueling DQN introduce una arquitectura diferente de red neuronal que separa el cálculo del valor del estado y la ventaja de cada acción. Lo que se pretende es que en muchos estados, la elección de la acción no es tan relevante, y es más útil aprender primero lo bueno que es el estado en sí mismo.\n"
      ],
      "metadata": {
        "id": "b4sMbBaDQMh6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  La red se divide en dos ramas:\n",
        "\n",
        "- Valor del estado\n",
        "𝑉\n",
        "(\n",
        "𝑠\n",
        ")\n",
        "\n",
        "- Ventaja relativa de cada acción\n",
        "𝐴\n",
        "(\n",
        "𝑠\n",
        ",\n",
        "𝑎\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "khMM8_7_RJNn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La salida de la red se debe combinar así:\n",
        "\n",
        "\n",
        "$$\n",
        "Q(s,a) = V(s) + \\left( A(s,a) - \\frac{1}{|\\mathcal{A}|} \\sum_{a'} A(s,a') \\right)\n",
        "$$\n",
        "\n",
        "Permitiendo a la red aprender de forma más eficiente y generalizar mejor en estados donde muchas acciones son similares."
      ],
      "metadata": {
        "id": "VZtf3FqSRjRX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Ventajas\n",
        "- Mejora el rendimiento especialmente en entornos con muchas acciones irrelevantes.\n",
        "- Acelera el aprendizaje y mejora la convergencia.\n",
        "\n",
        "###  Desventajas\n",
        "- Aumenta la complejidad del modelo.\n",
        "- Puede no aportar mejora en entornos muy simples.\n",
        "\n",
        "*Referencia*:\n",
        "Wang, Z., Schaul, T., Hessel, M., van Hasselt, H., Lanctot, M., & de Freitas, N. (2016). Dueling Network Architectures for Deep Reinforcement Learning."
      ],
      "metadata": {
        "id": "M6OclYNjR-64"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "-12q-m22MiUz"
      }
    }
  ]
}