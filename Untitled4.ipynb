{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/smoyarodrigo/03MAIR-Algoritmos-de-Optimizacion/blob/main/Untitled4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5Lr59O0NTlm",
        "outputId": "cfb970a8-c407-4f90-e7b9-8fc56595667c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ejecutando en Colab: True\n"
          ]
        }
      ],
      "source": [
        "# 1.2 Localizar entorno de trabajo\n",
        "mount = '/content/gdrive'\n",
        "drive_root = mount + \"/My Drive/08_MIAR/actividades/proyecto practico\"\n",
        "\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    IN_COLAB = True\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "print(\"Ejecutando en Colab:\", IN_COLAB)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1.3 Montar carpeta de datos local (solo Colab)\n",
        "import os\n",
        "\n",
        "if IN_COLAB:\n",
        "    print(\"Colab: montando Google Drive en\", mount)\n",
        "    drive.mount(mount)\n",
        "\n",
        "    # Asegurarse de que la carpeta de la práctica existe\n",
        "    os.makedirs(drive_root, exist_ok=True)\n",
        "    print(\"Carpeta verificada:\", drive_root)\n",
        "\n",
        "    # Cambiar al directorio de la práctica\n",
        "    %cd $drive_root\n",
        "\n",
        "# Verificamos que estamos en la ruta correcta\n",
        "print(\"Directorio actual:\", os.getcwd())\n",
        "print(\"Archivos disponibles:\", os.listdir())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sSmcj_jBNy3A",
        "outputId": "7c4a481e-4ee6-4d81-b983-d7dbff51cb6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab: montando Google Drive en /content/gdrive\n",
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Carpeta verificada: /content/gdrive/My Drive/08_MIAR/actividades/proyecto practico\n",
            "/content/gdrive/My Drive/08_MIAR/actividades/proyecto practico\n",
            "Directorio actual: /content/gdrive/My Drive/08_MIAR/actividades/proyecto practico\n",
            "Archivos disponibles: ['baseline_Acrobot-v1_weights.h5f.index', 'baseline_Acrobot-v1_weights.h5f.data-00000-of-00001', 'peso_dqn_mejorado_Acrobot-v1_paso_5000.h5f.index', 'peso_dqn_mejorado_Acrobot-v1_paso_5000.h5f.data-00000-of-00001', 'peso_dqn_mejorado_Acrobot-v1_paso_10000.h5f.index', 'peso_dqn_mejorado_Acrobot-v1_paso_10000.h5f.data-00000-of-00001', 'peso_dqn_mejorado_Acrobot-v1_paso_15000.h5f.index', 'peso_dqn_mejorado_Acrobot-v1_paso_15000.h5f.data-00000-of-00001', 'peso_dqn_mejorado_Acrobot-v1_paso_20000.h5f.index', 'peso_dqn_mejorado_Acrobot-v1_paso_20000.h5f.data-00000-of-00001', 'peso_dqn_mejorado_Acrobot-v1_paso_25000.h5f.index', 'peso_dqn_mejorado_Acrobot-v1_paso_25000.h5f.data-00000-of-00001', 'peso_dqn_mejorado_Acrobot-v1_paso_30000.h5f.index', 'peso_dqn_mejorado_Acrobot-v1_paso_30000.h5f.data-00000-of-00001', 'peso_dqn_mejorado_Acrobot-v1_paso_35000.h5f.index', 'peso_dqn_mejorado_Acrobot-v1_paso_35000.h5f.data-00000-of-00001', 'peso_dqn_mejorado_Acrobot-v1_paso_40000.h5f.index', 'peso_dqn_mejorado_Acrobot-v1_paso_40000.h5f.data-00000-of-00001', 'peso_dqn_mejorado_Acrobot-v1_paso_45000.h5f.index', 'peso_dqn_mejorado_Acrobot-v1_paso_45000.h5f.data-00000-of-00001', 'peso_dqn_mejorado_Acrobot-v1_paso_50000.h5f.index', 'peso_dqn_mejorado_Acrobot-v1_paso_50000.h5f.data-00000-of-00001', 'peso_dqn_mejorado_Acrobot-v1_paso_55000.h5f.index', 'peso_dqn_mejorado_Acrobot-v1_paso_55000.h5f.data-00000-of-00001', 'peso_dqn_mejorado_Acrobot-v1_paso_60000.h5f.index', 'peso_dqn_mejorado_Acrobot-v1_paso_60000.h5f.data-00000-of-00001', 'peso_dqn_mejorado_Acrobot-v1_paso_65000.h5f.index', 'peso_dqn_mejorado_Acrobot-v1_paso_65000.h5f.data-00000-of-00001', 'peso_dqn_mejorado_Acrobot-v1_paso_70000.h5f.index', 'peso_dqn_mejorado_Acrobot-v1_paso_70000.h5f.data-00000-of-00001', 'peso_dqn_mejorado_Acrobot-v1_paso_75000.h5f.index', 'peso_dqn_mejorado_Acrobot-v1_paso_75000.h5f.data-00000-of-00001', 'peso_dqn_mejorado_Acrobot-v1_paso_80000.h5f.index', 'peso_dqn_mejorado_Acrobot-v1_paso_80000.h5f.data-00000-of-00001', 'peso_dqn_mejorado_Acrobot-v1_paso_85000.h5f.index', 'peso_dqn_mejorado_Acrobot-v1_paso_85000.h5f.data-00000-of-00001', 'peso_dqn_mejorado_Acrobot-v1_paso_90000.h5f.index', 'peso_dqn_mejorado_Acrobot-v1_paso_90000.h5f.data-00000-of-00001', 'peso_dqn_mejorado_Acrobot-v1_paso_95000.h5f.index', 'peso_dqn_mejorado_Acrobot-v1_paso_95000.h5f.data-00000-of-00001', 'peso_dqn_mejorado_Acrobot-v1_paso_100000.h5f.index', 'dqn_mejorado_Acrobot-v1_final.h5f.index', 'dqn_mejorado_Acrobot-v1_log.json', 'peso_dqn_mejorado_Acrobot-v1_paso_100000.h5f.data-00000-of-00001', 'dqn_mejorado_Acrobot-v1_final.h5f.data-00000-of-00001', 'modelo_keras_completo.h5', 'dqn_weights.h5f.data-00000-of-00001', 'dqn_weights.h5f.index', 'ddqn_weights.h5f.data-00000-of-00001', 'ddqn_weights.h5f.index', 'dueling_dqn_weights.h5f.data-00000-of-00001', 'dueling_dqn_weights.h5f.index', 'checkpoint', 'dqn_full_model.h5', 'ddqn_full_model.h5', 'dueling_dqn_full_model.h5']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install gym==0.17.3\n",
        "%pip install git+https://github.com/Kojoley/atari-py.git\n",
        "%pip install keras-rl2==1.0.5\n",
        "%pip install tensorflow==2.12.0\n",
        "%pip install protobuf==3.20.*\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GPFKSfewOFiP",
        "outputId": "30c1fd47-1ca7-443b-83c4-90ab3070cb55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Ignoring invalid distribution ~rotobuf (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~rotobuf (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: gym==0.17.3 in /usr/local/lib/python3.11/dist-packages (0.17.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from gym==0.17.3) (1.15.3)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.11/dist-packages (from gym==0.17.3) (1.23.5)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from gym==0.17.3) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gym==0.17.3) (1.6.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.11/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym==0.17.3) (1.0.0)\n",
            "\u001b[33mWARNING: Ignoring invalid distribution ~rotobuf (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~rotobuf (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~rotobuf (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting git+https://github.com/Kojoley/atari-py.git\n",
            "  Cloning https://github.com/Kojoley/atari-py.git to /tmp/pip-req-build-ikefyvcp\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/Kojoley/atari-py.git /tmp/pip-req-build-ikefyvcp\n",
            "  Resolved https://github.com/Kojoley/atari-py.git to commit 86a1e05c0a95e9e6233c3a413521fdb34ca8a089\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from atari-py==1.2.2) (1.23.5)\n",
            "\u001b[33mWARNING: Ignoring invalid distribution ~rotobuf (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~rotobuf (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~rotobuf (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: keras-rl2==1.0.5 in /usr/local/lib/python3.11/dist-packages (1.0.5)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (from keras-rl2==1.0.5) (2.12.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (25.2.10)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (1.72.1)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (3.13.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (0.4.30)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (2.12.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (18.1.1)\n",
            "Requirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (24.2)\n",
            "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow->keras-rl2==1.0.5)\n",
            "  Using cached protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (1.17.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (2.12.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (4.14.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow->keras-rl2==1.0.5) (0.45.1)\n",
            "Requirement already satisfied: jaxlib<=0.4.30,>=0.4.27 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow->keras-rl2==1.0.5) (0.4.30)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow->keras-rl2==1.0.5) (0.4.1)\n",
            "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow->keras-rl2==1.0.5) (1.15.3)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (2.38.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (3.8)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (3.1.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (4.9.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (2.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (2025.4.26)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (3.2.2)\n",
            "Using cached protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[33mWARNING: Ignoring invalid distribution ~rotobuf (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mInstalling collected packages: protobuf\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "grpcio-status 1.71.0 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.8 which is incompatible.\n",
            "tf-keras 2.18.0 requires tensorflow<2.19,>=2.18, but you have tensorflow 2.12.0 which is incompatible.\n",
            "tensorflow-decision-forests 1.11.0 requires tensorflow==2.18.0, but you have tensorflow 2.12.0 which is incompatible.\n",
            "bigframes 2.5.0 requires cloudpickle>=2.0.0, but you have cloudpickle 1.6.0 which is incompatible.\n",
            "bigframes 2.5.0 requires numpy>=1.24.0, but you have numpy 1.23.5 which is incompatible.\n",
            "tensorflow-text 2.18.1 requires tensorflow<2.19,>=2.18.0, but you have tensorflow 2.12.0 which is incompatible.\n",
            "ydf 0.12.0 requires protobuf<6.0.0,>=5.29.1, but you have protobuf 4.25.8 which is incompatible.\n",
            "orbax-checkpoint 0.11.13 requires jax>=0.5.0, but you have jax 0.4.30 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed protobuf-4.25.8\n",
            "Requirement already satisfied: tensorflow==2.12.0 in /usr/local/lib/python3.11/dist-packages (2.12.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (25.2.10)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (1.72.1)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (3.13.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (0.4.30)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (2.12.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (18.1.1)\n",
            "Requirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (4.25.8)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (1.17.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (2.12.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (4.14.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow==2.12.0) (0.45.1)\n",
            "Requirement already satisfied: jaxlib<=0.4.30,>=0.4.27 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow==2.12.0) (0.4.30)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow==2.12.0) (0.4.1)\n",
            "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow==2.12.0) (1.15.3)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.38.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.8)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.1.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (4.9.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2025.4.26)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.2.2)\n",
            "Collecting protobuf==3.20.*\n",
            "  Using cached protobuf-3.20.3-py2.py3-none-any.whl.metadata (720 bytes)\n",
            "Using cached protobuf-3.20.3-py2.py3-none-any.whl (162 kB)\n",
            "Installing collected packages: protobuf\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 4.25.8\n",
            "    Uninstalling protobuf-4.25.8:\n",
            "      Successfully uninstalled protobuf-4.25.8\n",
            "\u001b[33mWARNING: Ignoring invalid distribution ~rotobuf (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "grpcio-status 1.71.0 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 3.20.3 which is incompatible.\n",
            "tf-keras 2.18.0 requires tensorflow<2.19,>=2.18, but you have tensorflow 2.12.0 which is incompatible.\n",
            "tensorflow-decision-forests 1.11.0 requires tensorflow==2.18.0, but you have tensorflow 2.12.0 which is incompatible.\n",
            "bigframes 2.5.0 requires cloudpickle>=2.0.0, but you have cloudpickle 1.6.0 which is incompatible.\n",
            "bigframes 2.5.0 requires numpy>=1.24.0, but you have numpy 1.23.5 which is incompatible.\n",
            "tensorflow-text 2.18.1 requires tensorflow<2.19,>=2.18.0, but you have tensorflow 2.12.0 which is incompatible.\n",
            "ydf 0.12.0 requires protobuf<6.0.0,>=5.29.1, but you have protobuf 3.20.3 which is incompatible.\n",
            "orbax-checkpoint 0.11.13 requires jax>=0.5.0, but you have jax 0.4.30 which is incompatible.\n",
            "tensorflow-metadata 1.17.1 requires protobuf<6.0.0,>=4.25.2; python_version >= \"3.11\", but you have protobuf 3.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed protobuf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Imports generales\n",
        "from __future__ import division\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "# Entorno Gym\n",
        "import gym\n",
        "\n",
        "# Deep Learning\n",
        "import tensorflow as tf\n",
        "\n",
        "#Keras-RL\n",
        "from rl.agents.dqn import DQNAgent\n",
        "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy\n",
        "from rl.memory import SequentialMemory\n",
        "from rl.callbacks import FileLogger\n",
        "\n",
        "# capa de compatibilidad con los optimizadores viejo:\n",
        "from tensorflow.keras.optimizers.legacy import Adam"
      ],
      "metadata": {
        "id": "DZE803sbP7J6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env_name = 'Acrobot-v1'\n",
        "env = gym.make(env_name)\n",
        "\n",
        "# Semillas para reproducibilidad\n",
        "np.random.seed(123)\n",
        "env.seed(123)\n",
        "\n",
        "n_obs = env.observation_space.shape\n",
        "nb_actions = env.action_space.n\n"
      ],
      "metadata": {
        "id": "dRnU8gXJQP_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MODELO BASE"
      ],
      "metadata": {
        "id": "M5pjD5t1S3Q8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#\tHyper-params\n",
        "memory_limit\t=\t10000\n",
        "window_length\t=\t4\n",
        "max_eps,\tmin_eps,\ttest_eps\t=\t1.0,\t0.1,\t0.05\n",
        "steps_exploration\t=\t25000\n",
        "steps_warmup\t=\t100\n",
        "gamma\t=\t0.99\n",
        "target_model_update\t=\t1000\n",
        "train_interval\t=\t4\n",
        "learning_rate\t=\t1e-3\n",
        "nb_steps\t=\t50000\n",
        "batch_size\t=\t128\n",
        "#\tModelo\tsimple\n",
        "input_shape\t=\t(n_obs)\n",
        "model\t=\ttf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.Flatten(input_shape=(window_length,)\t+\tenv.observation_space.shape))\n",
        "model.add(tf.keras.layers.Dense(16))\n",
        "model.add(tf.keras.layers.Activation('relu'))\n",
        "model.add(tf.keras.layers.Dense(16))\n",
        "model.add(tf.keras.layers.Activation('relu'))\n",
        "model.add(tf.keras.layers.Dense(16))\n",
        "model.add(tf.keras.layers.Activation('relu'))\n",
        "model.add(tf.keras.layers.Dense(nb_actions))\n",
        "model.add(tf.keras.layers.Activation('linear'))\n",
        "print(model.summary())\n",
        "#\tMemory\n",
        "memory\t=\tSequentialMemory(limit=memory_limit,\twindow_length=window_length)\n",
        "#\tPolicy\n",
        "policy\t=\tLinearAnnealedPolicy(EpsGreedyQPolicy(),\tattr='eps',\tvalue_max=max_eps,\tvalue_min=min_eps,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tvalue_test=test_eps,\tnb_steps=steps_exploration)\n",
        "#\tAgente\n",
        "egreed_dqn\t=\tDQNAgent(model=model,\tnb_actions=nb_actions,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tpolicy=policy,\tmemory=memory,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnb_steps_warmup=steps_warmup,\tgamma=gamma,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\ttarget_model_update=target_model_update,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\ttrain_interval=train_interval,\tbatch_size=batch_size)\n",
        "#\tCompilar\n",
        "egreed_dqn.compile(tf.keras.optimizers.Adam(learning_rate=learning_rate),\tmetrics=['mae'])\n",
        "#\tEntrenar\n",
        "log_filename\t=\t'baseline_{}_log.json'.format(env_name)\n",
        "callbacks\t=\t[FileLogger(log_filename,\tinterval=100)]\n",
        "#egreed_dqn.fit(env,\tcallbacks=callbacks,\tnb_steps=nb_steps,\tvisualize=False,\tverbose=2)\n",
        "egreed_dqn.compile(Adam(learning_rate=learning_rate), metrics=['mae'])\n",
        "#\tSalvar\tpesos\n",
        "egreed_dqn.save_weights('baseline_{}_weights.h5f'.format(env_name),\toverwrite=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oytJL81tSJYI",
        "outputId": "f519f764-2a63-4407-ed4d-da50049776bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten (Flatten)           (None, 24)                0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 16)                400       \n",
            "                                                                 \n",
            " activation (Activation)     (None, 16)                0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 16)                272       \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 16)                0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 16)                272       \n",
            "                                                                 \n",
            " activation_2 (Activation)   (None, 16)                0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 3)                 51        \n",
            "                                                                 \n",
            " activation_3 (Activation)   (None, 3)                 0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 995\n",
            "Trainable params: 995\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#\tTestear\n",
        "egreed_dqn.load_weights('baseline_{}_weights.h5f'.format(env_name))\n",
        "egreed_dqn.test(env,\tnb_episodes=10,\tvisualize=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QpT4cSS4WNPZ",
        "outputId": "a9bce3d5-6918-422b-b0a4-e471a2699f47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing for 10 episodes ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1: reward: -500.000, steps: 500\n",
            "Episode 2: reward: -500.000, steps: 500\n",
            "Episode 3: reward: -500.000, steps: 500\n",
            "Episode 4: reward: -500.000, steps: 500\n",
            "Episode 5: reward: -500.000, steps: 500\n",
            "Episode 6: reward: -500.000, steps: 500\n",
            "Episode 7: reward: -500.000, steps: 500\n",
            "Episode 8: reward: -500.000, steps: 500\n",
            "Episode 9: reward: -500.000, steps: 500\n",
            "Episode 10: reward: -500.000, steps: 500\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x796461951650>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importación de bibliotecas estándar para manejo numérico, reproducibilidad y visualización\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import random\n",
        "import gym\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Importación de módulos necesarios de Keras y Keras-RL\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation, Flatten, BatchNormalization, Dropout\n",
        "from tensorflow.keras.optimizers.legacy import Adam\n",
        "from rl.agents.dqn import DQNAgent\n",
        "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy\n",
        "from rl.memory import SequentialMemory\n",
        "\n",
        "# -------------------- CONFIGURACIÓN GLOBAL --------------------\n",
        "\n",
        "# Establecimiento de semillas para garantizar la reproducibilidad de los experimentos\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "# Inicialización del entorno Acrobot-v1 de OpenAI Gym\n",
        "env = gym.make('Acrobot-v1')\n",
        "env.seed(SEED)\n",
        "env.action_space.seed(SEED)\n",
        "env.observation_space.seed(SEED)\n",
        "\n",
        "# Dimensiones del espacio de acción y observación\n",
        "nb_actions = env.action_space.n\n",
        "obs_shape = env.observation_space.shape\n",
        "\n",
        "# -------------------- DEFINICIÓN DE ARQUITECTURAS --------------------\n",
        "\n",
        "# Mejora 1: Arquitectura con normalización por lotes y tres capas densas\n",
        "def build_model_mejora1():\n",
        "    model = Sequential()\n",
        "    model.add(Flatten(input_shape=(1,) + obs_shape))\n",
        "    model.add(Dense(64))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dense(64))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dense(32, activation='relu'))\n",
        "    model.add(Dense(nb_actions, activation='linear'))\n",
        "    return model\n",
        "\n",
        "# Mejora 2: Red más profunda con Dropout para regularización\n",
        "def build_model_mejora2():\n",
        "    model = Sequential()\n",
        "    model.add(Flatten(input_shape=(1,) + obs_shape))\n",
        "    model.add(Dense(256, activation='relu'))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Dense(256, activation='relu'))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dense(nb_actions, activation='linear'))\n",
        "    return model\n",
        "\n",
        "# Mejora 3: Arquitectura intermedia con activación y normalización combinadas\n",
        "def build_model_mejora3():\n",
        "    model = Sequential()\n",
        "    model.add(Flatten(input_shape=(1,) + obs_shape))\n",
        "    model.add(Dense(128))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(128))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dense(nb_actions))\n",
        "    model.add(Activation('linear'))\n",
        "    return model\n",
        "\n",
        "# Mejora 4 (Baseline): Red simple con capas densas pequeñas\n",
        "def build_model_mejora4():\n",
        "    model = Sequential()\n",
        "    model.add(Flatten(input_shape=(1,) + obs_shape))\n",
        "    model.add(Dense(16, activation='relu'))\n",
        "    model.add(Dense(16, activation='relu'))\n",
        "    model.add(Dense(16, activation='relu'))\n",
        "    model.add(Dense(nb_actions, activation='linear'))\n",
        "    return model\n",
        "\n",
        "# -------------------- CONFIGURACIÓN DE EXPERIMENTOS --------------------\n",
        "\n",
        "# Lista de configuraciones con arquitectura y parámetros específicos por mejora\n",
        "parametros = [\n",
        "    {\n",
        "        'modelo': build_model_mejora1,\n",
        "        'nombre': 'Mejora 1',\n",
        "        'params': dict(memory_limit=50000, eps_min=0.01, eps_test=0.01, steps_exploration=100000,\n",
        "                       warmup_steps=1000, gamma=0.98, target_model_update=500,\n",
        "                       batch_size=64, learning_rate=5e-4, nb_steps=100000)\n",
        "    },\n",
        "    {\n",
        "        'modelo': build_model_mejora2,\n",
        "        'nombre': 'Mejora 2',\n",
        "        'params': dict(memory_limit=50000, eps_min=0.02, eps_test=0.02, steps_exploration=200000,\n",
        "                       warmup_steps=5000, gamma=0.995, target_model_update=250,\n",
        "                       batch_size=256, learning_rate=3e-4, nb_steps=200000)\n",
        "    },\n",
        "    {\n",
        "        'modelo': build_model_mejora3,\n",
        "        'nombre': 'Mejora 3',\n",
        "        'params': dict(memory_limit=100000, eps_min=0.05, eps_test=0.01, steps_exploration=150000,\n",
        "                       warmup_steps=5000, gamma=0.99, target_model_update=1000,\n",
        "                       batch_size=128, learning_rate=1e-4, nb_steps=200000)\n",
        "    },\n",
        "    {\n",
        "        'modelo': build_model_mejora4,\n",
        "        'nombre': 'Base',\n",
        "        'params': dict(memory_limit=10000, eps_min=0.1, eps_test=0.05, steps_exploration=25000,\n",
        "                       warmup_steps=100, gamma=0.99, target_model_update=1000,\n",
        "                       batch_size=128, learning_rate=1e-3, nb_steps=50000)\n",
        "    }\n",
        "]\n",
        "\n",
        "# -------------------- FUNCIÓN DE ENTRENAMIENTO --------------------\n",
        "\n",
        "def entrenar_evaluar(model_func, nombre, p):\n",
        "    \"\"\"\n",
        "    Entrena y evalúa un agente DQN con arquitectura y configuración específicas.\n",
        "\n",
        "    Args:\n",
        "        model_func: función constructora del modelo.\n",
        "        nombre: identificador del experimento.\n",
        "        p: diccionario de hiperparámetros.\n",
        "    Returns:\n",
        "        recompensas: arreglo NumPy con las recompensas por episodio durante la evaluación.\n",
        "    \"\"\"\n",
        "    model = model_func()\n",
        "    memory = SequentialMemory(limit=p['memory_limit'], window_length=1)\n",
        "    policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps',\n",
        "                                  value_max=1.0, value_min=p['eps_min'],\n",
        "                                  value_test=p['eps_test'], nb_steps=p['steps_exploration'])\n",
        "\n",
        "    agente = DQNAgent(model=model, nb_actions=nb_actions, memory=memory,\n",
        "                      policy=policy, nb_steps_warmup=p['warmup_steps'],\n",
        "                      gamma=p['gamma'], target_model_update=p['target_model_update'],\n",
        "                      train_interval=1, batch_size=p['batch_size'])\n",
        "\n",
        "    agente.compile(Adam(learning_rate=p['learning_rate']), metrics=['mae'])\n",
        "\n",
        "    print(f\"\\nEntrenando {nombre}...\")\n",
        "    agente.fit(env, nb_steps=p['nb_steps'], visualize=False, verbose=2)\n",
        "\n",
        "    print(f\"\\nEvaluando {nombre}...\")\n",
        "    resultados = agente.test(env, nb_episodes=20, visualize=False, verbose=0)\n",
        "    recompensas = np.array(resultados.history['episode_reward'])\n",
        "    print(f\"{nombre} → Recompensa media: {recompensas.mean():.2f} ± {recompensas.std():.2f}\")\n",
        "    return recompensas\n",
        "\n",
        "# -------------------- EJECUCIÓN DE EXPERIMENTOS --------------------\n",
        "\n",
        "# Diccionario para almacenar las recompensas de cada agente\n",
        "resultados = {}\n",
        "for item in parametros:\n",
        "    recompensas = entrenar_evaluar(item['modelo'], item['nombre'], item['params'])\n",
        "    resultados[item['nombre']] = recompensas\n",
        "\n",
        "# -------------------- VISUALIZACIÓN DE RESULTADOS --------------------\n",
        "\n",
        "# Histograma comparativo de las distribuciones de recompensa\n",
        "plt.figure(figsize=(10, 6))\n",
        "for nombre, recompensas in resultados.items():\n",
        "    plt.hist(recompensas, bins=10, alpha=0.5, label=nombre)\n",
        "plt.title('Comparación de Recompensas - Todas las Mejoras')\n",
        "plt.xlabel('Recompensa por Episodio')\n",
        "plt.ylabel('Frecuencia')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Tabla resumen de estadísticos descriptivos\n",
        "df_comparacion = pd.DataFrame(resultados)\n",
        "resumen = df_comparacion.describe().loc[['mean', 'std']].T\n",
        "resumen.columns = ['Media', 'Desviación estándar']\n",
        "print(\"\\nResumen comparativo de todas las mejoras:\\n\")\n",
        "print(resumen)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "mpeV8kLO90A8",
        "outputId": "92ea9867-cd87-4083-ae5f-6a605c962366"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.11/dist-packages/keras/layers/normalization/batch_normalization.py:581: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Entrenando Mejora 1...\n",
            "Training for 100000 steps ...\n",
            "   500/100000: episode: 1, duration: 0.761s, episode steps: 500, steps per second: 657, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.948 [0.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  1000/100000: episode: 2, duration: 0.537s, episode steps: 500, steps per second: 931, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.912 [0.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  1500/100000: episode: 3, duration: 6.612s, episode steps: 500, steps per second:  76, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.038998, mae: 0.591000, mean_q: -0.665666, mean_eps: 0.987625\n",
            "  2000/100000: episode: 4, duration: 4.009s, episode steps: 500, steps per second: 125, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.036 [0.000, 2.000],  loss: 0.014133, mae: 1.226697, mean_q: -1.715486, mean_eps: 0.982680\n",
            "  2500/100000: episode: 5, duration: 4.313s, episode steps: 500, steps per second: 116, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.028 [0.000, 2.000],  loss: 0.016119, mae: 1.869331, mean_q: -2.685463, mean_eps: 0.977730\n",
            "  3000/100000: episode: 6, duration: 4.739s, episode steps: 500, steps per second: 106, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.076 [0.000, 2.000],  loss: 0.020726, mae: 2.495324, mean_q: -3.616948, mean_eps: 0.972780\n",
            "  3500/100000: episode: 7, duration: 3.874s, episode steps: 500, steps per second: 129, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.986 [0.000, 2.000],  loss: 0.026740, mae: 3.118897, mean_q: -4.541459, mean_eps: 0.967830\n",
            "  4000/100000: episode: 8, duration: 3.892s, episode steps: 500, steps per second: 128, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.968 [0.000, 2.000],  loss: 0.039624, mae: 3.710595, mean_q: -5.409771, mean_eps: 0.962880\n",
            "  4500/100000: episode: 9, duration: 5.090s, episode steps: 500, steps per second:  98, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.046712, mae: 4.289356, mean_q: -6.270119, mean_eps: 0.957930\n",
            "  5000/100000: episode: 10, duration: 4.141s, episode steps: 500, steps per second: 121, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.028 [0.000, 2.000],  loss: 0.060293, mae: 4.874527, mean_q: -7.138987, mean_eps: 0.952980\n",
            "  5500/100000: episode: 11, duration: 4.091s, episode steps: 500, steps per second: 122, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.069064, mae: 5.387935, mean_q: -7.903308, mean_eps: 0.948030\n",
            "  6000/100000: episode: 12, duration: 5.262s, episode steps: 500, steps per second:  95, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.056 [0.000, 2.000],  loss: 0.074094, mae: 5.949295, mean_q: -8.732438, mean_eps: 0.943080\n",
            "  6500/100000: episode: 13, duration: 3.940s, episode steps: 500, steps per second: 127, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.054 [0.000, 2.000],  loss: 0.087473, mae: 6.419215, mean_q: -9.427242, mean_eps: 0.938130\n",
            "  7000/100000: episode: 14, duration: 3.948s, episode steps: 500, steps per second: 127, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.012 [0.000, 2.000],  loss: 0.110630, mae: 6.928768, mean_q: -10.170558, mean_eps: 0.933180\n",
            "  7500/100000: episode: 15, duration: 5.195s, episode steps: 500, steps per second:  96, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.066 [0.000, 2.000],  loss: 0.119263, mae: 7.460303, mean_q: -10.965220, mean_eps: 0.928230\n",
            "  8000/100000: episode: 16, duration: 4.060s, episode steps: 500, steps per second: 123, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.004 [0.000, 2.000],  loss: 0.138667, mae: 7.990169, mean_q: -11.752705, mean_eps: 0.923280\n",
            "  8500/100000: episode: 17, duration: 4.057s, episode steps: 500, steps per second: 123, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.956 [0.000, 2.000],  loss: 0.142924, mae: 8.432426, mean_q: -12.414807, mean_eps: 0.918330\n",
            "  9000/100000: episode: 18, duration: 5.518s, episode steps: 500, steps per second:  91, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.162271, mae: 8.901625, mean_q: -13.110835, mean_eps: 0.913380\n",
            "  9500/100000: episode: 19, duration: 4.307s, episode steps: 500, steps per second: 116, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.896 [0.000, 2.000],  loss: 0.158959, mae: 9.364130, mean_q: -13.787264, mean_eps: 0.908430\n",
            " 10000/100000: episode: 20, duration: 4.249s, episode steps: 500, steps per second: 118, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.928 [0.000, 2.000],  loss: 0.215445, mae: 9.761039, mean_q: -14.366409, mean_eps: 0.903480\n",
            " 10500/100000: episode: 21, duration: 5.222s, episode steps: 500, steps per second:  96, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.978 [0.000, 2.000],  loss: 0.265909, mae: 10.182644, mean_q: -14.980414, mean_eps: 0.898530\n",
            " 11000/100000: episode: 22, duration: 4.213s, episode steps: 500, steps per second: 119, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.207190, mae: 10.553680, mean_q: -15.555704, mean_eps: 0.893580\n",
            " 11500/100000: episode: 23, duration: 4.038s, episode steps: 500, steps per second: 124, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.034 [0.000, 2.000],  loss: 0.275755, mae: 10.940802, mean_q: -16.107811, mean_eps: 0.888630\n",
            " 12000/100000: episode: 24, duration: 5.624s, episode steps: 500, steps per second:  89, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.072 [0.000, 2.000],  loss: 0.247616, mae: 11.120741, mean_q: -16.385618, mean_eps: 0.883680\n",
            " 12467/100000: episode: 25, duration: 3.853s, episode steps: 467, steps per second: 121, episode reward: -466.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.086 [0.000, 2.000],  loss: 0.280375, mae: 11.509246, mean_q: -16.953000, mean_eps: 0.878893\n",
            " 12967/100000: episode: 26, duration: 4.035s, episode steps: 500, steps per second: 124, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.962 [0.000, 2.000],  loss: 0.323101, mae: 11.835587, mean_q: -17.411261, mean_eps: 0.874107\n",
            " 13467/100000: episode: 27, duration: 5.190s, episode steps: 500, steps per second:  96, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.956 [0.000, 2.000],  loss: 0.357463, mae: 12.207296, mean_q: -17.971872, mean_eps: 0.869157\n",
            " 13724/100000: episode: 28, duration: 2.120s, episode steps: 257, steps per second: 121, episode reward: -256.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.023 [0.000, 2.000],  loss: 0.481789, mae: 12.535990, mean_q: -18.429651, mean_eps: 0.865410\n",
            " 14113/100000: episode: 29, duration: 3.282s, episode steps: 389, steps per second: 119, episode reward: -388.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.059 [0.000, 2.000],  loss: 0.403911, mae: 12.658618, mean_q: -18.625223, mean_eps: 0.862212\n",
            " 14613/100000: episode: 30, duration: 4.296s, episode steps: 500, steps per second: 116, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.004 [0.000, 2.000],  loss: 0.448720, mae: 12.920584, mean_q: -19.000773, mean_eps: 0.857811\n",
            " 15113/100000: episode: 31, duration: 5.163s, episode steps: 500, steps per second:  97, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.443079, mae: 13.188722, mean_q: -19.411746, mean_eps: 0.852861\n",
            " 15613/100000: episode: 32, duration: 4.228s, episode steps: 500, steps per second: 118, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.490673, mae: 13.405175, mean_q: -19.713795, mean_eps: 0.847911\n",
            " 16113/100000: episode: 33, duration: 4.361s, episode steps: 500, steps per second: 115, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.098 [0.000, 2.000],  loss: 0.424920, mae: 13.606047, mean_q: -20.013565, mean_eps: 0.842961\n",
            " 16529/100000: episode: 34, duration: 4.355s, episode steps: 416, steps per second:  96, episode reward: -415.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.053 [0.000, 2.000],  loss: 0.446897, mae: 13.724058, mean_q: -20.170448, mean_eps: 0.838427\n",
            " 17029/100000: episode: 35, duration: 4.040s, episode steps: 500, steps per second: 124, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.054 [0.000, 2.000],  loss: 0.524553, mae: 13.962857, mean_q: -20.503177, mean_eps: 0.833893\n",
            " 17450/100000: episode: 36, duration: 3.479s, episode steps: 421, steps per second: 121, episode reward: -420.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.969 [0.000, 2.000],  loss: 0.543396, mae: 14.062124, mean_q: -20.624382, mean_eps: 0.829334\n",
            " 17846/100000: episode: 37, duration: 4.247s, episode steps: 396, steps per second:  93, episode reward: -395.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.058 [0.000, 2.000],  loss: 0.598696, mae: 14.251623, mean_q: -20.891299, mean_eps: 0.825290\n",
            " 18346/100000: episode: 38, duration: 4.389s, episode steps: 500, steps per second: 114, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.032 [0.000, 2.000],  loss: 0.571272, mae: 14.340226, mean_q: -21.025356, mean_eps: 0.820855\n",
            " 18805/100000: episode: 39, duration: 3.779s, episode steps: 459, steps per second: 121, episode reward: -458.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.085 [0.000, 2.000],  loss: 0.594541, mae: 14.543614, mean_q: -21.329121, mean_eps: 0.816107\n",
            " 19305/100000: episode: 40, duration: 5.351s, episode steps: 500, steps per second:  93, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.966 [0.000, 2.000],  loss: 0.624100, mae: 14.767582, mean_q: -21.669717, mean_eps: 0.811360\n",
            " 19805/100000: episode: 41, duration: 4.474s, episode steps: 500, steps per second: 112, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.982 [0.000, 2.000],  loss: 0.628692, mae: 14.986702, mean_q: -21.989749, mean_eps: 0.806410\n",
            " 20219/100000: episode: 42, duration: 3.464s, episode steps: 414, steps per second: 120, episode reward: -413.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.005 [0.000, 2.000],  loss: 0.616460, mae: 15.041634, mean_q: -22.064840, mean_eps: 0.801886\n",
            " 20512/100000: episode: 43, duration: 2.451s, episode steps: 293, steps per second: 120, episode reward: -292.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.014 [0.000, 2.000],  loss: 0.621287, mae: 15.027445, mean_q: -22.049842, mean_eps: 0.798386\n",
            " 20838/100000: episode: 44, duration: 3.841s, episode steps: 326, steps per second:  85, episode reward: -325.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.061 [0.000, 2.000],  loss: 0.709361, mae: 15.081975, mean_q: -22.091932, mean_eps: 0.795322\n",
            " 21110/100000: episode: 45, duration: 2.260s, episode steps: 272, steps per second: 120, episode reward: -271.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.066 [0.000, 2.000],  loss: 0.644417, mae: 15.096778, mean_q: -22.096300, mean_eps: 0.792362\n",
            " 21525/100000: episode: 46, duration: 3.426s, episode steps: 415, steps per second: 121, episode reward: -414.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.077 [0.000, 2.000],  loss: 0.852787, mae: 15.126624, mean_q: -22.145595, mean_eps: 0.788962\n",
            " 21821/100000: episode: 47, duration: 2.451s, episode steps: 296, steps per second: 121, episode reward: -295.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.179 [0.000, 2.000],  loss: 0.757270, mae: 15.357618, mean_q: -22.488473, mean_eps: 0.785442\n",
            " 22079/100000: episode: 48, duration: 2.399s, episode steps: 258, steps per second: 108, episode reward: -257.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.031 [0.000, 2.000],  loss: 0.741063, mae: 15.455010, mean_q: -22.602217, mean_eps: 0.782700\n",
            " 22449/100000: episode: 49, duration: 4.078s, episode steps: 370, steps per second:  91, episode reward: -369.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.035 [0.000, 2.000],  loss: 0.739761, mae: 15.749858, mean_q: -23.047168, mean_eps: 0.779591\n",
            " 22917/100000: episode: 50, duration: 4.059s, episode steps: 468, steps per second: 115, episode reward: -467.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.062 [0.000, 2.000],  loss: 0.739958, mae: 15.881173, mean_q: -23.231916, mean_eps: 0.775443\n",
            " 23201/100000: episode: 51, duration: 2.340s, episode steps: 284, steps per second: 121, episode reward: -283.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.951 [0.000, 2.000],  loss: 0.835799, mae: 16.124010, mean_q: -23.572601, mean_eps: 0.771721\n",
            " 23507/100000: episode: 52, duration: 2.622s, episode steps: 306, steps per second: 117, episode reward: -305.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.056 [0.000, 2.000],  loss: 0.760388, mae: 16.184094, mean_q: -23.669801, mean_eps: 0.768800\n",
            " 23913/100000: episode: 53, duration: 4.505s, episode steps: 406, steps per second:  90, episode reward: -405.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.052 [0.000, 2.000],  loss: 0.813176, mae: 16.503870, mean_q: -24.131704, mean_eps: 0.765276\n",
            " 24264/100000: episode: 54, duration: 2.901s, episode steps: 351, steps per second: 121, episode reward: -350.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.108 [0.000, 2.000],  loss: 0.918127, mae: 16.592914, mean_q: -24.270448, mean_eps: 0.761529\n",
            " 24453/100000: episode: 55, duration: 1.559s, episode steps: 189, steps per second: 121, episode reward: -188.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.915 [0.000, 2.000],  loss: 0.808734, mae: 16.617168, mean_q: -24.328347, mean_eps: 0.758856\n",
            " 24776/100000: episode: 56, duration: 2.679s, episode steps: 323, steps per second: 121, episode reward: -322.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.969 [0.000, 2.000],  loss: 0.851079, mae: 17.031190, mean_q: -24.950828, mean_eps: 0.756321\n",
            " 25052/100000: episode: 57, duration: 2.675s, episode steps: 276, steps per second: 103, episode reward: -275.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.007 [0.000, 2.000],  loss: 0.941826, mae: 17.162409, mean_q: -25.111572, mean_eps: 0.753356\n",
            " 25197/100000: episode: 58, duration: 1.858s, episode steps: 145, steps per second:  78, episode reward: -144.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.897 [0.000, 2.000],  loss: 0.920545, mae: 17.165260, mean_q: -25.089393, mean_eps: 0.751272\n",
            " 25514/100000: episode: 59, duration: 3.082s, episode steps: 317, steps per second: 103, episode reward: -316.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.161 [0.000, 2.000],  loss: 0.913894, mae: 17.158936, mean_q: -25.059722, mean_eps: 0.748985\n",
            " 25909/100000: episode: 60, duration: 3.342s, episode steps: 395, steps per second: 118, episode reward: -394.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.005 [0.000, 2.000],  loss: 0.927603, mae: 17.087181, mean_q: -24.827783, mean_eps: 0.745461\n",
            " 26146/100000: episode: 61, duration: 2.032s, episode steps: 237, steps per second: 117, episode reward: -236.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.004 [0.000, 2.000],  loss: 1.022337, mae: 17.164542, mean_q: -24.928506, mean_eps: 0.742333\n",
            " 26646/100000: episode: 62, duration: 5.148s, episode steps: 500, steps per second:  97, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.972 [0.000, 2.000],  loss: 1.007163, mae: 17.234125, mean_q: -25.017581, mean_eps: 0.738685\n",
            " 27047/100000: episode: 63, duration: 3.771s, episode steps: 401, steps per second: 106, episode reward: -400.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.035 [0.000, 2.000],  loss: 1.024121, mae: 17.379541, mean_q: -25.226215, mean_eps: 0.734225\n",
            " 27430/100000: episode: 64, duration: 3.328s, episode steps: 383, steps per second: 115, episode reward: -382.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.008 [0.000, 2.000],  loss: 0.964681, mae: 17.601498, mean_q: -25.579570, mean_eps: 0.730344\n",
            " 27845/100000: episode: 65, duration: 3.583s, episode steps: 415, steps per second: 116, episode reward: -414.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.092 [0.000, 2.000],  loss: 1.082838, mae: 17.829364, mean_q: -25.904751, mean_eps: 0.726394\n",
            " 28074/100000: episode: 66, duration: 2.846s, episode steps: 229, steps per second:  80, episode reward: -228.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.956 [0.000, 2.000],  loss: 1.126511, mae: 17.890883, mean_q: -25.963214, mean_eps: 0.723206\n",
            " 28373/100000: episode: 67, duration: 2.888s, episode steps: 299, steps per second: 104, episode reward: -298.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.161 [0.000, 2.000],  loss: 1.112914, mae: 18.033149, mean_q: -26.162867, mean_eps: 0.720592\n",
            " 28599/100000: episode: 68, duration: 2.105s, episode steps: 226, steps per second: 107, episode reward: -225.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.102 [0.000, 2.000],  loss: 1.166389, mae: 18.140489, mean_q: -26.293202, mean_eps: 0.717994\n",
            " 28860/100000: episode: 69, duration: 2.306s, episode steps: 261, steps per second: 113, episode reward: -260.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.027 [0.000, 2.000],  loss: 1.126724, mae: 18.347379, mean_q: -26.661717, mean_eps: 0.715583\n",
            " 29098/100000: episode: 70, duration: 2.131s, episode steps: 238, steps per second: 112, episode reward: -237.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.975 [0.000, 2.000],  loss: 1.381502, mae: 18.379195, mean_q: -26.647475, mean_eps: 0.713113\n",
            " 29392/100000: episode: 71, duration: 3.039s, episode steps: 294, steps per second:  97, episode reward: -293.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.061 [0.000, 2.000],  loss: 1.267145, mae: 18.416396, mean_q: -26.664440, mean_eps: 0.710479\n",
            " 29796/100000: episode: 72, duration: 4.019s, episode steps: 404, steps per second: 101, episode reward: -403.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.074 [0.000, 2.000],  loss: 1.168016, mae: 18.329743, mean_q: -26.568412, mean_eps: 0.707024\n",
            " 30053/100000: episode: 73, duration: 2.229s, episode steps: 257, steps per second: 115, episode reward: -256.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.179 [0.000, 2.000],  loss: 1.014946, mae: 18.280007, mean_q: -26.495635, mean_eps: 0.703752\n",
            " 30361/100000: episode: 74, duration: 2.631s, episode steps: 308, steps per second: 117, episode reward: -307.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.081 [0.000, 2.000],  loss: 1.286342, mae: 18.193170, mean_q: -26.272157, mean_eps: 0.700956\n",
            " 30557/100000: episode: 75, duration: 1.729s, episode steps: 196, steps per second: 113, episode reward: -195.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.112 [0.000, 2.000],  loss: 1.208372, mae: 18.200056, mean_q: -26.309753, mean_eps: 0.698461\n",
            " 30732/100000: episode: 76, duration: 1.678s, episode steps: 175, steps per second: 104, episode reward: -174.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.114 [0.000, 2.000],  loss: 1.128361, mae: 18.269823, mean_q: -26.416219, mean_eps: 0.696624\n",
            " 30941/100000: episode: 77, duration: 2.685s, episode steps: 209, steps per second:  78, episode reward: -208.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.091 [0.000, 2.000],  loss: 1.255934, mae: 18.275791, mean_q: -26.426904, mean_eps: 0.694724\n",
            " 31246/100000: episode: 78, duration: 2.860s, episode steps: 305, steps per second: 107, episode reward: -304.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.108 [0.000, 2.000],  loss: 1.205814, mae: 18.375088, mean_q: -26.598581, mean_eps: 0.692179\n",
            " 31549/100000: episode: 79, duration: 2.674s, episode steps: 303, steps per second: 113, episode reward: -302.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.046 [0.000, 2.000],  loss: 1.169598, mae: 18.400837, mean_q: -26.673372, mean_eps: 0.689170\n",
            " 31848/100000: episode: 80, duration: 2.711s, episode steps: 299, steps per second: 110, episode reward: -298.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.017 [0.000, 2.000],  loss: 1.300380, mae: 18.448222, mean_q: -26.663707, mean_eps: 0.686190\n",
            " 32088/100000: episode: 81, duration: 2.140s, episode steps: 240, steps per second: 112, episode reward: -239.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.979 [0.000, 2.000],  loss: 1.172199, mae: 18.448794, mean_q: -26.685768, mean_eps: 0.683522\n",
            " 32336/100000: episode: 82, duration: 3.223s, episode steps: 248, steps per second:  77, episode reward: -247.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.004 [0.000, 2.000],  loss: 1.339886, mae: 18.348781, mean_q: -26.452710, mean_eps: 0.681106\n",
            " 32553/100000: episode: 83, duration: 2.006s, episode steps: 217, steps per second: 108, episode reward: -216.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.088 [0.000, 2.000],  loss: 1.380793, mae: 18.333194, mean_q: -26.455195, mean_eps: 0.678804\n",
            " 32805/100000: episode: 84, duration: 2.170s, episode steps: 252, steps per second: 116, episode reward: -251.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.952 [0.000, 2.000],  loss: 1.330700, mae: 18.379644, mean_q: -26.473877, mean_eps: 0.676483\n",
            " 33057/100000: episode: 85, duration: 2.218s, episode steps: 252, steps per second: 114, episode reward: -251.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.075 [0.000, 2.000],  loss: 1.446163, mae: 18.312243, mean_q: -26.409439, mean_eps: 0.673988\n",
            " 33248/100000: episode: 86, duration: 1.686s, episode steps: 191, steps per second: 113, episode reward: -190.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.963 [0.000, 2.000],  loss: 1.380180, mae: 18.259228, mean_q: -26.259698, mean_eps: 0.671795\n",
            " 33585/100000: episode: 87, duration: 3.174s, episode steps: 337, steps per second: 106, episode reward: -336.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.964 [0.000, 2.000],  loss: 1.157606, mae: 18.329675, mean_q: -26.453193, mean_eps: 0.669182\n",
            " 33863/100000: episode: 88, duration: 3.197s, episode steps: 278, steps per second:  87, episode reward: -277.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.978 [0.000, 2.000],  loss: 1.331079, mae: 18.399425, mean_q: -26.520249, mean_eps: 0.666137\n",
            " 34116/100000: episode: 89, duration: 2.267s, episode steps: 253, steps per second: 112, episode reward: -252.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.032 [0.000, 2.000],  loss: 1.458984, mae: 18.491906, mean_q: -26.659044, mean_eps: 0.663509\n",
            " 34346/100000: episode: 90, duration: 2.045s, episode steps: 230, steps per second: 112, episode reward: -229.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.187 [0.000, 2.000],  loss: 1.532704, mae: 18.613333, mean_q: -26.694949, mean_eps: 0.661118\n",
            " 34525/100000: episode: 91, duration: 1.629s, episode steps: 179, steps per second: 110, episode reward: -178.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.117 [0.000, 2.000],  loss: 1.405710, mae: 18.663956, mean_q: -26.860543, mean_eps: 0.659093\n",
            " 34779/100000: episode: 92, duration: 2.370s, episode steps: 254, steps per second: 107, episode reward: -253.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.067 [0.000, 2.000],  loss: 1.504073, mae: 19.019226, mean_q: -27.410043, mean_eps: 0.656950\n",
            " 34965/100000: episode: 93, duration: 2.038s, episode steps: 186, steps per second:  91, episode reward: -185.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.177 [0.000, 2.000],  loss: 1.420137, mae: 18.966661, mean_q: -27.320935, mean_eps: 0.654772\n",
            " 35106/100000: episode: 94, duration: 1.825s, episode steps: 141, steps per second:  77, episode reward: -140.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.078 [0.000, 2.000],  loss: 1.289542, mae: 18.885197, mean_q: -27.271963, mean_eps: 0.653153\n",
            " 35383/100000: episode: 95, duration: 2.658s, episode steps: 277, steps per second: 104, episode reward: -276.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.931 [0.000, 2.000],  loss: 1.464820, mae: 18.830457, mean_q: -27.094945, mean_eps: 0.651084\n",
            " 35699/100000: episode: 96, duration: 2.771s, episode steps: 316, steps per second: 114, episode reward: -315.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.003 [0.000, 2.000],  loss: 1.394683, mae: 18.743020, mean_q: -26.970856, mean_eps: 0.648149\n",
            " 35902/100000: episode: 97, duration: 1.755s, episode steps: 203, steps per second: 116, episode reward: -202.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.049 [0.000, 2.000],  loss: 1.585127, mae: 18.625051, mean_q: -26.715781, mean_eps: 0.645580\n",
            " 36108/100000: episode: 98, duration: 1.768s, episode steps: 206, steps per second: 116, episode reward: -205.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.951 [0.000, 2.000],  loss: 1.475112, mae: 18.785554, mean_q: -26.891036, mean_eps: 0.643555\n",
            " 36277/100000: episode: 99, duration: 1.457s, episode steps: 169, steps per second: 116, episode reward: -168.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.929 [0.000, 2.000],  loss: 1.655172, mae: 18.816038, mean_q: -27.028132, mean_eps: 0.641699\n",
            " 36491/100000: episode: 100, duration: 2.581s, episode steps: 214, steps per second:  83, episode reward: -213.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.028 [0.000, 2.000],  loss: 1.731027, mae: 18.800600, mean_q: -26.976274, mean_eps: 0.639803\n",
            " 36991/100000: episode: 101, duration: 4.576s, episode steps: 500, steps per second: 109, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.730 [0.000, 2.000],  loss: 1.493555, mae: 18.933204, mean_q: -27.253335, mean_eps: 0.636269\n",
            " 37266/100000: episode: 102, duration: 2.363s, episode steps: 275, steps per second: 116, episode reward: -274.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.876 [0.000, 2.000],  loss: 1.509455, mae: 19.098935, mean_q: -27.431139, mean_eps: 0.632433\n",
            " 37510/100000: episode: 103, duration: 2.104s, episode steps: 244, steps per second: 116, episode reward: -243.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.004 [0.000, 2.000],  loss: 1.502791, mae: 19.168811, mean_q: -27.652498, mean_eps: 0.629864\n",
            " 37752/100000: episode: 104, duration: 2.143s, episode steps: 242, steps per second: 113, episode reward: -241.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.025 [0.000, 2.000],  loss: 1.670763, mae: 19.133362, mean_q: -27.433956, mean_eps: 0.627458\n",
            " 38099/100000: episode: 105, duration: 4.104s, episode steps: 347, steps per second:  85, episode reward: -346.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.121 [0.000, 2.000],  loss: 1.628448, mae: 19.029978, mean_q: -27.311777, mean_eps: 0.624542\n",
            " 38240/100000: episode: 106, duration: 1.287s, episode steps: 141, steps per second: 110, episode reward: -140.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.014 [0.000, 2.000],  loss: 1.707056, mae: 19.031982, mean_q: -27.263796, mean_eps: 0.622127\n",
            " 38433/100000: episode: 107, duration: 1.722s, episode steps: 193, steps per second: 112, episode reward: -192.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.855 [0.000, 2.000],  loss: 1.695958, mae: 18.963640, mean_q: -27.125749, mean_eps: 0.620474\n",
            " 38627/100000: episode: 108, duration: 1.791s, episode steps: 194, steps per second: 108, episode reward: -193.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.918 [0.000, 2.000],  loss: 1.759646, mae: 18.996545, mean_q: -27.155293, mean_eps: 0.618558\n",
            " 38812/100000: episode: 109, duration: 1.676s, episode steps: 185, steps per second: 110, episode reward: -184.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.081 [0.000, 2.000],  loss: 1.760009, mae: 19.024815, mean_q: -27.164425, mean_eps: 0.616682\n",
            " 38979/100000: episode: 110, duration: 3.160s, episode steps: 167, steps per second:  53, episode reward: -166.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.042 [0.000, 2.000],  loss: 1.661295, mae: 19.112569, mean_q: -27.393929, mean_eps: 0.614939\n",
            " 39162/100000: episode: 111, duration: 4.666s, episode steps: 183, steps per second:  39, episode reward: -182.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.016 [0.000, 2.000],  loss: 1.654528, mae: 19.059968, mean_q: -27.169873, mean_eps: 0.613207\n",
            " 39348/100000: episode: 112, duration: 2.378s, episode steps: 186, steps per second:  78, episode reward: -185.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.097 [0.000, 2.000],  loss: 1.804974, mae: 19.009885, mean_q: -27.053793, mean_eps: 0.611380\n",
            " 39561/100000: episode: 113, duration: 1.942s, episode steps: 213, steps per second: 110, episode reward: -212.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.972 [0.000, 2.000],  loss: 1.604787, mae: 19.045699, mean_q: -27.139877, mean_eps: 0.609405\n",
            " 39783/100000: episode: 114, duration: 2.014s, episode steps: 222, steps per second: 110, episode reward: -221.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.959 [0.000, 2.000],  loss: 1.674466, mae: 19.126763, mean_q: -27.364429, mean_eps: 0.607252\n",
            " 39935/100000: episode: 115, duration: 1.320s, episode steps: 152, steps per second: 115, episode reward: -151.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.033 [0.000, 2.000],  loss: 1.906905, mae: 19.082417, mean_q: -27.197760, mean_eps: 0.605401\n",
            " 40164/100000: episode: 116, duration: 1.998s, episode steps: 229, steps per second: 115, episode reward: -228.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.987 [0.000, 2.000],  loss: 1.727840, mae: 19.197996, mean_q: -27.474957, mean_eps: 0.603515\n",
            " 40412/100000: episode: 117, duration: 2.219s, episode steps: 248, steps per second: 112, episode reward: -247.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.060 [0.000, 2.000],  loss: 1.370891, mae: 19.245325, mean_q: -27.571366, mean_eps: 0.601154\n",
            " 40660/100000: episode: 118, duration: 3.237s, episode steps: 248, steps per second:  77, episode reward: -247.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.008 [0.000, 2.000],  loss: 1.554694, mae: 19.087298, mean_q: -27.363736, mean_eps: 0.598699\n",
            " 40825/100000: episode: 119, duration: 1.604s, episode steps: 165, steps per second: 103, episode reward: -164.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.903 [0.000, 2.000],  loss: 1.536311, mae: 19.100608, mean_q: -27.373283, mean_eps: 0.596654\n",
            " 40995/100000: episode: 120, duration: 1.520s, episode steps: 170, steps per second: 112, episode reward: -169.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.794 [0.000, 2.000],  loss: 1.499484, mae: 18.977020, mean_q: -27.237590, mean_eps: 0.594996\n",
            " 41337/100000: episode: 121, duration: 3.132s, episode steps: 342, steps per second: 109, episode reward: -341.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.161 [0.000, 2.000],  loss: 1.515829, mae: 19.189801, mean_q: -27.553279, mean_eps: 0.592462\n",
            " 41521/100000: episode: 122, duration: 1.642s, episode steps: 184, steps per second: 112, episode reward: -183.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.935 [0.000, 2.000],  loss: 1.483297, mae: 19.110989, mean_q: -27.262494, mean_eps: 0.589858\n",
            " 41833/100000: episode: 123, duration: 3.012s, episode steps: 312, steps per second: 104, episode reward: -311.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.176 [0.000, 2.000],  loss: 1.553763, mae: 18.940921, mean_q: -27.014945, mean_eps: 0.587403\n",
            " 41987/100000: episode: 124, duration: 1.973s, episode steps: 154, steps per second:  78, episode reward: -153.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.026 [0.000, 2.000],  loss: 1.736815, mae: 18.873260, mean_q: -26.913887, mean_eps: 0.585096\n",
            " 42215/100000: episode: 125, duration: 2.459s, episode steps: 228, steps per second:  93, episode reward: -227.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.180 [0.000, 2.000],  loss: 1.848806, mae: 19.092114, mean_q: -27.240495, mean_eps: 0.583205\n",
            " 42465/100000: episode: 126, duration: 2.345s, episode steps: 250, steps per second: 107, episode reward: -249.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.984 [0.000, 2.000],  loss: 1.500329, mae: 19.091667, mean_q: -27.240935, mean_eps: 0.580839\n",
            " 42642/100000: episode: 127, duration: 1.673s, episode steps: 177, steps per second: 106, episode reward: -176.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.983 [0.000, 2.000],  loss: 1.429584, mae: 19.248514, mean_q: -27.565236, mean_eps: 0.578725\n",
            " 42792/100000: episode: 128, duration: 1.468s, episode steps: 150, steps per second: 102, episode reward: -149.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.080 [0.000, 2.000],  loss: 1.408915, mae: 19.280375, mean_q: -27.667625, mean_eps: 0.577107\n",
            " 42998/100000: episode: 129, duration: 1.897s, episode steps: 206, steps per second: 109, episode reward: -205.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.175 [0.000, 2.000],  loss: 1.493840, mae: 19.236062, mean_q: -27.527615, mean_eps: 0.575344\n",
            " 43191/100000: episode: 130, duration: 2.019s, episode steps: 193, steps per second:  96, episode reward: -192.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.285 [0.000, 2.000],  loss: 1.451781, mae: 19.241109, mean_q: -27.490048, mean_eps: 0.573369\n",
            " 43378/100000: episode: 131, duration: 2.505s, episode steps: 187, steps per second:  75, episode reward: -186.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.080 [0.000, 2.000],  loss: 1.622856, mae: 19.205600, mean_q: -27.368497, mean_eps: 0.571488\n",
            " 43563/100000: episode: 132, duration: 1.784s, episode steps: 185, steps per second: 104, episode reward: -184.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.200 [0.000, 2.000],  loss: 1.501307, mae: 19.071497, mean_q: -27.114322, mean_eps: 0.569647\n",
            " 43765/100000: episode: 133, duration: 1.879s, episode steps: 202, steps per second: 107, episode reward: -201.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.069 [0.000, 2.000],  loss: 1.463489, mae: 18.993195, mean_q: -27.119250, mean_eps: 0.567731\n",
            " 43944/100000: episode: 134, duration: 1.700s, episode steps: 179, steps per second: 105, episode reward: -178.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.050 [0.000, 2.000],  loss: 1.610869, mae: 18.987248, mean_q: -27.106793, mean_eps: 0.565845\n",
            " 44157/100000: episode: 135, duration: 2.094s, episode steps: 213, steps per second: 102, episode reward: -212.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.089 [0.000, 2.000],  loss: 1.628976, mae: 18.924981, mean_q: -27.005237, mean_eps: 0.563905\n",
            " 44393/100000: episode: 136, duration: 2.459s, episode steps: 236, steps per second:  96, episode reward: -235.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.195 [0.000, 2.000],  loss: 1.536354, mae: 18.960840, mean_q: -27.126634, mean_eps: 0.561682\n",
            " 44661/100000: episode: 137, duration: 3.770s, episode steps: 268, steps per second:  71, episode reward: -267.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.209 [0.000, 2.000],  loss: 1.453587, mae: 18.706610, mean_q: -26.629814, mean_eps: 0.559188\n",
            " 44799/100000: episode: 138, duration: 1.432s, episode steps: 138, steps per second:  96, episode reward: -137.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.949 [0.000, 2.000],  loss: 1.302126, mae: 18.653457, mean_q: -26.541371, mean_eps: 0.557178\n",
            " 45045/100000: episode: 139, duration: 2.541s, episode steps: 246, steps per second:  97, episode reward: -245.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.073 [0.000, 2.000],  loss: 1.480767, mae: 18.633632, mean_q: -26.410930, mean_eps: 0.555277\n",
            " 45259/100000: episode: 140, duration: 2.267s, episode steps: 214, steps per second:  94, episode reward: -213.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.019 [0.000, 2.000],  loss: 1.671146, mae: 18.899417, mean_q: -26.851040, mean_eps: 0.553000\n",
            " 45444/100000: episode: 141, duration: 1.817s, episode steps: 185, steps per second: 102, episode reward: -184.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.968 [0.000, 2.000],  loss: 1.764114, mae: 18.881152, mean_q: -26.841967, mean_eps: 0.551025\n",
            " 45617/100000: episode: 142, duration: 1.705s, episode steps: 173, steps per second: 101, episode reward: -172.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.092 [0.000, 2.000],  loss: 1.725806, mae: 18.756340, mean_q: -26.636447, mean_eps: 0.549253\n",
            " 45772/100000: episode: 143, duration: 1.988s, episode steps: 155, steps per second:  78, episode reward: -154.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.916 [0.000, 2.000],  loss: 1.673048, mae: 18.742024, mean_q: -26.673611, mean_eps: 0.547629\n",
            " 45951/100000: episode: 144, duration: 2.247s, episode steps: 179, steps per second:  80, episode reward: -178.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.955 [0.000, 2.000],  loss: 1.696672, mae: 18.799196, mean_q: -26.764764, mean_eps: 0.545976\n",
            " 46094/100000: episode: 145, duration: 1.410s, episode steps: 143, steps per second: 101, episode reward: -142.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.238 [0.000, 2.000],  loss: 1.719654, mae: 18.811252, mean_q: -26.790756, mean_eps: 0.544382\n",
            " 46310/100000: episode: 146, duration: 2.080s, episode steps: 216, steps per second: 104, episode reward: -215.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.111 [0.000, 2.000],  loss: 1.662973, mae: 18.925805, mean_q: -26.970088, mean_eps: 0.542605\n",
            " 46516/100000: episode: 147, duration: 1.991s, episode steps: 206, steps per second: 103, episode reward: -205.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.922 [0.000, 2.000],  loss: 1.656433, mae: 18.964625, mean_q: -27.022418, mean_eps: 0.540516\n",
            " 46653/100000: episode: 148, duration: 1.297s, episode steps: 137, steps per second: 106, episode reward: -136.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.891 [0.000, 2.000],  loss: 1.848740, mae: 19.128705, mean_q: -27.258759, mean_eps: 0.538818\n",
            " 46989/100000: episode: 149, duration: 3.390s, episode steps: 336, steps per second:  99, episode reward: -335.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.827 [0.000, 2.000],  loss: 1.465136, mae: 19.163428, mean_q: -27.442798, mean_eps: 0.536477\n",
            " 47198/100000: episode: 150, duration: 2.879s, episode steps: 209, steps per second:  73, episode reward: -208.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.900 [0.000, 2.000],  loss: 1.490909, mae: 19.157985, mean_q: -27.457080, mean_eps: 0.533779\n",
            " 47456/100000: episode: 151, duration: 2.345s, episode steps: 258, steps per second: 110, episode reward: -257.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.895 [0.000, 2.000],  loss: 1.562557, mae: 19.158916, mean_q: -27.501818, mean_eps: 0.531468\n",
            " 47672/100000: episode: 152, duration: 2.007s, episode steps: 216, steps per second: 108, episode reward: -215.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.819 [0.000, 2.000],  loss: 1.587928, mae: 19.062563, mean_q: -27.381452, mean_eps: 0.529121\n",
            " 48086/100000: episode: 153, duration: 3.858s, episode steps: 414, steps per second: 107, episode reward: -413.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.234 [0.000, 2.000],  loss: 1.580927, mae: 19.023001, mean_q: -27.266381, mean_eps: 0.526003\n",
            " 48258/100000: episode: 154, duration: 1.611s, episode steps: 172, steps per second: 107, episode reward: -171.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.814 [0.000, 2.000],  loss: 1.566925, mae: 19.164450, mean_q: -27.597991, mean_eps: 0.523102\n",
            " 48399/100000: episode: 155, duration: 1.872s, episode steps: 141, steps per second:  75, episode reward: -140.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.943 [0.000, 2.000],  loss: 1.639168, mae: 19.164868, mean_q: -27.543082, mean_eps: 0.521553\n",
            " 48548/100000: episode: 156, duration: 1.982s, episode steps: 149, steps per second:  75, episode reward: -148.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.987 [0.000, 2.000],  loss: 1.579954, mae: 19.149653, mean_q: -27.581915, mean_eps: 0.520117\n",
            " 48769/100000: episode: 157, duration: 2.154s, episode steps: 221, steps per second: 103, episode reward: -220.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.145 [0.000, 2.000],  loss: 1.439850, mae: 19.021233, mean_q: -27.410387, mean_eps: 0.518286\n",
            " 48903/100000: episode: 158, duration: 1.304s, episode steps: 134, steps per second: 103, episode reward: -133.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.843 [0.000, 2.000],  loss: 1.535188, mae: 18.899593, mean_q: -27.220663, mean_eps: 0.516529\n",
            " 49084/100000: episode: 159, duration: 1.786s, episode steps: 181, steps per second: 101, episode reward: -180.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.989 [0.000, 2.000],  loss: 1.562854, mae: 19.025705, mean_q: -27.426727, mean_eps: 0.514969\n",
            " 49240/100000: episode: 160, duration: 1.548s, episode steps: 156, steps per second: 101, episode reward: -155.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.026 [0.000, 2.000],  loss: 1.406935, mae: 19.108938, mean_q: -27.581694, mean_eps: 0.513301\n",
            " 49414/100000: episode: 161, duration: 1.723s, episode steps: 174, steps per second: 101, episode reward: -173.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.914 [0.000, 2.000],  loss: 1.546238, mae: 19.151592, mean_q: -27.613620, mean_eps: 0.511668\n",
            " 49576/100000: episode: 162, duration: 1.784s, episode steps: 162, steps per second:  91, episode reward: -161.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.062 [0.000, 2.000],  loss: 1.343477, mae: 19.138969, mean_q: -27.591268, mean_eps: 0.510004\n",
            " 49752/100000: episode: 163, duration: 2.402s, episode steps: 176, steps per second:  73, episode reward: -175.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.267 [0.000, 2.000],  loss: 1.601877, mae: 19.156208, mean_q: -27.606733, mean_eps: 0.508331\n",
            " 49915/100000: episode: 164, duration: 1.813s, episode steps: 163, steps per second:  90, episode reward: -162.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.055 [0.000, 2.000],  loss: 1.593976, mae: 19.115949, mean_q: -27.604219, mean_eps: 0.506653\n",
            " 50071/100000: episode: 165, duration: 1.521s, episode steps: 156, steps per second: 103, episode reward: -155.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.212 [0.000, 2.000],  loss: 1.533810, mae: 19.229745, mean_q: -27.802074, mean_eps: 0.505074\n",
            " 50240/100000: episode: 166, duration: 1.640s, episode steps: 169, steps per second: 103, episode reward: -168.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.136 [0.000, 2.000],  loss: 1.290793, mae: 19.231361, mean_q: -27.866658, mean_eps: 0.503466\n",
            " 50444/100000: episode: 167, duration: 1.974s, episode steps: 204, steps per second: 103, episode reward: -203.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.034 [0.000, 2.000],  loss: 1.605568, mae: 19.271889, mean_q: -27.856036, mean_eps: 0.501619\n",
            " 50634/100000: episode: 168, duration: 1.872s, episode steps: 190, steps per second: 102, episode reward: -189.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.158 [0.000, 2.000],  loss: 1.463290, mae: 19.254384, mean_q: -27.932379, mean_eps: 0.499669\n",
            " 50804/100000: episode: 169, duration: 1.641s, episode steps: 170, steps per second: 104, episode reward: -169.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.247 [0.000, 2.000],  loss: 1.507197, mae: 19.334725, mean_q: -28.024555, mean_eps: 0.497887\n",
            " 51003/100000: episode: 170, duration: 2.587s, episode steps: 199, steps per second:  77, episode reward: -198.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.136 [0.000, 2.000],  loss: 1.442844, mae: 19.371350, mean_q: -28.101556, mean_eps: 0.496060\n",
            " 51407/100000: episode: 171, duration: 4.302s, episode steps: 404, steps per second:  94, episode reward: -403.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.109 [0.000, 2.000],  loss: 1.575476, mae: 19.309470, mean_q: -27.981649, mean_eps: 0.493075\n",
            " 51533/100000: episode: 172, duration: 1.180s, episode steps: 126, steps per second: 107, episode reward: -125.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.079 [0.000, 2.000],  loss: 1.619920, mae: 19.222758, mean_q: -27.788918, mean_eps: 0.490452\n",
            " 51657/100000: episode: 173, duration: 1.169s, episode steps: 124, steps per second: 106, episode reward: -123.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.048 [0.000, 2.000],  loss: 1.437325, mae: 19.121777, mean_q: -27.647027, mean_eps: 0.489214\n",
            " 51803/100000: episode: 174, duration: 1.396s, episode steps: 146, steps per second: 105, episode reward: -145.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 1.578590, mae: 19.111858, mean_q: -27.624437, mean_eps: 0.487878\n",
            " 51974/100000: episode: 175, duration: 1.642s, episode steps: 171, steps per second: 104, episode reward: -170.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.029 [0.000, 2.000],  loss: 1.582948, mae: 19.214415, mean_q: -27.794008, mean_eps: 0.486309\n",
            " 52091/100000: episode: 176, duration: 1.174s, episode steps: 117, steps per second: 100, episode reward: -116.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.077 [0.000, 2.000],  loss: 1.343145, mae: 19.055134, mean_q: -27.594540, mean_eps: 0.484883\n",
            " 52280/100000: episode: 177, duration: 2.344s, episode steps: 189, steps per second:  81, episode reward: -188.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.884 [0.000, 2.000],  loss: 1.742712, mae: 18.945658, mean_q: -27.398454, mean_eps: 0.483369\n",
            " 52448/100000: episode: 178, duration: 2.032s, episode steps: 168, steps per second:  83, episode reward: -167.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.923 [0.000, 2.000],  loss: 1.545234, mae: 18.719062, mean_q: -27.101049, mean_eps: 0.481601\n",
            " 52626/100000: episode: 179, duration: 1.683s, episode steps: 178, steps per second: 106, episode reward: -177.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.034 [0.000, 2.000],  loss: 1.377668, mae: 18.887556, mean_q: -27.387817, mean_eps: 0.479889\n",
            " 52774/100000: episode: 180, duration: 1.417s, episode steps: 148, steps per second: 104, episode reward: -147.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.973 [0.000, 2.000],  loss: 1.428126, mae: 18.906383, mean_q: -27.410962, mean_eps: 0.478275\n",
            " 52927/100000: episode: 181, duration: 1.435s, episode steps: 153, steps per second: 107, episode reward: -152.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.993 [0.000, 2.000],  loss: 1.359721, mae: 18.775336, mean_q: -27.221567, mean_eps: 0.476785\n",
            " 53058/100000: episode: 182, duration: 1.184s, episode steps: 131, steps per second: 111, episode reward: -130.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.969 [0.000, 2.000],  loss: 1.451148, mae: 18.901124, mean_q: -27.393520, mean_eps: 0.475379\n",
            " 53209/100000: episode: 183, duration: 1.363s, episode steps: 151, steps per second: 111, episode reward: -150.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.020 [0.000, 2.000],  loss: 1.321962, mae: 18.759647, mean_q: -27.222084, mean_eps: 0.473983\n",
            " 53353/100000: episode: 184, duration: 1.340s, episode steps: 144, steps per second: 107, episode reward: -143.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.965 [0.000, 2.000],  loss: 1.433537, mae: 18.843072, mean_q: -27.331834, mean_eps: 0.472523\n",
            " 53521/100000: episode: 185, duration: 1.797s, episode steps: 168, steps per second:  93, episode reward: -167.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.006 [0.000, 2.000],  loss: 1.295025, mae: 18.783835, mean_q: -27.244344, mean_eps: 0.470979\n",
            " 53678/100000: episode: 186, duration: 1.970s, episode steps: 157, steps per second:  80, episode reward: -156.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.885 [0.000, 2.000],  loss: 1.387734, mae: 18.863361, mean_q: -27.330555, mean_eps: 0.469370\n",
            " 53829/100000: episode: 187, duration: 1.688s, episode steps: 151, steps per second:  89, episode reward: -150.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.033 [0.000, 2.000],  loss: 1.437937, mae: 18.920219, mean_q: -27.447968, mean_eps: 0.467845\n",
            " 53967/100000: episode: 188, duration: 1.238s, episode steps: 138, steps per second: 111, episode reward: -137.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.833 [0.000, 2.000],  loss: 1.429927, mae: 18.795008, mean_q: -27.310104, mean_eps: 0.466415\n",
            " 54116/100000: episode: 189, duration: 1.341s, episode steps: 149, steps per second: 111, episode reward: -148.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.094 [0.000, 2.000],  loss: 1.563590, mae: 18.865716, mean_q: -27.366090, mean_eps: 0.464994\n",
            " 54236/100000: episode: 190, duration: 1.152s, episode steps: 120, steps per second: 104, episode reward: -119.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.017 [0.000, 2.000],  loss: 1.537896, mae: 18.968454, mean_q: -27.516090, mean_eps: 0.463663\n",
            " 54387/100000: episode: 191, duration: 1.394s, episode steps: 151, steps per second: 108, episode reward: -150.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.126 [0.000, 2.000],  loss: 1.355393, mae: 19.027241, mean_q: -27.618159, mean_eps: 0.462321\n",
            " 54560/100000: episode: 192, duration: 1.638s, episode steps: 173, steps per second: 106, episode reward: -172.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.092 [0.000, 2.000],  loss: 1.315483, mae: 18.935543, mean_q: -27.463597, mean_eps: 0.460717\n",
            " 54712/100000: episode: 193, duration: 1.431s, episode steps: 152, steps per second: 106, episode reward: -151.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.928 [0.000, 2.000],  loss: 1.294182, mae: 18.732566, mean_q: -27.155093, mean_eps: 0.459109\n",
            " 54853/100000: episode: 194, duration: 1.619s, episode steps: 141, steps per second:  87, episode reward: -140.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.121 [0.000, 2.000],  loss: 1.215095, mae: 18.726463, mean_q: -27.175422, mean_eps: 0.457658\n",
            " 54999/100000: episode: 195, duration: 1.944s, episode steps: 146, steps per second:  75, episode reward: -145.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.034 [0.000, 2.000],  loss: 1.405127, mae: 18.684322, mean_q: -27.120806, mean_eps: 0.456238\n",
            " 55158/100000: episode: 196, duration: 1.792s, episode steps: 159, steps per second:  89, episode reward: -158.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.069 [0.000, 2.000],  loss: 1.278698, mae: 18.894873, mean_q: -27.429148, mean_eps: 0.454728\n",
            " 55356/100000: episode: 197, duration: 1.928s, episode steps: 198, steps per second: 103, episode reward: -197.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.197 [0.000, 2.000],  loss: 1.339411, mae: 18.971156, mean_q: -27.543439, mean_eps: 0.452961\n",
            " 55513/100000: episode: 198, duration: 1.503s, episode steps: 157, steps per second: 104, episode reward: -156.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.064 [0.000, 2.000],  loss: 1.279190, mae: 18.969893, mean_q: -27.538852, mean_eps: 0.451203\n",
            " 55656/100000: episode: 199, duration: 1.410s, episode steps: 143, steps per second: 101, episode reward: -142.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.147 [0.000, 2.000],  loss: 1.405613, mae: 19.106373, mean_q: -27.722092, mean_eps: 0.449718\n",
            " 55776/100000: episode: 200, duration: 1.250s, episode steps: 120, steps per second:  96, episode reward: -119.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.108 [0.000, 2.000],  loss: 1.337473, mae: 19.086296, mean_q: -27.669639, mean_eps: 0.448417\n",
            " 55880/100000: episode: 201, duration: 0.997s, episode steps: 104, steps per second: 104, episode reward: -103.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.058 [0.000, 2.000],  loss: 1.325177, mae: 18.924526, mean_q: -27.448325, mean_eps: 0.447308\n",
            " 56018/100000: episode: 202, duration: 1.287s, episode steps: 138, steps per second: 107, episode reward: -137.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.087 [0.000, 2.000],  loss: 1.374625, mae: 18.990113, mean_q: -27.512029, mean_eps: 0.446110\n",
            " 56161/100000: episode: 203, duration: 1.543s, episode steps: 143, steps per second:  93, episode reward: -142.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.119 [0.000, 2.000],  loss: 1.443414, mae: 19.063744, mean_q: -27.619990, mean_eps: 0.444719\n",
            " 56271/100000: episode: 204, duration: 1.460s, episode steps: 110, steps per second:  75, episode reward: -109.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.136 [0.000, 2.000],  loss: 1.308741, mae: 18.998590, mean_q: -27.514534, mean_eps: 0.443467\n",
            " 56418/100000: episode: 205, duration: 1.797s, episode steps: 147, steps per second:  82, episode reward: -146.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.238 [0.000, 2.000],  loss: 1.321213, mae: 19.061049, mean_q: -27.615343, mean_eps: 0.442194\n",
            " 56536/100000: episode: 206, duration: 1.163s, episode steps: 118, steps per second: 102, episode reward: -117.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.136 [0.000, 2.000],  loss: 1.177761, mae: 19.111113, mean_q: -27.745444, mean_eps: 0.440883\n",
            " 56652/100000: episode: 207, duration: 1.107s, episode steps: 116, steps per second: 105, episode reward: -115.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.052 [0.000, 2.000],  loss: 1.292188, mae: 18.904397, mean_q: -27.459232, mean_eps: 0.439724\n",
            " 56816/100000: episode: 208, duration: 1.638s, episode steps: 164, steps per second: 100, episode reward: -163.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.030 [0.000, 2.000],  loss: 1.243746, mae: 18.967993, mean_q: -27.521669, mean_eps: 0.438338\n",
            " 56940/100000: episode: 209, duration: 1.196s, episode steps: 124, steps per second: 104, episode reward: -123.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.121 [0.000, 2.000],  loss: 1.410331, mae: 18.781695, mean_q: -27.201296, mean_eps: 0.436913\n",
            " 57056/100000: episode: 210, duration: 1.125s, episode steps: 116, steps per second: 103, episode reward: -115.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.991 [0.000, 2.000],  loss: 1.296266, mae: 18.987616, mean_q: -27.498740, mean_eps: 0.435725\n",
            " 57273/100000: episode: 211, duration: 2.110s, episode steps: 217, steps per second: 103, episode reward: -216.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.083 [0.000, 2.000],  loss: 1.319378, mae: 18.989160, mean_q: -27.568946, mean_eps: 0.434076\n",
            " 57455/100000: episode: 212, duration: 1.898s, episode steps: 182, steps per second:  96, episode reward: -181.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.066 [0.000, 2.000],  loss: 1.174663, mae: 18.988238, mean_q: -27.588041, mean_eps: 0.432101\n",
            " 57554/100000: episode: 213, duration: 1.292s, episode steps:  99, steps per second:  77, episode reward: -98.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.990 [0.000, 2.000],  loss: 1.242029, mae: 19.096538, mean_q: -27.759909, mean_eps: 0.430710\n",
            " 57702/100000: episode: 214, duration: 1.799s, episode steps: 148, steps per second:  82, episode reward: -147.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.061 [0.000, 2.000],  loss: 1.135806, mae: 19.169836, mean_q: -27.876058, mean_eps: 0.429488\n",
            " 57884/100000: episode: 215, duration: 1.700s, episode steps: 182, steps per second: 107, episode reward: -181.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.066 [0.000, 2.000],  loss: 1.445344, mae: 19.224450, mean_q: -27.941952, mean_eps: 0.427854\n",
            " 58024/100000: episode: 216, duration: 1.353s, episode steps: 140, steps per second: 103, episode reward: -139.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.943 [0.000, 2.000],  loss: 1.356538, mae: 19.150450, mean_q: -27.852163, mean_eps: 0.426260\n",
            " 58217/100000: episode: 217, duration: 1.838s, episode steps: 193, steps per second: 105, episode reward: -192.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.021 [0.000, 2.000],  loss: 1.317416, mae: 18.842489, mean_q: -27.338312, mean_eps: 0.424612\n",
            " 58363/100000: episode: 218, duration: 1.440s, episode steps: 146, steps per second: 101, episode reward: -145.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.021 [0.000, 2.000],  loss: 1.406035, mae: 18.803133, mean_q: -27.269713, mean_eps: 0.422934\n",
            " 58569/100000: episode: 219, duration: 1.989s, episode steps: 206, steps per second: 104, episode reward: -205.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.942 [0.000, 2.000],  loss: 1.116792, mae: 18.833540, mean_q: -27.370071, mean_eps: 0.421192\n",
            " 58713/100000: episode: 220, duration: 1.412s, episode steps: 144, steps per second: 102, episode reward: -143.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.757 [0.000, 2.000],  loss: 1.258900, mae: 18.827159, mean_q: -27.319362, mean_eps: 0.419459\n",
            " 58817/100000: episode: 221, duration: 1.374s, episode steps: 104, steps per second:  76, episode reward: -103.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.942 [0.000, 2.000],  loss: 1.346486, mae: 18.755017, mean_q: -27.208964, mean_eps: 0.418231\n",
            " 58926/100000: episode: 222, duration: 1.407s, episode steps: 109, steps per second:  77, episode reward: -108.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.991 [0.000, 2.000],  loss: 1.205699, mae: 18.870337, mean_q: -27.401578, mean_eps: 0.417177\n",
            " 59158/100000: episode: 223, duration: 2.349s, episode steps: 232, steps per second:  99, episode reward: -231.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.974 [0.000, 2.000],  loss: 1.432215, mae: 18.857856, mean_q: -27.372636, mean_eps: 0.415489\n",
            " 59353/100000: episode: 224, duration: 1.902s, episode steps: 195, steps per second: 103, episode reward: -194.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.113 [0.000, 2.000],  loss: 1.333909, mae: 18.867969, mean_q: -27.400833, mean_eps: 0.413376\n",
            " 59499/100000: episode: 225, duration: 1.366s, episode steps: 146, steps per second: 107, episode reward: -145.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.027 [0.000, 2.000],  loss: 1.159750, mae: 18.840009, mean_q: -27.402207, mean_eps: 0.411688\n",
            " 59625/100000: episode: 226, duration: 1.234s, episode steps: 126, steps per second: 102, episode reward: -125.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.214 [0.000, 2.000],  loss: 1.344334, mae: 19.098198, mean_q: -27.734981, mean_eps: 0.410341\n",
            " 59761/100000: episode: 227, duration: 1.292s, episode steps: 136, steps per second: 105, episode reward: -135.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.206 [0.000, 2.000],  loss: 1.114569, mae: 19.073582, mean_q: -27.733908, mean_eps: 0.409044\n",
            " 59892/100000: episode: 228, duration: 1.234s, episode steps: 131, steps per second: 106, episode reward: -130.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.137 [0.000, 2.000],  loss: 1.431265, mae: 18.963460, mean_q: -27.514728, mean_eps: 0.407723\n",
            " 60048/100000: episode: 229, duration: 1.641s, episode steps: 156, steps per second:  95, episode reward: -155.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.083 [0.000, 2.000],  loss: 1.288469, mae: 18.924871, mean_q: -27.441162, mean_eps: 0.406302\n",
            " 60211/100000: episode: 230, duration: 2.115s, episode steps: 163, steps per second:  77, episode reward: -162.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.123 [0.000, 2.000],  loss: 1.225994, mae: 18.904027, mean_q: -27.436803, mean_eps: 0.404723\n",
            " 60361/100000: episode: 231, duration: 1.699s, episode steps: 150, steps per second:  88, episode reward: -149.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.147 [0.000, 2.000],  loss: 1.189255, mae: 18.967062, mean_q: -27.533609, mean_eps: 0.403174\n",
            " 60507/100000: episode: 232, duration: 1.389s, episode steps: 146, steps per second: 105, episode reward: -145.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.007 [0.000, 2.000],  loss: 1.120652, mae: 18.878875, mean_q: -27.448339, mean_eps: 0.401708\n",
            " 60660/100000: episode: 233, duration: 1.449s, episode steps: 153, steps per second: 106, episode reward: -152.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.020 [0.000, 2.000],  loss: 1.337878, mae: 18.928298, mean_q: -27.458440, mean_eps: 0.400228\n",
            " 60842/100000: episode: 234, duration: 1.751s, episode steps: 182, steps per second: 104, episode reward: -181.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.088 [0.000, 2.000],  loss: 1.413556, mae: 18.880877, mean_q: -27.366461, mean_eps: 0.398570\n",
            " 60970/100000: episode: 235, duration: 1.208s, episode steps: 128, steps per second: 106, episode reward: -127.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.125 [0.000, 2.000],  loss: 1.471730, mae: 18.757222, mean_q: -27.164626, mean_eps: 0.397036\n",
            " 61122/100000: episode: 236, duration: 1.434s, episode steps: 152, steps per second: 106, episode reward: -151.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.296 [0.000, 2.000],  loss: 1.370152, mae: 18.790283, mean_q: -27.218787, mean_eps: 0.395650\n",
            " 61268/100000: episode: 237, duration: 1.442s, episode steps: 146, steps per second: 101, episode reward: -145.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.041 [0.000, 2.000],  loss: 1.361474, mae: 18.769565, mean_q: -27.201205, mean_eps: 0.394174\n",
            " 61433/100000: episode: 238, duration: 1.941s, episode steps: 165, steps per second:  85, episode reward: -164.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.030 [0.000, 2.000],  loss: 1.385683, mae: 18.851344, mean_q: -27.327989, mean_eps: 0.392635\n",
            " 61607/100000: episode: 239, duration: 2.296s, episode steps: 174, steps per second:  76, episode reward: -173.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.092 [0.000, 2.000],  loss: 1.259857, mae: 18.855263, mean_q: -27.370869, mean_eps: 0.390957\n",
            " 61801/100000: episode: 240, duration: 1.848s, episode steps: 194, steps per second: 105, episode reward: -193.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.067 [0.000, 2.000],  loss: 1.240172, mae: 18.832806, mean_q: -27.354471, mean_eps: 0.389135\n",
            " 61932/100000: episode: 241, duration: 1.263s, episode steps: 131, steps per second: 104, episode reward: -130.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.122 [0.000, 2.000],  loss: 1.457878, mae: 18.779561, mean_q: -27.245276, mean_eps: 0.387527\n",
            " 62044/100000: episode: 242, duration: 1.081s, episode steps: 112, steps per second: 104, episode reward: -111.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.188 [0.000, 2.000],  loss: 1.297269, mae: 18.712775, mean_q: -27.153184, mean_eps: 0.386324\n",
            " 62205/100000: episode: 243, duration: 1.569s, episode steps: 161, steps per second: 103, episode reward: -160.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.031 [0.000, 2.000],  loss: 1.394704, mae: 18.834287, mean_q: -27.323384, mean_eps: 0.384972\n",
            " 62343/100000: episode: 244, duration: 1.342s, episode steps: 138, steps per second: 103, episode reward: -137.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.022 [0.000, 2.000],  loss: 1.353299, mae: 18.833751, mean_q: -27.332606, mean_eps: 0.383492\n",
            " 62459/100000: episode: 245, duration: 1.103s, episode steps: 116, steps per second: 105, episode reward: -115.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.078 [0.000, 2.000],  loss: 1.236462, mae: 18.725482, mean_q: -27.171112, mean_eps: 0.382235\n",
            " 62563/100000: episode: 246, duration: 1.004s, episode steps: 104, steps per second: 104, episode reward: -103.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.067 [0.000, 2.000],  loss: 1.342320, mae: 18.740325, mean_q: -27.162304, mean_eps: 0.381146\n",
            " 62694/100000: episode: 247, duration: 1.486s, episode steps: 131, steps per second:  88, episode reward: -130.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.916 [0.000, 2.000],  loss: 1.334321, mae: 18.668245, mean_q: -27.046555, mean_eps: 0.379983\n",
            " 62785/100000: episode: 248, duration: 1.228s, episode steps:  91, steps per second:  74, episode reward: -90.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.956 [0.000, 2.000],  loss: 1.326649, mae: 18.696797, mean_q: -27.066330, mean_eps: 0.378884\n",
            " 62886/100000: episode: 249, duration: 1.374s, episode steps: 101, steps per second:  73, episode reward: -100.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.782 [0.000, 2.000],  loss: 1.368826, mae: 18.692259, mean_q: -27.090127, mean_eps: 0.377933\n",
            " 63010/100000: episode: 250, duration: 1.146s, episode steps: 124, steps per second: 108, episode reward: -123.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.121 [0.000, 2.000],  loss: 1.304177, mae: 18.705360, mean_q: -27.104484, mean_eps: 0.376820\n",
            " 63122/100000: episode: 251, duration: 1.078s, episode steps: 112, steps per second: 104, episode reward: -111.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.009 [0.000, 2.000],  loss: 1.434880, mae: 18.715472, mean_q: -27.146063, mean_eps: 0.375652\n",
            " 63275/100000: episode: 252, duration: 1.423s, episode steps: 153, steps per second: 108, episode reward: -152.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.850 [0.000, 2.000],  loss: 1.157242, mae: 18.617455, mean_q: -27.015102, mean_eps: 0.374340\n",
            " 63374/100000: episode: 253, duration: 0.944s, episode steps:  99, steps per second: 105, episode reward: -98.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.980 [0.000, 2.000],  loss: 1.491748, mae: 18.631098, mean_q: -27.012095, mean_eps: 0.373092\n",
            " 63530/100000: episode: 254, duration: 1.544s, episode steps: 156, steps per second: 101, episode reward: -155.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.891 [0.000, 2.000],  loss: 1.283436, mae: 18.781527, mean_q: -27.252092, mean_eps: 0.371830\n",
            " 63672/100000: episode: 255, duration: 1.397s, episode steps: 142, steps per second: 102, episode reward: -141.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.789 [0.000, 2.000],  loss: 1.305323, mae: 18.699660, mean_q: -27.123565, mean_eps: 0.370355\n",
            " 63803/100000: episode: 256, duration: 1.299s, episode steps: 131, steps per second: 101, episode reward: -130.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.931 [0.000, 2.000],  loss: 1.253391, mae: 18.677623, mean_q: -27.122727, mean_eps: 0.369004\n",
            " 63967/100000: episode: 257, duration: 1.710s, episode steps: 164, steps per second:  96, episode reward: -163.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.738 [0.000, 2.000],  loss: 1.294037, mae: 18.692498, mean_q: -27.115891, mean_eps: 0.367543\n",
            " 64092/100000: episode: 258, duration: 1.613s, episode steps: 125, steps per second:  77, episode reward: -124.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.936 [0.000, 2.000],  loss: 1.465343, mae: 18.874596, mean_q: -27.342604, mean_eps: 0.366113\n",
            " 64221/100000: episode: 259, duration: 1.619s, episode steps: 129, steps per second:  80, episode reward: -128.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.977 [0.000, 2.000],  loss: 1.409401, mae: 18.807764, mean_q: -27.289122, mean_eps: 0.364856\n",
            " 64352/100000: episode: 260, duration: 1.217s, episode steps: 131, steps per second: 108, episode reward: -130.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.962 [0.000, 2.000],  loss: 1.298099, mae: 18.885989, mean_q: -27.447466, mean_eps: 0.363569\n",
            " 64528/100000: episode: 261, duration: 1.705s, episode steps: 176, steps per second: 103, episode reward: -175.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.017 [0.000, 2.000],  loss: 1.275416, mae: 18.836288, mean_q: -27.378186, mean_eps: 0.362049\n",
            " 64662/100000: episode: 262, duration: 1.277s, episode steps: 134, steps per second: 105, episode reward: -133.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.933 [0.000, 2.000],  loss: 1.197777, mae: 18.602332, mean_q: -26.995572, mean_eps: 0.360514\n",
            " 64770/100000: episode: 263, duration: 1.033s, episode steps: 108, steps per second: 105, episode reward: -107.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.037 [0.000, 2.000],  loss: 1.505559, mae: 18.631681, mean_q: -26.996077, mean_eps: 0.359317\n",
            " 64859/100000: episode: 264, duration: 0.828s, episode steps:  89, steps per second: 107, episode reward: -88.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.034 [0.000, 2.000],  loss: 1.301163, mae: 18.688391, mean_q: -27.138156, mean_eps: 0.358341\n",
            " 64997/100000: episode: 265, duration: 1.244s, episode steps: 138, steps per second: 111, episode reward: -137.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 1.365816, mae: 18.599647, mean_q: -26.990411, mean_eps: 0.357218\n",
            " 65106/100000: episode: 266, duration: 1.007s, episode steps: 109, steps per second: 108, episode reward: -108.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.018 [0.000, 2.000],  loss: 1.402874, mae: 18.842492, mean_q: -27.363721, mean_eps: 0.355995\n",
            " 65238/100000: episode: 267, duration: 1.238s, episode steps: 132, steps per second: 107, episode reward: -131.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.939 [0.000, 2.000],  loss: 1.313465, mae: 18.733090, mean_q: -27.166846, mean_eps: 0.354802\n",
            " 65351/100000: episode: 268, duration: 1.425s, episode steps: 113, steps per second:  79, episode reward: -112.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.133 [0.000, 2.000],  loss: 1.306828, mae: 18.727665, mean_q: -27.188136, mean_eps: 0.353589\n",
            " 65464/100000: episode: 269, duration: 1.428s, episode steps: 113, steps per second:  79, episode reward: -112.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.938 [0.000, 2.000],  loss: 1.371878, mae: 18.687958, mean_q: -27.090132, mean_eps: 0.352471\n",
            " 65561/100000: episode: 270, duration: 1.218s, episode steps:  97, steps per second:  80, episode reward: -96.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.979 [0.000, 2.000],  loss: 1.376160, mae: 18.672624, mean_q: -27.091387, mean_eps: 0.351431\n",
            " 65679/100000: episode: 271, duration: 1.118s, episode steps: 118, steps per second: 106, episode reward: -117.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.102 [0.000, 2.000],  loss: 1.334682, mae: 18.727091, mean_q: -27.145495, mean_eps: 0.350367\n",
            " 65827/100000: episode: 272, duration: 1.470s, episode steps: 148, steps per second: 101, episode reward: -147.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.068 [0.000, 2.000],  loss: 1.320729, mae: 18.733168, mean_q: -27.150136, mean_eps: 0.349050\n",
            " 65944/100000: episode: 273, duration: 1.103s, episode steps: 117, steps per second: 106, episode reward: -116.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.060 [0.000, 2.000],  loss: 1.449045, mae: 18.629816, mean_q: -27.008862, mean_eps: 0.347738\n",
            " 66048/100000: episode: 274, duration: 1.036s, episode steps: 104, steps per second: 100, episode reward: -103.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.087 [0.000, 2.000],  loss: 1.398364, mae: 18.592338, mean_q: -26.923503, mean_eps: 0.346645\n",
            " 66161/100000: episode: 275, duration: 1.072s, episode steps: 113, steps per second: 105, episode reward: -112.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.133 [0.000, 2.000],  loss: 1.525085, mae: 18.395909, mean_q: -26.609786, mean_eps: 0.345570\n",
            " 66276/100000: episode: 276, duration: 1.153s, episode steps: 115, steps per second: 100, episode reward: -114.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.209 [0.000, 2.000],  loss: 1.422529, mae: 18.531894, mean_q: -26.815346, mean_eps: 0.344442\n",
            " 66384/100000: episode: 277, duration: 1.106s, episode steps: 108, steps per second:  98, episode reward: -107.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.074 [0.000, 2.000],  loss: 1.383200, mae: 18.530721, mean_q: -26.817810, mean_eps: 0.343338\n",
            " 66493/100000: episode: 278, duration: 1.215s, episode steps: 109, steps per second:  90, episode reward: -108.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.083 [0.000, 2.000],  loss: 1.272158, mae: 18.580272, mean_q: -26.909093, mean_eps: 0.342264\n",
            " 66590/100000: episode: 279, duration: 1.325s, episode steps:  97, steps per second:  73, episode reward: -96.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.124 [0.000, 2.000],  loss: 1.476892, mae: 18.334544, mean_q: -26.497452, mean_eps: 0.341244\n",
            " 66707/100000: episode: 280, duration: 1.784s, episode steps: 117, steps per second:  66, episode reward: -116.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.085 [0.000, 2.000],  loss: 1.292869, mae: 18.486534, mean_q: -26.763425, mean_eps: 0.340185\n",
            " 66836/100000: episode: 281, duration: 1.555s, episode steps: 129, steps per second:  83, episode reward: -128.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.287 [0.000, 2.000],  loss: 1.358778, mae: 18.415769, mean_q: -26.595298, mean_eps: 0.338967\n",
            " 67005/100000: episode: 282, duration: 1.669s, episode steps: 169, steps per second: 101, episode reward: -168.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.077 [0.000, 2.000],  loss: 1.419883, mae: 18.404346, mean_q: -26.623304, mean_eps: 0.337492\n",
            " 67125/100000: episode: 283, duration: 1.201s, episode steps: 120, steps per second: 100, episode reward: -119.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.933 [0.000, 2.000],  loss: 1.370709, mae: 18.220081, mean_q: -26.302992, mean_eps: 0.336061\n",
            " 67233/100000: episode: 284, duration: 1.100s, episode steps: 108, steps per second:  98, episode reward: -107.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.926 [0.000, 2.000],  loss: 1.255368, mae: 18.284113, mean_q: -26.454797, mean_eps: 0.334933\n",
            " 67335/100000: episode: 285, duration: 1.011s, episode steps: 102, steps per second: 101, episode reward: -101.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.147 [0.000, 2.000],  loss: 1.532352, mae: 18.209360, mean_q: -26.277679, mean_eps: 0.333893\n",
            " 67442/100000: episode: 286, duration: 1.030s, episode steps: 107, steps per second: 104, episode reward: -106.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.879 [0.000, 2.000],  loss: 1.294975, mae: 18.224307, mean_q: -26.358367, mean_eps: 0.332859\n",
            " 67580/100000: episode: 287, duration: 1.301s, episode steps: 138, steps per second: 106, episode reward: -137.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.036 [0.000, 2.000],  loss: 1.433629, mae: 18.342498, mean_q: -26.513811, mean_eps: 0.331646\n",
            " 67726/100000: episode: 288, duration: 1.419s, episode steps: 146, steps per second: 103, episode reward: -145.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.075 [0.000, 2.000],  loss: 1.411345, mae: 18.427994, mean_q: -26.657498, mean_eps: 0.330240\n",
            " 67829/100000: episode: 289, duration: 1.170s, episode steps: 103, steps per second:  88, episode reward: -102.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 1.541874, mae: 18.395827, mean_q: -26.602933, mean_eps: 0.329008\n",
            " 67943/100000: episode: 290, duration: 1.590s, episode steps: 114, steps per second:  72, episode reward: -113.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.053 [0.000, 2.000],  loss: 1.253675, mae: 18.370181, mean_q: -26.617333, mean_eps: 0.327934\n",
            " 68051/100000: episode: 291, duration: 1.472s, episode steps: 108, steps per second:  73, episode reward: -107.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.972 [0.000, 2.000],  loss: 1.233289, mae: 18.309272, mean_q: -26.604982, mean_eps: 0.326835\n",
            " 68172/100000: episode: 292, duration: 1.173s, episode steps: 121, steps per second: 103, episode reward: -120.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.174 [0.000, 2.000],  loss: 1.256295, mae: 18.344779, mean_q: -26.616129, mean_eps: 0.325701\n",
            " 68290/100000: episode: 293, duration: 1.160s, episode steps: 118, steps per second: 102, episode reward: -117.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.822 [0.000, 2.000],  loss: 1.282410, mae: 18.201451, mean_q: -26.395960, mean_eps: 0.324518\n",
            " 68455/100000: episode: 294, duration: 1.597s, episode steps: 165, steps per second: 103, episode reward: -164.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.891 [0.000, 2.000],  loss: 1.305710, mae: 18.375088, mean_q: -26.648489, mean_eps: 0.323117\n",
            " 68555/100000: episode: 295, duration: 0.994s, episode steps: 100, steps per second: 101, episode reward: -99.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.080 [0.000, 2.000],  loss: 1.260166, mae: 18.327491, mean_q: -26.579794, mean_eps: 0.321805\n",
            " 68650/100000: episode: 296, duration: 0.946s, episode steps:  95, steps per second: 100, episode reward: -94.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.874 [0.000, 2.000],  loss: 1.285871, mae: 18.295667, mean_q: -26.535499, mean_eps: 0.320840\n",
            " 68790/100000: episode: 297, duration: 1.390s, episode steps: 140, steps per second: 101, episode reward: -139.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.957 [0.000, 2.000],  loss: 1.258934, mae: 18.280674, mean_q: -26.508111, mean_eps: 0.319677\n",
            " 68879/100000: episode: 298, duration: 0.841s, episode steps:  89, steps per second: 106, episode reward: -88.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.011 [0.000, 2.000],  loss: 1.267267, mae: 18.147972, mean_q: -26.328839, mean_eps: 0.318543\n",
            " 68999/100000: episode: 299, duration: 1.139s, episode steps: 120, steps per second: 105, episode reward: -119.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.942 [0.000, 2.000],  loss: 1.317642, mae: 18.241355, mean_q: -26.501946, mean_eps: 0.317509\n",
            " 69153/100000: episode: 300, duration: 1.830s, episode steps: 154, steps per second:  84, episode reward: -153.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.123 [0.000, 2.000],  loss: 1.376131, mae: 18.307662, mean_q: -26.531615, mean_eps: 0.316153\n",
            " 69278/100000: episode: 301, duration: 1.657s, episode steps: 125, steps per second:  75, episode reward: -124.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.008 [0.000, 2.000],  loss: 1.193115, mae: 18.314054, mean_q: -26.567026, mean_eps: 0.314771\n",
            " 69455/100000: episode: 302, duration: 2.007s, episode steps: 177, steps per second:  88, episode reward: -176.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.203 [0.000, 2.000],  loss: 1.356462, mae: 18.316952, mean_q: -26.550681, mean_eps: 0.313277\n",
            " 69552/100000: episode: 303, duration: 1.035s, episode steps:  97, steps per second:  94, episode reward: -96.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.010 [0.000, 2.000],  loss: 1.333620, mae: 18.512236, mean_q: -26.815628, mean_eps: 0.311920\n",
            " 69669/100000: episode: 304, duration: 1.178s, episode steps: 117, steps per second:  99, episode reward: -116.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.034 [0.000, 2.000],  loss: 1.207038, mae: 18.613373, mean_q: -26.978371, mean_eps: 0.310861\n",
            " 69784/100000: episode: 305, duration: 1.185s, episode steps: 115, steps per second:  97, episode reward: -114.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.965 [0.000, 2.000],  loss: 1.294709, mae: 18.568825, mean_q: -26.911961, mean_eps: 0.309713\n",
            " 69892/100000: episode: 306, duration: 1.085s, episode steps: 108, steps per second: 100, episode reward: -107.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.981 [0.000, 2.000],  loss: 1.222696, mae: 18.611148, mean_q: -26.981881, mean_eps: 0.308609\n",
            " 70015/100000: episode: 307, duration: 1.236s, episode steps: 123, steps per second: 100, episode reward: -122.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.927 [0.000, 2.000],  loss: 1.331867, mae: 18.625410, mean_q: -26.991291, mean_eps: 0.307465\n",
            " 70150/100000: episode: 308, duration: 1.320s, episode steps: 135, steps per second: 102, episode reward: -134.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.074 [0.000, 2.000],  loss: 1.303860, mae: 18.748208, mean_q: -27.149442, mean_eps: 0.306188\n",
            " 70257/100000: episode: 309, duration: 1.021s, episode steps: 107, steps per second: 105, episode reward: -106.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.047 [0.000, 2.000],  loss: 1.379924, mae: 18.793543, mean_q: -27.201375, mean_eps: 0.304990\n",
            " 70367/100000: episode: 310, duration: 1.206s, episode steps: 110, steps per second:  91, episode reward: -109.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.800 [0.000, 2.000],  loss: 1.275752, mae: 18.756040, mean_q: -27.175401, mean_eps: 0.303916\n",
            " 70477/100000: episode: 311, duration: 1.457s, episode steps: 110, steps per second:  76, episode reward: -109.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.009 [0.000, 2.000],  loss: 1.313597, mae: 18.794930, mean_q: -27.225265, mean_eps: 0.302827\n",
            " 70591/100000: episode: 312, duration: 1.568s, episode steps: 114, steps per second:  73, episode reward: -113.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.904 [0.000, 2.000],  loss: 1.425506, mae: 19.031044, mean_q: -27.569173, mean_eps: 0.301718\n",
            " 70718/100000: episode: 313, duration: 1.293s, episode steps: 127, steps per second:  98, episode reward: -126.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.874 [0.000, 2.000],  loss: 1.393120, mae: 18.890280, mean_q: -27.360047, mean_eps: 0.300525\n",
            " 70839/100000: episode: 314, duration: 1.190s, episode steps: 121, steps per second: 102, episode reward: -120.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.860 [0.000, 2.000],  loss: 1.371382, mae: 18.906691, mean_q: -27.373945, mean_eps: 0.299298\n",
            " 70962/100000: episode: 315, duration: 1.225s, episode steps: 123, steps per second: 100, episode reward: -122.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.911 [0.000, 2.000],  loss: 1.452332, mae: 18.906593, mean_q: -27.373286, mean_eps: 0.298090\n",
            " 71075/100000: episode: 316, duration: 1.111s, episode steps: 113, steps per second: 102, episode reward: -112.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.053 [0.000, 2.000],  loss: 1.452192, mae: 18.919750, mean_q: -27.433748, mean_eps: 0.296922\n",
            " 71171/100000: episode: 317, duration: 0.937s, episode steps:  96, steps per second: 102, episode reward: -95.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.083 [0.000, 2.000],  loss: 1.575695, mae: 18.957854, mean_q: -27.434070, mean_eps: 0.295887\n",
            " 71260/100000: episode: 318, duration: 0.854s, episode steps:  89, steps per second: 104, episode reward: -88.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.090 [0.000, 2.000],  loss: 1.378231, mae: 18.900612, mean_q: -27.358669, mean_eps: 0.294971\n",
            " 71373/100000: episode: 319, duration: 1.064s, episode steps: 113, steps per second: 106, episode reward: -112.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.053 [0.000, 2.000],  loss: 1.401248, mae: 18.923255, mean_q: -27.419346, mean_eps: 0.293972\n",
            " 71468/100000: episode: 320, duration: 0.935s, episode steps:  95, steps per second: 102, episode reward: -94.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.032 [0.000, 2.000],  loss: 1.409832, mae: 18.963994, mean_q: -27.468479, mean_eps: 0.292942\n",
            " 71593/100000: episode: 321, duration: 1.195s, episode steps: 125, steps per second: 105, episode reward: -124.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.096 [0.000, 2.000],  loss: 1.501542, mae: 19.160634, mean_q: -27.749370, mean_eps: 0.291853\n",
            " 71685/100000: episode: 322, duration: 1.135s, episode steps:  92, steps per second:  81, episode reward: -91.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.924 [0.000, 2.000],  loss: 1.575677, mae: 19.148035, mean_q: -27.707489, mean_eps: 0.290779\n",
            " 71776/100000: episode: 323, duration: 1.208s, episode steps:  91, steps per second:  75, episode reward: -90.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.912 [0.000, 2.000],  loss: 1.282801, mae: 19.175079, mean_q: -27.782165, mean_eps: 0.289873\n",
            " 71882/100000: episode: 324, duration: 1.372s, episode steps: 106, steps per second:  77, episode reward: -105.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.972 [0.000, 2.000],  loss: 1.567260, mae: 19.018619, mean_q: -27.474119, mean_eps: 0.288898\n",
            " 72006/100000: episode: 325, duration: 1.204s, episode steps: 124, steps per second: 103, episode reward: -123.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.089 [0.000, 2.000],  loss: 1.478628, mae: 19.061627, mean_q: -27.604150, mean_eps: 0.287759\n",
            " 72141/100000: episode: 326, duration: 1.252s, episode steps: 135, steps per second: 108, episode reward: -134.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.230 [0.000, 2.000],  loss: 1.512445, mae: 19.254973, mean_q: -27.901835, mean_eps: 0.286477\n",
            " 72258/100000: episode: 327, duration: 1.120s, episode steps: 117, steps per second: 104, episode reward: -116.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.949 [0.000, 2.000],  loss: 1.434888, mae: 19.283312, mean_q: -27.948489, mean_eps: 0.285230\n",
            " 72410/100000: episode: 328, duration: 1.439s, episode steps: 152, steps per second: 106, episode reward: -151.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.059 [0.000, 2.000],  loss: 1.474818, mae: 19.208318, mean_q: -27.862973, mean_eps: 0.283898\n",
            " 72533/100000: episode: 329, duration: 1.223s, episode steps: 123, steps per second: 101, episode reward: -122.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.203 [0.000, 2.000],  loss: 1.554596, mae: 19.251110, mean_q: -27.875576, mean_eps: 0.282537\n",
            " 72686/100000: episode: 330, duration: 1.508s, episode steps: 153, steps per second: 101, episode reward: -152.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.353 [0.000, 2.000],  loss: 1.411763, mae: 19.358151, mean_q: -28.079161, mean_eps: 0.281171\n",
            " 72811/100000: episode: 331, duration: 1.160s, episode steps: 125, steps per second: 108, episode reward: -124.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.160 [0.000, 2.000],  loss: 1.333902, mae: 19.317110, mean_q: -28.022792, mean_eps: 0.279795\n",
            " 72927/100000: episode: 332, duration: 1.183s, episode steps: 116, steps per second:  98, episode reward: -115.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.172 [0.000, 2.000],  loss: 1.513568, mae: 19.230724, mean_q: -27.885645, mean_eps: 0.278602\n",
            " 73036/100000: episode: 333, duration: 1.457s, episode steps: 109, steps per second:  75, episode reward: -108.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.927 [0.000, 2.000],  loss: 1.473640, mae: 19.215689, mean_q: -27.842866, mean_eps: 0.277488\n",
            " 73121/100000: episode: 334, duration: 1.059s, episode steps:  85, steps per second:  80, episode reward: -84.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.071 [0.000, 2.000],  loss: 1.711670, mae: 19.129649, mean_q: -27.678181, mean_eps: 0.276528\n",
            " 73213/100000: episode: 335, duration: 1.144s, episode steps:  92, steps per second:  80, episode reward: -91.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.902 [0.000, 2.000],  loss: 1.456544, mae: 19.208589, mean_q: -27.794790, mean_eps: 0.275652\n",
            " 73303/100000: episode: 336, duration: 0.828s, episode steps:  90, steps per second: 109, episode reward: -89.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.944 [0.000, 2.000],  loss: 1.316165, mae: 19.228781, mean_q: -27.889637, mean_eps: 0.274751\n",
            " 73409/100000: episode: 337, duration: 1.044s, episode steps: 106, steps per second: 102, episode reward: -105.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.075 [0.000, 2.000],  loss: 1.364159, mae: 19.150080, mean_q: -27.749345, mean_eps: 0.273781\n",
            " 73497/100000: episode: 338, duration: 0.866s, episode steps:  88, steps per second: 102, episode reward: -87.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 1.628931, mae: 19.232959, mean_q: -27.824271, mean_eps: 0.272820\n",
            " 73596/100000: episode: 339, duration: 0.986s, episode steps:  99, steps per second: 100, episode reward: -98.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.879 [0.000, 2.000],  loss: 1.509492, mae: 19.306427, mean_q: -27.936041, mean_eps: 0.271895\n",
            " 73701/100000: episode: 340, duration: 1.082s, episode steps: 105, steps per second:  97, episode reward: -104.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.105 [0.000, 2.000],  loss: 1.410012, mae: 19.371369, mean_q: -27.995994, mean_eps: 0.270885\n",
            " 73851/100000: episode: 341, duration: 1.576s, episode steps: 150, steps per second:  95, episode reward: -149.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.080 [0.000, 2.000],  loss: 1.463140, mae: 19.378874, mean_q: -28.048540, mean_eps: 0.269623\n",
            " 73996/100000: episode: 342, duration: 1.559s, episode steps: 145, steps per second:  93, episode reward: -144.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.014 [0.000, 2.000],  loss: 1.447651, mae: 19.328742, mean_q: -28.030720, mean_eps: 0.268162\n",
            " 74117/100000: episode: 343, duration: 1.234s, episode steps: 121, steps per second:  98, episode reward: -120.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.033 [0.000, 2.000],  loss: 1.650082, mae: 19.571300, mean_q: -28.378502, mean_eps: 0.266846\n",
            " 74204/100000: episode: 344, duration: 1.017s, episode steps:  87, steps per second:  86, episode reward: -86.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.862 [0.000, 2.000],  loss: 1.495574, mae: 19.527077, mean_q: -28.316596, mean_eps: 0.265816\n",
            " 74309/100000: episode: 345, duration: 1.433s, episode steps: 105, steps per second:  73, episode reward: -104.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.962 [0.000, 2.000],  loss: 1.533893, mae: 19.420349, mean_q: -28.163161, mean_eps: 0.264866\n",
            " 74416/100000: episode: 346, duration: 1.505s, episode steps: 107, steps per second:  71, episode reward: -106.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.112 [0.000, 2.000],  loss: 1.552677, mae: 19.396279, mean_q: -28.099892, mean_eps: 0.263816\n",
            " 74500/100000: episode: 347, duration: 0.838s, episode steps:  84, steps per second: 100, episode reward: -83.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.952 [0.000, 2.000],  loss: 1.520510, mae: 19.441309, mean_q: -28.144416, mean_eps: 0.262871\n",
            " 74621/100000: episode: 348, duration: 1.166s, episode steps: 121, steps per second: 104, episode reward: -120.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.769 [0.000, 2.000],  loss: 1.510931, mae: 19.136399, mean_q: -27.688336, mean_eps: 0.261856\n",
            " 74713/100000: episode: 349, duration: 0.900s, episode steps:  92, steps per second: 102, episode reward: -91.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.054 [0.000, 2.000],  loss: 1.615627, mae: 19.263684, mean_q: -27.894821, mean_eps: 0.260802\n",
            " 74818/100000: episode: 350, duration: 1.028s, episode steps: 105, steps per second: 102, episode reward: -104.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.867 [0.000, 2.000],  loss: 1.718145, mae: 19.168942, mean_q: -27.742639, mean_eps: 0.259826\n",
            " 74938/100000: episode: 351, duration: 1.164s, episode steps: 120, steps per second: 103, episode reward: -119.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.992 [0.000, 2.000],  loss: 1.556261, mae: 19.208528, mean_q: -27.788582, mean_eps: 0.258713\n",
            " 75052/100000: episode: 352, duration: 1.141s, episode steps: 114, steps per second: 100, episode reward: -113.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.982 [0.000, 2.000],  loss: 1.735764, mae: 19.211532, mean_q: -27.808187, mean_eps: 0.257554\n",
            " 75167/100000: episode: 353, duration: 1.140s, episode steps: 115, steps per second: 101, episode reward: -114.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.052 [0.000, 2.000],  loss: 1.653590, mae: 19.229420, mean_q: -27.822731, mean_eps: 0.256421\n",
            " 75332/100000: episode: 354, duration: 1.650s, episode steps: 165, steps per second: 100, episode reward: -164.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.006 [0.000, 2.000],  loss: 1.720776, mae: 19.331269, mean_q: -28.006179, mean_eps: 0.255035\n",
            " 75469/100000: episode: 355, duration: 1.516s, episode steps: 137, steps per second:  90, episode reward: -136.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.036 [0.000, 2.000],  loss: 1.490025, mae: 19.302299, mean_q: -27.962246, mean_eps: 0.253540\n",
            " 75585/100000: episode: 356, duration: 1.643s, episode steps: 116, steps per second:  71, episode reward: -115.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.879 [0.000, 2.000],  loss: 1.696537, mae: 19.303756, mean_q: -27.955044, mean_eps: 0.252288\n",
            " 75685/100000: episode: 357, duration: 1.436s, episode steps: 100, steps per second:  70, episode reward: -99.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.920 [0.000, 2.000],  loss: 1.684754, mae: 19.363328, mean_q: -28.041542, mean_eps: 0.251218\n",
            " 75778/100000: episode: 358, duration: 1.003s, episode steps:  93, steps per second:  93, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 1.620327, mae: 19.317142, mean_q: -27.947638, mean_eps: 0.250263\n",
            " 75878/100000: episode: 359, duration: 0.982s, episode steps: 100, steps per second: 102, episode reward: -99.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 1.613111, mae: 19.278869, mean_q: -27.856745, mean_eps: 0.249308\n",
            " 75980/100000: episode: 360, duration: 0.991s, episode steps: 102, steps per second: 103, episode reward: -101.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.069 [0.000, 2.000],  loss: 1.742998, mae: 19.294259, mean_q: -27.882996, mean_eps: 0.248308\n",
            " 76058/100000: episode: 361, duration: 0.762s, episode steps:  78, steps per second: 102, episode reward: -77.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.090 [0.000, 2.000],  loss: 1.506733, mae: 19.292500, mean_q: -27.911710, mean_eps: 0.247417\n",
            " 76176/100000: episode: 362, duration: 1.132s, episode steps: 118, steps per second: 104, episode reward: -117.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.915 [0.000, 2.000],  loss: 1.522467, mae: 19.386748, mean_q: -28.027408, mean_eps: 0.246447\n",
            " 76259/100000: episode: 363, duration: 0.813s, episode steps:  83, steps per second: 102, episode reward: -82.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.036 [0.000, 2.000],  loss: 1.663253, mae: 19.378746, mean_q: -27.982250, mean_eps: 0.245452\n",
            " 76368/100000: episode: 364, duration: 1.083s, episode steps: 109, steps per second: 101, episode reward: -108.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.963 [0.000, 2.000],  loss: 1.458443, mae: 19.305796, mean_q: -27.931065, mean_eps: 0.244501\n",
            " 76452/100000: episode: 365, duration: 0.817s, episode steps:  84, steps per second: 103, episode reward: -83.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.060 [0.000, 2.000],  loss: 1.517315, mae: 19.468642, mean_q: -28.158802, mean_eps: 0.243546\n",
            " 76541/100000: episode: 366, duration: 0.855s, episode steps:  89, steps per second: 104, episode reward: -88.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.079 [0.000, 2.000],  loss: 1.615361, mae: 19.291295, mean_q: -27.926280, mean_eps: 0.242690\n",
            " 76623/100000: episode: 367, duration: 0.788s, episode steps:  82, steps per second: 104, episode reward: -81.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.951 [0.000, 2.000],  loss: 1.665733, mae: 19.136510, mean_q: -27.639250, mean_eps: 0.241843\n",
            " 76723/100000: episode: 368, duration: 1.138s, episode steps: 100, steps per second:  88, episode reward: -99.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.090 [0.000, 2.000],  loss: 1.620750, mae: 19.103538, mean_q: -27.611691, mean_eps: 0.240942\n",
            " 76893/100000: episode: 369, duration: 2.257s, episode steps: 170, steps per second:  75, episode reward: -169.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.941 [0.000, 2.000],  loss: 1.702099, mae: 19.196513, mean_q: -27.754255, mean_eps: 0.239606\n",
            " 77000/100000: episode: 370, duration: 1.347s, episode steps: 107, steps per second:  79, episode reward: -106.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.888 [0.000, 2.000],  loss: 1.693908, mae: 19.207258, mean_q: -27.702900, mean_eps: 0.238235\n",
            " 77094/100000: episode: 371, duration: 0.941s, episode steps:  94, steps per second: 100, episode reward: -93.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.181 [0.000, 2.000],  loss: 1.650585, mae: 19.065455, mean_q: -27.548358, mean_eps: 0.237240\n",
            " 77219/100000: episode: 372, duration: 1.209s, episode steps: 125, steps per second: 103, episode reward: -124.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.200 [0.000, 2.000],  loss: 1.751333, mae: 19.091110, mean_q: -27.587677, mean_eps: 0.236156\n",
            " 77337/100000: episode: 373, duration: 1.179s, episode steps: 118, steps per second: 100, episode reward: -117.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.102 [0.000, 2.000],  loss: 1.725126, mae: 19.053107, mean_q: -27.496002, mean_eps: 0.234953\n",
            " 77436/100000: episode: 374, duration: 0.973s, episode steps:  99, steps per second: 102, episode reward: -98.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.949 [0.000, 2.000],  loss: 1.571454, mae: 18.934425, mean_q: -27.305083, mean_eps: 0.233879\n",
            " 77527/100000: episode: 375, duration: 0.927s, episode steps:  91, steps per second:  98, episode reward: -90.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.055 [0.000, 2.000],  loss: 1.725709, mae: 19.025914, mean_q: -27.495460, mean_eps: 0.232938\n",
            " 77682/100000: episode: 376, duration: 1.571s, episode steps: 155, steps per second:  99, episode reward: -154.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.439 [0.000, 2.000],  loss: 1.782537, mae: 19.305435, mean_q: -27.858771, mean_eps: 0.231720\n",
            " 77793/100000: episode: 377, duration: 1.101s, episode steps: 111, steps per second: 101, episode reward: -110.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.081 [0.000, 2.000],  loss: 1.577968, mae: 19.350591, mean_q: -27.968684, mean_eps: 0.230404\n",
            " 77946/100000: episode: 378, duration: 1.545s, episode steps: 153, steps per second:  99, episode reward: -152.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.255 [0.000, 2.000],  loss: 1.724794, mae: 19.358199, mean_q: -27.919035, mean_eps: 0.229097\n",
            " 78063/100000: episode: 379, duration: 1.595s, episode steps: 117, steps per second:  73, episode reward: -116.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.385 [0.000, 2.000],  loss: 1.709763, mae: 19.252188, mean_q: -27.754012, mean_eps: 0.227760\n",
            " 78231/100000: episode: 380, duration: 2.219s, episode steps: 168, steps per second:  76, episode reward: -167.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.143 [0.000, 2.000],  loss: 1.698281, mae: 19.242524, mean_q: -27.813198, mean_eps: 0.226350\n",
            " 78331/100000: episode: 381, duration: 0.991s, episode steps: 100, steps per second: 101, episode reward: -99.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.290 [0.000, 2.000],  loss: 1.631333, mae: 19.229028, mean_q: -27.794195, mean_eps: 0.225023\n",
            " 78430/100000: episode: 382, duration: 0.990s, episode steps:  99, steps per second: 100, episode reward: -98.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.192 [0.000, 2.000],  loss: 1.723720, mae: 19.234771, mean_q: -27.818328, mean_eps: 0.224038\n",
            " 78514/100000: episode: 383, duration: 0.848s, episode steps:  84, steps per second:  99, episode reward: -83.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.190 [0.000, 2.000],  loss: 1.611754, mae: 19.253307, mean_q: -27.815201, mean_eps: 0.223132\n",
            " 78619/100000: episode: 384, duration: 1.038s, episode steps: 105, steps per second: 101, episode reward: -104.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.981 [0.000, 2.000],  loss: 1.828639, mae: 18.977569, mean_q: -27.383604, mean_eps: 0.222197\n",
            " 78735/100000: episode: 385, duration: 1.110s, episode steps: 116, steps per second: 105, episode reward: -115.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.095 [0.000, 2.000],  loss: 1.754629, mae: 18.947172, mean_q: -27.359885, mean_eps: 0.221103\n",
            " 78914/100000: episode: 386, duration: 1.754s, episode steps: 179, steps per second: 102, episode reward: -178.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.151 [0.000, 2.000],  loss: 1.719194, mae: 19.060546, mean_q: -27.493570, mean_eps: 0.219642\n",
            " 79027/100000: episode: 387, duration: 1.108s, episode steps: 113, steps per second: 102, episode reward: -112.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.204 [0.000, 2.000],  loss: 1.768104, mae: 18.972844, mean_q: -27.350947, mean_eps: 0.218197\n",
            " 79127/100000: episode: 388, duration: 0.958s, episode steps: 100, steps per second: 104, episode reward: -99.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.940 [0.000, 2.000],  loss: 1.834043, mae: 19.008331, mean_q: -27.378939, mean_eps: 0.217143\n",
            " 79231/100000: episode: 389, duration: 1.026s, episode steps: 104, steps per second: 101, episode reward: -103.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.885 [0.000, 2.000],  loss: 1.742073, mae: 18.840806, mean_q: -27.159785, mean_eps: 0.216133\n",
            " 79374/100000: episode: 390, duration: 1.953s, episode steps: 143, steps per second:  73, episode reward: -142.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.853 [0.000, 2.000],  loss: 1.684116, mae: 18.916980, mean_q: -27.291602, mean_eps: 0.214910\n",
            " 79493/100000: episode: 391, duration: 1.660s, episode steps: 119, steps per second:  72, episode reward: -118.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.891 [0.000, 2.000],  loss: 1.629679, mae: 19.074397, mean_q: -27.535663, mean_eps: 0.213613\n",
            " 79583/100000: episode: 392, duration: 0.909s, episode steps:  90, steps per second:  99, episode reward: -89.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.044 [0.000, 2.000],  loss: 1.742490, mae: 18.752895, mean_q: -27.011752, mean_eps: 0.212579\n",
            " 79686/100000: episode: 393, duration: 1.036s, episode steps: 103, steps per second:  99, episode reward: -102.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.835 [0.000, 2.000],  loss: 1.628501, mae: 18.918661, mean_q: -27.283770, mean_eps: 0.211623\n",
            " 79813/100000: episode: 394, duration: 1.284s, episode steps: 127, steps per second:  99, episode reward: -126.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.992 [0.000, 2.000],  loss: 1.577085, mae: 18.978445, mean_q: -27.383203, mean_eps: 0.210485\n",
            " 79909/100000: episode: 395, duration: 1.028s, episode steps:  96, steps per second:  93, episode reward: -95.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.021 [0.000, 2.000],  loss: 1.579272, mae: 18.919417, mean_q: -27.299334, mean_eps: 0.209381\n",
            " 80005/100000: episode: 396, duration: 0.982s, episode steps:  96, steps per second:  98, episode reward: -95.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.917 [0.000, 2.000],  loss: 1.817580, mae: 18.971942, mean_q: -27.353710, mean_eps: 0.208431\n",
            " 80099/100000: episode: 397, duration: 0.968s, episode steps:  94, steps per second:  97, episode reward: -93.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.170 [0.000, 2.000],  loss: 1.953379, mae: 18.810245, mean_q: -27.093677, mean_eps: 0.207490\n",
            " 80177/100000: episode: 398, duration: 0.785s, episode steps:  78, steps per second:  99, episode reward: -77.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.115 [0.000, 2.000],  loss: 1.748721, mae: 18.843080, mean_q: -27.184289, mean_eps: 0.206639\n",
            " 80290/100000: episode: 399, duration: 1.127s, episode steps: 113, steps per second: 100, episode reward: -112.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.044 [0.000, 2.000],  loss: 1.743720, mae: 18.694453, mean_q: -26.948652, mean_eps: 0.205693\n",
            " 80371/100000: episode: 400, duration: 0.808s, episode steps:  81, steps per second: 100, episode reward: -80.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.074 [0.000, 2.000],  loss: 1.724325, mae: 18.780827, mean_q: -27.051938, mean_eps: 0.204733\n",
            " 80471/100000: episode: 401, duration: 1.176s, episode steps: 100, steps per second:  85, episode reward: -99.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.880 [0.000, 2.000],  loss: 1.499554, mae: 18.656724, mean_q: -26.922045, mean_eps: 0.203837\n",
            " 80617/100000: episode: 402, duration: 2.052s, episode steps: 146, steps per second:  71, episode reward: -145.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.062 [0.000, 2.000],  loss: 1.654464, mae: 18.712086, mean_q: -26.980221, mean_eps: 0.202619\n",
            " 80689/100000: episode: 403, duration: 1.034s, episode steps:  72, steps per second:  70, episode reward: -71.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.958 [0.000, 2.000],  loss: 1.700614, mae: 18.757996, mean_q: -27.057110, mean_eps: 0.201540\n",
            " 80786/100000: episode: 404, duration: 1.057s, episode steps:  97, steps per second:  92, episode reward: -96.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.052 [0.000, 2.000],  loss: 1.443726, mae: 18.801184, mean_q: -27.151727, mean_eps: 0.200704\n",
            " 80888/100000: episode: 405, duration: 1.000s, episode steps: 102, steps per second: 102, episode reward: -101.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.755 [0.000, 2.000],  loss: 1.485071, mae: 18.661951, mean_q: -26.953811, mean_eps: 0.199719\n",
            " 80963/100000: episode: 406, duration: 0.766s, episode steps:  75, steps per second:  98, episode reward: -74.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.040 [0.000, 2.000],  loss: 1.789669, mae: 18.737466, mean_q: -26.986373, mean_eps: 0.198843\n",
            " 81099/100000: episode: 407, duration: 1.384s, episode steps: 136, steps per second:  98, episode reward: -135.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.132 [0.000, 2.000],  loss: 1.765150, mae: 18.664541, mean_q: -26.902492, mean_eps: 0.197798\n",
            " 81258/100000: episode: 408, duration: 1.649s, episode steps: 159, steps per second:  96, episode reward: -158.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.264 [0.000, 2.000],  loss: 1.709195, mae: 18.623473, mean_q: -26.839032, mean_eps: 0.196338\n",
            " 81385/100000: episode: 409, duration: 1.386s, episode steps: 127, steps per second:  92, episode reward: -126.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.031 [0.000, 2.000],  loss: 1.592257, mae: 18.667558, mean_q: -26.927835, mean_eps: 0.194922\n",
            " 81504/100000: episode: 410, duration: 1.218s, episode steps: 119, steps per second:  98, episode reward: -118.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.866 [0.000, 2.000],  loss: 1.719716, mae: 18.670028, mean_q: -26.886318, mean_eps: 0.193704\n",
            " 81601/100000: episode: 411, duration: 0.937s, episode steps:  97, steps per second: 103, episode reward: -96.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.959 [0.000, 2.000],  loss: 1.545528, mae: 18.720999, mean_q: -26.984975, mean_eps: 0.192635\n",
            " 81724/100000: episode: 412, duration: 1.359s, episode steps: 123, steps per second:  90, episode reward: -122.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.309 [0.000, 2.000],  loss: 1.832850, mae: 18.702051, mean_q: -26.931296, mean_eps: 0.191546\n",
            " 81826/100000: episode: 413, duration: 1.395s, episode steps: 102, steps per second:  73, episode reward: -101.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.088 [0.000, 2.000],  loss: 1.865022, mae: 18.682946, mean_q: -26.886926, mean_eps: 0.190432\n",
            " 81980/100000: episode: 414, duration: 2.017s, episode steps: 154, steps per second:  76, episode reward: -153.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.922 [0.000, 2.000],  loss: 1.720673, mae: 18.595751, mean_q: -26.769894, mean_eps: 0.189165\n",
            " 82087/100000: episode: 415, duration: 1.049s, episode steps: 107, steps per second: 102, episode reward: -106.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.093 [0.000, 2.000],  loss: 1.861261, mae: 18.833574, mean_q: -27.111196, mean_eps: 0.187873\n",
            " 82208/100000: episode: 416, duration: 1.219s, episode steps: 121, steps per second:  99, episode reward: -120.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.124 [0.000, 2.000],  loss: 1.899181, mae: 18.956695, mean_q: -27.264894, mean_eps: 0.186745\n",
            " 82335/100000: episode: 417, duration: 1.253s, episode steps: 127, steps per second: 101, episode reward: -126.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.110 [0.000, 2.000],  loss: 1.882079, mae: 18.868584, mean_q: -27.146542, mean_eps: 0.185517\n",
            " 82449/100000: episode: 418, duration: 1.098s, episode steps: 114, steps per second: 104, episode reward: -113.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.228 [0.000, 2.000],  loss: 1.864388, mae: 18.847773, mean_q: -27.128872, mean_eps: 0.184324\n",
            " 82531/100000: episode: 419, duration: 0.806s, episode steps:  82, steps per second: 102, episode reward: -81.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.073 [0.000, 2.000],  loss: 1.713533, mae: 18.805227, mean_q: -27.035263, mean_eps: 0.183354\n",
            " 82620/100000: episode: 420, duration: 0.903s, episode steps:  89, steps per second:  99, episode reward: -88.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.124 [0.000, 2.000],  loss: 1.820207, mae: 18.819296, mean_q: -27.037655, mean_eps: 0.182508\n",
            " 82734/100000: episode: 421, duration: 1.089s, episode steps: 114, steps per second: 105, episode reward: -113.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.018 [0.000, 2.000],  loss: 1.589401, mae: 18.876551, mean_q: -27.181772, mean_eps: 0.181503\n",
            " 82854/100000: episode: 422, duration: 1.164s, episode steps: 120, steps per second: 103, episode reward: -119.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.108 [0.000, 2.000],  loss: 1.653001, mae: 18.802137, mean_q: -27.082029, mean_eps: 0.180344\n",
            " 82951/100000: episode: 423, duration: 0.936s, episode steps:  97, steps per second: 104, episode reward: -96.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.041 [0.000, 2.000],  loss: 1.665651, mae: 18.841397, mean_q: -27.130092, mean_eps: 0.179270\n",
            " 83028/100000: episode: 424, duration: 0.983s, episode steps:  77, steps per second:  78, episode reward: -76.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.117 [0.000, 2.000],  loss: 1.580655, mae: 18.783391, mean_q: -27.040020, mean_eps: 0.178409\n",
            " 83108/100000: episode: 425, duration: 1.089s, episode steps:  80, steps per second:  73, episode reward: -79.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.113 [0.000, 2.000],  loss: 1.843043, mae: 18.720253, mean_q: -26.923914, mean_eps: 0.177632\n",
            " 83212/100000: episode: 426, duration: 1.441s, episode steps: 104, steps per second:  72, episode reward: -103.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.971 [0.000, 2.000],  loss: 1.555708, mae: 18.691332, mean_q: -26.943678, mean_eps: 0.176721\n",
            " 83307/100000: episode: 427, duration: 0.928s, episode steps:  95, steps per second: 102, episode reward: -94.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.137 [0.000, 2.000],  loss: 1.560665, mae: 18.646194, mean_q: -26.860090, mean_eps: 0.175736\n",
            " 83428/100000: episode: 428, duration: 1.177s, episode steps: 121, steps per second: 103, episode reward: -120.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.099 [0.000, 2.000],  loss: 1.771987, mae: 18.602943, mean_q: -26.805574, mean_eps: 0.174667\n",
            " 83527/100000: episode: 429, duration: 1.008s, episode steps:  99, steps per second:  98, episode reward: -98.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.010 [0.000, 2.000],  loss: 1.559080, mae: 18.528454, mean_q: -26.722912, mean_eps: 0.173578\n",
            " 83599/100000: episode: 430, duration: 0.721s, episode steps:  72, steps per second: 100, episode reward: -71.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.097 [0.000, 2.000],  loss: 1.602447, mae: 18.677146, mean_q: -26.954014, mean_eps: 0.172731\n",
            " 83698/100000: episode: 431, duration: 0.943s, episode steps:  99, steps per second: 105, episode reward: -98.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.030 [0.000, 2.000],  loss: 1.679216, mae: 18.601059, mean_q: -26.792734, mean_eps: 0.171885\n",
            " 83803/100000: episode: 432, duration: 0.985s, episode steps: 105, steps per second: 107, episode reward: -104.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.038 [0.000, 2.000],  loss: 1.585921, mae: 18.689200, mean_q: -26.945918, mean_eps: 0.170875\n",
            " 83911/100000: episode: 433, duration: 1.031s, episode steps: 108, steps per second: 105, episode reward: -107.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.972 [0.000, 2.000],  loss: 1.655064, mae: 18.734806, mean_q: -27.025543, mean_eps: 0.169821\n",
            " 84019/100000: episode: 434, duration: 1.020s, episode steps: 108, steps per second: 106, episode reward: -107.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.898 [0.000, 2.000],  loss: 1.605659, mae: 18.603623, mean_q: -26.864934, mean_eps: 0.168751\n",
            " 84129/100000: episode: 435, duration: 1.072s, episode steps: 110, steps per second: 103, episode reward: -109.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.191 [0.000, 2.000],  loss: 1.606087, mae: 18.847576, mean_q: -27.215080, mean_eps: 0.167672\n",
            " 84235/100000: episode: 436, duration: 1.016s, episode steps: 106, steps per second: 104, episode reward: -105.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.953 [0.000, 2.000],  loss: 1.538609, mae: 18.949785, mean_q: -27.415567, mean_eps: 0.166603\n",
            " 84331/100000: episode: 437, duration: 1.226s, episode steps:  96, steps per second:  78, episode reward: -95.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.979 [0.000, 2.000],  loss: 1.524764, mae: 18.882677, mean_q: -27.285982, mean_eps: 0.165603\n",
            " 84408/100000: episode: 438, duration: 1.042s, episode steps:  77, steps per second:  74, episode reward: -76.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.987 [0.000, 2.000],  loss: 1.492642, mae: 18.878272, mean_q: -27.306171, mean_eps: 0.164747\n",
            " 84487/100000: episode: 439, duration: 1.150s, episode steps:  79, steps per second:  69, episode reward: -78.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.089 [0.000, 2.000],  loss: 1.535445, mae: 18.832965, mean_q: -27.231814, mean_eps: 0.163975\n",
            " 84593/100000: episode: 440, duration: 1.177s, episode steps: 106, steps per second:  90, episode reward: -105.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.170 [0.000, 2.000],  loss: 1.716560, mae: 18.882999, mean_q: -27.255512, mean_eps: 0.163059\n",
            " 84674/100000: episode: 441, duration: 0.847s, episode steps:  81, steps per second:  96, episode reward: -80.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.963 [0.000, 2.000],  loss: 1.611754, mae: 18.849165, mean_q: -27.247083, mean_eps: 0.162133\n",
            " 84754/100000: episode: 442, duration: 0.855s, episode steps:  80, steps per second:  94, episode reward: -79.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.988 [0.000, 2.000],  loss: 1.573877, mae: 18.855183, mean_q: -27.240225, mean_eps: 0.161336\n",
            " 84845/100000: episode: 443, duration: 0.923s, episode steps:  91, steps per second:  99, episode reward: -90.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.802 [0.000, 2.000],  loss: 1.617494, mae: 18.802772, mean_q: -27.141446, mean_eps: 0.160490\n",
            " 84948/100000: episode: 444, duration: 1.018s, episode steps: 103, steps per second: 101, episode reward: -102.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.971 [0.000, 2.000],  loss: 1.696439, mae: 18.821773, mean_q: -27.174319, mean_eps: 0.159530\n",
            " 85032/100000: episode: 445, duration: 0.851s, episode steps:  84, steps per second:  99, episode reward: -83.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.976 [0.000, 2.000],  loss: 1.545492, mae: 18.869575, mean_q: -27.281988, mean_eps: 0.158604\n",
            " 85152/100000: episode: 446, duration: 1.240s, episode steps: 120, steps per second:  97, episode reward: -119.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.867 [0.000, 2.000],  loss: 1.766091, mae: 18.747981, mean_q: -27.045574, mean_eps: 0.157594\n",
            " 85234/100000: episode: 447, duration: 0.800s, episode steps:  82, steps per second: 102, episode reward: -81.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.866 [0.000, 2.000],  loss: 1.515798, mae: 18.827995, mean_q: -27.252068, mean_eps: 0.156594\n",
            " 85327/100000: episode: 448, duration: 0.917s, episode steps:  93, steps per second: 101, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.065 [0.000, 2.000],  loss: 1.403905, mae: 18.618140, mean_q: -26.884899, mean_eps: 0.155728\n",
            " 85441/100000: episode: 449, duration: 1.095s, episode steps: 114, steps per second: 104, episode reward: -113.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.009 [0.000, 2.000],  loss: 1.684659, mae: 18.829870, mean_q: -27.182540, mean_eps: 0.154703\n",
            " 85533/100000: episode: 450, duration: 1.072s, episode steps:  92, steps per second:  86, episode reward: -91.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.772 [0.000, 2.000],  loss: 1.675432, mae: 18.663122, mean_q: -26.915277, mean_eps: 0.153684\n",
            " 85636/100000: episode: 451, duration: 1.416s, episode steps: 103, steps per second:  73, episode reward: -102.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.864 [0.000, 2.000],  loss: 1.499245, mae: 18.546531, mean_q: -26.743623, mean_eps: 0.152718\n",
            " 85733/100000: episode: 452, duration: 1.342s, episode steps:  97, steps per second:  72, episode reward: -96.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.814 [0.000, 2.000],  loss: 1.631098, mae: 18.870315, mean_q: -27.248220, mean_eps: 0.151728\n",
            " 85860/100000: episode: 453, duration: 1.366s, episode steps: 127, steps per second:  93, episode reward: -126.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.906 [0.000, 2.000],  loss: 1.550541, mae: 18.575758, mean_q: -26.815256, mean_eps: 0.150620\n",
            " 85962/100000: episode: 454, duration: 1.052s, episode steps: 102, steps per second:  97, episode reward: -101.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.863 [0.000, 2.000],  loss: 1.544590, mae: 18.746239, mean_q: -27.055712, mean_eps: 0.149486\n",
            " 86048/100000: episode: 455, duration: 0.880s, episode steps:  86, steps per second:  98, episode reward: -85.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.860 [0.000, 2.000],  loss: 1.616971, mae: 18.662446, mean_q: -26.898343, mean_eps: 0.148555\n",
            " 86171/100000: episode: 456, duration: 1.221s, episode steps: 123, steps per second: 101, episode reward: -122.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.959 [0.000, 2.000],  loss: 1.472913, mae: 18.559849, mean_q: -26.790668, mean_eps: 0.147521\n",
            " 86285/100000: episode: 457, duration: 1.110s, episode steps: 114, steps per second: 103, episode reward: -113.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.912 [0.000, 2.000],  loss: 1.536644, mae: 18.592790, mean_q: -26.856879, mean_eps: 0.146348\n",
            " 86393/100000: episode: 458, duration: 1.077s, episode steps: 108, steps per second: 100, episode reward: -107.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.139 [0.000, 2.000],  loss: 1.625831, mae: 18.560535, mean_q: -26.780154, mean_eps: 0.145249\n",
            " 86498/100000: episode: 459, duration: 1.027s, episode steps: 105, steps per second: 102, episode reward: -104.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.076 [0.000, 2.000],  loss: 1.837443, mae: 18.618335, mean_q: -26.806228, mean_eps: 0.144195\n",
            " 86617/100000: episode: 460, duration: 1.154s, episode steps: 119, steps per second: 103, episode reward: -118.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.160 [0.000, 2.000],  loss: 1.629456, mae: 18.340726, mean_q: -26.431060, mean_eps: 0.143086\n",
            " 86725/100000: episode: 461, duration: 1.048s, episode steps: 108, steps per second: 103, episode reward: -107.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.093 [0.000, 2.000],  loss: 1.652920, mae: 18.358574, mean_q: -26.447863, mean_eps: 0.141962\n",
            " 86872/100000: episode: 462, duration: 1.807s, episode steps: 147, steps per second:  81, episode reward: -146.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.156 [0.000, 2.000],  loss: 1.620581, mae: 18.375815, mean_q: -26.519788, mean_eps: 0.140700\n",
            " 86972/100000: episode: 463, duration: 1.259s, episode steps: 100, steps per second:  79, episode reward: -99.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.980 [0.000, 2.000],  loss: 1.706303, mae: 18.232778, mean_q: -26.284187, mean_eps: 0.139477\n",
            " 87068/100000: episode: 464, duration: 1.125s, episode steps:  96, steps per second:  85, episode reward: -95.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.229 [0.000, 2.000],  loss: 1.692707, mae: 18.339936, mean_q: -26.399998, mean_eps: 0.138507\n",
            " 87182/100000: episode: 465, duration: 1.065s, episode steps: 114, steps per second: 107, episode reward: -113.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.088 [0.000, 2.000],  loss: 1.674105, mae: 18.457283, mean_q: -26.640362, mean_eps: 0.137467\n",
            " 87274/100000: episode: 466, duration: 0.876s, episode steps:  92, steps per second: 105, episode reward: -91.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.043 [0.000, 2.000],  loss: 1.482146, mae: 18.358131, mean_q: -26.477982, mean_eps: 0.136448\n",
            " 87392/100000: episode: 467, duration: 1.138s, episode steps: 118, steps per second: 104, episode reward: -117.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.017 [0.000, 2.000],  loss: 1.643690, mae: 18.311141, mean_q: -26.366022, mean_eps: 0.135408\n",
            " 87524/100000: episode: 468, duration: 1.260s, episode steps: 132, steps per second: 105, episode reward: -131.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.083 [0.000, 2.000],  loss: 1.723316, mae: 18.350755, mean_q: -26.442068, mean_eps: 0.134171\n",
            " 87616/100000: episode: 469, duration: 0.878s, episode steps:  92, steps per second: 105, episode reward: -91.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.033 [0.000, 2.000],  loss: 1.826320, mae: 18.128636, mean_q: -26.070475, mean_eps: 0.133062\n",
            " 87742/100000: episode: 470, duration: 1.177s, episode steps: 126, steps per second: 107, episode reward: -125.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.357 [0.000, 2.000],  loss: 1.851944, mae: 18.233168, mean_q: -26.219020, mean_eps: 0.131983\n",
            " 87824/100000: episode: 471, duration: 0.778s, episode steps:  82, steps per second: 105, episode reward: -81.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.110 [0.000, 2.000],  loss: 1.895712, mae: 18.166689, mean_q: -26.099817, mean_eps: 0.130953\n",
            " 87910/100000: episode: 472, duration: 0.854s, episode steps:  86, steps per second: 101, episode reward: -85.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.151 [0.000, 2.000],  loss: 1.908064, mae: 18.187189, mean_q: -26.103658, mean_eps: 0.130122\n",
            " 87996/100000: episode: 473, duration: 0.845s, episode steps:  86, steps per second: 102, episode reward: -85.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.907 [0.000, 2.000],  loss: 1.650940, mae: 18.278642, mean_q: -26.283820, mean_eps: 0.129270\n",
            " 88149/100000: episode: 474, duration: 1.761s, episode steps: 153, steps per second:  87, episode reward: -152.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.235 [0.000, 2.000],  loss: 1.801647, mae: 18.161073, mean_q: -26.035682, mean_eps: 0.128087\n",
            " 88248/100000: episode: 475, duration: 1.334s, episode steps:  99, steps per second:  74, episode reward: -98.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.970 [0.000, 2.000],  loss: 1.661030, mae: 18.024505, mean_q: -25.869412, mean_eps: 0.126840\n",
            " 88348/100000: episode: 476, duration: 1.318s, episode steps: 100, steps per second:  76, episode reward: -99.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.910 [0.000, 2.000],  loss: 1.612477, mae: 18.044701, mean_q: -25.897571, mean_eps: 0.125855\n",
            " 88454/100000: episode: 477, duration: 1.036s, episode steps: 106, steps per second: 102, episode reward: -105.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.953 [0.000, 2.000],  loss: 1.607118, mae: 17.920034, mean_q: -25.765151, mean_eps: 0.124835\n",
            " 88568/100000: episode: 478, duration: 1.095s, episode steps: 114, steps per second: 104, episode reward: -113.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.930 [0.000, 2.000],  loss: 1.819071, mae: 18.042096, mean_q: -25.820585, mean_eps: 0.123746\n",
            " 88663/100000: episode: 479, duration: 0.917s, episode steps:  95, steps per second: 104, episode reward: -94.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.884 [0.000, 2.000],  loss: 1.648746, mae: 17.892036, mean_q: -25.720091, mean_eps: 0.122711\n",
            " 88756/100000: episode: 480, duration: 0.909s, episode steps:  93, steps per second: 102, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.032 [0.000, 2.000],  loss: 2.139450, mae: 17.909752, mean_q: -25.633917, mean_eps: 0.121781\n",
            " 88866/100000: episode: 481, duration: 1.107s, episode steps: 110, steps per second:  99, episode reward: -109.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.227 [0.000, 2.000],  loss: 1.668105, mae: 17.901788, mean_q: -25.733660, mean_eps: 0.120776\n",
            " 88964/100000: episode: 482, duration: 0.957s, episode steps:  98, steps per second: 102, episode reward: -97.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.051 [0.000, 2.000],  loss: 1.853575, mae: 17.838368, mean_q: -25.615532, mean_eps: 0.119746\n",
            " 89063/100000: episode: 483, duration: 1.033s, episode steps:  99, steps per second:  96, episode reward: -98.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.051 [0.000, 2.000],  loss: 1.639625, mae: 17.956818, mean_q: -25.840026, mean_eps: 0.118771\n",
            " 89158/100000: episode: 484, duration: 0.968s, episode steps:  95, steps per second:  98, episode reward: -94.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 1.736450, mae: 17.880717, mean_q: -25.753907, mean_eps: 0.117811\n",
            " 89248/100000: episode: 485, duration: 0.864s, episode steps:  90, steps per second: 104, episode reward: -89.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.167 [0.000, 2.000],  loss: 1.618152, mae: 17.723097, mean_q: -25.510108, mean_eps: 0.116895\n",
            " 89319/100000: episode: 486, duration: 0.681s, episode steps:  71, steps per second: 104, episode reward: -70.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.958 [0.000, 2.000],  loss: 1.713685, mae: 17.787922, mean_q: -25.536141, mean_eps: 0.116098\n",
            " 89415/100000: episode: 487, duration: 1.231s, episode steps:  96, steps per second:  78, episode reward: -95.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.094 [0.000, 2.000],  loss: 1.622920, mae: 17.796343, mean_q: -25.559766, mean_eps: 0.115272\n",
            " 89509/100000: episode: 488, duration: 1.297s, episode steps:  94, steps per second:  72, episode reward: -93.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.223 [0.000, 2.000],  loss: 1.658209, mae: 17.672636, mean_q: -25.409881, mean_eps: 0.114331\n",
            " 89635/100000: episode: 489, duration: 1.572s, episode steps: 126, steps per second:  80, episode reward: -125.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.786 [0.000, 2.000],  loss: 1.636215, mae: 17.578793, mean_q: -25.306281, mean_eps: 0.113242\n",
            " 89734/100000: episode: 490, duration: 1.005s, episode steps:  99, steps per second:  98, episode reward: -98.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 1.688592, mae: 17.582252, mean_q: -25.300335, mean_eps: 0.112128\n",
            " 89817/100000: episode: 491, duration: 0.820s, episode steps:  83, steps per second: 101, episode reward: -82.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.880 [0.000, 2.000],  loss: 1.527966, mae: 17.563393, mean_q: -25.215402, mean_eps: 0.111227\n",
            " 89924/100000: episode: 492, duration: 0.989s, episode steps: 107, steps per second: 108, episode reward: -106.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.925 [0.000, 2.000],  loss: 1.441665, mae: 17.576353, mean_q: -25.257539, mean_eps: 0.110287\n",
            " 90001/100000: episode: 493, duration: 0.733s, episode steps:  77, steps per second: 105, episode reward: -76.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.117 [0.000, 2.000],  loss: 1.614671, mae: 17.634110, mean_q: -25.372029, mean_eps: 0.109376\n",
            " 90090/100000: episode: 494, duration: 0.829s, episode steps:  89, steps per second: 107, episode reward: -88.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.730 [0.000, 2.000],  loss: 1.764482, mae: 17.490097, mean_q: -25.099504, mean_eps: 0.108554\n",
            " 90176/100000: episode: 495, duration: 0.822s, episode steps:  86, steps per second: 105, episode reward: -85.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.930 [0.000, 2.000],  loss: 1.576883, mae: 17.542441, mean_q: -25.152839, mean_eps: 0.107688\n",
            " 90264/100000: episode: 496, duration: 0.839s, episode steps:  88, steps per second: 105, episode reward: -87.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.943 [0.000, 2.000],  loss: 1.633907, mae: 17.593718, mean_q: -25.251324, mean_eps: 0.106827\n",
            " 90350/100000: episode: 497, duration: 0.839s, episode steps:  86, steps per second: 102, episode reward: -85.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.128 [0.000, 2.000],  loss: 1.572322, mae: 17.558706, mean_q: -25.148450, mean_eps: 0.105966\n",
            " 90471/100000: episode: 498, duration: 1.135s, episode steps: 121, steps per second: 107, episode reward: -120.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.868 [0.000, 2.000],  loss: 1.698809, mae: 17.526133, mean_q: -25.155565, mean_eps: 0.104941\n",
            " 90566/100000: episode: 499, duration: 0.923s, episode steps:  95, steps per second: 103, episode reward: -94.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.768 [0.000, 2.000],  loss: 1.495300, mae: 17.595464, mean_q: -25.252832, mean_eps: 0.103872\n",
            " 90664/100000: episode: 500, duration: 1.026s, episode steps:  98, steps per second:  95, episode reward: -97.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.837 [0.000, 2.000],  loss: 1.731652, mae: 17.622123, mean_q: -25.230806, mean_eps: 0.102916\n",
            " 90770/100000: episode: 501, duration: 1.432s, episode steps: 106, steps per second:  74, episode reward: -105.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.774 [0.000, 2.000],  loss: 1.575438, mae: 17.630520, mean_q: -25.252929, mean_eps: 0.101907\n",
            " 90873/100000: episode: 502, duration: 1.394s, episode steps: 103, steps per second:  74, episode reward: -102.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.786 [0.000, 2.000],  loss: 1.539976, mae: 17.535720, mean_q: -25.159410, mean_eps: 0.100872\n",
            " 90993/100000: episode: 503, duration: 1.233s, episode steps: 120, steps per second:  97, episode reward: -119.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.725 [0.000, 2.000],  loss: 1.409615, mae: 17.593735, mean_q: -25.302959, mean_eps: 0.099768\n",
            " 91083/100000: episode: 504, duration: 0.879s, episode steps:  90, steps per second: 102, episode reward: -89.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.033 [0.000, 2.000],  loss: 1.659477, mae: 17.366435, mean_q: -24.921813, mean_eps: 0.098729\n",
            " 91179/100000: episode: 505, duration: 0.939s, episode steps:  96, steps per second: 102, episode reward: -95.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.188 [0.000, 2.000],  loss: 1.568536, mae: 17.361899, mean_q: -24.942584, mean_eps: 0.097808\n",
            " 91267/100000: episode: 506, duration: 0.873s, episode steps:  88, steps per second: 101, episode reward: -87.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.125 [0.000, 2.000],  loss: 1.492100, mae: 17.291055, mean_q: -24.832892, mean_eps: 0.096897\n",
            " 91355/100000: episode: 507, duration: 0.851s, episode steps:  88, steps per second: 103, episode reward: -87.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.989 [0.000, 2.000],  loss: 1.626470, mae: 17.299852, mean_q: -24.815697, mean_eps: 0.096026\n",
            " 91456/100000: episode: 508, duration: 0.961s, episode steps: 101, steps per second: 105, episode reward: -100.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.950 [0.000, 2.000],  loss: 1.544400, mae: 17.392670, mean_q: -24.959370, mean_eps: 0.095090\n",
            " 91544/100000: episode: 509, duration: 0.865s, episode steps:  88, steps per second: 102, episode reward: -87.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.852 [0.000, 2.000],  loss: 1.664607, mae: 17.159511, mean_q: -24.556754, mean_eps: 0.094155\n",
            " 91644/100000: episode: 510, duration: 0.970s, episode steps: 100, steps per second: 103, episode reward: -99.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.780 [0.000, 2.000],  loss: 1.586508, mae: 17.067524, mean_q: -24.435188, mean_eps: 0.093224\n",
            " 91729/100000: episode: 511, duration: 0.860s, episode steps:  85, steps per second:  99, episode reward: -84.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.894 [0.000, 2.000],  loss: 1.682678, mae: 17.072023, mean_q: -24.460919, mean_eps: 0.092309\n",
            " 91832/100000: episode: 512, duration: 1.015s, episode steps: 103, steps per second: 102, episode reward: -102.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.767 [0.000, 2.000],  loss: 1.514342, mae: 17.095777, mean_q: -24.482700, mean_eps: 0.091378\n",
            " 91908/100000: episode: 513, duration: 0.776s, episode steps:  76, steps per second:  98, episode reward: -75.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.961 [0.000, 2.000],  loss: 1.768228, mae: 17.195687, mean_q: -24.555810, mean_eps: 0.090492\n",
            " 91998/100000: episode: 514, duration: 1.340s, episode steps:  90, steps per second:  67, episode reward: -89.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.889 [0.000, 2.000],  loss: 1.560446, mae: 17.096161, mean_q: -24.509095, mean_eps: 0.089670\n",
            " 92120/100000: episode: 515, duration: 1.746s, episode steps: 122, steps per second:  70, episode reward: -121.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.967 [0.000, 2.000],  loss: 1.564195, mae: 16.822808, mean_q: -24.072749, mean_eps: 0.088621\n",
            " 92226/100000: episode: 516, duration: 1.252s, episode steps: 106, steps per second:  85, episode reward: -105.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.811 [0.000, 2.000],  loss: 1.567725, mae: 16.842425, mean_q: -24.129809, mean_eps: 0.087492\n",
            " 92334/100000: episode: 517, duration: 1.053s, episode steps: 108, steps per second: 103, episode reward: -107.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.907 [0.000, 2.000],  loss: 1.636073, mae: 16.913093, mean_q: -24.218801, mean_eps: 0.086433\n",
            " 92420/100000: episode: 518, duration: 0.870s, episode steps:  86, steps per second:  99, episode reward: -85.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.872 [0.000, 2.000],  loss: 1.749631, mae: 16.886477, mean_q: -24.121924, mean_eps: 0.085473\n",
            " 92542/100000: episode: 519, duration: 1.214s, episode steps: 122, steps per second: 101, episode reward: -121.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.811 [0.000, 2.000],  loss: 1.694291, mae: 16.932369, mean_q: -24.247554, mean_eps: 0.084443\n",
            " 92649/100000: episode: 520, duration: 1.056s, episode steps: 107, steps per second: 101, episode reward: -106.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.206 [0.000, 2.000],  loss: 1.478168, mae: 16.895550, mean_q: -24.229067, mean_eps: 0.083310\n",
            " 92744/100000: episode: 521, duration: 0.902s, episode steps:  95, steps per second: 105, episode reward: -94.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.916 [0.000, 2.000],  loss: 1.371308, mae: 16.995024, mean_q: -24.392816, mean_eps: 0.082310\n",
            " 92821/100000: episode: 522, duration: 0.733s, episode steps:  77, steps per second: 105, episode reward: -76.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.935 [0.000, 2.000],  loss: 1.503940, mae: 16.893006, mean_q: -24.244010, mean_eps: 0.081458\n",
            " 92926/100000: episode: 523, duration: 0.963s, episode steps: 105, steps per second: 109, episode reward: -104.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.057 [0.000, 2.000],  loss: 1.542266, mae: 16.982158, mean_q: -24.349105, mean_eps: 0.080557\n",
            " 93016/100000: episode: 524, duration: 0.843s, episode steps:  90, steps per second: 107, episode reward: -89.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.256 [0.000, 2.000],  loss: 1.403195, mae: 17.047724, mean_q: -24.485333, mean_eps: 0.079592\n",
            " 93113/100000: episode: 525, duration: 0.942s, episode steps:  97, steps per second: 103, episode reward: -96.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.103 [0.000, 2.000],  loss: 1.716475, mae: 17.016320, mean_q: -24.359129, mean_eps: 0.078666\n",
            " 93218/100000: episode: 526, duration: 1.093s, episode steps: 105, steps per second:  96, episode reward: -104.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.133 [0.000, 2.000],  loss: 1.557160, mae: 17.026447, mean_q: -24.454160, mean_eps: 0.077666\n",
            " 93295/100000: episode: 527, duration: 1.029s, episode steps:  77, steps per second:  75, episode reward: -76.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.195 [0.000, 2.000],  loss: 1.526479, mae: 16.750721, mean_q: -24.001751, mean_eps: 0.076766\n",
            " 93402/100000: episode: 528, duration: 1.394s, episode steps: 107, steps per second:  77, episode reward: -106.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.626 [0.000, 2.000],  loss: 1.562464, mae: 16.942047, mean_q: -24.289653, mean_eps: 0.075855\n",
            " 93508/100000: episode: 529, duration: 1.194s, episode steps: 106, steps per second:  89, episode reward: -105.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.934 [0.000, 2.000],  loss: 1.613064, mae: 16.726380, mean_q: -23.975217, mean_eps: 0.074800\n",
            " 93587/100000: episode: 530, duration: 0.772s, episode steps:  79, steps per second: 102, episode reward: -78.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.785 [0.000, 2.000],  loss: 1.763203, mae: 16.810296, mean_q: -24.048237, mean_eps: 0.073885\n",
            " 93650/100000: episode: 531, duration: 0.582s, episode steps:  63, steps per second: 108, episode reward: -62.000, mean reward: -0.984 [-1.000,  0.000], mean action: 1.032 [0.000, 2.000],  loss: 1.775208, mae: 16.687251, mean_q: -23.861116, mean_eps: 0.073182\n",
            " 93752/100000: episode: 532, duration: 0.973s, episode steps: 102, steps per second: 105, episode reward: -101.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.961 [0.000, 2.000],  loss: 1.501045, mae: 16.660947, mean_q: -23.851995, mean_eps: 0.072365\n",
            " 93847/100000: episode: 533, duration: 0.873s, episode steps:  95, steps per second: 109, episode reward: -94.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.874 [0.000, 2.000],  loss: 1.813463, mae: 16.553369, mean_q: -23.637279, mean_eps: 0.071390\n",
            " 93954/100000: episode: 534, duration: 1.018s, episode steps: 107, steps per second: 105, episode reward: -106.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.318 [0.000, 2.000],  loss: 1.676729, mae: 16.622501, mean_q: -23.765816, mean_eps: 0.070390\n",
            " 94040/100000: episode: 535, duration: 0.812s, episode steps:  86, steps per second: 106, episode reward: -85.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.942 [0.000, 2.000],  loss: 1.488112, mae: 16.478481, mean_q: -23.562432, mean_eps: 0.069435\n",
            " 94129/100000: episode: 536, duration: 0.832s, episode steps:  89, steps per second: 107, episode reward: -88.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.843 [0.000, 2.000],  loss: 1.437766, mae: 16.560759, mean_q: -23.659631, mean_eps: 0.068568\n",
            " 94250/100000: episode: 537, duration: 1.162s, episode steps: 121, steps per second: 104, episode reward: -120.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.132 [0.000, 2.000],  loss: 1.525748, mae: 16.507712, mean_q: -23.585915, mean_eps: 0.067529\n",
            " 94335/100000: episode: 538, duration: 0.823s, episode steps:  85, steps per second: 103, episode reward: -84.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.929 [0.000, 2.000],  loss: 1.636981, mae: 16.463245, mean_q: -23.513749, mean_eps: 0.066509\n",
            " 94448/100000: episode: 539, duration: 1.097s, episode steps: 113, steps per second: 103, episode reward: -112.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.628 [0.000, 2.000],  loss: 1.595497, mae: 16.478300, mean_q: -23.554986, mean_eps: 0.065529\n",
            " 94517/100000: episode: 540, duration: 0.718s, episode steps:  69, steps per second:  96, episode reward: -68.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.783 [0.000, 2.000],  loss: 1.516272, mae: 16.559808, mean_q: -23.711626, mean_eps: 0.064628\n",
            " 94615/100000: episode: 541, duration: 1.377s, episode steps:  98, steps per second:  71, episode reward: -97.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.878 [0.000, 2.000],  loss: 1.507926, mae: 16.675424, mean_q: -23.908084, mean_eps: 0.063802\n",
            " 94686/100000: episode: 542, duration: 0.953s, episode steps:  71, steps per second:  74, episode reward: -70.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.761 [0.000, 2.000],  loss: 1.397265, mae: 16.660423, mean_q: -23.890879, mean_eps: 0.062965\n",
            " 94761/100000: episode: 543, duration: 1.080s, episode steps:  75, steps per second:  69, episode reward: -74.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.933 [0.000, 2.000],  loss: 1.585117, mae: 16.727975, mean_q: -23.936269, mean_eps: 0.062242\n",
            " 94833/100000: episode: 544, duration: 0.696s, episode steps:  72, steps per second: 103, episode reward: -71.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.111 [0.000, 2.000],  loss: 1.575376, mae: 16.590365, mean_q: -23.686405, mean_eps: 0.061515\n",
            " 94930/100000: episode: 545, duration: 0.963s, episode steps:  97, steps per second: 101, episode reward: -96.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.794 [0.000, 2.000],  loss: 1.333600, mae: 16.552793, mean_q: -23.703356, mean_eps: 0.060678\n",
            " 95054/100000: episode: 546, duration: 1.249s, episode steps: 124, steps per second:  99, episode reward: -123.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.056 [0.000, 2.000],  loss: 1.253001, mae: 16.771096, mean_q: -24.066630, mean_eps: 0.059584\n",
            " 95162/100000: episode: 547, duration: 1.063s, episode steps: 108, steps per second: 102, episode reward: -107.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.981 [0.000, 2.000],  loss: 1.486966, mae: 16.777711, mean_q: -24.078111, mean_eps: 0.058436\n",
            " 95254/100000: episode: 548, duration: 0.883s, episode steps:  92, steps per second: 104, episode reward: -91.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.293 [0.000, 2.000],  loss: 1.376612, mae: 16.626533, mean_q: -23.895780, mean_eps: 0.057446\n",
            " 95355/100000: episode: 549, duration: 1.005s, episode steps: 101, steps per second: 100, episode reward: -100.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.307 [0.000, 2.000],  loss: 1.395727, mae: 16.534060, mean_q: -23.711378, mean_eps: 0.056490\n",
            " 95483/100000: episode: 550, duration: 1.235s, episode steps: 128, steps per second: 104, episode reward: -127.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.055 [0.000, 2.000],  loss: 1.456771, mae: 16.700227, mean_q: -23.969882, mean_eps: 0.055357\n",
            " 95575/100000: episode: 551, duration: 0.907s, episode steps:  92, steps per second: 101, episode reward: -91.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.109 [0.000, 2.000],  loss: 1.352043, mae: 16.828521, mean_q: -24.188041, mean_eps: 0.054268\n",
            " 95665/100000: episode: 552, duration: 0.868s, episode steps:  90, steps per second: 104, episode reward: -89.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.889 [0.000, 2.000],  loss: 1.310045, mae: 16.714819, mean_q: -23.988011, mean_eps: 0.053367\n",
            " 95750/100000: episode: 553, duration: 0.869s, episode steps:  85, steps per second:  98, episode reward: -84.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.976 [0.000, 2.000],  loss: 1.303411, mae: 16.733796, mean_q: -23.997234, mean_eps: 0.052501\n",
            " 95864/100000: episode: 554, duration: 1.479s, episode steps: 114, steps per second:  77, episode reward: -113.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.825 [0.000, 2.000],  loss: 1.283174, mae: 16.852689, mean_q: -24.192096, mean_eps: 0.051516\n",
            " 95944/100000: episode: 555, duration: 1.096s, episode steps:  80, steps per second:  73, episode reward: -79.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.900 [0.000, 2.000],  loss: 1.287321, mae: 16.904989, mean_q: -24.258522, mean_eps: 0.050555\n",
            " 96039/100000: episode: 556, duration: 1.367s, episode steps:  95, steps per second:  70, episode reward: -94.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.989 [0.000, 2.000],  loss: 1.255554, mae: 16.792090, mean_q: -24.126797, mean_eps: 0.049689\n",
            " 96175/100000: episode: 557, duration: 1.376s, episode steps: 136, steps per second:  99, episode reward: -135.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.978 [0.000, 2.000],  loss: 1.519786, mae: 16.753746, mean_q: -23.988637, mean_eps: 0.048546\n",
            " 96259/100000: episode: 558, duration: 0.870s, episode steps:  84, steps per second:  97, episode reward: -83.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.095 [0.000, 2.000],  loss: 1.320713, mae: 16.768428, mean_q: -24.022101, mean_eps: 0.047457\n",
            " 96341/100000: episode: 559, duration: 0.819s, episode steps:  82, steps per second: 100, episode reward: -81.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.817 [0.000, 2.000],  loss: 1.399192, mae: 16.836166, mean_q: -24.132692, mean_eps: 0.046635\n",
            " 96421/100000: episode: 560, duration: 0.799s, episode steps:  80, steps per second: 100, episode reward: -79.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.975 [0.000, 2.000],  loss: 1.257403, mae: 16.766585, mean_q: -24.069863, mean_eps: 0.045833\n",
            " 96527/100000: episode: 561, duration: 1.023s, episode steps: 106, steps per second: 104, episode reward: -105.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.217 [0.000, 2.000],  loss: 1.352178, mae: 16.769565, mean_q: -24.045456, mean_eps: 0.044912\n",
            " 96610/100000: episode: 562, duration: 0.931s, episode steps:  83, steps per second:  89, episode reward: -82.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.012 [0.000, 2.000],  loss: 1.368519, mae: 16.996612, mean_q: -24.432490, mean_eps: 0.043977\n",
            " 96704/100000: episode: 563, duration: 1.162s, episode steps:  94, steps per second:  81, episode reward: -93.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.085 [0.000, 2.000],  loss: 1.326176, mae: 16.921477, mean_q: -24.292832, mean_eps: 0.043101\n",
            " 96805/100000: episode: 564, duration: 1.269s, episode steps: 101, steps per second:  80, episode reward: -100.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 1.417687, mae: 16.882272, mean_q: -24.210615, mean_eps: 0.042135\n",
            " 96892/100000: episode: 565, duration: 3.556s, episode steps:  87, steps per second:  24, episode reward: -86.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.069 [0.000, 2.000],  loss: 1.499069, mae: 16.920754, mean_q: -24.192662, mean_eps: 0.041205\n",
            " 96982/100000: episode: 566, duration: 1.799s, episode steps:  90, steps per second:  50, episode reward: -89.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.144 [0.000, 2.000],  loss: 1.343045, mae: 17.009296, mean_q: -24.458733, mean_eps: 0.040329\n",
            " 97059/100000: episode: 567, duration: 0.994s, episode steps:  77, steps per second:  77, episode reward: -76.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.052 [0.000, 2.000],  loss: 1.436351, mae: 16.973400, mean_q: -24.367582, mean_eps: 0.039502\n",
            " 97161/100000: episode: 568, duration: 1.401s, episode steps: 102, steps per second:  73, episode reward: -101.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.020 [0.000, 2.000],  loss: 1.312519, mae: 17.002274, mean_q: -24.433992, mean_eps: 0.038616\n",
            " 97264/100000: episode: 569, duration: 1.023s, episode steps: 103, steps per second: 101, episode reward: -102.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.903 [0.000, 2.000],  loss: 1.317903, mae: 16.945728, mean_q: -24.319121, mean_eps: 0.037601\n",
            " 97363/100000: episode: 570, duration: 0.956s, episode steps:  99, steps per second: 104, episode reward: -98.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.848 [0.000, 2.000],  loss: 1.379044, mae: 17.014752, mean_q: -24.428996, mean_eps: 0.036601\n",
            " 97458/100000: episode: 571, duration: 0.936s, episode steps:  95, steps per second: 101, episode reward: -94.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.379 [0.000, 2.000],  loss: 1.334401, mae: 17.061138, mean_q: -24.493041, mean_eps: 0.035641\n",
            " 97572/100000: episode: 572, duration: 1.124s, episode steps: 114, steps per second: 101, episode reward: -113.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.140 [0.000, 2.000],  loss: 1.410172, mae: 16.974073, mean_q: -24.369516, mean_eps: 0.034606\n",
            " 97667/100000: episode: 573, duration: 0.938s, episode steps:  95, steps per second: 101, episode reward: -94.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.084 [0.000, 2.000],  loss: 1.338435, mae: 17.017558, mean_q: -24.433079, mean_eps: 0.033572\n",
            " 97777/100000: episode: 574, duration: 1.052s, episode steps: 110, steps per second: 105, episode reward: -109.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.064 [0.000, 2.000],  loss: 1.430517, mae: 17.069436, mean_q: -24.484822, mean_eps: 0.032557\n",
            " 97864/100000: episode: 575, duration: 0.842s, episode steps:  87, steps per second: 103, episode reward: -86.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.310 [0.000, 2.000],  loss: 1.320371, mae: 17.093840, mean_q: -24.533215, mean_eps: 0.031582\n",
            " 97960/100000: episode: 576, duration: 0.986s, episode steps:  96, steps per second:  97, episode reward: -95.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.167 [0.000, 2.000],  loss: 1.465038, mae: 17.087141, mean_q: -24.513511, mean_eps: 0.030676\n",
            " 98054/100000: episode: 577, duration: 0.936s, episode steps:  94, steps per second: 100, episode reward: -93.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.170 [0.000, 2.000],  loss: 1.403018, mae: 17.107575, mean_q: -24.586106, mean_eps: 0.029736\n",
            " 98160/100000: episode: 578, duration: 1.102s, episode steps: 106, steps per second:  96, episode reward: -105.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.943 [0.000, 2.000],  loss: 1.340282, mae: 17.158747, mean_q: -24.655259, mean_eps: 0.028746\n",
            " 98313/100000: episode: 579, duration: 2.083s, episode steps: 153, steps per second:  73, episode reward: -152.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.673 [0.000, 2.000],  loss: 1.313910, mae: 17.141721, mean_q: -24.668027, mean_eps: 0.027464\n",
            " 98411/100000: episode: 580, duration: 1.289s, episode steps:  98, steps per second:  76, episode reward: -97.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.918 [0.000, 2.000],  loss: 1.243385, mae: 17.219838, mean_q: -24.769730, mean_eps: 0.026221\n",
            " 98475/100000: episode: 581, duration: 0.622s, episode steps:  64, steps per second: 103, episode reward: -63.000, mean reward: -0.984 [-1.000,  0.000], mean action: 0.859 [0.000, 2.000],  loss: 1.517985, mae: 17.071150, mean_q: -24.478329, mean_eps: 0.025419\n",
            " 98577/100000: episode: 582, duration: 1.021s, episode steps: 102, steps per second: 100, episode reward: -101.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.010 [0.000, 2.000],  loss: 1.555515, mae: 17.447126, mean_q: -25.058958, mean_eps: 0.024598\n",
            " 98807/100000: episode: 583, duration: 2.153s, episode steps: 230, steps per second: 107, episode reward: -229.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.652 [0.000, 2.000],  loss: 1.400960, mae: 17.633776, mean_q: -25.378528, mean_eps: 0.022954\n",
            " 98891/100000: episode: 584, duration: 0.805s, episode steps:  84, steps per second: 104, episode reward: -83.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.036 [0.000, 2.000],  loss: 1.303674, mae: 17.598649, mean_q: -25.374579, mean_eps: 0.021400\n",
            " 98991/100000: episode: 585, duration: 0.930s, episode steps: 100, steps per second: 108, episode reward: -99.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.890 [0.000, 2.000],  loss: 1.380539, mae: 17.622219, mean_q: -25.351299, mean_eps: 0.020489\n",
            " 99095/100000: episode: 586, duration: 1.022s, episode steps: 104, steps per second: 102, episode reward: -103.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.952 [0.000, 2.000],  loss: 1.400851, mae: 17.692919, mean_q: -25.490816, mean_eps: 0.019479\n",
            " 99202/100000: episode: 587, duration: 1.032s, episode steps: 107, steps per second: 104, episode reward: -106.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.514 [0.000, 2.000],  loss: 1.324214, mae: 17.855660, mean_q: -25.695993, mean_eps: 0.018435\n",
            " 99301/100000: episode: 588, duration: 0.962s, episode steps:  99, steps per second: 103, episode reward: -98.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.889 [0.000, 2.000],  loss: 1.408662, mae: 17.988352, mean_q: -25.942715, mean_eps: 0.017415\n",
            " 99532/100000: episode: 589, duration: 2.487s, episode steps: 231, steps per second:  93, episode reward: -230.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.455 [0.000, 2.000],  loss: 1.354794, mae: 17.746344, mean_q: -25.512098, mean_eps: 0.015782\n",
            " 99626/100000: episode: 590, duration: 1.318s, episode steps:  94, steps per second:  71, episode reward: -93.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.819 [0.000, 2.000],  loss: 1.636366, mae: 17.879719, mean_q: -25.665945, mean_eps: 0.014173\n",
            " 99698/100000: episode: 591, duration: 1.007s, episode steps:  72, steps per second:  72, episode reward: -71.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.847 [0.000, 2.000],  loss: 1.495181, mae: 17.930942, mean_q: -25.747488, mean_eps: 0.013351\n",
            " 99781/100000: episode: 592, duration: 0.806s, episode steps:  83, steps per second: 103, episode reward: -82.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.976 [0.000, 2.000],  loss: 1.430148, mae: 17.927065, mean_q: -25.732829, mean_eps: 0.012584\n",
            " 99875/100000: episode: 593, duration: 0.890s, episode steps:  94, steps per second: 106, episode reward: -93.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.809 [0.000, 2.000],  loss: 1.436104, mae: 17.929641, mean_q: -25.770897, mean_eps: 0.011708\n",
            " 99965/100000: episode: 594, duration: 0.835s, episode steps:  90, steps per second: 108, episode reward: -89.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.756 [0.000, 2.000],  loss: 1.466984, mae: 17.947525, mean_q: -25.777437, mean_eps: 0.010797\n",
            "done, took 1002.361 seconds\n",
            "\n",
            "Evaluando Mejora 1...\n",
            "Mejora 1 → Recompensa media: -94.95 ± 13.70\n",
            "\n",
            "Entrenando Mejora 2...\n",
            "Training for 200000 steps ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    500/200000: episode: 1, duration: 0.808s, episode steps: 500, steps per second: 619, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.996 [0.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   1000/200000: episode: 2, duration: 0.498s, episode steps: 500, steps per second: 1004, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.952 [0.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   1500/200000: episode: 3, duration: 0.490s, episode steps: 500, steps per second: 1020, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.004 [0.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   2000/200000: episode: 4, duration: 0.519s, episode steps: 500, steps per second: 964, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.016 [0.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   2500/200000: episode: 5, duration: 0.476s, episode steps: 500, steps per second: 1051, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.012 [0.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   3000/200000: episode: 6, duration: 0.506s, episode steps: 500, steps per second: 988, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   3500/200000: episode: 7, duration: 0.500s, episode steps: 500, steps per second: 1001, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.968 [0.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   4000/200000: episode: 8, duration: 0.623s, episode steps: 500, steps per second: 803, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.048 [0.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   4500/200000: episode: 9, duration: 0.719s, episode steps: 500, steps per second: 696, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.962 [0.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   5000/200000: episode: 10, duration: 0.710s, episode steps: 500, steps per second: 704, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.008 [0.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   5500/200000: episode: 11, duration: 11.907s, episode steps: 500, steps per second:  42, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.054 [0.000, 2.000],  loss: 0.029995, mae: 0.933024, mean_q: -1.256795, mean_eps: 0.974275\n",
            "   6000/200000: episode: 12, duration: 10.887s, episode steps: 500, steps per second:  46, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.026 [0.000, 2.000],  loss: 0.035604, mae: 2.067143, mean_q: -2.957794, mean_eps: 0.971827\n",
            "   6500/200000: episode: 13, duration: 10.439s, episode steps: 500, steps per second:  48, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.002 [0.000, 2.000],  loss: 0.039070, mae: 2.837654, mean_q: -4.113577, mean_eps: 0.969377\n",
            "   7000/200000: episode: 14, duration: 10.501s, episode steps: 500, steps per second:  48, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.908 [0.000, 2.000],  loss: 0.034616, mae: 3.268769, mean_q: -4.773091, mean_eps: 0.966927\n",
            "   7500/200000: episode: 15, duration: 9.085s, episode steps: 500, steps per second:  55, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.956 [0.000, 2.000],  loss: 0.035875, mae: 3.701626, mean_q: -5.433355, mean_eps: 0.964477\n",
            "   8000/200000: episode: 16, duration: 10.700s, episode steps: 500, steps per second:  47, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.046 [0.000, 2.000],  loss: 0.040661, mae: 4.320723, mean_q: -6.366715, mean_eps: 0.962027\n",
            "   8500/200000: episode: 17, duration: 10.529s, episode steps: 500, steps per second:  47, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.956 [0.000, 2.000],  loss: 0.050582, mae: 5.055273, mean_q: -7.467463, mean_eps: 0.959577\n",
            "   9000/200000: episode: 18, duration: 10.532s, episode steps: 500, steps per second:  47, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.982 [0.000, 2.000],  loss: 0.064896, mae: 5.905621, mean_q: -8.731554, mean_eps: 0.957127\n",
            "   9500/200000: episode: 19, duration: 9.295s, episode steps: 500, steps per second:  54, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.024 [0.000, 2.000],  loss: 0.087747, mae: 6.785276, mean_q: -10.039667, mean_eps: 0.954677\n",
            "  10000/200000: episode: 20, duration: 10.886s, episode steps: 500, steps per second:  46, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.052 [0.000, 2.000],  loss: 0.106119, mae: 7.703075, mean_q: -11.406007, mean_eps: 0.952227\n",
            "  10500/200000: episode: 21, duration: 10.991s, episode steps: 500, steps per second:  45, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.936 [0.000, 2.000],  loss: 0.136525, mae: 8.637558, mean_q: -12.786256, mean_eps: 0.949777\n",
            "  11000/200000: episode: 22, duration: 10.557s, episode steps: 500, steps per second:  47, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.014 [0.000, 2.000],  loss: 0.157967, mae: 9.520634, mean_q: -14.092987, mean_eps: 0.947327\n",
            "  11500/200000: episode: 23, duration: 9.680s, episode steps: 500, steps per second:  52, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.028 [0.000, 2.000],  loss: 0.174668, mae: 10.441735, mean_q: -15.452356, mean_eps: 0.944877\n",
            "  12000/200000: episode: 24, duration: 10.407s, episode steps: 500, steps per second:  48, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.016 [0.000, 2.000],  loss: 0.243036, mae: 11.264516, mean_q: -16.661876, mean_eps: 0.942427\n",
            "  12500/200000: episode: 25, duration: 10.583s, episode steps: 500, steps per second:  47, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.968 [0.000, 2.000],  loss: 0.263807, mae: 12.055255, mean_q: -17.832330, mean_eps: 0.939977\n",
            "  13000/200000: episode: 26, duration: 10.821s, episode steps: 500, steps per second:  46, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.996 [0.000, 2.000],  loss: 0.308683, mae: 12.781773, mean_q: -18.910505, mean_eps: 0.937527\n",
            "  13500/200000: episode: 27, duration: 9.769s, episode steps: 500, steps per second:  51, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.992 [0.000, 2.000],  loss: 0.341903, mae: 13.484734, mean_q: -19.954393, mean_eps: 0.935077\n",
            "  14000/200000: episode: 28, duration: 10.080s, episode steps: 500, steps per second:  50, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.922 [0.000, 2.000],  loss: 0.359983, mae: 14.254163, mean_q: -21.098813, mean_eps: 0.932627\n",
            "  14500/200000: episode: 29, duration: 10.578s, episode steps: 500, steps per second:  47, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.028 [0.000, 2.000],  loss: 0.419342, mae: 14.799712, mean_q: -21.869732, mean_eps: 0.930177\n",
            "  14937/200000: episode: 30, duration: 9.422s, episode steps: 437, steps per second:  46, episode reward: -436.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.986 [0.000, 2.000],  loss: 0.409262, mae: 15.387558, mean_q: -22.720099, mean_eps: 0.927882\n",
            "  15437/200000: episode: 31, duration: 9.786s, episode steps: 500, steps per second:  51, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.460397, mae: 15.802739, mean_q: -23.350441, mean_eps: 0.925586\n",
            "  15937/200000: episode: 32, duration: 10.500s, episode steps: 500, steps per second:  48, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.026 [0.000, 2.000],  loss: 0.488732, mae: 16.297749, mean_q: -24.051165, mean_eps: 0.923136\n",
            "  16437/200000: episode: 33, duration: 10.724s, episode steps: 500, steps per second:  47, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.016 [0.000, 2.000],  loss: 0.525357, mae: 16.696858, mean_q: -24.611427, mean_eps: 0.920686\n",
            "  16937/200000: episode: 34, duration: 10.735s, episode steps: 500, steps per second:  47, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.506835, mae: 16.996958, mean_q: -25.067265, mean_eps: 0.918236\n",
            "  17406/200000: episode: 35, duration: 9.450s, episode steps: 469, steps per second:  50, episode reward: -468.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.985 [0.000, 2.000],  loss: 0.536149, mae: 17.296208, mean_q: -25.485277, mean_eps: 0.915862\n",
            "  17906/200000: episode: 36, duration: 10.699s, episode steps: 500, steps per second:  47, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.962 [0.000, 2.000],  loss: 0.504774, mae: 17.475060, mean_q: -25.737546, mean_eps: 0.913488\n",
            "  18406/200000: episode: 37, duration: 10.586s, episode steps: 500, steps per second:  47, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.044 [0.000, 2.000],  loss: 0.549231, mae: 17.738695, mean_q: -26.155659, mean_eps: 0.911038\n",
            "  18906/200000: episode: 38, duration: 10.741s, episode steps: 500, steps per second:  47, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.032 [0.000, 2.000],  loss: 0.536483, mae: 18.114870, mean_q: -26.707749, mean_eps: 0.908588\n",
            "  19318/200000: episode: 39, duration: 8.276s, episode steps: 412, steps per second:  50, episode reward: -411.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.053 [0.000, 2.000],  loss: 0.620195, mae: 18.331006, mean_q: -27.020095, mean_eps: 0.906354\n",
            "  19818/200000: episode: 40, duration: 10.655s, episode steps: 500, steps per second:  47, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.569244, mae: 18.561928, mean_q: -27.392647, mean_eps: 0.904119\n",
            "  20318/200000: episode: 41, duration: 11.184s, episode steps: 500, steps per second:  45, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.982 [0.000, 2.000],  loss: 0.615803, mae: 18.835670, mean_q: -27.793273, mean_eps: 0.901669\n",
            "  20818/200000: episode: 42, duration: 10.833s, episode steps: 500, steps per second:  46, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.693167, mae: 19.209791, mean_q: -28.330501, mean_eps: 0.899219\n",
            "  21318/200000: episode: 43, duration: 10.969s, episode steps: 500, steps per second:  46, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.934 [0.000, 2.000],  loss: 0.813003, mae: 19.635292, mean_q: -28.986022, mean_eps: 0.896769\n",
            "  21818/200000: episode: 44, duration: 9.681s, episode steps: 500, steps per second:  52, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.906 [0.000, 2.000],  loss: 0.775642, mae: 20.047365, mean_q: -29.575890, mean_eps: 0.894319\n",
            "  22318/200000: episode: 45, duration: 11.376s, episode steps: 500, steps per second:  44, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.002 [0.000, 2.000],  loss: 0.885034, mae: 20.285843, mean_q: -29.893393, mean_eps: 0.891869\n",
            "  22818/200000: episode: 46, duration: 11.118s, episode steps: 500, steps per second:  45, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.008 [0.000, 2.000],  loss: 0.822032, mae: 20.335360, mean_q: -29.978681, mean_eps: 0.889419\n",
            "  23318/200000: episode: 47, duration: 11.420s, episode steps: 500, steps per second:  44, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.064 [0.000, 2.000],  loss: 0.846263, mae: 20.471700, mean_q: -30.169556, mean_eps: 0.886969\n",
            "  23745/200000: episode: 48, duration: 9.033s, episode steps: 427, steps per second:  47, episode reward: -426.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.988 [0.000, 2.000],  loss: 0.795813, mae: 20.602850, mean_q: -30.360764, mean_eps: 0.884698\n",
            "  24135/200000: episode: 49, duration: 8.443s, episode steps: 390, steps per second:  46, episode reward: -389.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.005 [0.000, 2.000],  loss: 0.837869, mae: 20.741407, mean_q: -30.573300, mean_eps: 0.882696\n",
            "  24635/200000: episode: 50, duration: 11.011s, episode steps: 500, steps per second:  45, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.994 [0.000, 2.000],  loss: 0.861515, mae: 20.837683, mean_q: -30.695155, mean_eps: 0.880516\n",
            "  25135/200000: episode: 51, duration: 11.330s, episode steps: 500, steps per second:  44, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.022 [0.000, 2.000],  loss: 0.931942, mae: 20.937799, mean_q: -30.823249, mean_eps: 0.878066\n",
            "  25462/200000: episode: 52, duration: 6.376s, episode steps: 327, steps per second:  51, episode reward: -326.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.092 [0.000, 2.000],  loss: 0.915022, mae: 21.009370, mean_q: -30.936511, mean_eps: 0.876040\n",
            "  25962/200000: episode: 53, duration: 11.114s, episode steps: 500, steps per second:  45, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.944 [0.000, 2.000],  loss: 0.885320, mae: 21.078998, mean_q: -31.043413, mean_eps: 0.874014\n",
            "  26236/200000: episode: 54, duration: 5.898s, episode steps: 274, steps per second:  46, episode reward: -273.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.941070, mae: 21.173933, mean_q: -31.156299, mean_eps: 0.872117\n",
            "  26708/200000: episode: 55, duration: 10.138s, episode steps: 472, steps per second:  47, episode reward: -471.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.015 [0.000, 2.000],  loss: 0.918579, mae: 21.153734, mean_q: -31.118817, mean_eps: 0.870290\n",
            "  27131/200000: episode: 56, duration: 9.935s, episode steps: 423, steps per second:  43, episode reward: -422.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.021 [0.000, 2.000],  loss: 0.849755, mae: 21.144653, mean_q: -31.123090, mean_eps: 0.868097\n",
            "  27631/200000: episode: 57, duration: 11.121s, episode steps: 500, steps per second:  45, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.028 [0.000, 2.000],  loss: 0.892052, mae: 21.160737, mean_q: -31.162989, mean_eps: 0.865836\n",
            "  28131/200000: episode: 58, duration: 10.606s, episode steps: 500, steps per second:  47, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.869684, mae: 21.173805, mean_q: -31.185108, mean_eps: 0.863386\n",
            "  28427/200000: episode: 59, duration: 6.416s, episode steps: 296, steps per second:  46, episode reward: -295.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.986 [0.000, 2.000],  loss: 0.825783, mae: 21.185097, mean_q: -31.195137, mean_eps: 0.861435\n",
            "  28927/200000: episode: 60, duration: 11.154s, episode steps: 500, steps per second:  45, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.994 [0.000, 2.000],  loss: 0.858597, mae: 21.226459, mean_q: -31.263672, mean_eps: 0.859485\n",
            "  29427/200000: episode: 61, duration: 10.913s, episode steps: 500, steps per second:  46, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.008 [0.000, 2.000],  loss: 0.869437, mae: 21.308822, mean_q: -31.407409, mean_eps: 0.857035\n",
            "  29927/200000: episode: 62, duration: 10.589s, episode steps: 500, steps per second:  47, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.956 [0.000, 2.000],  loss: 0.907609, mae: 21.495528, mean_q: -31.694681, mean_eps: 0.854585\n",
            "  30427/200000: episode: 63, duration: 11.076s, episode steps: 500, steps per second:  45, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.996 [0.000, 2.000],  loss: 0.844871, mae: 21.645133, mean_q: -31.911354, mean_eps: 0.852135\n",
            "  30927/200000: episode: 64, duration: 11.064s, episode steps: 500, steps per second:  45, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.968 [0.000, 2.000],  loss: 0.827054, mae: 21.664409, mean_q: -31.948110, mean_eps: 0.849685\n",
            "  31179/200000: episode: 65, duration: 5.402s, episode steps: 252, steps per second:  47, episode reward: -251.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.960 [0.000, 2.000],  loss: 0.877298, mae: 21.713902, mean_q: -32.036406, mean_eps: 0.847843\n",
            "  31613/200000: episode: 66, duration: 13.599s, episode steps: 434, steps per second:  32, episode reward: -433.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.988 [0.000, 2.000],  loss: 0.887959, mae: 21.755194, mean_q: -32.097791, mean_eps: 0.846162\n",
            "  32065/200000: episode: 67, duration: 10.351s, episode steps: 452, steps per second:  44, episode reward: -451.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.064 [0.000, 2.000],  loss: 0.894349, mae: 21.774376, mean_q: -32.103632, mean_eps: 0.843991\n",
            "  32565/200000: episode: 68, duration: 11.361s, episode steps: 500, steps per second:  44, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.944 [0.000, 2.000],  loss: 0.941692, mae: 21.780937, mean_q: -32.103305, mean_eps: 0.841659\n",
            "  33065/200000: episode: 69, duration: 11.502s, episode steps: 500, steps per second:  43, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.996972, mae: 21.799287, mean_q: -32.141332, mean_eps: 0.839209\n",
            "  33565/200000: episode: 70, duration: 11.745s, episode steps: 500, steps per second:  43, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.971696, mae: 21.867007, mean_q: -32.247416, mean_eps: 0.836759\n",
            "  34065/200000: episode: 71, duration: 11.418s, episode steps: 500, steps per second:  44, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.995040, mae: 22.037250, mean_q: -32.479906, mean_eps: 0.834309\n",
            "  34565/200000: episode: 72, duration: 11.132s, episode steps: 500, steps per second:  45, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.992 [0.000, 2.000],  loss: 0.946990, mae: 22.116894, mean_q: -32.607330, mean_eps: 0.831859\n",
            "  35065/200000: episode: 73, duration: 11.493s, episode steps: 500, steps per second:  44, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.038 [0.000, 2.000],  loss: 0.975261, mae: 22.110942, mean_q: -32.603323, mean_eps: 0.829409\n",
            "  35381/200000: episode: 74, duration: 7.954s, episode steps: 316, steps per second:  40, episode reward: -315.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.095 [0.000, 2.000],  loss: 0.920458, mae: 22.073128, mean_q: -32.531902, mean_eps: 0.827410\n",
            "  35634/200000: episode: 75, duration: 5.337s, episode steps: 253, steps per second:  47, episode reward: -252.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.036 [0.000, 2.000],  loss: 0.935638, mae: 22.081896, mean_q: -32.560476, mean_eps: 0.826016\n",
            "  36003/200000: episode: 76, duration: 9.285s, episode steps: 369, steps per second:  40, episode reward: -368.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.927 [0.000, 2.000],  loss: 0.924932, mae: 22.042958, mean_q: -32.518309, mean_eps: 0.824492\n",
            "  36203/200000: episode: 77, duration: 4.246s, episode steps: 200, steps per second:  47, episode reward: -199.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.950 [0.000, 2.000],  loss: 0.962616, mae: 22.138634, mean_q: -32.653831, mean_eps: 0.823098\n",
            "  36703/200000: episode: 78, duration: 11.709s, episode steps: 500, steps per second:  43, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.905020, mae: 22.243122, mean_q: -32.830048, mean_eps: 0.821383\n",
            "  37012/200000: episode: 79, duration: 8.039s, episode steps: 309, steps per second:  38, episode reward: -308.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.023 [0.000, 2.000],  loss: 0.898914, mae: 22.389894, mean_q: -33.059709, mean_eps: 0.819401\n",
            "  37512/200000: episode: 80, duration: 11.236s, episode steps: 500, steps per second:  45, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.098 [0.000, 2.000],  loss: 0.929396, mae: 22.443993, mean_q: -33.139591, mean_eps: 0.817419\n",
            "  37892/200000: episode: 81, duration: 8.737s, episode steps: 380, steps per second:  43, episode reward: -379.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.013 [0.000, 2.000],  loss: 0.920232, mae: 22.613046, mean_q: -33.420730, mean_eps: 0.815263\n",
            "  38388/200000: episode: 82, duration: 11.764s, episode steps: 496, steps per second:  42, episode reward: -495.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.046 [0.000, 2.000],  loss: 1.003748, mae: 22.683213, mean_q: -33.510614, mean_eps: 0.813116\n",
            "  38679/200000: episode: 83, duration: 6.917s, episode steps: 291, steps per second:  42, episode reward: -290.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.048 [0.000, 2.000],  loss: 0.941382, mae: 22.775225, mean_q: -33.642915, mean_eps: 0.811188\n",
            "  38949/200000: episode: 84, duration: 6.492s, episode steps: 270, steps per second:  42, episode reward: -269.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.052 [0.000, 2.000],  loss: 1.079724, mae: 22.904510, mean_q: -33.830201, mean_eps: 0.809814\n",
            "  39348/200000: episode: 85, duration: 9.816s, episode steps: 399, steps per second:  41, episode reward: -398.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.942 [0.000, 2.000],  loss: 1.064389, mae: 22.942358, mean_q: -33.886123, mean_eps: 0.808175\n",
            "  39848/200000: episode: 86, duration: 11.317s, episode steps: 500, steps per second:  44, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.984 [0.000, 2.000],  loss: 1.103383, mae: 23.024896, mean_q: -34.024426, mean_eps: 0.805972\n",
            "  40348/200000: episode: 87, duration: 11.057s, episode steps: 500, steps per second:  45, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.002 [0.000, 2.000],  loss: 1.067013, mae: 23.188121, mean_q: -34.252958, mean_eps: 0.803522\n",
            "  40792/200000: episode: 88, duration: 10.798s, episode steps: 444, steps per second:  41, episode reward: -443.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.955 [0.000, 2.000],  loss: 1.177541, mae: 23.215192, mean_q: -34.253877, mean_eps: 0.801209\n",
            "  41267/200000: episode: 89, duration: 11.283s, episode steps: 475, steps per second:  42, episode reward: -474.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.061 [0.000, 2.000],  loss: 1.169834, mae: 23.258157, mean_q: -34.305039, mean_eps: 0.798958\n",
            "  41767/200000: episode: 90, duration: 12.055s, episode steps: 500, steps per second:  41, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.968 [0.000, 2.000],  loss: 1.070358, mae: 23.322739, mean_q: -34.395670, mean_eps: 0.796569\n",
            "  42266/200000: episode: 91, duration: 11.822s, episode steps: 499, steps per second:  42, episode reward: -498.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.040 [0.000, 2.000],  loss: 1.140733, mae: 23.325527, mean_q: -34.396270, mean_eps: 0.794122\n",
            "  42637/200000: episode: 92, duration: 8.049s, episode steps: 371, steps per second:  46, episode reward: -370.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.995 [0.000, 2.000],  loss: 1.136800, mae: 23.239490, mean_q: -34.279811, mean_eps: 0.791990\n",
            "  43042/200000: episode: 93, duration: 10.272s, episode steps: 405, steps per second:  39, episode reward: -404.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.064 [0.000, 2.000],  loss: 1.158066, mae: 23.201407, mean_q: -34.218012, mean_eps: 0.790089\n",
            "  43401/200000: episode: 94, duration: 8.974s, episode steps: 359, steps per second:  40, episode reward: -358.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.028 [0.000, 2.000],  loss: 1.080528, mae: 23.258331, mean_q: -34.334759, mean_eps: 0.788217\n",
            "  43762/200000: episode: 95, duration: 7.896s, episode steps: 361, steps per second:  46, episode reward: -360.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.028 [0.000, 2.000],  loss: 1.102400, mae: 23.319675, mean_q: -34.414404, mean_eps: 0.786453\n",
            "  44085/200000: episode: 96, duration: 8.500s, episode steps: 323, steps per second:  38, episode reward: -322.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.012 [0.000, 2.000],  loss: 1.064353, mae: 23.329965, mean_q: -34.423001, mean_eps: 0.784777\n",
            "  44417/200000: episode: 97, duration: 7.807s, episode steps: 332, steps per second:  43, episode reward: -331.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.006 [0.000, 2.000],  loss: 1.114053, mae: 23.355819, mean_q: -34.460505, mean_eps: 0.783173\n",
            "  44724/200000: episode: 98, duration: 6.920s, episode steps: 307, steps per second:  44, episode reward: -306.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 1.118018, mae: 23.508380, mean_q: -34.703764, mean_eps: 0.781607\n",
            "  45135/200000: episode: 99, duration: 9.973s, episode steps: 411, steps per second:  41, episode reward: -410.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.100 [0.000, 2.000],  loss: 1.125231, mae: 23.538877, mean_q: -34.756168, mean_eps: 0.779848\n",
            "  45441/200000: episode: 100, duration: 6.516s, episode steps: 306, steps per second:  47, episode reward: -305.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.993 [0.000, 2.000],  loss: 1.117175, mae: 23.661037, mean_q: -34.944744, mean_eps: 0.778091\n",
            "  45941/200000: episode: 101, duration: 11.862s, episode steps: 500, steps per second:  42, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.044 [0.000, 2.000],  loss: 1.196316, mae: 23.706293, mean_q: -34.992620, mean_eps: 0.776117\n",
            "  46381/200000: episode: 102, duration: 10.753s, episode steps: 440, steps per second:  41, episode reward: -439.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.952 [0.000, 2.000],  loss: 1.196617, mae: 23.787830, mean_q: -35.120730, mean_eps: 0.773814\n",
            "  46707/200000: episode: 103, duration: 8.195s, episode steps: 326, steps per second:  40, episode reward: -325.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.951 [0.000, 2.000],  loss: 1.325579, mae: 23.858158, mean_q: -35.210826, mean_eps: 0.771937\n",
            "  47001/200000: episode: 104, duration: 6.743s, episode steps: 294, steps per second:  44, episode reward: -293.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.990 [0.000, 2.000],  loss: 1.051128, mae: 23.855103, mean_q: -35.231998, mean_eps: 0.770418\n",
            "  47396/200000: episode: 105, duration: 9.816s, episode steps: 395, steps per second:  40, episode reward: -394.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.932 [0.000, 2.000],  loss: 1.247301, mae: 23.911360, mean_q: -35.317891, mean_eps: 0.768730\n",
            "  47896/200000: episode: 106, duration: 12.267s, episode steps: 500, steps per second:  41, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.944 [0.000, 2.000],  loss: 1.218414, mae: 23.972071, mean_q: -35.402777, mean_eps: 0.766537\n",
            "  48396/200000: episode: 107, duration: 12.208s, episode steps: 500, steps per second:  41, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.078 [0.000, 2.000],  loss: 1.240213, mae: 24.114563, mean_q: -35.629459, mean_eps: 0.764087\n",
            "  48835/200000: episode: 108, duration: 9.910s, episode steps: 439, steps per second:  44, episode reward: -438.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.995 [0.000, 2.000],  loss: 1.264013, mae: 24.118847, mean_q: -35.627817, mean_eps: 0.761786\n",
            "  49320/200000: episode: 109, duration: 12.176s, episode steps: 485, steps per second:  40, episode reward: -484.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.078 [0.000, 2.000],  loss: 1.240880, mae: 24.129599, mean_q: -35.641071, mean_eps: 0.759523\n",
            "  49665/200000: episode: 110, duration: 8.769s, episode steps: 345, steps per second:  39, episode reward: -344.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.994 [0.000, 2.000],  loss: 1.240079, mae: 24.160861, mean_q: -35.701956, mean_eps: 0.757489\n",
            "  49893/200000: episode: 111, duration: 5.000s, episode steps: 228, steps per second:  46, episode reward: -227.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.996 [0.000, 2.000],  loss: 1.305033, mae: 24.130674, mean_q: -35.650933, mean_eps: 0.756085\n",
            "  50329/200000: episode: 112, duration: 10.665s, episode steps: 436, steps per second:  41, episode reward: -435.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.972 [0.000, 2.000],  loss: 1.292581, mae: 24.161380, mean_q: -35.707842, mean_eps: 0.754459\n",
            "  50644/200000: episode: 113, duration: 8.131s, episode steps: 315, steps per second:  39, episode reward: -314.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.079 [0.000, 2.000],  loss: 1.292255, mae: 24.290317, mean_q: -35.881718, mean_eps: 0.752619\n",
            "  50935/200000: episode: 114, duration: 6.478s, episode steps: 291, steps per second:  45, episode reward: -290.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.072 [0.000, 2.000],  loss: 1.478452, mae: 24.287158, mean_q: -35.863550, mean_eps: 0.751134\n",
            "  51303/200000: episode: 115, duration: 9.810s, episode steps: 368, steps per second:  38, episode reward: -367.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.946 [0.000, 2.000],  loss: 1.367688, mae: 24.285448, mean_q: -35.849480, mean_eps: 0.749519\n",
            "  51729/200000: episode: 116, duration: 10.857s, episode steps: 426, steps per second:  39, episode reward: -425.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.007 [0.000, 2.000],  loss: 1.265329, mae: 24.201516, mean_q: -35.705012, mean_eps: 0.747574\n",
            "  52017/200000: episode: 117, duration: 6.405s, episode steps: 288, steps per second:  45, episode reward: -287.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.052 [0.000, 2.000],  loss: 1.516635, mae: 24.166865, mean_q: -35.632773, mean_eps: 0.745825\n",
            "  52511/200000: episode: 118, duration: 12.170s, episode steps: 494, steps per second:  41, episode reward: -493.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.945 [0.000, 2.000],  loss: 1.456682, mae: 24.067152, mean_q: -35.491635, mean_eps: 0.743909\n",
            "  53011/200000: episode: 119, duration: 12.195s, episode steps: 500, steps per second:  41, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 1.438911, mae: 24.008163, mean_q: -35.383636, mean_eps: 0.741474\n",
            "  53413/200000: episode: 120, duration: 10.164s, episode steps: 402, steps per second:  40, episode reward: -401.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.037 [0.000, 2.000],  loss: 1.538539, mae: 23.941555, mean_q: -35.252176, mean_eps: 0.739264\n",
            "  53751/200000: episode: 121, duration: 7.997s, episode steps: 338, steps per second:  42, episode reward: -337.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.098 [0.000, 2.000],  loss: 1.445753, mae: 23.893481, mean_q: -35.154884, mean_eps: 0.737451\n",
            "  54062/200000: episode: 122, duration: 8.325s, episode steps: 311, steps per second:  37, episode reward: -310.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.032 [0.000, 2.000],  loss: 1.464987, mae: 23.808012, mean_q: -35.018580, mean_eps: 0.735861\n",
            "  54376/200000: episode: 123, duration: 7.099s, episode steps: 314, steps per second:  44, episode reward: -313.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.952 [0.000, 2.000],  loss: 1.371419, mae: 23.682032, mean_q: -34.842480, mean_eps: 0.734329\n",
            "  54638/200000: episode: 124, duration: 6.896s, episode steps: 262, steps per second:  38, episode reward: -261.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.977 [0.000, 2.000],  loss: 1.432957, mae: 23.556936, mean_q: -34.650199, mean_eps: 0.732918\n",
            "  55077/200000: episode: 125, duration: 10.819s, episode steps: 439, steps per second:  41, episode reward: -438.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.954 [0.000, 2.000],  loss: 1.401256, mae: 23.297202, mean_q: -34.236573, mean_eps: 0.731201\n",
            "  55447/200000: episode: 126, duration: 8.068s, episode steps: 370, steps per second:  46, episode reward: -369.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.949 [0.000, 2.000],  loss: 1.370459, mae: 23.010919, mean_q: -33.809601, mean_eps: 0.729219\n",
            "  55774/200000: episode: 127, duration: 8.345s, episode steps: 327, steps per second:  39, episode reward: -326.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.966 [0.000, 2.000],  loss: 1.304712, mae: 22.772124, mean_q: -33.440353, mean_eps: 0.727511\n",
            "  56120/200000: episode: 128, duration: 8.707s, episode steps: 346, steps per second:  40, episode reward: -345.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.942 [0.000, 2.000],  loss: 1.327596, mae: 22.495720, mean_q: -33.020125, mean_eps: 0.725862\n",
            "  56473/200000: episode: 129, duration: 8.206s, episode steps: 353, steps per second:  43, episode reward: -352.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.102 [0.000, 2.000],  loss: 1.310532, mae: 22.220593, mean_q: -32.596903, mean_eps: 0.724150\n",
            "  56919/200000: episode: 130, duration: 10.918s, episode steps: 446, steps per second:  41, episode reward: -445.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.993 [0.000, 2.000],  loss: 1.319883, mae: 21.859131, mean_q: -32.059741, mean_eps: 0.722192\n",
            "  57181/200000: episode: 131, duration: 6.053s, episode steps: 262, steps per second:  43, episode reward: -261.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.065 [0.000, 2.000],  loss: 1.146716, mae: 21.533963, mean_q: -31.585788, mean_eps: 0.720457\n",
            "  57463/200000: episode: 132, duration: 7.323s, episode steps: 282, steps per second:  39, episode reward: -281.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.032 [0.000, 2.000],  loss: 1.193640, mae: 21.315572, mean_q: -31.272959, mean_eps: 0.719125\n",
            "  57947/200000: episode: 133, duration: 12.044s, episode steps: 484, steps per second:  40, episode reward: -483.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.081 [0.000, 2.000],  loss: 1.145045, mae: 20.866439, mean_q: -30.573080, mean_eps: 0.717248\n",
            "  58207/200000: episode: 134, duration: 5.695s, episode steps: 260, steps per second:  46, episode reward: -259.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.069 [0.000, 2.000],  loss: 1.139467, mae: 20.566450, mean_q: -30.125020, mean_eps: 0.715425\n",
            "  58443/200000: episode: 135, duration: 6.450s, episode steps: 236, steps per second:  37, episode reward: -235.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.076 [0.000, 2.000],  loss: 1.060010, mae: 20.363895, mean_q: -29.828166, mean_eps: 0.714210\n",
            "  58806/200000: episode: 136, duration: 8.588s, episode steps: 363, steps per second:  42, episode reward: -362.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.072 [0.000, 2.000],  loss: 1.074825, mae: 20.114402, mean_q: -29.459424, mean_eps: 0.712742\n",
            "  59063/200000: episode: 137, duration: 6.916s, episode steps: 257, steps per second:  37, episode reward: -256.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.117 [0.000, 2.000],  loss: 1.020164, mae: 19.897865, mean_q: -29.155487, mean_eps: 0.711223\n",
            "  59403/200000: episode: 138, duration: 8.265s, episode steps: 340, steps per second:  41, episode reward: -339.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.032 [0.000, 2.000],  loss: 1.074057, mae: 19.654745, mean_q: -28.794569, mean_eps: 0.709761\n",
            "  59710/200000: episode: 139, duration: 7.348s, episode steps: 307, steps per second:  42, episode reward: -306.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.033 [0.000, 2.000],  loss: 1.071424, mae: 19.503142, mean_q: -28.584960, mean_eps: 0.708176\n",
            "  59947/200000: episode: 140, duration: 5.739s, episode steps: 237, steps per second:  41, episode reward: -236.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.063 [0.000, 2.000],  loss: 0.898658, mae: 19.313675, mean_q: -28.301822, mean_eps: 0.706843\n",
            "  60198/200000: episode: 141, duration: 6.359s, episode steps: 251, steps per second:  39, episode reward: -250.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.092 [0.000, 2.000],  loss: 0.915006, mae: 19.170483, mean_q: -28.097229, mean_eps: 0.705647\n",
            "  60496/200000: episode: 142, duration: 6.975s, episode steps: 298, steps per second:  43, episode reward: -297.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.030 [0.000, 2.000],  loss: 0.893761, mae: 19.058946, mean_q: -27.934304, mean_eps: 0.704302\n",
            "  60774/200000: episode: 143, duration: 7.096s, episode steps: 278, steps per second:  39, episode reward: -277.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.018 [0.000, 2.000],  loss: 0.880340, mae: 19.006546, mean_q: -27.879284, mean_eps: 0.702891\n",
            "  61155/200000: episode: 144, duration: 9.941s, episode steps: 381, steps per second:  38, episode reward: -380.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.024 [0.000, 2.000],  loss: 0.874894, mae: 18.946496, mean_q: -27.789612, mean_eps: 0.701276\n",
            "  61523/200000: episode: 145, duration: 8.079s, episode steps: 368, steps per second:  46, episode reward: -367.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.054 [0.000, 2.000],  loss: 0.892386, mae: 18.870483, mean_q: -27.659066, mean_eps: 0.699441\n",
            "  61827/200000: episode: 146, duration: 7.863s, episode steps: 304, steps per second:  39, episode reward: -303.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.023 [0.000, 2.000],  loss: 0.887308, mae: 18.690878, mean_q: -27.396312, mean_eps: 0.697795\n",
            "  62217/200000: episode: 147, duration: 9.850s, episode steps: 390, steps per second:  40, episode reward: -389.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.144 [0.000, 2.000],  loss: 0.885541, mae: 18.631264, mean_q: -27.321298, mean_eps: 0.696095\n",
            "  62717/200000: episode: 148, duration: 11.702s, episode steps: 500, steps per second:  43, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.116 [0.000, 2.000],  loss: 0.873480, mae: 18.417469, mean_q: -26.989898, mean_eps: 0.693914\n",
            "  63217/200000: episode: 149, duration: 11.935s, episode steps: 500, steps per second:  42, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.094 [0.000, 2.000],  loss: 0.803010, mae: 18.240898, mean_q: -26.733811, mean_eps: 0.691464\n",
            "  63479/200000: episode: 150, duration: 7.392s, episode steps: 262, steps per second:  35, episode reward: -261.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.057 [0.000, 2.000],  loss: 0.793660, mae: 18.156511, mean_q: -26.616273, mean_eps: 0.689597\n",
            "  63693/200000: episode: 151, duration: 4.799s, episode steps: 214, steps per second:  45, episode reward: -213.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.084 [0.000, 2.000],  loss: 0.795903, mae: 17.999115, mean_q: -26.374064, mean_eps: 0.688431\n",
            "  64003/200000: episode: 152, duration: 8.270s, episode steps: 310, steps per second:  37, episode reward: -309.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.990 [0.000, 2.000],  loss: 0.760165, mae: 17.949211, mean_q: -26.320736, mean_eps: 0.687147\n",
            "  64431/200000: episode: 153, duration: 10.978s, episode steps: 428, steps per second:  39, episode reward: -427.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.002 [0.000, 2.000],  loss: 0.749090, mae: 17.860528, mean_q: -26.206352, mean_eps: 0.685339\n",
            "  64931/200000: episode: 154, duration: 12.217s, episode steps: 500, steps per second:  41, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.968 [0.000, 2.000],  loss: 0.719947, mae: 17.830250, mean_q: -26.164033, mean_eps: 0.683066\n",
            "  65341/200000: episode: 155, duration: 9.289s, episode steps: 410, steps per second:  44, episode reward: -409.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.051 [0.000, 2.000],  loss: 0.734884, mae: 17.692414, mean_q: -25.946037, mean_eps: 0.680836\n",
            "  65678/200000: episode: 156, duration: 8.677s, episode steps: 337, steps per second:  39, episode reward: -336.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.125 [0.000, 2.000],  loss: 0.688963, mae: 17.607524, mean_q: -25.826646, mean_eps: 0.679006\n",
            "  65957/200000: episode: 157, duration: 6.811s, episode steps: 279, steps per second:  41, episode reward: -278.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.057 [0.000, 2.000],  loss: 0.741290, mae: 17.549269, mean_q: -25.753610, mean_eps: 0.677497\n",
            "  66280/200000: episode: 158, duration: 8.128s, episode steps: 323, steps per second:  40, episode reward: -322.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.009 [0.000, 2.000],  loss: 0.696229, mae: 17.445715, mean_q: -25.599904, mean_eps: 0.676022\n",
            "  66657/200000: episode: 159, duration: 9.443s, episode steps: 377, steps per second:  40, episode reward: -376.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.072 [0.000, 2.000],  loss: 0.705988, mae: 17.444083, mean_q: -25.606180, mean_eps: 0.674307\n",
            "  67013/200000: episode: 160, duration: 7.902s, episode steps: 356, steps per second:  45, episode reward: -355.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.107 [0.000, 2.000],  loss: 0.678117, mae: 17.399728, mean_q: -25.540917, mean_eps: 0.672511\n",
            "  67318/200000: episode: 161, duration: 8.077s, episode steps: 305, steps per second:  38, episode reward: -304.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.039 [0.000, 2.000],  loss: 0.666170, mae: 17.405228, mean_q: -25.561779, mean_eps: 0.670891\n",
            "  67709/200000: episode: 162, duration: 9.933s, episode steps: 391, steps per second:  39, episode reward: -390.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.141 [0.000, 2.000],  loss: 0.654222, mae: 17.423383, mean_q: -25.590509, mean_eps: 0.669186\n",
            "  68015/200000: episode: 163, duration: 6.856s, episode steps: 306, steps per second:  45, episode reward: -305.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.144 [0.000, 2.000],  loss: 0.675289, mae: 17.399249, mean_q: -25.544424, mean_eps: 0.667479\n",
            "  68318/200000: episode: 164, duration: 8.125s, episode steps: 303, steps per second:  37, episode reward: -302.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.063 [0.000, 2.000],  loss: 0.716267, mae: 17.392103, mean_q: -25.517388, mean_eps: 0.665987\n",
            "  68728/200000: episode: 165, duration: 9.690s, episode steps: 410, steps per second:  42, episode reward: -409.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.188 [0.000, 2.000],  loss: 0.681158, mae: 17.432430, mean_q: -25.583064, mean_eps: 0.664240\n",
            "  68912/200000: episode: 166, duration: 4.858s, episode steps: 184, steps per second:  38, episode reward: -183.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.940 [0.000, 2.000],  loss: 0.729920, mae: 17.497733, mean_q: -25.684106, mean_eps: 0.662784\n",
            "  69094/200000: episode: 167, duration: 4.055s, episode steps: 182, steps per second:  45, episode reward: -181.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.005 [0.000, 2.000],  loss: 0.698942, mae: 17.520673, mean_q: -25.735004, mean_eps: 0.661888\n",
            "  69426/200000: episode: 168, duration: 8.547s, episode steps: 332, steps per second:  39, episode reward: -331.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.937 [0.000, 2.000],  loss: 0.683073, mae: 17.587636, mean_q: -25.837987, mean_eps: 0.660628\n",
            "  69650/200000: episode: 169, duration: 4.919s, episode steps: 224, steps per second:  46, episode reward: -223.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.134 [0.000, 2.000],  loss: 0.628519, mae: 17.561238, mean_q: -25.814509, mean_eps: 0.659266\n",
            "  69967/200000: episode: 170, duration: 8.202s, episode steps: 317, steps per second:  39, episode reward: -316.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.019 [0.000, 2.000],  loss: 0.678986, mae: 17.566287, mean_q: -25.812184, mean_eps: 0.657941\n",
            "  70297/200000: episode: 171, duration: 7.329s, episode steps: 330, steps per second:  45, episode reward: -329.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.070 [0.000, 2.000],  loss: 0.672917, mae: 17.561645, mean_q: -25.805729, mean_eps: 0.656356\n",
            "  70630/200000: episode: 172, duration: 8.892s, episode steps: 333, steps per second:  37, episode reward: -332.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.078 [0.000, 2.000],  loss: 0.641211, mae: 17.573081, mean_q: -25.823680, mean_eps: 0.654731\n",
            "  70882/200000: episode: 173, duration: 5.706s, episode steps: 252, steps per second:  44, episode reward: -251.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.190 [0.000, 2.000],  loss: 0.661493, mae: 17.664780, mean_q: -25.952926, mean_eps: 0.653298\n",
            "  71296/200000: episode: 174, duration: 10.360s, episode steps: 414, steps per second:  40, episode reward: -413.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.077 [0.000, 2.000],  loss: 0.729497, mae: 17.712688, mean_q: -26.018245, mean_eps: 0.651666\n",
            "  71687/200000: episode: 175, duration: 9.732s, episode steps: 391, steps per second:  40, episode reward: -390.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.077 [0.000, 2.000],  loss: 0.682721, mae: 17.824246, mean_q: -26.179912, mean_eps: 0.649694\n",
            "  71916/200000: episode: 176, duration: 5.193s, episode steps: 229, steps per second:  44, episode reward: -228.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.083 [0.000, 2.000],  loss: 0.707165, mae: 17.877029, mean_q: -26.246890, mean_eps: 0.648175\n",
            "  72179/200000: episode: 177, duration: 7.088s, episode steps: 263, steps per second:  37, episode reward: -262.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.053 [0.000, 2.000],  loss: 0.703568, mae: 17.914422, mean_q: -26.299811, mean_eps: 0.646970\n",
            "  72478/200000: episode: 178, duration: 6.584s, episode steps: 299, steps per second:  45, episode reward: -298.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.040 [0.000, 2.000],  loss: 0.702585, mae: 17.957882, mean_q: -26.376611, mean_eps: 0.645593\n",
            "  72863/200000: episode: 179, duration: 9.759s, episode steps: 385, steps per second:  39, episode reward: -384.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.021 [0.000, 2.000],  loss: 0.708186, mae: 17.983291, mean_q: -26.407803, mean_eps: 0.643917\n",
            "  73061/200000: episode: 180, duration: 4.696s, episode steps: 198, steps per second:  42, episode reward: -197.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.040 [0.000, 2.000],  loss: 0.679511, mae: 18.000330, mean_q: -26.438524, mean_eps: 0.642489\n",
            "  73319/200000: episode: 181, duration: 7.008s, episode steps: 258, steps per second:  37, episode reward: -257.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.996 [0.000, 2.000],  loss: 0.705722, mae: 17.967085, mean_q: -26.377745, mean_eps: 0.641371\n",
            "  73599/200000: episode: 182, duration: 6.326s, episode steps: 280, steps per second:  44, episode reward: -279.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.986 [0.000, 2.000],  loss: 0.651893, mae: 17.952747, mean_q: -26.373388, mean_eps: 0.640053\n",
            "  73980/200000: episode: 183, duration: 9.455s, episode steps: 381, steps per second:  40, episode reward: -380.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.110 [0.000, 2.000],  loss: 0.712906, mae: 17.945543, mean_q: -26.352349, mean_eps: 0.638434\n",
            "  74374/200000: episode: 184, duration: 9.826s, episode steps: 394, steps per second:  40, episode reward: -393.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.107 [0.000, 2.000],  loss: 0.704613, mae: 17.961973, mean_q: -26.380124, mean_eps: 0.636535\n",
            "  74618/200000: episode: 185, duration: 5.298s, episode steps: 244, steps per second:  46, episode reward: -243.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.061 [0.000, 2.000],  loss: 0.653756, mae: 17.970922, mean_q: -26.402102, mean_eps: 0.634972\n",
            "  74822/200000: episode: 186, duration: 4.962s, episode steps: 204, steps per second:  41, episode reward: -203.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.064 [0.000, 2.000],  loss: 0.721155, mae: 17.937458, mean_q: -26.331153, mean_eps: 0.633874\n",
            "  75130/200000: episode: 187, duration: 7.586s, episode steps: 308, steps per second:  41, episode reward: -307.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.166 [0.000, 2.000],  loss: 0.669505, mae: 17.948794, mean_q: -26.370299, mean_eps: 0.632620\n",
            "  75494/200000: episode: 188, duration: 9.379s, episode steps: 364, steps per second:  39, episode reward: -363.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.069 [0.000, 2.000],  loss: 0.665613, mae: 18.005008, mean_q: -26.445525, mean_eps: 0.630974\n",
            "  75868/200000: episode: 189, duration: 8.401s, episode steps: 374, steps per second:  45, episode reward: -373.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.024 [0.000, 2.000],  loss: 0.654465, mae: 18.040252, mean_q: -26.492468, mean_eps: 0.629166\n",
            "  76135/200000: episode: 190, duration: 7.037s, episode steps: 267, steps per second:  38, episode reward: -266.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.966 [0.000, 2.000],  loss: 0.658888, mae: 17.988633, mean_q: -26.411476, mean_eps: 0.627595\n",
            "  76442/200000: episode: 191, duration: 6.696s, episode steps: 307, steps per second:  46, episode reward: -306.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.902 [0.000, 2.000],  loss: 0.684219, mae: 17.971105, mean_q: -26.383377, mean_eps: 0.626189\n",
            "  76755/200000: episode: 192, duration: 8.266s, episode steps: 313, steps per second:  38, episode reward: -312.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.038 [0.000, 2.000],  loss: 0.667488, mae: 18.009535, mean_q: -26.448311, mean_eps: 0.624670\n",
            "  77044/200000: episode: 193, duration: 6.985s, episode steps: 289, steps per second:  41, episode reward: -288.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.087 [0.000, 2.000],  loss: 0.625006, mae: 18.007526, mean_q: -26.465696, mean_eps: 0.623195\n",
            "  77306/200000: episode: 194, duration: 6.484s, episode steps: 262, steps per second:  40, episode reward: -261.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.019 [0.000, 2.000],  loss: 0.627574, mae: 18.083443, mean_q: -26.576209, mean_eps: 0.621845\n",
            "  77580/200000: episode: 195, duration: 6.277s, episode steps: 274, steps per second:  44, episode reward: -273.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.131 [0.000, 2.000],  loss: 0.655668, mae: 18.128877, mean_q: -26.642753, mean_eps: 0.620532\n",
            "  77900/200000: episode: 196, duration: 8.255s, episode steps: 320, steps per second:  39, episode reward: -319.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.041 [0.000, 2.000],  loss: 0.634626, mae: 18.164502, mean_q: -26.684430, mean_eps: 0.619076\n",
            "  78245/200000: episode: 197, duration: 8.876s, episode steps: 345, steps per second:  39, episode reward: -344.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.041 [0.000, 2.000],  loss: 0.654983, mae: 18.116126, mean_q: -26.617570, mean_eps: 0.617447\n",
            "  78490/200000: episode: 198, duration: 5.477s, episode steps: 245, steps per second:  45, episode reward: -244.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.224 [0.000, 2.000],  loss: 0.674448, mae: 18.111387, mean_q: -26.599442, mean_eps: 0.616002\n",
            "  78838/200000: episode: 199, duration: 8.895s, episode steps: 348, steps per second:  39, episode reward: -347.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.026 [0.000, 2.000],  loss: 0.664257, mae: 18.043616, mean_q: -26.500194, mean_eps: 0.614549\n",
            "  79082/200000: episode: 200, duration: 5.348s, episode steps: 244, steps per second:  46, episode reward: -243.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.057 [0.000, 2.000],  loss: 0.689939, mae: 18.081409, mean_q: -26.546238, mean_eps: 0.613098\n",
            "  79369/200000: episode: 201, duration: 7.459s, episode steps: 287, steps per second:  38, episode reward: -286.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.003 [0.000, 2.000],  loss: 0.670437, mae: 18.012442, mean_q: -26.433440, mean_eps: 0.611798\n",
            "  79855/200000: episode: 202, duration: 11.593s, episode steps: 486, steps per second:  42, episode reward: -485.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.903 [0.000, 2.000],  loss: 0.664303, mae: 17.975206, mean_q: -26.379692, mean_eps: 0.609904\n",
            "  80110/200000: episode: 203, duration: 6.056s, episode steps: 255, steps per second:  42, episode reward: -254.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.918 [0.000, 2.000],  loss: 0.674214, mae: 17.938799, mean_q: -26.333123, mean_eps: 0.608088\n",
            "  80440/200000: episode: 204, duration: 8.767s, episode steps: 330, steps per second:  38, episode reward: -329.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.973 [0.000, 2.000],  loss: 0.639735, mae: 17.961286, mean_q: -26.378610, mean_eps: 0.606655\n",
            "  80610/200000: episode: 205, duration: 3.790s, episode steps: 170, steps per second:  45, episode reward: -169.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.100 [0.000, 2.000],  loss: 0.650383, mae: 17.891363, mean_q: -26.265884, mean_eps: 0.605430\n",
            "  81110/200000: episode: 206, duration: 12.102s, episode steps: 500, steps per second:  41, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.204 [0.000, 2.000],  loss: 0.683732, mae: 17.835765, mean_q: -26.166619, mean_eps: 0.603788\n",
            "  81259/200000: episode: 207, duration: 3.266s, episode steps: 149, steps per second:  46, episode reward: -148.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.101 [0.000, 2.000],  loss: 0.675029, mae: 17.840466, mean_q: -26.182165, mean_eps: 0.602198\n",
            "  81759/200000: episode: 208, duration: 12.190s, episode steps: 500, steps per second:  41, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.996 [0.000, 2.000],  loss: 0.657499, mae: 17.828720, mean_q: -26.168512, mean_eps: 0.600608\n",
            "  82057/200000: episode: 209, duration: 7.260s, episode steps: 298, steps per second:  41, episode reward: -297.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.997 [0.000, 2.000],  loss: 0.647949, mae: 17.758791, mean_q: -26.081345, mean_eps: 0.598653\n",
            "  82211/200000: episode: 210, duration: 3.887s, episode steps: 154, steps per second:  40, episode reward: -153.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.006 [0.000, 2.000],  loss: 0.623418, mae: 17.769475, mean_q: -26.092636, mean_eps: 0.597546\n",
            "  82511/200000: episode: 211, duration: 6.572s, episode steps: 300, steps per second:  46, episode reward: -299.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.023 [0.000, 2.000],  loss: 0.633395, mae: 17.731015, mean_q: -26.040521, mean_eps: 0.596434\n",
            "  82720/200000: episode: 212, duration: 6.014s, episode steps: 209, steps per second:  35, episode reward: -208.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.062 [0.000, 2.000],  loss: 0.650726, mae: 17.711994, mean_q: -25.998170, mean_eps: 0.595187\n",
            "  83180/200000: episode: 213, duration: 11.066s, episode steps: 460, steps per second:  42, episode reward: -459.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.052 [0.000, 2.000],  loss: 0.627180, mae: 17.806742, mean_q: -26.142590, mean_eps: 0.593547\n",
            "  83477/200000: episode: 214, duration: 7.188s, episode steps: 297, steps per second:  41, episode reward: -296.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.987 [0.000, 2.000],  loss: 0.674324, mae: 17.820205, mean_q: -26.146416, mean_eps: 0.591693\n",
            "  83734/200000: episode: 215, duration: 6.556s, episode steps: 257, steps per second:  39, episode reward: -256.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.981 [0.000, 2.000],  loss: 0.679754, mae: 17.795673, mean_q: -26.107478, mean_eps: 0.590336\n",
            "  83916/200000: episode: 216, duration: 4.232s, episode steps: 182, steps per second:  43, episode reward: -181.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.077 [0.000, 2.000],  loss: 0.651806, mae: 17.807642, mean_q: -26.130726, mean_eps: 0.589260\n",
            "  84155/200000: episode: 217, duration: 5.219s, episode steps: 239, steps per second:  46, episode reward: -238.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.050 [0.000, 2.000],  loss: 0.645451, mae: 17.827082, mean_q: -26.171614, mean_eps: 0.588229\n",
            "  84509/200000: episode: 218, duration: 8.975s, episode steps: 354, steps per second:  39, episode reward: -353.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.994 [0.000, 2.000],  loss: 0.649971, mae: 17.844471, mean_q: -26.199601, mean_eps: 0.586776\n",
            "  84873/200000: episode: 219, duration: 9.075s, episode steps: 364, steps per second:  40, episode reward: -363.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.069 [0.000, 2.000],  loss: 0.622975, mae: 17.848922, mean_q: -26.214176, mean_eps: 0.585017\n",
            "  85106/200000: episode: 220, duration: 5.566s, episode steps: 233, steps per second:  42, episode reward: -232.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.009 [0.000, 2.000],  loss: 0.596085, mae: 17.850056, mean_q: -26.216531, mean_eps: 0.583554\n",
            "  85384/200000: episode: 221, duration: 6.739s, episode steps: 278, steps per second:  41, episode reward: -277.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.993 [0.000, 2.000],  loss: 0.623817, mae: 17.858135, mean_q: -26.229105, mean_eps: 0.582302\n",
            "  85664/200000: episode: 222, duration: 6.731s, episode steps: 280, steps per second:  42, episode reward: -279.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.132 [0.000, 2.000],  loss: 0.636120, mae: 17.937366, mean_q: -26.346972, mean_eps: 0.580935\n",
            "  85956/200000: episode: 223, duration: 7.132s, episode steps: 292, steps per second:  41, episode reward: -291.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.188 [0.000, 2.000],  loss: 0.597584, mae: 17.980242, mean_q: -26.417192, mean_eps: 0.579533\n",
            "  86229/200000: episode: 224, duration: 6.365s, episode steps: 273, steps per second:  43, episode reward: -272.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.073 [0.000, 2.000],  loss: 0.617142, mae: 17.938940, mean_q: -26.352422, mean_eps: 0.578149\n",
            "  86610/200000: episode: 225, duration: 9.596s, episode steps: 381, steps per second:  40, episode reward: -380.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.152 [0.000, 2.000],  loss: 0.602235, mae: 17.913739, mean_q: -26.317301, mean_eps: 0.576547\n",
            "  86879/200000: episode: 226, duration: 5.835s, episode steps: 269, steps per second:  46, episode reward: -268.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.963 [0.000, 2.000],  loss: 0.640199, mae: 17.895676, mean_q: -26.281823, mean_eps: 0.574954\n",
            "  87165/200000: episode: 227, duration: 7.491s, episode steps: 286, steps per second:  38, episode reward: -285.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.178 [0.000, 2.000],  loss: 0.603027, mae: 17.927502, mean_q: -26.336930, mean_eps: 0.573595\n",
            "  87495/200000: episode: 228, duration: 7.430s, episode steps: 330, steps per second:  44, episode reward: -329.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.061 [0.000, 2.000],  loss: 0.601154, mae: 17.966894, mean_q: -26.410704, mean_eps: 0.572085\n",
            "  87896/200000: episode: 229, duration: 9.923s, episode steps: 401, steps per second:  40, episode reward: -400.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.047 [0.000, 2.000],  loss: 0.607122, mae: 18.001676, mean_q: -26.469654, mean_eps: 0.570295\n",
            "  88054/200000: episode: 230, duration: 3.474s, episode steps: 158, steps per second:  45, episode reward: -157.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.076 [0.000, 2.000],  loss: 0.619084, mae: 18.043162, mean_q: -26.531791, mean_eps: 0.568925\n",
            "  88311/200000: episode: 231, duration: 6.934s, episode steps: 257, steps per second:  37, episode reward: -256.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.043 [0.000, 2.000],  loss: 0.616464, mae: 18.016869, mean_q: -26.493268, mean_eps: 0.567908\n",
            "  88487/200000: episode: 232, duration: 3.806s, episode steps: 176, steps per second:  46, episode reward: -175.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.943 [0.000, 2.000],  loss: 0.591387, mae: 18.018772, mean_q: -26.503283, mean_eps: 0.566847\n",
            "  88667/200000: episode: 233, duration: 3.991s, episode steps: 180, steps per second:  45, episode reward: -179.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.967 [0.000, 2.000],  loss: 0.655106, mae: 17.978467, mean_q: -26.438111, mean_eps: 0.565975\n",
            "  88927/200000: episode: 234, duration: 6.848s, episode steps: 260, steps per second:  38, episode reward: -259.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.038 [0.000, 2.000],  loss: 0.599346, mae: 17.985566, mean_q: -26.448564, mean_eps: 0.564897\n",
            "  89060/200000: episode: 235, duration: 2.881s, episode steps: 133, steps per second:  46, episode reward: -132.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.023 [0.000, 2.000],  loss: 0.594423, mae: 18.011110, mean_q: -26.498737, mean_eps: 0.563934\n",
            "  89270/200000: episode: 236, duration: 4.912s, episode steps: 210, steps per second:  43, episode reward: -209.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.990 [0.000, 2.000],  loss: 0.609415, mae: 17.935537, mean_q: -26.384503, mean_eps: 0.563094\n",
            "  89549/200000: episode: 237, duration: 7.140s, episode steps: 279, steps per second:  39, episode reward: -278.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.918 [0.000, 2.000],  loss: 0.619293, mae: 18.011266, mean_q: -26.499520, mean_eps: 0.561896\n",
            "  89934/200000: episode: 238, duration: 10.186s, episode steps: 385, steps per second:  38, episode reward: -384.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.005 [0.000, 2.000],  loss: 0.601820, mae: 18.057537, mean_q: -26.568493, mean_eps: 0.560269\n",
            "  90160/200000: episode: 239, duration: 5.141s, episode steps: 226, steps per second:  44, episode reward: -225.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.075 [0.000, 2.000],  loss: 0.626948, mae: 18.039422, mean_q: -26.524888, mean_eps: 0.558772\n",
            "  90312/200000: episode: 240, duration: 3.376s, episode steps: 152, steps per second:  45, episode reward: -151.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.118 [0.000, 2.000],  loss: 0.625143, mae: 18.052771, mean_q: -26.534762, mean_eps: 0.557846\n",
            "  90535/200000: episode: 241, duration: 6.157s, episode steps: 223, steps per second:  36, episode reward: -222.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.009 [0.000, 2.000],  loss: 0.636024, mae: 18.091867, mean_q: -26.586791, mean_eps: 0.556927\n",
            "  90800/200000: episode: 242, duration: 5.980s, episode steps: 265, steps per second:  44, episode reward: -264.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.004 [0.000, 2.000],  loss: 0.639900, mae: 18.188205, mean_q: -26.737104, mean_eps: 0.555732\n",
            "  91010/200000: episode: 243, duration: 5.705s, episode steps: 210, steps per second:  37, episode reward: -209.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.995 [0.000, 2.000],  loss: 0.675391, mae: 18.275471, mean_q: -26.862796, mean_eps: 0.554568\n",
            "  91220/200000: episode: 244, duration: 4.631s, episode steps: 210, steps per second:  45, episode reward: -209.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.010 [0.000, 2.000],  loss: 0.709344, mae: 18.443398, mean_q: -27.106727, mean_eps: 0.553539\n",
            "  91441/200000: episode: 245, duration: 4.935s, episode steps: 221, steps per second:  45, episode reward: -220.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.977 [0.000, 2.000],  loss: 0.652798, mae: 18.495743, mean_q: -27.182936, mean_eps: 0.552483\n",
            "  91602/200000: episode: 246, duration: 4.743s, episode steps: 161, steps per second:  34, episode reward: -160.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.845 [0.000, 2.000],  loss: 0.674361, mae: 18.604692, mean_q: -27.339488, mean_eps: 0.551547\n",
            "  91762/200000: episode: 247, duration: 3.473s, episode steps: 160, steps per second:  46, episode reward: -159.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.131 [0.000, 2.000],  loss: 0.665798, mae: 18.689045, mean_q: -27.477218, mean_eps: 0.550761\n",
            "  91928/200000: episode: 248, duration: 3.669s, episode steps: 166, steps per second:  45, episode reward: -165.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.940 [0.000, 2.000],  loss: 0.683587, mae: 18.748147, mean_q: -27.553669, mean_eps: 0.549962\n",
            "  92084/200000: episode: 249, duration: 4.332s, episode steps: 156, steps per second:  36, episode reward: -155.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.051 [0.000, 2.000],  loss: 0.676098, mae: 18.795551, mean_q: -27.640584, mean_eps: 0.549173\n",
            "  92523/200000: episode: 250, duration: 10.348s, episode steps: 439, steps per second:  42, episode reward: -438.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.895 [0.000, 2.000],  loss: 0.679285, mae: 18.864311, mean_q: -27.734175, mean_eps: 0.547715\n",
            "  92790/200000: episode: 251, duration: 7.158s, episode steps: 267, steps per second:  37, episode reward: -266.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.955 [0.000, 2.000],  loss: 0.709992, mae: 18.871585, mean_q: -27.725509, mean_eps: 0.545986\n",
            "  93127/200000: episode: 252, duration: 7.861s, episode steps: 337, steps per second:  43, episode reward: -336.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.030 [0.000, 2.000],  loss: 0.688950, mae: 18.875505, mean_q: -27.739118, mean_eps: 0.544506\n",
            "  93299/200000: episode: 253, duration: 4.794s, episode steps: 172, steps per second:  36, episode reward: -171.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.907 [0.000, 2.000],  loss: 0.710825, mae: 18.910567, mean_q: -27.782853, mean_eps: 0.543259\n",
            "  93665/200000: episode: 254, duration: 7.943s, episode steps: 366, steps per second:  46, episode reward: -365.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.079 [0.000, 2.000],  loss: 0.694785, mae: 18.827201, mean_q: -27.646613, mean_eps: 0.541941\n",
            "  94046/200000: episode: 255, duration: 9.451s, episode steps: 381, steps per second:  40, episode reward: -380.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.142 [0.000, 2.000],  loss: 0.672262, mae: 18.853661, mean_q: -27.691204, mean_eps: 0.540111\n",
            "  94296/200000: episode: 256, duration: 6.280s, episode steps: 250, steps per second:  40, episode reward: -249.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.084 [0.000, 2.000],  loss: 0.666520, mae: 18.838725, mean_q: -27.664251, mean_eps: 0.538565\n",
            "  94469/200000: episode: 257, duration: 4.269s, episode steps: 173, steps per second:  41, episode reward: -172.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.133 [0.000, 2.000],  loss: 0.676774, mae: 18.871464, mean_q: -27.714235, mean_eps: 0.537528\n",
            "  94834/200000: episode: 258, duration: 8.936s, episode steps: 365, steps per second:  41, episode reward: -364.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.148 [0.000, 2.000],  loss: 0.634283, mae: 18.839432, mean_q: -27.677079, mean_eps: 0.536210\n",
            "  95107/200000: episode: 259, duration: 6.604s, episode steps: 273, steps per second:  41, episode reward: -272.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.051 [0.000, 2.000],  loss: 0.658923, mae: 18.808610, mean_q: -27.644875, mean_eps: 0.534647\n",
            "  95324/200000: episode: 260, duration: 4.760s, episode steps: 217, steps per second:  46, episode reward: -216.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.046 [0.000, 2.000],  loss: 0.647350, mae: 18.777097, mean_q: -27.605068, mean_eps: 0.533447\n",
            "  95510/200000: episode: 261, duration: 5.367s, episode steps: 186, steps per second:  35, episode reward: -185.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.668587, mae: 18.778706, mean_q: -27.595326, mean_eps: 0.532459\n",
            "  95680/200000: episode: 262, duration: 3.702s, episode steps: 170, steps per second:  46, episode reward: -169.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.935 [0.000, 2.000],  loss: 0.663340, mae: 18.708469, mean_q: -27.478840, mean_eps: 0.531587\n",
            "  95892/200000: episode: 263, duration: 4.626s, episode steps: 212, steps per second:  46, episode reward: -211.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.882 [0.000, 2.000],  loss: 0.683352, mae: 18.633089, mean_q: -27.350756, mean_eps: 0.530651\n",
            "  96064/200000: episode: 264, duration: 4.957s, episode steps: 172, steps per second:  35, episode reward: -171.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.953 [0.000, 2.000],  loss: 0.708275, mae: 18.549002, mean_q: -27.219116, mean_eps: 0.529710\n",
            "  96292/200000: episode: 265, duration: 5.021s, episode steps: 228, steps per second:  45, episode reward: -227.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.908 [0.000, 2.000],  loss: 0.648680, mae: 18.541213, mean_q: -27.225565, mean_eps: 0.528730\n",
            "  96556/200000: episode: 266, duration: 6.970s, episode steps: 264, steps per second:  38, episode reward: -263.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.630796, mae: 18.459280, mean_q: -27.095724, mean_eps: 0.527525\n",
            "  96786/200000: episode: 267, duration: 5.122s, episode steps: 230, steps per second:  45, episode reward: -229.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.991 [0.000, 2.000],  loss: 0.627049, mae: 18.390657, mean_q: -27.005563, mean_eps: 0.526315\n",
            "  96931/200000: episode: 268, duration: 3.218s, episode steps: 145, steps per second:  45, episode reward: -144.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.041 [0.000, 2.000],  loss: 0.622978, mae: 18.469134, mean_q: -27.122951, mean_eps: 0.525396\n",
            "  97212/200000: episode: 269, duration: 7.643s, episode steps: 281, steps per second:  37, episode reward: -280.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.982 [0.000, 2.000],  loss: 0.594838, mae: 18.355239, mean_q: -26.953900, mean_eps: 0.524352\n",
            "  97405/200000: episode: 270, duration: 4.246s, episode steps: 193, steps per second:  45, episode reward: -192.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.016 [0.000, 2.000],  loss: 0.547294, mae: 18.345563, mean_q: -26.977412, mean_eps: 0.523191\n",
            "  97564/200000: episode: 271, duration: 3.689s, episode steps: 159, steps per second:  43, episode reward: -158.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.119 [0.000, 2.000],  loss: 0.559172, mae: 18.251943, mean_q: -26.816386, mean_eps: 0.522328\n",
            "  97900/200000: episode: 272, duration: 8.510s, episode steps: 336, steps per second:  39, episode reward: -335.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.946 [0.000, 2.000],  loss: 0.581941, mae: 18.251055, mean_q: -26.818559, mean_eps: 0.521116\n",
            "  98063/200000: episode: 273, duration: 3.695s, episode steps: 163, steps per second:  44, episode reward: -162.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.975 [0.000, 2.000],  loss: 0.560995, mae: 18.202793, mean_q: -26.756617, mean_eps: 0.519893\n",
            "  98273/200000: episode: 274, duration: 5.807s, episode steps: 210, steps per second:  36, episode reward: -209.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.967 [0.000, 2.000],  loss: 0.585191, mae: 18.162494, mean_q: -26.685125, mean_eps: 0.518979\n",
            "  98467/200000: episode: 275, duration: 4.298s, episode steps: 194, steps per second:  45, episode reward: -193.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.990 [0.000, 2.000],  loss: 0.577731, mae: 18.221400, mean_q: -26.787107, mean_eps: 0.517989\n",
            "  98636/200000: episode: 276, duration: 3.764s, episode steps: 169, steps per second:  45, episode reward: -168.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.012 [0.000, 2.000],  loss: 0.570104, mae: 18.235940, mean_q: -26.821600, mean_eps: 0.517100\n",
            "  98815/200000: episode: 277, duration: 5.209s, episode steps: 179, steps per second:  34, episode reward: -178.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.078 [0.000, 2.000],  loss: 0.543601, mae: 18.272438, mean_q: -26.873706, mean_eps: 0.516248\n",
            "  99071/200000: episode: 278, duration: 5.644s, episode steps: 256, steps per second:  45, episode reward: -255.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.871 [0.000, 2.000],  loss: 0.559980, mae: 18.222559, mean_q: -26.797611, mean_eps: 0.515182\n",
            "  99270/200000: episode: 279, duration: 4.924s, episode steps: 199, steps per second:  40, episode reward: -198.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.995 [0.000, 2.000],  loss: 0.540206, mae: 18.232767, mean_q: -26.812995, mean_eps: 0.514067\n",
            "  99469/200000: episode: 280, duration: 5.095s, episode steps: 199, steps per second:  39, episode reward: -198.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.869 [0.000, 2.000],  loss: 0.545191, mae: 18.227744, mean_q: -26.798135, mean_eps: 0.513092\n",
            "  99632/200000: episode: 281, duration: 3.933s, episode steps: 163, steps per second:  41, episode reward: -162.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.104 [0.000, 2.000],  loss: 0.563933, mae: 18.164443, mean_q: -26.709843, mean_eps: 0.512205\n",
            "  99790/200000: episode: 282, duration: 3.740s, episode steps: 158, steps per second:  42, episode reward: -157.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.956 [0.000, 2.000],  loss: 0.594623, mae: 18.132698, mean_q: -26.651866, mean_eps: 0.511419\n",
            "  99954/200000: episode: 283, duration: 4.566s, episode steps: 164, steps per second:  36, episode reward: -163.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.006 [0.000, 2.000],  loss: 0.572996, mae: 18.055972, mean_q: -26.549650, mean_eps: 0.510630\n",
            " 100179/200000: episode: 284, duration: 4.965s, episode steps: 225, steps per second:  45, episode reward: -224.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.191 [0.000, 2.000],  loss: 0.569097, mae: 18.058904, mean_q: -26.563042, mean_eps: 0.509677\n",
            " 100584/200000: episode: 285, duration: 10.229s, episode steps: 405, steps per second:  40, episode reward: -404.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.854 [0.000, 2.000],  loss: 0.574639, mae: 18.074553, mean_q: -26.587703, mean_eps: 0.508133\n",
            " 100733/200000: episode: 286, duration: 3.260s, episode steps: 149, steps per second:  46, episode reward: -148.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.007 [0.000, 2.000],  loss: 0.588142, mae: 18.029105, mean_q: -26.515565, mean_eps: 0.506776\n",
            " 100965/200000: episode: 287, duration: 5.961s, episode steps: 232, steps per second:  39, episode reward: -231.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.944 [0.000, 2.000],  loss: 0.586962, mae: 18.056206, mean_q: -26.568006, mean_eps: 0.505842\n",
            " 101145/200000: episode: 288, duration: 4.282s, episode steps: 180, steps per second:  42, episode reward: -179.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.006 [0.000, 2.000],  loss: 0.578547, mae: 18.031948, mean_q: -26.525347, mean_eps: 0.504833\n",
            " 101374/200000: episode: 289, duration: 5.146s, episode steps: 229, steps per second:  44, episode reward: -228.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.948 [0.000, 2.000],  loss: 0.580005, mae: 18.067919, mean_q: -26.573085, mean_eps: 0.503831\n",
            " 101568/200000: episode: 290, duration: 5.543s, episode steps: 194, steps per second:  35, episode reward: -193.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.578957, mae: 18.026056, mean_q: -26.517094, mean_eps: 0.502795\n",
            " 101810/200000: episode: 291, duration: 5.446s, episode steps: 242, steps per second:  44, episode reward: -241.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.037 [0.000, 2.000],  loss: 0.558245, mae: 18.076624, mean_q: -26.585947, mean_eps: 0.501726\n",
            " 101986/200000: episode: 292, duration: 4.371s, episode steps: 176, steps per second:  40, episode reward: -175.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.182 [0.000, 2.000],  loss: 0.553182, mae: 18.101600, mean_q: -26.625364, mean_eps: 0.500702\n",
            " 102163/200000: episode: 293, duration: 4.968s, episode steps: 177, steps per second:  36, episode reward: -176.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.870 [0.000, 2.000],  loss: 0.542714, mae: 18.130017, mean_q: -26.681551, mean_eps: 0.499837\n",
            " 102349/200000: episode: 294, duration: 4.145s, episode steps: 186, steps per second:  45, episode reward: -185.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.973 [0.000, 2.000],  loss: 0.549747, mae: 18.187472, mean_q: -26.781904, mean_eps: 0.498948\n",
            " 102681/200000: episode: 295, duration: 8.550s, episode steps: 332, steps per second:  39, episode reward: -331.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.952 [0.000, 2.000],  loss: 0.556886, mae: 18.297496, mean_q: -26.931886, mean_eps: 0.497679\n",
            " 102844/200000: episode: 296, duration: 3.741s, episode steps: 163, steps per second:  44, episode reward: -162.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.600182, mae: 18.359775, mean_q: -27.011025, mean_eps: 0.496466\n",
            " 103015/200000: episode: 297, duration: 3.905s, episode steps: 171, steps per second:  44, episode reward: -170.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.988 [0.000, 2.000],  loss: 0.563196, mae: 18.361654, mean_q: -27.009415, mean_eps: 0.495648\n",
            " 103168/200000: episode: 298, duration: 4.534s, episode steps: 153, steps per second:  34, episode reward: -152.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.935 [0.000, 2.000],  loss: 0.576792, mae: 18.379186, mean_q: -27.028005, mean_eps: 0.494854\n",
            " 103367/200000: episode: 299, duration: 4.454s, episode steps: 199, steps per second:  45, episode reward: -198.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.940 [0.000, 2.000],  loss: 0.568404, mae: 18.458601, mean_q: -27.154887, mean_eps: 0.493992\n",
            " 103556/200000: episode: 300, duration: 4.166s, episode steps: 189, steps per second:  45, episode reward: -188.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.937 [0.000, 2.000],  loss: 0.564673, mae: 18.461304, mean_q: -27.161393, mean_eps: 0.493041\n",
            " 103752/200000: episode: 301, duration: 5.484s, episode steps: 196, steps per second:  36, episode reward: -195.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.036 [0.000, 2.000],  loss: 0.566690, mae: 18.540368, mean_q: -27.277015, mean_eps: 0.492098\n",
            " 104023/200000: episode: 302, duration: 6.035s, episode steps: 271, steps per second:  45, episode reward: -270.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.993 [0.000, 2.000],  loss: 0.563768, mae: 18.598771, mean_q: -27.362485, mean_eps: 0.490954\n",
            " 104267/200000: episode: 303, duration: 6.751s, episode steps: 244, steps per second:  36, episode reward: -243.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.996 [0.000, 2.000],  loss: 0.570285, mae: 18.645009, mean_q: -27.434625, mean_eps: 0.489692\n",
            " 104407/200000: episode: 304, duration: 3.616s, episode steps: 140, steps per second:  39, episode reward: -139.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.021 [0.000, 2.000],  loss: 0.583966, mae: 18.621412, mean_q: -27.387539, mean_eps: 0.488751\n",
            " 104587/200000: episode: 305, duration: 4.061s, episode steps: 180, steps per second:  44, episode reward: -179.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.867 [0.000, 2.000],  loss: 0.611835, mae: 18.598625, mean_q: -27.354788, mean_eps: 0.487967\n",
            " 104845/200000: episode: 306, duration: 7.030s, episode steps: 258, steps per second:  37, episode reward: -257.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.942 [0.000, 2.000],  loss: 0.597499, mae: 18.630879, mean_q: -27.396084, mean_eps: 0.486894\n",
            " 105055/200000: episode: 307, duration: 4.670s, episode steps: 210, steps per second:  45, episode reward: -209.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.981 [0.000, 2.000],  loss: 0.595175, mae: 18.602620, mean_q: -27.360011, mean_eps: 0.485747\n",
            " 105287/200000: episode: 308, duration: 5.490s, episode steps: 232, steps per second:  42, episode reward: -231.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.901 [0.000, 2.000],  loss: 0.561361, mae: 18.645157, mean_q: -27.431929, mean_eps: 0.484665\n",
            " 105657/200000: episode: 309, duration: 9.121s, episode steps: 370, steps per second:  41, episode reward: -369.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.019 [0.000, 2.000],  loss: 0.576545, mae: 18.656460, mean_q: -27.445040, mean_eps: 0.483190\n",
            " 105811/200000: episode: 310, duration: 3.491s, episode steps: 154, steps per second:  44, episode reward: -153.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.266 [0.000, 2.000],  loss: 0.564180, mae: 18.655751, mean_q: -27.462635, mean_eps: 0.481906\n",
            " 106001/200000: episode: 311, duration: 5.437s, episode steps: 190, steps per second:  35, episode reward: -189.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.058 [0.000, 2.000],  loss: 0.572793, mae: 18.688588, mean_q: -27.491928, mean_eps: 0.481063\n",
            " 106251/200000: episode: 312, duration: 5.613s, episode steps: 250, steps per second:  45, episode reward: -249.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.056 [0.000, 2.000],  loss: 0.557586, mae: 18.653400, mean_q: -27.436994, mean_eps: 0.479985\n",
            " 106502/200000: episode: 313, duration: 6.907s, episode steps: 251, steps per second:  36, episode reward: -250.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.012 [0.000, 2.000],  loss: 0.537453, mae: 18.629545, mean_q: -27.402145, mean_eps: 0.478758\n",
            " 106754/200000: episode: 314, duration: 6.127s, episode steps: 252, steps per second:  41, episode reward: -251.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.048 [0.000, 2.000],  loss: 0.559435, mae: 18.624696, mean_q: -27.392299, mean_eps: 0.477525\n",
            " 106955/200000: episode: 315, duration: 5.297s, episode steps: 201, steps per second:  38, episode reward: -200.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.990 [0.000, 2.000],  loss: 0.581139, mae: 18.708404, mean_q: -27.524191, mean_eps: 0.476415\n",
            " 107176/200000: episode: 316, duration: 5.395s, episode steps: 221, steps per second:  41, episode reward: -220.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.018 [0.000, 2.000],  loss: 0.548124, mae: 18.707146, mean_q: -27.520849, mean_eps: 0.475382\n",
            " 107328/200000: episode: 317, duration: 3.767s, episode steps: 152, steps per second:  40, episode reward: -151.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.046 [0.000, 2.000],  loss: 0.586072, mae: 18.710356, mean_q: -27.522921, mean_eps: 0.474468\n",
            " 107581/200000: episode: 318, duration: 7.244s, episode steps: 253, steps per second:  35, episode reward: -252.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.043 [0.000, 2.000],  loss: 0.539477, mae: 18.696758, mean_q: -27.514080, mean_eps: 0.473475\n",
            " 107869/200000: episode: 319, duration: 7.145s, episode steps: 288, steps per second:  40, episode reward: -287.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.938 [0.000, 2.000],  loss: 0.570677, mae: 18.744418, mean_q: -27.591247, mean_eps: 0.472150\n",
            " 108072/200000: episode: 320, duration: 5.844s, episode steps: 203, steps per second:  35, episode reward: -202.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.897 [0.000, 2.000],  loss: 0.551894, mae: 18.801485, mean_q: -27.677911, mean_eps: 0.470947\n",
            " 108193/200000: episode: 321, duration: 2.687s, episode steps: 121, steps per second:  45, episode reward: -120.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.553887, mae: 18.805621, mean_q: -27.670315, mean_eps: 0.470153\n",
            " 108338/200000: episode: 322, duration: 3.164s, episode steps: 145, steps per second:  46, episode reward: -144.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.986 [0.000, 2.000],  loss: 0.559576, mae: 18.829947, mean_q: -27.704614, mean_eps: 0.469502\n",
            " 108518/200000: episode: 323, duration: 4.441s, episode steps: 180, steps per second:  41, episode reward: -179.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.833 [0.000, 2.000],  loss: 0.576122, mae: 18.803064, mean_q: -27.646989, mean_eps: 0.468705\n",
            " 108715/200000: episode: 324, duration: 5.137s, episode steps: 197, steps per second:  38, episode reward: -196.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.020 [0.000, 2.000],  loss: 0.557748, mae: 18.851695, mean_q: -27.731583, mean_eps: 0.467782\n",
            " 108884/200000: episode: 325, duration: 3.857s, episode steps: 169, steps per second:  44, episode reward: -168.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.018 [0.000, 2.000],  loss: 0.578069, mae: 18.858702, mean_q: -27.741190, mean_eps: 0.466885\n",
            " 109084/200000: episode: 326, duration: 5.087s, episode steps: 200, steps per second:  39, episode reward: -199.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.240 [0.000, 2.000],  loss: 0.575397, mae: 18.908177, mean_q: -27.826249, mean_eps: 0.465981\n",
            " 109244/200000: episode: 327, duration: 4.472s, episode steps: 160, steps per second:  36, episode reward: -159.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.975 [0.000, 2.000],  loss: 0.616445, mae: 18.952747, mean_q: -27.883621, mean_eps: 0.465099\n",
            " 109598/200000: episode: 328, duration: 8.436s, episode steps: 354, steps per second:  42, episode reward: -353.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.102 [0.000, 2.000],  loss: 0.609962, mae: 19.026074, mean_q: -27.992103, mean_eps: 0.463840\n",
            " 109858/200000: episode: 329, duration: 6.643s, episode steps: 260, steps per second:  39, episode reward: -259.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.062 [0.000, 2.000],  loss: 0.623429, mae: 18.999158, mean_q: -27.944754, mean_eps: 0.462335\n",
            " 110011/200000: episode: 330, duration: 3.384s, episode steps: 153, steps per second:  45, episode reward: -152.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.941 [0.000, 2.000],  loss: 0.590741, mae: 19.022350, mean_q: -27.986541, mean_eps: 0.461323\n",
            " 110214/200000: episode: 331, duration: 5.672s, episode steps: 203, steps per second:  36, episode reward: -202.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.911 [0.000, 2.000],  loss: 0.612320, mae: 19.060678, mean_q: -28.027611, mean_eps: 0.460451\n",
            " 110407/200000: episode: 332, duration: 4.286s, episode steps: 193, steps per second:  45, episode reward: -192.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.839 [0.000, 2.000],  loss: 0.586186, mae: 19.023841, mean_q: -27.982855, mean_eps: 0.459481\n",
            " 110556/200000: episode: 333, duration: 3.417s, episode steps: 149, steps per second:  44, episode reward: -148.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.027 [0.000, 2.000],  loss: 0.588658, mae: 19.003228, mean_q: -27.952805, mean_eps: 0.458643\n",
            " 110725/200000: episode: 334, duration: 4.539s, episode steps: 169, steps per second:  37, episode reward: -168.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.030 [0.000, 2.000],  loss: 0.594084, mae: 18.968285, mean_q: -27.897020, mean_eps: 0.457864\n",
            " 110891/200000: episode: 335, duration: 4.261s, episode steps: 166, steps per second:  39, episode reward: -165.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.114 [0.000, 2.000],  loss: 0.603400, mae: 18.933808, mean_q: -27.829119, mean_eps: 0.457043\n",
            " 111032/200000: episode: 336, duration: 3.170s, episode steps: 141, steps per second:  44, episode reward: -140.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.965 [0.000, 2.000],  loss: 0.629907, mae: 18.903927, mean_q: -27.789822, mean_eps: 0.456291\n",
            " 111243/200000: episode: 337, duration: 5.180s, episode steps: 211, steps per second:  41, episode reward: -210.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.066 [0.000, 2.000],  loss: 0.597171, mae: 18.944926, mean_q: -27.846108, mean_eps: 0.455429\n",
            " 111404/200000: episode: 338, duration: 4.406s, episode steps: 161, steps per second:  37, episode reward: -160.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.950 [0.000, 2.000],  loss: 0.594763, mae: 18.952526, mean_q: -27.866742, mean_eps: 0.454517\n",
            " 111593/200000: episode: 339, duration: 4.558s, episode steps: 189, steps per second:  41, episode reward: -188.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.857 [0.000, 2.000],  loss: 0.604361, mae: 18.942482, mean_q: -27.846540, mean_eps: 0.453660\n",
            " 111842/200000: episode: 340, duration: 6.733s, episode steps: 249, steps per second:  37, episode reward: -248.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.024 [0.000, 2.000],  loss: 0.596082, mae: 18.878146, mean_q: -27.749886, mean_eps: 0.452587\n",
            " 111972/200000: episode: 341, duration: 2.981s, episode steps: 130, steps per second:  44, episode reward: -129.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.008 [0.000, 2.000],  loss: 0.608493, mae: 18.842903, mean_q: -27.695360, mean_eps: 0.451658\n",
            " 112106/200000: episode: 342, duration: 3.046s, episode steps: 134, steps per second:  44, episode reward: -133.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.022 [0.000, 2.000],  loss: 0.566999, mae: 18.830793, mean_q: -27.691701, mean_eps: 0.451011\n",
            " 112301/200000: episode: 343, duration: 4.577s, episode steps: 195, steps per second:  43, episode reward: -194.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.123 [0.000, 2.000],  loss: 0.591506, mae: 18.802637, mean_q: -27.648279, mean_eps: 0.450205\n",
            " 112557/200000: episode: 344, duration: 6.948s, episode steps: 256, steps per second:  37, episode reward: -255.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.195 [0.000, 2.000],  loss: 0.567412, mae: 18.798334, mean_q: -27.639634, mean_eps: 0.449100\n",
            " 112754/200000: episode: 345, duration: 4.431s, episode steps: 197, steps per second:  44, episode reward: -196.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.005 [0.000, 2.000],  loss: 0.531905, mae: 18.783951, mean_q: -27.630304, mean_eps: 0.447991\n",
            " 112984/200000: episode: 346, duration: 6.216s, episode steps: 230, steps per second:  37, episode reward: -229.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.061 [0.000, 2.000],  loss: 0.568222, mae: 18.715370, mean_q: -27.513934, mean_eps: 0.446944\n",
            " 113161/200000: episode: 347, duration: 3.901s, episode steps: 177, steps per second:  45, episode reward: -176.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.062 [0.000, 2.000],  loss: 0.563627, mae: 18.750256, mean_q: -27.578338, mean_eps: 0.445947\n",
            " 113376/200000: episode: 348, duration: 4.778s, episode steps: 215, steps per second:  45, episode reward: -214.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.172 [0.000, 2.000],  loss: 0.560968, mae: 18.689513, mean_q: -27.476357, mean_eps: 0.444987\n",
            " 113478/200000: episode: 349, duration: 3.166s, episode steps: 102, steps per second:  32, episode reward: -101.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.039 [0.000, 2.000],  loss: 0.551096, mae: 18.666753, mean_q: -27.457258, mean_eps: 0.444210\n",
            " 113659/200000: episode: 350, duration: 4.326s, episode steps: 181, steps per second:  42, episode reward: -180.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.022 [0.000, 2.000],  loss: 0.580322, mae: 18.614816, mean_q: -27.363011, mean_eps: 0.443517\n",
            " 113838/200000: episode: 351, duration: 4.073s, episode steps: 179, steps per second:  44, episode reward: -178.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.084 [0.000, 2.000],  loss: 0.569170, mae: 18.630064, mean_q: -27.375670, mean_eps: 0.442635\n",
            " 114013/200000: episode: 352, duration: 5.154s, episode steps: 175, steps per second:  34, episode reward: -174.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.983 [0.000, 2.000],  loss: 0.592832, mae: 18.633012, mean_q: -27.381570, mean_eps: 0.441768\n",
            " 114168/200000: episode: 353, duration: 3.733s, episode steps: 155, steps per second:  42, episode reward: -154.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.045 [0.000, 2.000],  loss: 0.571418, mae: 18.599945, mean_q: -27.343282, mean_eps: 0.440959\n",
            " 114326/200000: episode: 354, duration: 3.537s, episode steps: 158, steps per second:  45, episode reward: -157.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.152 [0.000, 2.000],  loss: 0.580147, mae: 18.558931, mean_q: -27.277298, mean_eps: 0.440192\n",
            " 114513/200000: episode: 355, duration: 4.490s, episode steps: 187, steps per second:  42, episode reward: -186.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.118 [0.000, 2.000],  loss: 0.565714, mae: 18.552801, mean_q: -27.280208, mean_eps: 0.439347\n",
            " 114646/200000: episode: 356, duration: 3.859s, episode steps: 133, steps per second:  34, episode reward: -132.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.985 [0.000, 2.000],  loss: 0.579954, mae: 18.406457, mean_q: -27.051238, mean_eps: 0.438563\n",
            " 114778/200000: episode: 357, duration: 2.942s, episode steps: 132, steps per second:  45, episode reward: -131.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.992 [0.000, 2.000],  loss: 0.557027, mae: 18.436468, mean_q: -27.089517, mean_eps: 0.437914\n",
            " 114997/200000: episode: 358, duration: 4.866s, episode steps: 219, steps per second:  45, episode reward: -218.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.895 [0.000, 2.000],  loss: 0.558075, mae: 18.412660, mean_q: -27.057569, mean_eps: 0.437054\n",
            " 115107/200000: episode: 359, duration: 3.240s, episode steps: 110, steps per second:  34, episode reward: -109.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.566720, mae: 18.406937, mean_q: -27.056505, mean_eps: 0.436248\n",
            " 115252/200000: episode: 360, duration: 3.611s, episode steps: 145, steps per second:  40, episode reward: -144.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.138 [0.000, 2.000],  loss: 0.544519, mae: 18.439259, mean_q: -27.107056, mean_eps: 0.435623\n",
            " 115453/200000: episode: 361, duration: 4.449s, episode steps: 201, steps per second:  45, episode reward: -200.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.065 [0.000, 2.000],  loss: 0.570613, mae: 18.415775, mean_q: -27.063764, mean_eps: 0.434775\n",
            " 115660/200000: episode: 362, duration: 5.294s, episode steps: 207, steps per second:  39, episode reward: -206.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.058 [0.000, 2.000],  loss: 0.552590, mae: 18.441154, mean_q: -27.113112, mean_eps: 0.433776\n",
            " 115865/200000: episode: 363, duration: 5.093s, episode steps: 205, steps per second:  40, episode reward: -204.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.122 [0.000, 2.000],  loss: 0.578135, mae: 18.441400, mean_q: -27.102319, mean_eps: 0.432766\n",
            " 116030/200000: episode: 364, duration: 3.674s, episode steps: 165, steps per second:  45, episode reward: -164.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.018 [0.000, 2.000],  loss: 0.574384, mae: 18.439749, mean_q: -27.102916, mean_eps: 0.431860\n",
            " 116218/200000: episode: 365, duration: 5.138s, episode steps: 188, steps per second:  37, episode reward: -187.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.995 [0.000, 2.000],  loss: 0.582523, mae: 18.506197, mean_q: -27.208964, mean_eps: 0.430995\n",
            " 116333/200000: episode: 366, duration: 2.961s, episode steps: 115, steps per second:  39, episode reward: -114.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.052 [0.000, 2.000],  loss: 0.609141, mae: 18.485972, mean_q: -27.168703, mean_eps: 0.430253\n",
            " 116559/200000: episode: 367, duration: 5.275s, episode steps: 226, steps per second:  43, episode reward: -225.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.898 [0.000, 2.000],  loss: 0.573468, mae: 18.530041, mean_q: -27.245490, mean_eps: 0.429417\n",
            " 116758/200000: episode: 368, duration: 5.272s, episode steps: 199, steps per second:  38, episode reward: -198.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.995 [0.000, 2.000],  loss: 0.558868, mae: 18.543276, mean_q: -27.271399, mean_eps: 0.428376\n",
            " 116924/200000: episode: 369, duration: 3.917s, episode steps: 166, steps per second:  42, episode reward: -165.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.030 [0.000, 2.000],  loss: 0.583686, mae: 18.527315, mean_q: -27.240161, mean_eps: 0.427482\n",
            " 117064/200000: episode: 370, duration: 3.105s, episode steps: 140, steps per second:  45, episode reward: -139.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.114 [0.000, 2.000],  loss: 0.613624, mae: 18.506275, mean_q: -27.179174, mean_eps: 0.426732\n",
            " 117288/200000: episode: 371, duration: 5.597s, episode steps: 224, steps per second:  40, episode reward: -223.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.884 [0.000, 2.000],  loss: 0.598442, mae: 18.527513, mean_q: -27.215606, mean_eps: 0.425840\n",
            " 117496/200000: episode: 372, duration: 5.296s, episode steps: 208, steps per second:  39, episode reward: -207.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.990 [0.000, 2.000],  loss: 0.577422, mae: 18.498650, mean_q: -27.173630, mean_eps: 0.424782\n",
            " 117678/200000: episode: 373, duration: 4.117s, episode steps: 182, steps per second:  44, episode reward: -181.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.984 [0.000, 2.000],  loss: 0.601118, mae: 18.539148, mean_q: -27.246053, mean_eps: 0.423826\n",
            " 117841/200000: episode: 374, duration: 4.343s, episode steps: 163, steps per second:  38, episode reward: -162.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.578981, mae: 18.510388, mean_q: -27.192633, mean_eps: 0.422981\n",
            " 117974/200000: episode: 375, duration: 3.478s, episode steps: 133, steps per second:  38, episode reward: -132.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.547049, mae: 18.471304, mean_q: -27.132862, mean_eps: 0.422256\n",
            " 118199/200000: episode: 376, duration: 5.034s, episode steps: 225, steps per second:  45, episode reward: -224.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.031 [0.000, 2.000],  loss: 0.609652, mae: 18.483695, mean_q: -27.149763, mean_eps: 0.421379\n",
            " 118356/200000: episode: 377, duration: 3.630s, episode steps: 157, steps per second:  43, episode reward: -156.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.121 [0.000, 2.000],  loss: 0.590534, mae: 18.469903, mean_q: -27.128692, mean_eps: 0.420443\n",
            " 118546/200000: episode: 378, duration: 5.220s, episode steps: 190, steps per second:  36, episode reward: -189.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.989 [0.000, 2.000],  loss: 0.560432, mae: 18.445138, mean_q: -27.095709, mean_eps: 0.419593\n",
            " 118670/200000: episode: 379, duration: 2.787s, episode steps: 124, steps per second:  44, episode reward: -123.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.081 [0.000, 2.000],  loss: 0.547200, mae: 18.446013, mean_q: -27.102002, mean_eps: 0.418823\n",
            " 118849/200000: episode: 380, duration: 4.280s, episode steps: 179, steps per second:  42, episode reward: -178.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.073 [0.000, 2.000],  loss: 0.543381, mae: 18.455092, mean_q: -27.110050, mean_eps: 0.418081\n",
            " 119027/200000: episode: 381, duration: 5.170s, episode steps: 178, steps per second:  34, episode reward: -177.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.949 [0.000, 2.000],  loss: 0.560883, mae: 18.469844, mean_q: -27.130110, mean_eps: 0.417206\n",
            " 119219/200000: episode: 382, duration: 4.268s, episode steps: 192, steps per second:  45, episode reward: -191.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.021 [0.000, 2.000],  loss: 0.593484, mae: 18.409821, mean_q: -27.046759, mean_eps: 0.416300\n",
            " 119378/200000: episode: 383, duration: 3.648s, episode steps: 159, steps per second:  44, episode reward: -158.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.075 [0.000, 2.000],  loss: 0.569384, mae: 18.429453, mean_q: -27.071579, mean_eps: 0.415440\n",
            " 119599/200000: episode: 384, duration: 6.259s, episode steps: 221, steps per second:  35, episode reward: -220.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.959 [0.000, 2.000],  loss: 0.583079, mae: 18.396527, mean_q: -27.015769, mean_eps: 0.414509\n",
            " 119740/200000: episode: 385, duration: 3.190s, episode steps: 141, steps per second:  44, episode reward: -140.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.965 [0.000, 2.000],  loss: 0.568073, mae: 18.407101, mean_q: -27.043228, mean_eps: 0.413622\n",
            " 119937/200000: episode: 386, duration: 4.469s, episode steps: 197, steps per second:  44, episode reward: -196.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.091 [0.000, 2.000],  loss: 0.572122, mae: 18.453525, mean_q: -27.111930, mean_eps: 0.412794\n",
            " 120060/200000: episode: 387, duration: 3.807s, episode steps: 123, steps per second:  32, episode reward: -122.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.935 [0.000, 2.000],  loss: 0.582116, mae: 18.462582, mean_q: -27.126859, mean_eps: 0.412010\n",
            " 120185/200000: episode: 388, duration: 2.922s, episode steps: 125, steps per second:  43, episode reward: -124.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.572745, mae: 18.459139, mean_q: -27.118855, mean_eps: 0.411402\n",
            " 120341/200000: episode: 389, duration: 3.423s, episode steps: 156, steps per second:  46, episode reward: -155.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.962 [0.000, 2.000],  loss: 0.559921, mae: 18.401245, mean_q: -27.026378, mean_eps: 0.410714\n",
            " 120470/200000: episode: 390, duration: 2.868s, episode steps: 129, steps per second:  45, episode reward: -128.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.054 [0.000, 2.000],  loss: 0.568903, mae: 18.398445, mean_q: -27.030623, mean_eps: 0.410016\n",
            " 120679/200000: episode: 391, duration: 5.886s, episode steps: 209, steps per second:  36, episode reward: -208.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.938 [0.000, 2.000],  loss: 0.548470, mae: 18.350280, mean_q: -26.951340, mean_eps: 0.409187\n",
            " 120792/200000: episode: 392, duration: 2.593s, episode steps: 113, steps per second:  44, episode reward: -112.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.991 [0.000, 2.000],  loss: 0.556690, mae: 18.343272, mean_q: -26.936777, mean_eps: 0.408399\n",
            " 120932/200000: episode: 393, duration: 3.202s, episode steps: 140, steps per second:  44, episode reward: -139.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.993 [0.000, 2.000],  loss: 0.559475, mae: 18.253235, mean_q: -26.797676, mean_eps: 0.407779\n",
            " 121092/200000: episode: 394, duration: 3.775s, episode steps: 160, steps per second:  42, episode reward: -159.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.944 [0.000, 2.000],  loss: 0.551963, mae: 18.286567, mean_q: -26.853283, mean_eps: 0.407044\n",
            " 121263/200000: episode: 395, duration: 4.961s, episode steps: 171, steps per second:  34, episode reward: -170.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.111 [0.000, 2.000],  loss: 0.567136, mae: 18.200995, mean_q: -26.716565, mean_eps: 0.406233\n",
            " 121448/200000: episode: 396, duration: 4.104s, episode steps: 185, steps per second:  45, episode reward: -184.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.162 [0.000, 2.000],  loss: 0.533974, mae: 18.120579, mean_q: -26.610226, mean_eps: 0.405361\n",
            " 121582/200000: episode: 397, duration: 3.011s, episode steps: 134, steps per second:  45, episode reward: -133.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.030 [0.000, 2.000],  loss: 0.562699, mae: 18.092572, mean_q: -26.565324, mean_eps: 0.404579\n",
            " 121763/200000: episode: 398, duration: 5.181s, episode steps: 181, steps per second:  35, episode reward: -180.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.017 [0.000, 2.000],  loss: 0.543462, mae: 18.056973, mean_q: -26.516724, mean_eps: 0.403807\n",
            " 122012/200000: episode: 399, duration: 5.597s, episode steps: 249, steps per second:  44, episode reward: -248.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.108 [0.000, 2.000],  loss: 0.541448, mae: 18.041829, mean_q: -26.492638, mean_eps: 0.402754\n",
            " 122205/200000: episode: 400, duration: 4.749s, episode steps: 193, steps per second:  41, episode reward: -192.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.036 [0.000, 2.000],  loss: 0.544198, mae: 18.015430, mean_q: -26.465142, mean_eps: 0.401671\n",
            " 122339/200000: episode: 401, duration: 3.793s, episode steps: 134, steps per second:  35, episode reward: -133.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.963 [0.000, 2.000],  loss: 0.527892, mae: 18.020409, mean_q: -26.466369, mean_eps: 0.400870\n",
            " 122517/200000: episode: 402, duration: 4.088s, episode steps: 178, steps per second:  44, episode reward: -177.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.152 [0.000, 2.000],  loss: 0.540342, mae: 18.023202, mean_q: -26.468677, mean_eps: 0.400105\n",
            " 122702/200000: episode: 403, duration: 4.143s, episode steps: 185, steps per second:  45, episode reward: -184.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.968 [0.000, 2.000],  loss: 0.542703, mae: 17.981035, mean_q: -26.400838, mean_eps: 0.399216\n",
            " 122883/200000: episode: 404, duration: 5.236s, episode steps: 181, steps per second:  35, episode reward: -180.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.028 [0.000, 2.000],  loss: 0.577218, mae: 17.944852, mean_q: -26.330189, mean_eps: 0.398319\n",
            " 123022/200000: episode: 405, duration: 3.095s, episode steps: 139, steps per second:  45, episode reward: -138.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.065 [0.000, 2.000],  loss: 0.580793, mae: 17.962494, mean_q: -26.358331, mean_eps: 0.397535\n",
            " 123135/200000: episode: 406, duration: 2.512s, episode steps: 113, steps per second:  45, episode reward: -112.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.071 [0.000, 2.000],  loss: 0.556807, mae: 17.931171, mean_q: -26.333676, mean_eps: 0.396918\n",
            " 123276/200000: episode: 407, duration: 3.312s, episode steps: 141, steps per second:  43, episode reward: -140.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.064 [0.000, 2.000],  loss: 0.526763, mae: 17.891438, mean_q: -26.272553, mean_eps: 0.396296\n",
            " 123467/200000: episode: 408, duration: 5.372s, episode steps: 191, steps per second:  36, episode reward: -190.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.162 [0.000, 2.000],  loss: 0.558266, mae: 17.834818, mean_q: -26.181408, mean_eps: 0.395482\n",
            " 123614/200000: episode: 409, duration: 3.655s, episode steps: 147, steps per second:  40, episode reward: -146.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.109 [0.000, 2.000],  loss: 0.532065, mae: 17.841561, mean_q: -26.203428, mean_eps: 0.394654\n",
            " 123769/200000: episode: 410, duration: 3.488s, episode steps: 155, steps per second:  44, episode reward: -154.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.135 [0.000, 2.000],  loss: 0.581061, mae: 17.823497, mean_q: -26.176637, mean_eps: 0.393914\n",
            " 123967/200000: episode: 411, duration: 5.665s, episode steps: 198, steps per second:  35, episode reward: -197.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.131 [0.000, 2.000],  loss: 0.581509, mae: 17.818505, mean_q: -26.155985, mean_eps: 0.393049\n",
            " 124129/200000: episode: 412, duration: 3.699s, episode steps: 162, steps per second:  44, episode reward: -161.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.117 [0.000, 2.000],  loss: 0.557564, mae: 17.833787, mean_q: -26.196006, mean_eps: 0.392167\n",
            " 124254/200000: episode: 413, duration: 2.873s, episode steps: 125, steps per second:  44, episode reward: -124.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.904 [0.000, 2.000],  loss: 0.502348, mae: 17.842062, mean_q: -26.206022, mean_eps: 0.391464\n",
            " 124454/200000: episode: 414, duration: 5.680s, episode steps: 200, steps per second:  35, episode reward: -199.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.165 [0.000, 2.000],  loss: 0.553106, mae: 17.817003, mean_q: -26.163716, mean_eps: 0.390668\n",
            " 124610/200000: episode: 415, duration: 3.557s, episode steps: 156, steps per second:  44, episode reward: -155.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.013 [0.000, 2.000],  loss: 0.565960, mae: 17.803291, mean_q: -26.131434, mean_eps: 0.389796\n",
            " 124768/200000: episode: 416, duration: 3.515s, episode steps: 158, steps per second:  45, episode reward: -157.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.968 [0.000, 2.000],  loss: 0.578201, mae: 17.806625, mean_q: -26.143569, mean_eps: 0.389026\n",
            " 124903/200000: episode: 417, duration: 3.184s, episode steps: 135, steps per second:  42, episode reward: -134.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.089 [0.000, 2.000],  loss: 0.581282, mae: 17.907145, mean_q: -26.283838, mean_eps: 0.388309\n",
            " 125058/200000: episode: 418, duration: 4.335s, episode steps: 155, steps per second:  36, episode reward: -154.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.039 [0.000, 2.000],  loss: 0.541373, mae: 17.907788, mean_q: -26.296347, mean_eps: 0.387598\n",
            " 125220/200000: episode: 419, duration: 3.592s, episode steps: 162, steps per second:  45, episode reward: -161.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.562752, mae: 17.853116, mean_q: -26.208786, mean_eps: 0.386821\n",
            " 125362/200000: episode: 420, duration: 3.127s, episode steps: 142, steps per second:  45, episode reward: -141.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.014 [0.000, 2.000],  loss: 0.616213, mae: 17.849205, mean_q: -26.209630, mean_eps: 0.386077\n",
            " 125506/200000: episode: 421, duration: 3.860s, episode steps: 144, steps per second:  37, episode reward: -143.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.965 [0.000, 2.000],  loss: 0.586471, mae: 17.903709, mean_q: -26.292928, mean_eps: 0.385376\n",
            " 125641/200000: episode: 422, duration: 3.563s, episode steps: 135, steps per second:  38, episode reward: -134.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.067 [0.000, 2.000],  loss: 0.585695, mae: 17.914647, mean_q: -26.296103, mean_eps: 0.384692\n",
            " 125757/200000: episode: 423, duration: 2.559s, episode steps: 116, steps per second:  45, episode reward: -115.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.034 [0.000, 2.000],  loss: 0.608266, mae: 17.892437, mean_q: -26.257940, mean_eps: 0.384077\n",
            " 125893/200000: episode: 424, duration: 3.027s, episode steps: 136, steps per second:  45, episode reward: -135.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.978 [0.000, 2.000],  loss: 0.557564, mae: 17.952677, mean_q: -26.362071, mean_eps: 0.383460\n",
            " 126082/200000: episode: 425, duration: 5.622s, episode steps: 189, steps per second:  34, episode reward: -188.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.063 [0.000, 2.000],  loss: 0.577186, mae: 17.892637, mean_q: -26.273375, mean_eps: 0.382664\n",
            " 126275/200000: episode: 426, duration: 4.501s, episode steps: 193, steps per second:  43, episode reward: -192.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.959 [0.000, 2.000],  loss: 0.589074, mae: 17.892735, mean_q: -26.267807, mean_eps: 0.381728\n",
            " 126420/200000: episode: 427, duration: 3.263s, episode steps: 145, steps per second:  44, episode reward: -144.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.938 [0.000, 2.000],  loss: 0.634133, mae: 17.886183, mean_q: -26.243684, mean_eps: 0.380900\n",
            " 126551/200000: episode: 428, duration: 3.176s, episode steps: 131, steps per second:  41, episode reward: -130.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.870 [0.000, 2.000],  loss: 0.595529, mae: 17.873194, mean_q: -26.228874, mean_eps: 0.380224\n",
            " 126694/200000: episode: 429, duration: 4.118s, episode steps: 143, steps per second:  35, episode reward: -142.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.168 [0.000, 2.000],  loss: 0.572781, mae: 17.878641, mean_q: -26.230049, mean_eps: 0.379552\n",
            " 126880/200000: episode: 430, duration: 4.262s, episode steps: 186, steps per second:  44, episode reward: -185.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.935 [0.000, 2.000],  loss: 0.624524, mae: 17.870701, mean_q: -26.207614, mean_eps: 0.378746\n",
            " 127021/200000: episode: 431, duration: 3.268s, episode steps: 141, steps per second:  43, episode reward: -140.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.572473, mae: 17.881697, mean_q: -26.245321, mean_eps: 0.377945\n",
            " 127144/200000: episode: 432, duration: 3.737s, episode steps: 123, steps per second:  33, episode reward: -122.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.935 [0.000, 2.000],  loss: 0.585786, mae: 17.898030, mean_q: -26.260455, mean_eps: 0.377298\n",
            " 127317/200000: episode: 433, duration: 4.320s, episode steps: 173, steps per second:  40, episode reward: -172.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.617654, mae: 17.928895, mean_q: -26.303046, mean_eps: 0.376573\n",
            " 127504/200000: episode: 434, duration: 4.288s, episode steps: 187, steps per second:  44, episode reward: -186.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.070 [0.000, 2.000],  loss: 0.635175, mae: 17.917189, mean_q: -26.288556, mean_eps: 0.375691\n",
            " 127626/200000: episode: 435, duration: 3.063s, episode steps: 122, steps per second:  40, episode reward: -121.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.975 [0.000, 2.000],  loss: 0.605396, mae: 17.950520, mean_q: -26.336625, mean_eps: 0.374934\n",
            " 127750/200000: episode: 436, duration: 3.904s, episode steps: 124, steps per second:  32, episode reward: -123.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.944 [0.000, 2.000],  loss: 0.611489, mae: 17.922494, mean_q: -26.280977, mean_eps: 0.374331\n",
            " 128011/200000: episode: 437, duration: 6.030s, episode steps: 261, steps per second:  43, episode reward: -260.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.069 [0.000, 2.000],  loss: 0.592124, mae: 17.933560, mean_q: -26.315309, mean_eps: 0.373388\n",
            " 128106/200000: episode: 438, duration: 2.192s, episode steps:  95, steps per second:  43, episode reward: -94.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.979 [0.000, 2.000],  loss: 0.586834, mae: 17.981824, mean_q: -26.384425, mean_eps: 0.372516\n",
            " 128327/200000: episode: 439, duration: 6.417s, episode steps: 221, steps per second:  34, episode reward: -220.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.991 [0.000, 2.000],  loss: 0.559816, mae: 17.943040, mean_q: -26.335566, mean_eps: 0.371742\n",
            " 128530/200000: episode: 440, duration: 4.985s, episode steps: 203, steps per second:  41, episode reward: -202.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.158 [0.000, 2.000],  loss: 0.572478, mae: 17.958281, mean_q: -26.358102, mean_eps: 0.370703\n",
            " 128653/200000: episode: 441, duration: 2.836s, episode steps: 123, steps per second:  43, episode reward: -122.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.130 [0.000, 2.000],  loss: 0.586716, mae: 17.893686, mean_q: -26.255039, mean_eps: 0.369904\n",
            " 128894/200000: episode: 442, duration: 6.671s, episode steps: 241, steps per second:  36, episode reward: -240.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.004 [0.000, 2.000],  loss: 0.593797, mae: 17.903678, mean_q: -26.262662, mean_eps: 0.369012\n",
            " 128994/200000: episode: 443, duration: 2.316s, episode steps: 100, steps per second:  43, episode reward: -99.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.050 [0.000, 2.000],  loss: 0.534607, mae: 17.911960, mean_q: -26.287240, mean_eps: 0.368177\n",
            " 129128/200000: episode: 444, duration: 3.043s, episode steps: 134, steps per second:  44, episode reward: -133.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.866 [0.000, 2.000],  loss: 0.573519, mae: 17.902524, mean_q: -26.255285, mean_eps: 0.367604\n",
            " 129240/200000: episode: 445, duration: 2.896s, episode steps: 112, steps per second:  39, episode reward: -111.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.080 [0.000, 2.000],  loss: 0.562277, mae: 18.012163, mean_q: -26.427992, mean_eps: 0.367001\n",
            " 129418/200000: episode: 446, duration: 4.968s, episode steps: 178, steps per second:  36, episode reward: -177.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.185 [0.000, 2.000],  loss: 0.588697, mae: 17.891223, mean_q: -26.239660, mean_eps: 0.366290\n",
            " 129519/200000: episode: 447, duration: 2.336s, episode steps: 101, steps per second:  43, episode reward: -100.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.208 [0.000, 2.000],  loss: 0.598831, mae: 17.848638, mean_q: -26.156624, mean_eps: 0.365607\n",
            " 129708/200000: episode: 448, duration: 4.326s, episode steps: 189, steps per second:  44, episode reward: -188.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.190 [0.000, 2.000],  loss: 0.577283, mae: 17.905299, mean_q: -26.247403, mean_eps: 0.364896\n",
            " 129832/200000: episode: 449, duration: 3.885s, episode steps: 124, steps per second:  32, episode reward: -123.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.169 [0.000, 2.000],  loss: 0.574342, mae: 17.889814, mean_q: -26.231123, mean_eps: 0.364129\n",
            " 129958/200000: episode: 450, duration: 2.958s, episode steps: 126, steps per second:  43, episode reward: -125.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.960 [0.000, 2.000],  loss: 0.596265, mae: 17.891292, mean_q: -26.231996, mean_eps: 0.363517\n",
            " 130117/200000: episode: 451, duration: 3.566s, episode steps: 159, steps per second:  45, episode reward: -158.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.088 [0.000, 2.000],  loss: 0.577898, mae: 17.874484, mean_q: -26.201979, mean_eps: 0.362819\n",
            " 130215/200000: episode: 452, duration: 2.188s, episode steps:  98, steps per second:  45, episode reward: -97.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.959 [0.000, 2.000],  loss: 0.554727, mae: 17.932624, mean_q: -26.296176, mean_eps: 0.362189\n",
            " 130397/200000: episode: 453, duration: 5.362s, episode steps: 182, steps per second:  34, episode reward: -181.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.077 [0.000, 2.000],  loss: 0.586320, mae: 17.857881, mean_q: -26.187099, mean_eps: 0.361503\n",
            " 130552/200000: episode: 454, duration: 3.649s, episode steps: 155, steps per second:  42, episode reward: -154.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.039 [0.000, 2.000],  loss: 0.591188, mae: 17.835454, mean_q: -26.150754, mean_eps: 0.360677\n",
            " 130722/200000: episode: 455, duration: 3.959s, episode steps: 170, steps per second:  43, episode reward: -169.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.100 [0.000, 2.000],  loss: 0.626146, mae: 17.840899, mean_q: -26.158182, mean_eps: 0.359881\n",
            " 130950/200000: episode: 456, duration: 6.768s, episode steps: 228, steps per second:  34, episode reward: -227.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.031 [0.000, 2.000],  loss: 0.609972, mae: 17.836446, mean_q: -26.162275, mean_eps: 0.358906\n",
            " 131089/200000: episode: 457, duration: 3.155s, episode steps: 139, steps per second:  44, episode reward: -138.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.993 [0.000, 2.000],  loss: 0.593307, mae: 17.801485, mean_q: -26.112705, mean_eps: 0.358007\n",
            " 131253/200000: episode: 458, duration: 3.739s, episode steps: 164, steps per second:  44, episode reward: -163.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.951 [0.000, 2.000],  loss: 0.589220, mae: 17.811826, mean_q: -26.108326, mean_eps: 0.357265\n",
            " 131383/200000: episode: 459, duration: 3.390s, episode steps: 130, steps per second:  38, episode reward: -129.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.915 [0.000, 2.000],  loss: 0.600852, mae: 17.786792, mean_q: -26.076885, mean_eps: 0.356544\n",
            " 131515/200000: episode: 460, duration: 3.780s, episode steps: 132, steps per second:  35, episode reward: -131.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.970 [0.000, 2.000],  loss: 0.594631, mae: 17.744717, mean_q: -26.019154, mean_eps: 0.355902\n",
            " 131647/200000: episode: 461, duration: 3.029s, episode steps: 132, steps per second:  44, episode reward: -131.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.038 [0.000, 2.000],  loss: 0.608035, mae: 17.760027, mean_q: -26.042791, mean_eps: 0.355256\n",
            " 131766/200000: episode: 462, duration: 2.686s, episode steps: 119, steps per second:  44, episode reward: -118.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.832 [0.000, 2.000],  loss: 0.549264, mae: 17.690478, mean_q: -25.948444, mean_eps: 0.354641\n",
            " 131948/200000: episode: 463, duration: 5.117s, episode steps: 182, steps per second:  36, episode reward: -181.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.907 [0.000, 2.000],  loss: 0.572222, mae: 17.688505, mean_q: -25.950019, mean_eps: 0.353903\n",
            " 132060/200000: episode: 464, duration: 3.113s, episode steps: 112, steps per second:  36, episode reward: -111.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.973 [0.000, 2.000],  loss: 0.577597, mae: 17.722674, mean_q: -25.994035, mean_eps: 0.353183\n",
            " 132247/200000: episode: 465, duration: 4.369s, episode steps: 187, steps per second:  43, episode reward: -186.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.904 [0.000, 2.000],  loss: 0.586410, mae: 17.663167, mean_q: -25.898040, mean_eps: 0.352450\n",
            " 132403/200000: episode: 466, duration: 3.644s, episode steps: 156, steps per second:  43, episode reward: -155.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.083 [0.000, 2.000],  loss: 0.587498, mae: 17.668453, mean_q: -25.909629, mean_eps: 0.351610\n",
            " 132520/200000: episode: 467, duration: 3.914s, episode steps: 117, steps per second:  30, episode reward: -116.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.077 [0.000, 2.000],  loss: 0.609820, mae: 17.694571, mean_q: -25.958059, mean_eps: 0.350941\n",
            " 132609/200000: episode: 468, duration: 2.101s, episode steps:  89, steps per second:  42, episode reward: -88.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.590944, mae: 17.648528, mean_q: -25.878838, mean_eps: 0.350436\n",
            " 132753/200000: episode: 469, duration: 3.335s, episode steps: 144, steps per second:  43, episode reward: -143.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.069 [0.000, 2.000],  loss: 0.613352, mae: 17.661153, mean_q: -25.892879, mean_eps: 0.349866\n",
            " 132859/200000: episode: 470, duration: 2.441s, episode steps: 106, steps per second:  43, episode reward: -105.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.132 [0.000, 2.000],  loss: 0.619193, mae: 17.744402, mean_q: -26.001283, mean_eps: 0.349253\n",
            " 133034/200000: episode: 471, duration: 5.146s, episode steps: 175, steps per second:  34, episode reward: -174.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.109 [0.000, 2.000],  loss: 0.589058, mae: 17.751762, mean_q: -26.023363, mean_eps: 0.348565\n",
            " 133159/200000: episode: 472, duration: 2.967s, episode steps: 125, steps per second:  42, episode reward: -124.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.976 [0.000, 2.000],  loss: 0.620225, mae: 17.768796, mean_q: -26.047582, mean_eps: 0.347830\n",
            " 133299/200000: episode: 473, duration: 3.528s, episode steps: 140, steps per second:  40, episode reward: -139.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.014 [0.000, 2.000],  loss: 0.617215, mae: 17.764799, mean_q: -26.045306, mean_eps: 0.347180\n",
            " 133454/200000: episode: 474, duration: 3.465s, episode steps: 155, steps per second:  45, episode reward: -154.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.903 [0.000, 2.000],  loss: 0.575419, mae: 17.830130, mean_q: -26.144435, mean_eps: 0.346458\n",
            " 133578/200000: episode: 475, duration: 3.930s, episode steps: 124, steps per second:  32, episode reward: -123.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.040 [0.000, 2.000],  loss: 0.622533, mae: 17.799392, mean_q: -26.085799, mean_eps: 0.345774\n",
            " 133741/200000: episode: 476, duration: 3.688s, episode steps: 163, steps per second:  44, episode reward: -162.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.926 [0.000, 2.000],  loss: 0.619866, mae: 17.817932, mean_q: -26.112833, mean_eps: 0.345071\n",
            " 133883/200000: episode: 477, duration: 3.227s, episode steps: 142, steps per second:  44, episode reward: -141.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.099 [0.000, 2.000],  loss: 0.639410, mae: 17.852060, mean_q: -26.158311, mean_eps: 0.344324\n",
            " 133999/200000: episode: 478, duration: 2.599s, episode steps: 116, steps per second:  45, episode reward: -115.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.129 [0.000, 2.000],  loss: 0.627776, mae: 17.862791, mean_q: -26.179240, mean_eps: 0.343692\n",
            " 134151/200000: episode: 479, duration: 4.659s, episode steps: 152, steps per second:  33, episode reward: -151.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.974 [0.000, 2.000],  loss: 0.597544, mae: 17.868327, mean_q: -26.200921, mean_eps: 0.343035\n",
            " 134297/200000: episode: 480, duration: 3.291s, episode steps: 146, steps per second:  44, episode reward: -145.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.952 [0.000, 2.000],  loss: 0.604270, mae: 17.878061, mean_q: -26.219680, mean_eps: 0.342305\n",
            " 134487/200000: episode: 481, duration: 4.220s, episode steps: 190, steps per second:  45, episode reward: -189.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.147 [0.000, 2.000],  loss: 0.631672, mae: 17.917734, mean_q: -26.273366, mean_eps: 0.341482\n",
            " 134719/200000: episode: 482, duration: 6.261s, episode steps: 232, steps per second:  37, episode reward: -231.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.901 [0.000, 2.000],  loss: 0.620265, mae: 17.982214, mean_q: -26.381534, mean_eps: 0.340448\n",
            " 134811/200000: episode: 483, duration: 2.031s, episode steps:  92, steps per second:  45, episode reward: -91.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.022 [0.000, 2.000],  loss: 0.594384, mae: 17.954582, mean_q: -26.328242, mean_eps: 0.339654\n",
            " 134936/200000: episode: 484, duration: 2.752s, episode steps: 125, steps per second:  45, episode reward: -124.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.176 [0.000, 2.000],  loss: 0.619525, mae: 17.959325, mean_q: -26.321671, mean_eps: 0.339122\n",
            " 135110/200000: episode: 485, duration: 3.823s, episode steps: 174, steps per second:  46, episode reward: -173.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.851 [0.000, 2.000],  loss: 0.633602, mae: 18.013687, mean_q: -26.408203, mean_eps: 0.338390\n",
            " 135459/200000: episode: 486, duration: 9.040s, episode steps: 349, steps per second:  39, episode reward: -348.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.244 [0.000, 2.000],  loss: 0.618874, mae: 17.987821, mean_q: -26.372245, mean_eps: 0.337108\n",
            " 135582/200000: episode: 487, duration: 2.794s, episode steps: 123, steps per second:  44, episode reward: -122.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.976 [0.000, 2.000],  loss: 0.652291, mae: 17.977165, mean_q: -26.343076, mean_eps: 0.335952\n",
            " 135706/200000: episode: 488, duration: 3.549s, episode steps: 124, steps per second:  35, episode reward: -123.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.008 [0.000, 2.000],  loss: 0.663088, mae: 17.945869, mean_q: -26.291698, mean_eps: 0.335347\n",
            " 135871/200000: episode: 489, duration: 4.376s, episode steps: 165, steps per second:  38, episode reward: -164.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.109 [0.000, 2.000],  loss: 0.644117, mae: 17.957641, mean_q: -26.310336, mean_eps: 0.334639\n",
            " 136034/200000: episode: 490, duration: 3.621s, episode steps: 163, steps per second:  45, episode reward: -162.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.074 [0.000, 2.000],  loss: 0.664362, mae: 17.891857, mean_q: -26.199070, mean_eps: 0.333835\n",
            " 136181/200000: episode: 491, duration: 3.331s, episode steps: 147, steps per second:  44, episode reward: -146.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.068 [0.000, 2.000],  loss: 0.609171, mae: 17.861617, mean_q: -26.190267, mean_eps: 0.333076\n",
            " 136333/200000: episode: 492, duration: 4.702s, episode steps: 152, steps per second:  32, episode reward: -151.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.086 [0.000, 2.000],  loss: 0.643453, mae: 17.855187, mean_q: -26.167582, mean_eps: 0.332343\n",
            " 136479/200000: episode: 493, duration: 3.281s, episode steps: 146, steps per second:  45, episode reward: -145.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.966 [0.000, 2.000],  loss: 0.614454, mae: 17.913359, mean_q: -26.260890, mean_eps: 0.331613\n",
            " 136598/200000: episode: 494, duration: 2.693s, episode steps: 119, steps per second:  44, episode reward: -118.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.975 [0.000, 2.000],  loss: 0.622476, mae: 17.832522, mean_q: -26.146229, mean_eps: 0.330964\n",
            " 136689/200000: episode: 495, duration: 2.041s, episode steps:  91, steps per second:  45, episode reward: -90.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.646155, mae: 17.809771, mean_q: -26.097987, mean_eps: 0.330449\n",
            " 136852/200000: episode: 496, duration: 4.796s, episode steps: 163, steps per second:  34, episode reward: -162.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.147 [0.000, 2.000],  loss: 0.590397, mae: 17.842196, mean_q: -26.154155, mean_eps: 0.329827\n",
            " 137012/200000: episode: 497, duration: 3.752s, episode steps: 160, steps per second:  43, episode reward: -159.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.094 [0.000, 2.000],  loss: 0.616708, mae: 17.833152, mean_q: -26.125836, mean_eps: 0.329036\n",
            " 137194/200000: episode: 498, duration: 4.086s, episode steps: 182, steps per second:  45, episode reward: -181.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.978 [0.000, 2.000],  loss: 0.649405, mae: 17.827554, mean_q: -26.114831, mean_eps: 0.328198\n",
            " 137359/200000: episode: 499, duration: 4.341s, episode steps: 165, steps per second:  38, episode reward: -164.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.036 [0.000, 2.000],  loss: 0.624964, mae: 17.752630, mean_q: -25.995227, mean_eps: 0.327348\n",
            " 137550/200000: episode: 500, duration: 4.820s, episode steps: 191, steps per second:  40, episode reward: -190.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.026 [0.000, 2.000],  loss: 0.613474, mae: 17.756488, mean_q: -25.997836, mean_eps: 0.326475\n",
            " 137691/200000: episode: 501, duration: 3.197s, episode steps: 141, steps per second:  44, episode reward: -140.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.149 [0.000, 2.000],  loss: 0.616209, mae: 17.678269, mean_q: -25.888142, mean_eps: 0.325662\n",
            " 137920/200000: episode: 502, duration: 6.134s, episode steps: 229, steps per second:  37, episode reward: -228.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.035 [0.000, 2.000],  loss: 0.642242, mae: 17.727704, mean_q: -25.955089, mean_eps: 0.324756\n",
            " 138071/200000: episode: 503, duration: 4.097s, episode steps: 151, steps per second:  37, episode reward: -150.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.007 [0.000, 2.000],  loss: 0.627107, mae: 17.689151, mean_q: -25.896745, mean_eps: 0.323825\n",
            " 138241/200000: episode: 504, duration: 3.908s, episode steps: 170, steps per second:  43, episode reward: -169.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.847 [0.000, 2.000],  loss: 0.596938, mae: 17.728437, mean_q: -25.963527, mean_eps: 0.323038\n",
            " 138408/200000: episode: 505, duration: 4.236s, episode steps: 167, steps per second:  39, episode reward: -166.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.066 [0.000, 2.000],  loss: 0.607209, mae: 17.710944, mean_q: -25.928795, mean_eps: 0.322212\n",
            " 138529/200000: episode: 506, duration: 3.615s, episode steps: 121, steps per second:  33, episode reward: -120.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.612147, mae: 17.713729, mean_q: -25.941668, mean_eps: 0.321507\n",
            " 138681/200000: episode: 507, duration: 3.710s, episode steps: 152, steps per second:  41, episode reward: -151.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.171 [0.000, 2.000],  loss: 0.608516, mae: 17.731443, mean_q: -25.965902, mean_eps: 0.320838\n",
            " 138789/200000: episode: 508, duration: 2.514s, episode steps: 108, steps per second:  43, episode reward: -107.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.111 [0.000, 2.000],  loss: 0.592341, mae: 17.702565, mean_q: -25.930546, mean_eps: 0.320201\n",
            " 138938/200000: episode: 509, duration: 3.997s, episode steps: 149, steps per second:  37, episode reward: -148.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.047 [0.000, 2.000],  loss: 0.624674, mae: 17.660602, mean_q: -25.859456, mean_eps: 0.319571\n",
            " 139096/200000: episode: 510, duration: 4.398s, episode steps: 158, steps per second:  36, episode reward: -157.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.044 [0.000, 2.000],  loss: 0.616292, mae: 17.618810, mean_q: -25.767581, mean_eps: 0.318819\n",
            " 139249/200000: episode: 511, duration: 3.420s, episode steps: 153, steps per second:  45, episode reward: -152.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.085 [0.000, 2.000],  loss: 0.618176, mae: 17.587441, mean_q: -25.736434, mean_eps: 0.318057\n",
            " 139372/200000: episode: 512, duration: 2.766s, episode steps: 123, steps per second:  44, episode reward: -122.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.033 [0.000, 2.000],  loss: 0.661699, mae: 17.532141, mean_q: -25.638224, mean_eps: 0.317381\n",
            " 139546/200000: episode: 513, duration: 5.127s, episode steps: 174, steps per second:  34, episode reward: -173.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.948 [0.000, 2.000],  loss: 0.598562, mae: 17.591061, mean_q: -25.745610, mean_eps: 0.316653\n",
            " 139690/200000: episode: 514, duration: 3.186s, episode steps: 144, steps per second:  45, episode reward: -143.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.167 [0.000, 2.000],  loss: 0.581987, mae: 17.516793, mean_q: -25.651078, mean_eps: 0.315874\n",
            " 139876/200000: episode: 515, duration: 4.240s, episode steps: 186, steps per second:  44, episode reward: -185.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.102 [0.000, 2.000],  loss: 0.600702, mae: 17.543724, mean_q: -25.692781, mean_eps: 0.315066\n",
            " 140022/200000: episode: 516, duration: 3.812s, episode steps: 146, steps per second:  38, episode reward: -145.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.048 [0.000, 2.000],  loss: 0.606760, mae: 17.502501, mean_q: -25.625962, mean_eps: 0.314252\n",
            " 140190/200000: episode: 517, duration: 5.005s, episode steps: 168, steps per second:  34, episode reward: -167.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.929 [0.000, 2.000],  loss: 0.602053, mae: 17.457627, mean_q: -25.561191, mean_eps: 0.313483\n",
            " 140310/200000: episode: 518, duration: 3.000s, episode steps: 120, steps per second:  40, episode reward: -119.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.908 [0.000, 2.000],  loss: 0.607497, mae: 17.428484, mean_q: -25.513647, mean_eps: 0.312777\n",
            " 140455/200000: episode: 519, duration: 3.839s, episode steps: 145, steps per second:  38, episode reward: -144.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.966 [0.000, 2.000],  loss: 0.599580, mae: 17.437546, mean_q: -25.536294, mean_eps: 0.312128\n",
            " 140648/200000: episode: 520, duration: 5.760s, episode steps: 193, steps per second:  34, episode reward: -192.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.166 [0.000, 2.000],  loss: 0.575427, mae: 17.417094, mean_q: -25.508684, mean_eps: 0.311300\n",
            " 140872/200000: episode: 521, duration: 5.326s, episode steps: 224, steps per second:  42, episode reward: -223.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.013 [0.000, 2.000],  loss: 0.583292, mae: 17.453060, mean_q: -25.562156, mean_eps: 0.310278\n",
            " 141060/200000: episode: 522, duration: 5.211s, episode steps: 188, steps per second:  36, episode reward: -187.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.005 [0.000, 2.000],  loss: 0.614027, mae: 17.479555, mean_q: -25.587241, mean_eps: 0.309269\n",
            " 141256/200000: episode: 523, duration: 5.128s, episode steps: 196, steps per second:  38, episode reward: -195.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.189 [0.000, 2.000],  loss: 0.587995, mae: 17.530678, mean_q: -25.673561, mean_eps: 0.308328\n",
            " 141424/200000: episode: 524, duration: 3.989s, episode steps: 168, steps per second:  42, episode reward: -167.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.179 [0.000, 2.000],  loss: 0.556070, mae: 17.535988, mean_q: -25.685088, mean_eps: 0.307436\n",
            " 141627/200000: episode: 525, duration: 5.959s, episode steps: 203, steps per second:  34, episode reward: -202.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.133 [0.000, 2.000],  loss: 0.572328, mae: 17.510896, mean_q: -25.653564, mean_eps: 0.306528\n",
            " 141785/200000: episode: 526, duration: 3.620s, episode steps: 158, steps per second:  44, episode reward: -157.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.120 [0.000, 2.000],  loss: 0.590013, mae: 17.471957, mean_q: -25.591488, mean_eps: 0.305643\n",
            " 141899/200000: episode: 527, duration: 2.590s, episode steps: 114, steps per second:  44, episode reward: -113.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.088 [0.000, 2.000],  loss: 0.571425, mae: 17.467401, mean_q: -25.575119, mean_eps: 0.304977\n",
            " 142038/200000: episode: 528, duration: 3.115s, episode steps: 139, steps per second:  45, episode reward: -138.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.036 [0.000, 2.000],  loss: 0.585059, mae: 17.463918, mean_q: -25.570628, mean_eps: 0.304357\n",
            " 142188/200000: episode: 529, duration: 4.635s, episode steps: 150, steps per second:  32, episode reward: -149.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.127 [0.000, 2.000],  loss: 0.626213, mae: 17.414646, mean_q: -25.495518, mean_eps: 0.303649\n",
            " 142400/200000: episode: 530, duration: 4.881s, episode steps: 212, steps per second:  43, episode reward: -211.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.118 [0.000, 2.000],  loss: 0.624880, mae: 17.380179, mean_q: -25.428393, mean_eps: 0.302762\n",
            " 142567/200000: episode: 531, duration: 3.914s, episode steps: 167, steps per second:  43, episode reward: -166.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.078 [0.000, 2.000],  loss: 0.609658, mae: 17.412344, mean_q: -25.475006, mean_eps: 0.301833\n",
            " 142712/200000: episode: 532, duration: 4.533s, episode steps: 145, steps per second:  32, episode reward: -144.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.959 [0.000, 2.000],  loss: 0.587508, mae: 17.352154, mean_q: -25.402279, mean_eps: 0.301069\n",
            " 142862/200000: episode: 533, duration: 3.653s, episode steps: 150, steps per second:  41, episode reward: -149.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.087 [0.000, 2.000],  loss: 0.628560, mae: 17.346495, mean_q: -25.387444, mean_eps: 0.300346\n",
            " 142981/200000: episode: 534, duration: 2.660s, episode steps: 119, steps per second:  45, episode reward: -118.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.916 [0.000, 2.000],  loss: 0.615844, mae: 17.326765, mean_q: -25.341752, mean_eps: 0.299687\n",
            " 143110/200000: episode: 535, duration: 2.977s, episode steps: 129, steps per second:  43, episode reward: -128.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.953 [0.000, 2.000],  loss: 0.615355, mae: 17.339917, mean_q: -25.373764, mean_eps: 0.299080\n",
            " 143248/200000: episode: 536, duration: 4.417s, episode steps: 138, steps per second:  31, episode reward: -137.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.920 [0.000, 2.000],  loss: 0.608873, mae: 17.334127, mean_q: -25.375084, mean_eps: 0.298425\n",
            " 143422/200000: episode: 537, duration: 3.979s, episode steps: 174, steps per second:  44, episode reward: -173.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.977 [0.000, 2.000],  loss: 0.615937, mae: 17.338395, mean_q: -25.402237, mean_eps: 0.297661\n",
            " 143544/200000: episode: 538, duration: 2.826s, episode steps: 122, steps per second:  43, episode reward: -121.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.959 [0.000, 2.000],  loss: 0.612579, mae: 17.383872, mean_q: -25.462236, mean_eps: 0.296936\n",
            " 143680/200000: episode: 539, duration: 3.332s, episode steps: 136, steps per second:  41, episode reward: -135.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.074 [0.000, 2.000],  loss: 0.663364, mae: 17.348637, mean_q: -25.403534, mean_eps: 0.296304\n",
            " 143836/200000: episode: 540, duration: 4.625s, episode steps: 156, steps per second:  34, episode reward: -155.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.083 [0.000, 2.000],  loss: 0.616629, mae: 17.392916, mean_q: -25.481232, mean_eps: 0.295588\n",
            " 144022/200000: episode: 541, duration: 4.240s, episode steps: 186, steps per second:  44, episode reward: -185.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.903 [0.000, 2.000],  loss: 0.593728, mae: 17.351912, mean_q: -25.431185, mean_eps: 0.294750\n",
            " 144154/200000: episode: 542, duration: 2.961s, episode steps: 132, steps per second:  45, episode reward: -131.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.076 [0.000, 2.000],  loss: 0.619768, mae: 17.333864, mean_q: -25.393664, mean_eps: 0.293971\n",
            " 144291/200000: episode: 543, duration: 3.951s, episode steps: 137, steps per second:  35, episode reward: -136.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.226 [0.000, 2.000],  loss: 0.634407, mae: 17.368737, mean_q: -25.447966, mean_eps: 0.293312\n",
            " 144523/200000: episode: 544, duration: 5.586s, episode steps: 232, steps per second:  42, episode reward: -231.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.103 [0.000, 2.000],  loss: 0.672656, mae: 17.407552, mean_q: -25.473701, mean_eps: 0.292408\n",
            " 144658/200000: episode: 545, duration: 3.063s, episode steps: 135, steps per second:  44, episode reward: -134.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.044 [0.000, 2.000],  loss: 0.636523, mae: 17.455509, mean_q: -25.543110, mean_eps: 0.291509\n",
            " 144790/200000: episode: 546, duration: 3.352s, episode steps: 132, steps per second:  39, episode reward: -131.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.098 [0.000, 2.000],  loss: 0.625475, mae: 17.432663, mean_q: -25.509952, mean_eps: 0.290855\n",
            " 144916/200000: episode: 547, duration: 3.882s, episode steps: 126, steps per second:  32, episode reward: -125.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.143 [0.000, 2.000],  loss: 0.627374, mae: 17.458263, mean_q: -25.554353, mean_eps: 0.290223\n",
            " 145025/200000: episode: 548, duration: 2.533s, episode steps: 109, steps per second:  43, episode reward: -108.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.927 [0.000, 2.000],  loss: 0.649158, mae: 17.504628, mean_q: -25.616568, mean_eps: 0.289647\n",
            " 145243/200000: episode: 549, duration: 5.294s, episode steps: 218, steps per second:  41, episode reward: -217.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.069 [0.000, 2.000],  loss: 0.669179, mae: 17.563134, mean_q: -25.722105, mean_eps: 0.288846\n",
            " 145374/200000: episode: 550, duration: 3.969s, episode steps: 131, steps per second:  33, episode reward: -130.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.191 [0.000, 2.000],  loss: 0.627721, mae: 17.593938, mean_q: -25.748276, mean_eps: 0.287991\n",
            " 145565/200000: episode: 551, duration: 4.519s, episode steps: 191, steps per second:  42, episode reward: -190.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.246 [0.000, 2.000],  loss: 0.608791, mae: 17.609893, mean_q: -25.778360, mean_eps: 0.287202\n",
            " 145715/200000: episode: 552, duration: 3.412s, episode steps: 150, steps per second:  44, episode reward: -149.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.140 [0.000, 2.000],  loss: 0.613968, mae: 17.587691, mean_q: -25.744140, mean_eps: 0.286366\n",
            " 145835/200000: episode: 553, duration: 2.968s, episode steps: 120, steps per second:  40, episode reward: -119.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.942 [0.000, 2.000],  loss: 0.672522, mae: 17.590864, mean_q: -25.723557, mean_eps: 0.285705\n",
            " 145958/200000: episode: 554, duration: 3.899s, episode steps: 123, steps per second:  32, episode reward: -122.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.862 [0.000, 2.000],  loss: 0.662212, mae: 17.637626, mean_q: -25.805688, mean_eps: 0.285110\n",
            " 146115/200000: episode: 555, duration: 3.632s, episode steps: 157, steps per second:  43, episode reward: -156.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.955 [0.000, 2.000],  loss: 0.674772, mae: 17.560791, mean_q: -25.690567, mean_eps: 0.284424\n",
            " 146273/200000: episode: 556, duration: 3.706s, episode steps: 158, steps per second:  43, episode reward: -157.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.981 [0.000, 2.000],  loss: 0.668335, mae: 17.601407, mean_q: -25.761818, mean_eps: 0.283652\n",
            " 146393/200000: episode: 557, duration: 3.290s, episode steps: 120, steps per second:  36, episode reward: -119.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.058 [0.000, 2.000],  loss: 0.677806, mae: 17.596007, mean_q: -25.761089, mean_eps: 0.282971\n",
            " 146522/200000: episode: 558, duration: 3.663s, episode steps: 129, steps per second:  35, episode reward: -128.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.194 [0.000, 2.000],  loss: 0.671431, mae: 17.686316, mean_q: -25.887395, mean_eps: 0.282361\n",
            " 146657/200000: episode: 559, duration: 3.075s, episode steps: 135, steps per second:  44, episode reward: -134.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.044 [0.000, 2.000],  loss: 0.643599, mae: 17.781721, mean_q: -26.045790, mean_eps: 0.281714\n",
            " 146761/200000: episode: 560, duration: 2.421s, episode steps: 104, steps per second:  43, episode reward: -103.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.971 [0.000, 2.000],  loss: 0.636883, mae: 17.736944, mean_q: -25.964925, mean_eps: 0.281128\n",
            " 146890/200000: episode: 561, duration: 3.113s, episode steps: 129, steps per second:  41, episode reward: -128.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.876 [0.000, 2.000],  loss: 0.647749, mae: 17.722992, mean_q: -25.945172, mean_eps: 0.280558\n",
            " 147000/200000: episode: 562, duration: 3.568s, episode steps: 110, steps per second:  31, episode reward: -109.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.900 [0.000, 2.000],  loss: 0.660435, mae: 17.750180, mean_q: -25.969623, mean_eps: 0.279972\n",
            " 147172/200000: episode: 563, duration: 3.898s, episode steps: 172, steps per second:  44, episode reward: -171.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.029 [0.000, 2.000],  loss: 0.631833, mae: 17.743570, mean_q: -25.977321, mean_eps: 0.279281\n",
            " 147309/200000: episode: 564, duration: 3.146s, episode steps: 137, steps per second:  44, episode reward: -136.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.102 [0.000, 2.000],  loss: 0.608938, mae: 17.755741, mean_q: -25.985433, mean_eps: 0.278524\n",
            " 147466/200000: episode: 565, duration: 4.041s, episode steps: 157, steps per second:  39, episode reward: -156.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.013 [0.000, 2.000],  loss: 0.673878, mae: 17.730873, mean_q: -25.934043, mean_eps: 0.277804\n",
            " 147568/200000: episode: 566, duration: 3.114s, episode steps: 102, steps per second:  33, episode reward: -101.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.990 [0.000, 2.000],  loss: 0.628709, mae: 17.718570, mean_q: -25.923361, mean_eps: 0.277169\n",
            " 147680/200000: episode: 567, duration: 2.893s, episode steps: 112, steps per second:  39, episode reward: -111.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.812 [0.000, 2.000],  loss: 0.617835, mae: 17.702842, mean_q: -25.903687, mean_eps: 0.276645\n",
            " 147808/200000: episode: 568, duration: 2.958s, episode steps: 128, steps per second:  43, episode reward: -127.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.945 [0.000, 2.000],  loss: 0.650195, mae: 17.636147, mean_q: -25.794343, mean_eps: 0.276057\n",
            " 147956/200000: episode: 569, duration: 3.395s, episode steps: 148, steps per second:  44, episode reward: -147.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.986 [0.000, 2.000],  loss: 0.652020, mae: 17.675669, mean_q: -25.865915, mean_eps: 0.275381\n",
            " 148086/200000: episode: 570, duration: 4.147s, episode steps: 130, steps per second:  31, episode reward: -129.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.885 [0.000, 2.000],  loss: 0.652501, mae: 17.599406, mean_q: -25.735557, mean_eps: 0.274700\n",
            " 148208/200000: episode: 571, duration: 2.871s, episode steps: 122, steps per second:  42, episode reward: -121.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.652301, mae: 17.630823, mean_q: -25.793976, mean_eps: 0.274082\n",
            " 148368/200000: episode: 572, duration: 3.742s, episode steps: 160, steps per second:  43, episode reward: -159.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.113 [0.000, 2.000],  loss: 0.652426, mae: 17.606129, mean_q: -25.760206, mean_eps: 0.273391\n",
            " 148490/200000: episode: 573, duration: 2.944s, episode steps: 122, steps per second:  41, episode reward: -121.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.221 [0.000, 2.000],  loss: 0.651271, mae: 17.608195, mean_q: -25.771178, mean_eps: 0.272700\n",
            " 148638/200000: episode: 574, duration: 4.500s, episode steps: 148, steps per second:  33, episode reward: -147.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.162 [0.000, 2.000],  loss: 0.653268, mae: 17.666597, mean_q: -25.856997, mean_eps: 0.272039\n",
            " 148784/200000: episode: 575, duration: 3.456s, episode steps: 146, steps per second:  42, episode reward: -145.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.048 [0.000, 2.000],  loss: 0.629101, mae: 17.665367, mean_q: -25.838341, mean_eps: 0.271319\n",
            " 148932/200000: episode: 576, duration: 3.370s, episode steps: 148, steps per second:  44, episode reward: -147.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.182 [0.000, 2.000],  loss: 0.678326, mae: 17.715166, mean_q: -25.900999, mean_eps: 0.270598\n",
            " 149052/200000: episode: 577, duration: 3.178s, episode steps: 120, steps per second:  38, episode reward: -119.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.092 [0.000, 2.000],  loss: 0.715296, mae: 17.614182, mean_q: -25.730046, mean_eps: 0.269942\n",
            " 149167/200000: episode: 578, duration: 3.533s, episode steps: 115, steps per second:  33, episode reward: -114.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.061 [0.000, 2.000],  loss: 0.692606, mae: 17.649985, mean_q: -25.805861, mean_eps: 0.269366\n",
            " 149364/200000: episode: 579, duration: 4.690s, episode steps: 197, steps per second:  42, episode reward: -196.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.995 [0.000, 2.000],  loss: 0.638699, mae: 17.618275, mean_q: -25.766812, mean_eps: 0.268602\n",
            " 149550/200000: episode: 580, duration: 4.611s, episode steps: 186, steps per second:  40, episode reward: -185.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.113 [0.000, 2.000],  loss: 0.641855, mae: 17.656348, mean_q: -25.822762, mean_eps: 0.267663\n",
            " 149680/200000: episode: 581, duration: 3.990s, episode steps: 130, steps per second:  33, episode reward: -129.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.208 [0.000, 2.000],  loss: 0.652162, mae: 17.632760, mean_q: -25.784461, mean_eps: 0.266889\n",
            " 149835/200000: episode: 582, duration: 3.517s, episode steps: 155, steps per second:  44, episode reward: -154.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.265 [0.000, 2.000],  loss: 0.640147, mae: 17.630117, mean_q: -25.770681, mean_eps: 0.266191\n",
            " 149994/200000: episode: 583, duration: 3.637s, episode steps: 159, steps per second:  44, episode reward: -158.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.843 [0.000, 2.000],  loss: 0.661939, mae: 17.598108, mean_q: -25.712010, mean_eps: 0.265421\n",
            " 150133/200000: episode: 584, duration: 4.154s, episode steps: 139, steps per second:  33, episode reward: -138.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.638994, mae: 17.546696, mean_q: -25.660090, mean_eps: 0.264691\n",
            " 150277/200000: episode: 585, duration: 3.700s, episode steps: 144, steps per second:  39, episode reward: -143.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.104 [0.000, 2.000],  loss: 0.672100, mae: 17.486536, mean_q: -25.568915, mean_eps: 0.263998\n",
            " 150385/200000: episode: 586, duration: 2.486s, episode steps: 108, steps per second:  43, episode reward: -107.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.065 [0.000, 2.000],  loss: 0.686702, mae: 17.454932, mean_q: -25.516099, mean_eps: 0.263381\n",
            " 150507/200000: episode: 587, duration: 2.853s, episode steps: 122, steps per second:  43, episode reward: -121.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.115 [0.000, 2.000],  loss: 0.597190, mae: 17.416104, mean_q: -25.470067, mean_eps: 0.262817\n",
            " 150659/200000: episode: 588, duration: 4.067s, episode steps: 152, steps per second:  37, episode reward: -151.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.967 [0.000, 2.000],  loss: 0.610899, mae: 17.417057, mean_q: -25.480304, mean_eps: 0.262146\n",
            " 150781/200000: episode: 589, duration: 3.467s, episode steps: 122, steps per second:  35, episode reward: -121.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.943 [0.000, 2.000],  loss: 0.611509, mae: 17.458790, mean_q: -25.547775, mean_eps: 0.261474\n",
            " 150894/200000: episode: 590, duration: 2.597s, episode steps: 113, steps per second:  44, episode reward: -112.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.973 [0.000, 2.000],  loss: 0.592424, mae: 17.437049, mean_q: -25.511826, mean_eps: 0.260899\n",
            " 151042/200000: episode: 591, duration: 3.411s, episode steps: 148, steps per second:  43, episode reward: -147.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.980 [0.000, 2.000],  loss: 0.619096, mae: 17.437284, mean_q: -25.500483, mean_eps: 0.260259\n",
            " 151208/200000: episode: 592, duration: 4.539s, episode steps: 166, steps per second:  37, episode reward: -165.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.024 [0.000, 2.000],  loss: 0.627658, mae: 17.399235, mean_q: -25.463286, mean_eps: 0.259490\n",
            " 151352/200000: episode: 593, duration: 3.660s, episode steps: 144, steps per second:  39, episode reward: -143.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.028 [0.000, 2.000],  loss: 0.598621, mae: 17.388954, mean_q: -25.443879, mean_eps: 0.258730\n",
            " 151474/200000: episode: 594, duration: 2.739s, episode steps: 122, steps per second:  45, episode reward: -121.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.025 [0.000, 2.000],  loss: 0.653354, mae: 17.323236, mean_q: -25.345937, mean_eps: 0.258079\n",
            " 151609/200000: episode: 595, duration: 3.034s, episode steps: 135, steps per second:  44, episode reward: -134.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.096 [0.000, 2.000],  loss: 0.624062, mae: 17.373766, mean_q: -25.429790, mean_eps: 0.257449\n",
            " 151715/200000: episode: 596, duration: 2.577s, episode steps: 106, steps per second:  41, episode reward: -105.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.028 [0.000, 2.000],  loss: 0.610846, mae: 17.346495, mean_q: -25.382267, mean_eps: 0.256859\n",
            " 151871/200000: episode: 597, duration: 4.503s, episode steps: 156, steps per second:  35, episode reward: -155.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.051 [0.000, 2.000],  loss: 0.576034, mae: 17.408014, mean_q: -25.485396, mean_eps: 0.256217\n",
            " 152002/200000: episode: 598, duration: 3.075s, episode steps: 131, steps per second:  43, episode reward: -130.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.885 [0.000, 2.000],  loss: 0.618163, mae: 17.420508, mean_q: -25.490349, mean_eps: 0.255514\n",
            " 152159/200000: episode: 599, duration: 3.621s, episode steps: 157, steps per second:  43, episode reward: -156.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.955 [0.000, 2.000],  loss: 0.594927, mae: 17.370197, mean_q: -25.410326, mean_eps: 0.254808\n",
            " 152291/200000: episode: 600, duration: 3.587s, episode steps: 132, steps per second:  37, episode reward: -131.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.144 [0.000, 2.000],  loss: 0.598335, mae: 17.424154, mean_q: -25.492527, mean_eps: 0.254100\n",
            " 152407/200000: episode: 601, duration: 3.212s, episode steps: 116, steps per second:  36, episode reward: -115.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.983 [0.000, 2.000],  loss: 0.581750, mae: 17.415691, mean_q: -25.485114, mean_eps: 0.253492\n",
            " 152496/200000: episode: 602, duration: 2.361s, episode steps:  89, steps per second:  38, episode reward: -88.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.944 [0.000, 2.000],  loss: 0.599873, mae: 17.416685, mean_q: -25.500939, mean_eps: 0.252990\n",
            " 152666/200000: episode: 603, duration: 4.001s, episode steps: 170, steps per second:  42, episode reward: -169.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.959 [0.000, 2.000],  loss: 0.623059, mae: 17.459211, mean_q: -25.548528, mean_eps: 0.252356\n",
            " 152831/200000: episode: 604, duration: 4.636s, episode steps: 165, steps per second:  36, episode reward: -164.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.097 [0.000, 2.000],  loss: 0.589747, mae: 17.445337, mean_q: -25.536460, mean_eps: 0.251535\n",
            " 152982/200000: episode: 605, duration: 3.843s, episode steps: 151, steps per second:  39, episode reward: -150.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.026 [0.000, 2.000],  loss: 0.610984, mae: 17.469724, mean_q: -25.552689, mean_eps: 0.250761\n",
            " 153153/200000: episode: 606, duration: 3.933s, episode steps: 171, steps per second:  43, episode reward: -170.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.205 [0.000, 2.000],  loss: 0.634597, mae: 17.410044, mean_q: -25.476488, mean_eps: 0.249972\n",
            " 153277/200000: episode: 607, duration: 2.850s, episode steps: 124, steps per second:  44, episode reward: -123.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.895 [0.000, 2.000],  loss: 0.607833, mae: 17.435773, mean_q: -25.497070, mean_eps: 0.249249\n",
            " 153444/200000: episode: 608, duration: 4.958s, episode steps: 167, steps per second:  34, episode reward: -166.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.952 [0.000, 2.000],  loss: 0.633886, mae: 17.443263, mean_q: -25.514572, mean_eps: 0.248536\n",
            " 153587/200000: episode: 609, duration: 3.225s, episode steps: 143, steps per second:  44, episode reward: -142.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.993 [0.000, 2.000],  loss: 0.639041, mae: 17.446450, mean_q: -25.511721, mean_eps: 0.247777\n",
            " 153709/200000: episode: 610, duration: 2.736s, episode steps: 122, steps per second:  45, episode reward: -121.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.049 [0.000, 2.000],  loss: 0.637418, mae: 17.417656, mean_q: -25.479882, mean_eps: 0.247127\n",
            " 153805/200000: episode: 611, duration: 2.186s, episode steps:  96, steps per second:  44, episode reward: -95.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.104 [0.000, 2.000],  loss: 0.632382, mae: 17.497736, mean_q: -25.586117, mean_eps: 0.246593\n",
            " 153938/200000: episode: 612, duration: 4.132s, episode steps: 133, steps per second:  32, episode reward: -132.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.917 [0.000, 2.000],  loss: 0.652304, mae: 17.478183, mean_q: -25.580003, mean_eps: 0.246032\n",
            " 154087/200000: episode: 613, duration: 3.461s, episode steps: 149, steps per second:  43, episode reward: -148.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.993 [0.000, 2.000],  loss: 0.639591, mae: 17.539570, mean_q: -25.659925, mean_eps: 0.245341\n",
            " 154283/200000: episode: 614, duration: 4.411s, episode steps: 196, steps per second:  44, episode reward: -195.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.163 [0.000, 2.000],  loss: 0.620263, mae: 17.569799, mean_q: -25.705171, mean_eps: 0.244496\n",
            " 154415/200000: episode: 615, duration: 3.333s, episode steps: 132, steps per second:  40, episode reward: -131.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.189 [0.000, 2.000],  loss: 0.620086, mae: 17.476148, mean_q: -25.556416, mean_eps: 0.243692\n",
            " 154558/200000: episode: 616, duration: 4.096s, episode steps: 143, steps per second:  35, episode reward: -142.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.986 [0.000, 2.000],  loss: 0.651663, mae: 17.490087, mean_q: -25.574724, mean_eps: 0.243019\n",
            " 154704/200000: episode: 617, duration: 3.375s, episode steps: 146, steps per second:  43, episode reward: -145.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.137 [0.000, 2.000],  loss: 0.658448, mae: 17.544542, mean_q: -25.658719, mean_eps: 0.242311\n",
            " 154824/200000: episode: 618, duration: 2.719s, episode steps: 120, steps per second:  44, episode reward: -119.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.017 [0.000, 2.000],  loss: 0.633587, mae: 17.608776, mean_q: -25.768140, mean_eps: 0.241659\n",
            " 154967/200000: episode: 619, duration: 4.070s, episode steps: 143, steps per second:  35, episode reward: -142.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.063 [0.000, 2.000],  loss: 0.639829, mae: 17.581013, mean_q: -25.720515, mean_eps: 0.241015\n",
            " 155069/200000: episode: 620, duration: 3.037s, episode steps: 102, steps per second:  34, episode reward: -101.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.049 [0.000, 2.000],  loss: 0.620153, mae: 17.600282, mean_q: -25.754584, mean_eps: 0.240414\n",
            " 155249/200000: episode: 621, duration: 4.176s, episode steps: 180, steps per second:  43, episode reward: -179.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.017 [0.000, 2.000],  loss: 0.642242, mae: 17.699872, mean_q: -25.900518, mean_eps: 0.239723\n",
            " 155404/200000: episode: 622, duration: 3.590s, episode steps: 155, steps per second:  43, episode reward: -154.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.702741, mae: 17.683007, mean_q: -25.866687, mean_eps: 0.238903\n",
            " 155510/200000: episode: 623, duration: 3.232s, episode steps: 106, steps per second:  33, episode reward: -105.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.075 [0.000, 2.000],  loss: 0.644169, mae: 17.620248, mean_q: -25.764427, mean_eps: 0.238263\n",
            " 155640/200000: episode: 624, duration: 3.578s, episode steps: 130, steps per second:  36, episode reward: -129.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.038 [0.000, 2.000],  loss: 0.700587, mae: 17.687743, mean_q: -25.877404, mean_eps: 0.237685\n",
            " 155900/200000: episode: 625, duration: 5.869s, episode steps: 260, steps per second:  44, episode reward: -259.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.854 [0.000, 2.000],  loss: 0.672765, mae: 17.721035, mean_q: -25.931827, mean_eps: 0.236729\n",
            " 156103/200000: episode: 626, duration: 5.759s, episode steps: 203, steps per second:  35, episode reward: -202.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.207 [0.000, 2.000],  loss: 0.642274, mae: 17.717398, mean_q: -25.933730, mean_eps: 0.235595\n",
            " 156257/200000: episode: 627, duration: 3.462s, episode steps: 154, steps per second:  44, episode reward: -153.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.013 [0.000, 2.000],  loss: 0.603775, mae: 17.707234, mean_q: -25.923199, mean_eps: 0.234720\n",
            " 156458/200000: episode: 628, duration: 4.566s, episode steps: 201, steps per second:  44, episode reward: -200.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.950 [0.000, 2.000],  loss: 0.640023, mae: 17.769795, mean_q: -26.007671, mean_eps: 0.233851\n",
            " 156563/200000: episode: 629, duration: 2.756s, episode steps: 105, steps per second:  38, episode reward: -104.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.933 [0.000, 2.000],  loss: 0.591230, mae: 17.820023, mean_q: -26.094611, mean_eps: 0.233101\n",
            " 156670/200000: episode: 630, duration: 3.332s, episode steps: 107, steps per second:  32, episode reward: -106.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.972 [0.000, 2.000],  loss: 0.612533, mae: 17.852390, mean_q: -26.128908, mean_eps: 0.232582\n",
            " 156796/200000: episode: 631, duration: 2.876s, episode steps: 126, steps per second:  44, episode reward: -125.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.944 [0.000, 2.000],  loss: 0.638765, mae: 17.811547, mean_q: -26.063869, mean_eps: 0.232011\n",
            " 156911/200000: episode: 632, duration: 2.593s, episode steps: 115, steps per second:  44, episode reward: -114.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.139 [0.000, 2.000],  loss: 0.643324, mae: 17.816200, mean_q: -26.087113, mean_eps: 0.231420\n",
            " 157047/200000: episode: 633, duration: 3.100s, episode steps: 136, steps per second:  44, episode reward: -135.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.199 [0.000, 2.000],  loss: 0.646957, mae: 17.840496, mean_q: -26.119037, mean_eps: 0.230805\n",
            " 157178/200000: episode: 634, duration: 4.208s, episode steps: 131, steps per second:  31, episode reward: -130.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.183 [0.000, 2.000],  loss: 0.639960, mae: 17.824731, mean_q: -26.100848, mean_eps: 0.230151\n",
            " 157300/200000: episode: 635, duration: 3.057s, episode steps: 122, steps per second:  40, episode reward: -121.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.098 [0.000, 2.000],  loss: 0.620337, mae: 17.859421, mean_q: -26.160418, mean_eps: 0.229531\n",
            " 157455/200000: episode: 636, duration: 3.549s, episode steps: 155, steps per second:  44, episode reward: -154.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.968 [0.000, 2.000],  loss: 0.648001, mae: 17.847972, mean_q: -26.128790, mean_eps: 0.228853\n",
            " 157562/200000: episode: 637, duration: 2.503s, episode steps: 107, steps per second:  43, episode reward: -106.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.638855, mae: 17.831297, mean_q: -26.095696, mean_eps: 0.228211\n",
            " 157735/200000: episode: 638, duration: 5.193s, episode steps: 173, steps per second:  33, episode reward: -172.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.786 [0.000, 2.000],  loss: 0.640752, mae: 17.890166, mean_q: -26.206873, mean_eps: 0.227525\n",
            " 157907/200000: episode: 639, duration: 4.026s, episode steps: 172, steps per second:  43, episode reward: -171.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.878 [0.000, 2.000],  loss: 0.650859, mae: 17.849338, mean_q: -26.134984, mean_eps: 0.226680\n",
            " 158072/200000: episode: 640, duration: 3.837s, episode steps: 165, steps per second:  43, episode reward: -164.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.915 [0.000, 2.000],  loss: 0.664325, mae: 17.858071, mean_q: -26.137776, mean_eps: 0.225854\n",
            " 158259/200000: episode: 641, duration: 5.360s, episode steps: 187, steps per second:  35, episode reward: -186.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.882 [0.000, 2.000],  loss: 0.614319, mae: 17.872904, mean_q: -26.176748, mean_eps: 0.224992\n",
            " 158387/200000: episode: 642, duration: 2.849s, episode steps: 128, steps per second:  45, episode reward: -127.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.086 [0.000, 2.000],  loss: 0.648796, mae: 17.884536, mean_q: -26.174622, mean_eps: 0.224220\n",
            " 158596/200000: episode: 643, duration: 4.722s, episode steps: 209, steps per second:  44, episode reward: -208.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.856 [0.000, 2.000],  loss: 0.668422, mae: 17.967435, mean_q: -26.323649, mean_eps: 0.223394\n",
            " 158739/200000: episode: 644, duration: 3.979s, episode steps: 143, steps per second:  36, episode reward: -142.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.986 [0.000, 2.000],  loss: 0.681820, mae: 17.938396, mean_q: -26.278762, mean_eps: 0.222532\n",
            " 158873/200000: episode: 645, duration: 3.607s, episode steps: 134, steps per second:  37, episode reward: -133.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.291 [0.000, 2.000],  loss: 0.662364, mae: 17.930762, mean_q: -26.251154, mean_eps: 0.221853\n",
            " 158978/200000: episode: 646, duration: 2.364s, episode steps: 105, steps per second:  44, episode reward: -104.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.124 [0.000, 2.000],  loss: 0.605676, mae: 17.967991, mean_q: -26.332022, mean_eps: 0.221268\n",
            " 159146/200000: episode: 647, duration: 3.881s, episode steps: 168, steps per second:  43, episode reward: -167.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.881 [0.000, 2.000],  loss: 0.644316, mae: 18.073608, mean_q: -26.469333, mean_eps: 0.220599\n",
            " 159264/200000: episode: 648, duration: 3.405s, episode steps: 118, steps per second:  35, episode reward: -117.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.051 [0.000, 2.000],  loss: 0.663403, mae: 18.077809, mean_q: -26.467470, mean_eps: 0.219898\n",
            " 159475/200000: episode: 649, duration: 5.515s, episode steps: 211, steps per second:  38, episode reward: -210.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.062 [0.000, 2.000],  loss: 0.696118, mae: 18.131902, mean_q: -26.534295, mean_eps: 0.219092\n",
            " 159637/200000: episode: 650, duration: 3.668s, episode steps: 162, steps per second:  44, episode reward: -161.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.160 [0.000, 2.000],  loss: 0.705751, mae: 18.165784, mean_q: -26.581960, mean_eps: 0.218178\n",
            " 159740/200000: episode: 651, duration: 2.676s, episode steps: 103, steps per second:  38, episode reward: -102.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.722259, mae: 18.076227, mean_q: -26.451366, mean_eps: 0.217529\n",
            " 159989/200000: episode: 652, duration: 6.815s, episode steps: 249, steps per second:  37, episode reward: -248.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.940 [0.000, 2.000],  loss: 0.686648, mae: 18.126127, mean_q: -26.524701, mean_eps: 0.216666\n",
            " 160086/200000: episode: 653, duration: 2.273s, episode steps:  97, steps per second:  43, episode reward: -96.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.052 [0.000, 2.000],  loss: 0.677665, mae: 18.206269, mean_q: -26.653896, mean_eps: 0.215819\n",
            " 160217/200000: episode: 654, duration: 3.037s, episode steps: 131, steps per second:  43, episode reward: -130.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.168 [0.000, 2.000],  loss: 0.657980, mae: 18.230761, mean_q: -26.702423, mean_eps: 0.215260\n",
            " 160423/200000: episode: 655, duration: 6.026s, episode steps: 206, steps per second:  34, episode reward: -205.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.816 [0.000, 2.000],  loss: 0.668126, mae: 18.256690, mean_q: -26.724091, mean_eps: 0.214434\n",
            " 160612/200000: episode: 656, duration: 4.307s, episode steps: 189, steps per second:  44, episode reward: -188.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.085 [0.000, 2.000],  loss: 0.674164, mae: 18.284890, mean_q: -26.768892, mean_eps: 0.213467\n",
            " 160728/200000: episode: 657, duration: 2.685s, episode steps: 116, steps per second:  43, episode reward: -115.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.879 [0.000, 2.000],  loss: 0.716979, mae: 18.248261, mean_q: -26.688180, mean_eps: 0.212719\n",
            " 160902/200000: episode: 658, duration: 4.859s, episode steps: 174, steps per second:  36, episode reward: -173.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.092 [0.000, 2.000],  loss: 0.712795, mae: 18.362513, mean_q: -26.876267, mean_eps: 0.212009\n",
            " 161039/200000: episode: 659, duration: 3.322s, episode steps: 137, steps per second:  41, episode reward: -136.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.920 [0.000, 2.000],  loss: 0.705269, mae: 18.437282, mean_q: -26.998003, mean_eps: 0.211247\n",
            " 161132/200000: episode: 660, duration: 2.121s, episode steps:  93, steps per second:  44, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.957 [0.000, 2.000],  loss: 0.689851, mae: 18.403063, mean_q: -26.918994, mean_eps: 0.210684\n",
            " 161271/200000: episode: 661, duration: 3.150s, episode steps: 139, steps per second:  44, episode reward: -138.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.964 [0.000, 2.000],  loss: 0.721954, mae: 18.447307, mean_q: -27.013288, mean_eps: 0.210115\n",
            " 161386/200000: episode: 662, duration: 2.869s, episode steps: 115, steps per second:  40, episode reward: -114.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.826 [0.000, 2.000],  loss: 0.696332, mae: 18.488365, mean_q: -27.073057, mean_eps: 0.209493\n",
            " 161535/200000: episode: 663, duration: 4.380s, episode steps: 149, steps per second:  34, episode reward: -148.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.913 [0.000, 2.000],  loss: 0.730542, mae: 18.426961, mean_q: -26.951309, mean_eps: 0.208846\n",
            " 161693/200000: episode: 664, duration: 3.593s, episode steps: 158, steps per second:  44, episode reward: -157.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.975 [0.000, 2.000],  loss: 0.754762, mae: 18.531728, mean_q: -27.103433, mean_eps: 0.208094\n",
            " 161802/200000: episode: 665, duration: 2.503s, episode steps: 109, steps per second:  44, episode reward: -108.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.872 [0.000, 2.000],  loss: 0.736080, mae: 18.518060, mean_q: -27.105548, mean_eps: 0.207440\n",
            " 161956/200000: episode: 666, duration: 4.148s, episode steps: 154, steps per second:  37, episode reward: -153.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.883 [0.000, 2.000],  loss: 0.714083, mae: 18.494845, mean_q: -27.069926, mean_eps: 0.206795\n",
            " 162057/200000: episode: 667, duration: 2.921s, episode steps: 101, steps per second:  35, episode reward: -100.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.693 [0.000, 2.000],  loss: 0.737444, mae: 18.678335, mean_q: -27.334511, mean_eps: 0.206171\n",
            " 162201/200000: episode: 668, duration: 3.629s, episode steps: 144, steps per second:  40, episode reward: -143.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.264 [0.000, 2.000],  loss: 0.777130, mae: 18.626467, mean_q: -27.242075, mean_eps: 0.205570\n",
            " 162375/200000: episode: 669, duration: 3.959s, episode steps: 174, steps per second:  44, episode reward: -173.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.011 [0.000, 2.000],  loss: 0.750150, mae: 18.718626, mean_q: -27.377732, mean_eps: 0.204791\n",
            " 162533/200000: episode: 670, duration: 4.789s, episode steps: 158, steps per second:  33, episode reward: -157.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.753 [0.000, 2.000],  loss: 0.773629, mae: 18.734046, mean_q: -27.380891, mean_eps: 0.203978\n",
            " 162692/200000: episode: 671, duration: 3.748s, episode steps: 159, steps per second:  42, episode reward: -158.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.044 [0.000, 2.000],  loss: 0.782708, mae: 18.774057, mean_q: -27.437415, mean_eps: 0.203201\n",
            " 162843/200000: episode: 672, duration: 3.506s, episode steps: 151, steps per second:  43, episode reward: -150.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.907 [0.000, 2.000],  loss: 0.763651, mae: 18.769471, mean_q: -27.438226, mean_eps: 0.202442\n",
            " 162955/200000: episode: 673, duration: 2.536s, episode steps: 112, steps per second:  44, episode reward: -111.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.804 [0.000, 2.000],  loss: 0.772877, mae: 18.712087, mean_q: -27.348001, mean_eps: 0.201797\n",
            " 163096/200000: episode: 674, duration: 4.425s, episode steps: 141, steps per second:  32, episode reward: -140.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.071 [0.000, 2.000],  loss: 0.760813, mae: 18.821946, mean_q: -27.499141, mean_eps: 0.201178\n",
            " 163243/200000: episode: 675, duration: 3.351s, episode steps: 147, steps per second:  44, episode reward: -146.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.245 [0.000, 2.000],  loss: 0.755972, mae: 18.799556, mean_q: -27.474162, mean_eps: 0.200472\n",
            " 163355/200000: episode: 676, duration: 2.613s, episode steps: 112, steps per second:  43, episode reward: -111.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.080 [0.000, 2.000],  loss: 0.776044, mae: 18.929274, mean_q: -27.682008, mean_eps: 0.199837\n",
            " 163465/200000: episode: 677, duration: 2.517s, episode steps: 110, steps per second:  44, episode reward: -109.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.027 [0.000, 2.000],  loss: 0.801926, mae: 18.926855, mean_q: -27.663889, mean_eps: 0.199293\n",
            " 163627/200000: episode: 678, duration: 4.861s, episode steps: 162, steps per second:  33, episode reward: -161.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.006 [0.000, 2.000],  loss: 0.791376, mae: 18.957159, mean_q: -27.713788, mean_eps: 0.198627\n",
            " 163731/200000: episode: 679, duration: 2.390s, episode steps: 104, steps per second:  44, episode reward: -103.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.904 [0.000, 2.000],  loss: 0.760032, mae: 18.940591, mean_q: -27.709937, mean_eps: 0.197975\n",
            " 163855/200000: episode: 680, duration: 2.871s, episode steps: 124, steps per second:  43, episode reward: -123.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.726 [0.000, 2.000],  loss: 0.766381, mae: 18.945681, mean_q: -27.707656, mean_eps: 0.197417\n",
            " 164007/200000: episode: 681, duration: 3.494s, episode steps: 152, steps per second:  44, episode reward: -151.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.921 [0.000, 2.000],  loss: 0.834734, mae: 18.863866, mean_q: -27.574897, mean_eps: 0.196741\n",
            " 164119/200000: episode: 682, duration: 3.516s, episode steps: 112, steps per second:  32, episode reward: -111.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.009 [0.000, 2.000],  loss: 0.782437, mae: 18.894418, mean_q: -27.626549, mean_eps: 0.196094\n",
            " 164262/200000: episode: 683, duration: 3.534s, episode steps: 143, steps per second:  40, episode reward: -142.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.035 [0.000, 2.000],  loss: 0.819256, mae: 18.894266, mean_q: -27.599153, mean_eps: 0.195469\n",
            " 164367/200000: episode: 684, duration: 2.532s, episode steps: 105, steps per second:  41, episode reward: -104.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.971 [0.000, 2.000],  loss: 0.801954, mae: 18.807403, mean_q: -27.442945, mean_eps: 0.194861\n",
            " 164482/200000: episode: 685, duration: 2.998s, episode steps: 115, steps per second:  38, episode reward: -114.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.009 [0.000, 2.000],  loss: 0.796004, mae: 18.796272, mean_q: -27.461675, mean_eps: 0.194322\n",
            " 164589/200000: episode: 686, duration: 2.837s, episode steps: 107, steps per second:  38, episode reward: -106.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.121 [0.000, 2.000],  loss: 0.793389, mae: 18.840051, mean_q: -27.523337, mean_eps: 0.193779\n",
            " 164692/200000: episode: 687, duration: 3.176s, episode steps: 103, steps per second:  32, episode reward: -102.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.107 [0.000, 2.000],  loss: 0.780630, mae: 18.823743, mean_q: -27.499346, mean_eps: 0.193264\n",
            " 164896/200000: episode: 688, duration: 4.761s, episode steps: 204, steps per second:  43, episode reward: -203.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.015 [0.000, 2.000],  loss: 0.769536, mae: 18.721434, mean_q: -27.329861, mean_eps: 0.192512\n",
            " 164995/200000: episode: 689, duration: 2.325s, episode steps:  99, steps per second:  43, episode reward: -98.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.939 [0.000, 2.000],  loss: 0.762282, mae: 18.742263, mean_q: -27.375585, mean_eps: 0.191770\n",
            " 165104/200000: episode: 690, duration: 2.820s, episode steps: 109, steps per second:  39, episode reward: -108.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.991 [0.000, 2.000],  loss: 0.778390, mae: 18.676289, mean_q: -27.274600, mean_eps: 0.191260\n",
            " 165282/200000: episode: 691, duration: 4.950s, episode steps: 178, steps per second:  36, episode reward: -177.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.882 [0.000, 2.000],  loss: 0.767213, mae: 18.726015, mean_q: -27.361109, mean_eps: 0.190557\n",
            " 165387/200000: episode: 692, duration: 2.395s, episode steps: 105, steps per second:  44, episode reward: -104.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.114 [0.000, 2.000],  loss: 0.795405, mae: 18.728427, mean_q: -27.335770, mean_eps: 0.189863\n",
            " 165505/200000: episode: 693, duration: 2.669s, episode steps: 118, steps per second:  44, episode reward: -117.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.110 [0.000, 2.000],  loss: 0.729556, mae: 18.728654, mean_q: -27.355828, mean_eps: 0.189317\n",
            " 165653/200000: episode: 694, duration: 3.643s, episode steps: 148, steps per second:  41, episode reward: -147.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.953 [0.000, 2.000],  loss: 0.776417, mae: 18.695726, mean_q: -27.306636, mean_eps: 0.188665\n",
            " 165896/200000: episode: 695, duration: 6.403s, episode steps: 243, steps per second:  38, episode reward: -242.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.008 [0.000, 2.000],  loss: 0.778062, mae: 18.698564, mean_q: -27.327811, mean_eps: 0.187707\n",
            " 165994/200000: episode: 696, duration: 2.227s, episode steps:  98, steps per second:  44, episode reward: -97.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.245 [0.000, 2.000],  loss: 0.747423, mae: 18.651332, mean_q: -27.275532, mean_eps: 0.186872\n",
            " 166172/200000: episode: 697, duration: 4.088s, episode steps: 178, steps per second:  44, episode reward: -177.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.888 [0.000, 2.000],  loss: 0.771163, mae: 18.595749, mean_q: -27.174247, mean_eps: 0.186196\n",
            " 166304/200000: episode: 698, duration: 4.353s, episode steps: 132, steps per second:  30, episode reward: -131.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.008 [0.000, 2.000],  loss: 0.777696, mae: 18.622536, mean_q: -27.207182, mean_eps: 0.185436\n",
            " 166445/200000: episode: 699, duration: 3.241s, episode steps: 141, steps per second:  44, episode reward: -140.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.957 [0.000, 2.000],  loss: 0.754223, mae: 18.634894, mean_q: -27.239522, mean_eps: 0.184767\n",
            " 166621/200000: episode: 700, duration: 4.000s, episode steps: 176, steps per second:  44, episode reward: -175.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.864 [0.000, 2.000],  loss: 0.740159, mae: 18.671294, mean_q: -27.277868, mean_eps: 0.183991\n",
            " 166792/200000: episode: 701, duration: 4.848s, episode steps: 171, steps per second:  35, episode reward: -170.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.871 [0.000, 2.000],  loss: 0.781922, mae: 18.579559, mean_q: -27.138796, mean_eps: 0.183141\n",
            " 166886/200000: episode: 702, duration: 2.711s, episode steps:  94, steps per second:  35, episode reward: -93.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.255 [0.000, 2.000],  loss: 0.783133, mae: 18.611342, mean_q: -27.196914, mean_eps: 0.182491\n",
            " 166988/200000: episode: 703, duration: 2.381s, episode steps: 102, steps per second:  43, episode reward: -101.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.843 [0.000, 2.000],  loss: 0.759258, mae: 18.634723, mean_q: -27.238414, mean_eps: 0.182011\n",
            " 167082/200000: episode: 704, duration: 2.242s, episode steps:  94, steps per second:  42, episode reward: -93.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.043 [0.000, 2.000],  loss: 0.745468, mae: 18.541621, mean_q: -27.090212, mean_eps: 0.181531\n",
            " 167227/200000: episode: 705, duration: 3.339s, episode steps: 145, steps per second:  43, episode reward: -144.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.876 [0.000, 2.000],  loss: 0.759325, mae: 18.589656, mean_q: -27.155309, mean_eps: 0.180945\n",
            " 167351/200000: episode: 706, duration: 4.050s, episode steps: 124, steps per second:  31, episode reward: -123.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.984 [0.000, 2.000],  loss: 0.759606, mae: 18.605024, mean_q: -27.180874, mean_eps: 0.180286\n",
            " 167474/200000: episode: 707, duration: 2.857s, episode steps: 123, steps per second:  43, episode reward: -122.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.976 [0.000, 2.000],  loss: 0.727359, mae: 18.584458, mean_q: -27.131624, mean_eps: 0.179681\n",
            " 167594/200000: episode: 708, duration: 2.692s, episode steps: 120, steps per second:  45, episode reward: -119.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.842 [0.000, 2.000],  loss: 0.755785, mae: 18.510610, mean_q: -27.042614, mean_eps: 0.179086\n",
            " 167687/200000: episode: 709, duration: 2.068s, episode steps:  93, steps per second:  45, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.785 [0.000, 2.000],  loss: 0.758948, mae: 18.558110, mean_q: -27.103606, mean_eps: 0.178564\n",
            " 167816/200000: episode: 710, duration: 3.342s, episode steps: 129, steps per second:  39, episode reward: -128.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.953 [0.000, 2.000],  loss: 0.749735, mae: 18.589442, mean_q: -27.152069, mean_eps: 0.178020\n",
            " 167950/200000: episode: 711, duration: 3.776s, episode steps: 134, steps per second:  35, episode reward: -133.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.216 [0.000, 2.000],  loss: 0.803788, mae: 18.564501, mean_q: -27.114629, mean_eps: 0.177376\n",
            " 168088/200000: episode: 712, duration: 3.132s, episode steps: 138, steps per second:  44, episode reward: -137.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.014 [0.000, 2.000],  loss: 0.820076, mae: 18.533805, mean_q: -27.065270, mean_eps: 0.176709\n",
            " 168185/200000: episode: 713, duration: 2.264s, episode steps:  97, steps per second:  43, episode reward: -96.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.866 [0.000, 2.000],  loss: 0.777013, mae: 18.569738, mean_q: -27.118109, mean_eps: 0.176134\n",
            " 168331/200000: episode: 714, duration: 3.524s, episode steps: 146, steps per second:  41, episode reward: -145.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.274 [0.000, 2.000],  loss: 0.800773, mae: 18.469131, mean_q: -26.963762, mean_eps: 0.175538\n",
            " 168441/200000: episode: 715, duration: 3.489s, episode steps: 110, steps per second:  32, episode reward: -109.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.036 [0.000, 2.000],  loss: 0.741511, mae: 18.519330, mean_q: -27.043622, mean_eps: 0.174911\n",
            " 168575/200000: episode: 716, duration: 3.027s, episode steps: 134, steps per second:  44, episode reward: -133.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.045 [0.000, 2.000],  loss: 0.744536, mae: 18.453163, mean_q: -26.948540, mean_eps: 0.174313\n",
            " 168693/200000: episode: 717, duration: 2.638s, episode steps: 118, steps per second:  45, episode reward: -117.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.161 [0.000, 2.000],  loss: 0.757255, mae: 18.461611, mean_q: -26.963722, mean_eps: 0.173696\n",
            " 168787/200000: episode: 718, duration: 2.127s, episode steps:  94, steps per second:  44, episode reward: -93.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.106 [0.000, 2.000],  loss: 0.756191, mae: 18.457859, mean_q: -26.970612, mean_eps: 0.173176\n",
            " 168890/200000: episode: 719, duration: 2.615s, episode steps: 103, steps per second:  39, episode reward: -102.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.650 [0.000, 2.000],  loss: 0.749993, mae: 18.425729, mean_q: -26.906833, mean_eps: 0.172694\n",
            " 169061/200000: episode: 720, duration: 4.697s, episode steps: 171, steps per second:  36, episode reward: -170.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.181 [0.000, 2.000],  loss: 0.710171, mae: 18.426122, mean_q: -26.916573, mean_eps: 0.172023\n",
            " 169212/200000: episode: 721, duration: 3.457s, episode steps: 151, steps per second:  44, episode reward: -150.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.967 [0.000, 2.000],  loss: 0.783331, mae: 18.360211, mean_q: -26.812945, mean_eps: 0.171234\n",
            " 169318/200000: episode: 722, duration: 2.702s, episode steps: 106, steps per second:  39, episode reward: -105.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.038 [0.000, 2.000],  loss: 0.783132, mae: 18.418804, mean_q: -26.919084, mean_eps: 0.170604\n",
            " 169437/200000: episode: 723, duration: 3.119s, episode steps: 119, steps per second:  38, episode reward: -118.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.924 [0.000, 2.000],  loss: 0.698365, mae: 18.382036, mean_q: -26.887081, mean_eps: 0.170053\n",
            " 169563/200000: episode: 724, duration: 3.596s, episode steps: 126, steps per second:  35, episode reward: -125.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.071 [0.000, 2.000],  loss: 0.666765, mae: 18.372924, mean_q: -26.867358, mean_eps: 0.169452\n",
            " 169695/200000: episode: 725, duration: 2.948s, episode steps: 132, steps per second:  45, episode reward: -131.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.076 [0.000, 2.000],  loss: 0.733485, mae: 18.262580, mean_q: -26.687024, mean_eps: 0.168820\n",
            " 169794/200000: episode: 726, duration: 2.377s, episode steps:  99, steps per second:  42, episode reward: -98.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.960 [0.000, 2.000],  loss: 0.707880, mae: 18.321980, mean_q: -26.793873, mean_eps: 0.168254\n",
            " 169896/200000: episode: 727, duration: 2.359s, episode steps: 102, steps per second:  43, episode reward: -101.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.010 [0.000, 2.000],  loss: 0.721566, mae: 18.279873, mean_q: -26.717756, mean_eps: 0.167762\n",
            " 170063/200000: episode: 728, duration: 5.002s, episode steps: 167, steps per second:  33, episode reward: -166.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.772 [0.000, 2.000],  loss: 0.719237, mae: 18.281311, mean_q: -26.716683, mean_eps: 0.167103\n",
            " 170149/200000: episode: 729, duration: 1.986s, episode steps:  86, steps per second:  43, episode reward: -85.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.198 [0.000, 2.000],  loss: 0.681424, mae: 18.324709, mean_q: -26.790378, mean_eps: 0.166483\n",
            " 170264/200000: episode: 730, duration: 2.553s, episode steps: 115, steps per second:  45, episode reward: -114.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.861 [0.000, 2.000],  loss: 0.721889, mae: 18.273103, mean_q: -26.703703, mean_eps: 0.165991\n",
            " 170353/200000: episode: 731, duration: 2.011s, episode steps:  89, steps per second:  44, episode reward: -88.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.899 [0.000, 2.000],  loss: 0.743453, mae: 18.349131, mean_q: -26.819018, mean_eps: 0.165491\n",
            " 170498/200000: episode: 732, duration: 3.569s, episode steps: 145, steps per second:  41, episode reward: -144.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.993 [0.000, 2.000],  loss: 0.778312, mae: 18.256547, mean_q: -26.667178, mean_eps: 0.164918\n",
            " 170597/200000: episode: 733, duration: 3.200s, episode steps:  99, steps per second:  31, episode reward: -98.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.929 [0.000, 2.000],  loss: 0.769942, mae: 18.219692, mean_q: -26.595096, mean_eps: 0.164320\n",
            " 170679/200000: episode: 734, duration: 1.836s, episode steps:  82, steps per second:  45, episode reward: -81.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.073 [0.000, 2.000],  loss: 0.767533, mae: 18.213100, mean_q: -26.591496, mean_eps: 0.163876\n",
            " 170793/200000: episode: 735, duration: 2.543s, episode steps: 114, steps per second:  45, episode reward: -113.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.991 [0.000, 2.000],  loss: 0.768340, mae: 18.207543, mean_q: -26.584047, mean_eps: 0.163396\n",
            " 170906/200000: episode: 736, duration: 2.535s, episode steps: 113, steps per second:  45, episode reward: -112.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.035 [0.000, 2.000],  loss: 0.732472, mae: 18.266442, mean_q: -26.683665, mean_eps: 0.162840\n",
            " 170993/200000: episode: 737, duration: 1.946s, episode steps:  87, steps per second:  45, episode reward: -86.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.954 [0.000, 2.000],  loss: 0.735144, mae: 18.211609, mean_q: -26.614221, mean_eps: 0.162350\n",
            " 171347/200000: episode: 738, duration: 9.257s, episode steps: 354, steps per second:  38, episode reward: -353.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.237 [0.000, 2.000],  loss: 0.756584, mae: 18.261901, mean_q: -26.653643, mean_eps: 0.161269\n",
            " 171453/200000: episode: 739, duration: 2.345s, episode steps: 106, steps per second:  45, episode reward: -105.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.934 [0.000, 2.000],  loss: 0.719178, mae: 18.241318, mean_q: -26.644798, mean_eps: 0.160142\n",
            " 171587/200000: episode: 740, duration: 3.195s, episode steps: 134, steps per second:  42, episode reward: -133.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.993 [0.000, 2.000],  loss: 0.721463, mae: 18.215267, mean_q: -26.595120, mean_eps: 0.159554\n",
            " 171689/200000: episode: 741, duration: 3.652s, episode steps: 102, steps per second:  28, episode reward: -101.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.951 [0.000, 2.000],  loss: 0.737939, mae: 18.163193, mean_q: -26.502195, mean_eps: 0.158976\n",
            " 171818/200000: episode: 742, duration: 2.930s, episode steps: 129, steps per second:  44, episode reward: -128.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.961 [0.000, 2.000],  loss: 0.682192, mae: 18.238804, mean_q: -26.632212, mean_eps: 0.158410\n",
            " 171905/200000: episode: 743, duration: 1.999s, episode steps:  87, steps per second:  44, episode reward: -86.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.011 [0.000, 2.000],  loss: 0.686677, mae: 18.219555, mean_q: -26.607912, mean_eps: 0.157881\n",
            " 172009/200000: episode: 744, duration: 2.399s, episode steps: 104, steps per second:  43, episode reward: -103.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.933 [0.000, 2.000],  loss: 0.726884, mae: 18.269468, mean_q: -26.676774, mean_eps: 0.157413\n",
            " 172131/200000: episode: 745, duration: 3.116s, episode steps: 122, steps per second:  39, episode reward: -121.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.787 [0.000, 2.000],  loss: 0.733321, mae: 18.235898, mean_q: -26.641126, mean_eps: 0.156859\n",
            " 172280/200000: episode: 746, duration: 4.177s, episode steps: 149, steps per second:  36, episode reward: -148.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.740514, mae: 18.190369, mean_q: -26.566460, mean_eps: 0.156196\n",
            " 172377/200000: episode: 747, duration: 2.163s, episode steps:  97, steps per second:  45, episode reward: -96.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.010 [0.000, 2.000],  loss: 0.692034, mae: 18.182352, mean_q: -26.559966, mean_eps: 0.155593\n",
            " 172565/200000: episode: 748, duration: 4.259s, episode steps: 188, steps per second:  44, episode reward: -187.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.856 [0.000, 2.000],  loss: 0.696512, mae: 18.144889, mean_q: -26.500555, mean_eps: 0.154895\n",
            " 172725/200000: episode: 749, duration: 4.454s, episode steps: 160, steps per second:  36, episode reward: -159.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.212 [0.000, 2.000],  loss: 0.656949, mae: 18.180989, mean_q: -26.557681, mean_eps: 0.154042\n",
            " 172821/200000: episode: 750, duration: 2.543s, episode steps:  96, steps per second:  38, episode reward: -95.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.958 [0.000, 2.000],  loss: 0.708351, mae: 18.121376, mean_q: -26.465064, mean_eps: 0.153415\n",
            " 172917/200000: episode: 751, duration: 2.213s, episode steps:  96, steps per second:  43, episode reward: -95.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.917 [0.000, 2.000],  loss: 0.719656, mae: 18.033708, mean_q: -26.314565, mean_eps: 0.152944\n",
            " 173018/200000: episode: 752, duration: 2.310s, episode steps: 101, steps per second:  44, episode reward: -100.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.891 [0.000, 2.000],  loss: 0.662773, mae: 18.149473, mean_q: -26.532093, mean_eps: 0.152462\n",
            " 173151/200000: episode: 753, duration: 3.085s, episode steps: 133, steps per second:  43, episode reward: -132.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.955 [0.000, 2.000],  loss: 0.724399, mae: 18.062924, mean_q: -26.380244, mean_eps: 0.151888\n",
            " 173247/200000: episode: 754, duration: 2.942s, episode steps:  96, steps per second:  33, episode reward: -95.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.792 [0.000, 2.000],  loss: 0.691392, mae: 18.056827, mean_q: -26.374684, mean_eps: 0.151327\n",
            " 173351/200000: episode: 755, duration: 2.908s, episode steps: 104, steps per second:  36, episode reward: -103.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.971 [0.000, 2.000],  loss: 0.705011, mae: 18.051504, mean_q: -26.365995, mean_eps: 0.150837\n",
            " 173449/200000: episode: 756, duration: 2.228s, episode steps:  98, steps per second:  44, episode reward: -97.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.796 [0.000, 2.000],  loss: 0.669285, mae: 18.053716, mean_q: -26.388309, mean_eps: 0.150342\n",
            " 173582/200000: episode: 757, duration: 3.027s, episode steps: 133, steps per second:  44, episode reward: -132.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.872 [0.000, 2.000],  loss: 0.722060, mae: 18.008025, mean_q: -26.295738, mean_eps: 0.149777\n",
            " 173676/200000: episode: 758, duration: 2.172s, episode steps:  94, steps per second:  43, episode reward: -93.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.053 [0.000, 2.000],  loss: 0.717430, mae: 18.017761, mean_q: -26.321592, mean_eps: 0.149220\n",
            " 173818/200000: episode: 759, duration: 4.369s, episode steps: 142, steps per second:  33, episode reward: -141.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.070 [0.000, 2.000],  loss: 0.733257, mae: 18.089171, mean_q: -26.430545, mean_eps: 0.148642\n",
            " 173914/200000: episode: 760, duration: 2.313s, episode steps:  96, steps per second:  42, episode reward: -95.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.781 [0.000, 2.000],  loss: 0.662468, mae: 18.101256, mean_q: -26.446427, mean_eps: 0.148059\n",
            " 174019/200000: episode: 761, duration: 2.492s, episode steps: 105, steps per second:  42, episode reward: -104.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.019 [0.000, 2.000],  loss: 0.681538, mae: 17.988555, mean_q: -26.282557, mean_eps: 0.147567\n",
            " 174131/200000: episode: 762, duration: 2.836s, episode steps: 112, steps per second:  39, episode reward: -111.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.929 [0.000, 2.000],  loss: 0.692197, mae: 18.016740, mean_q: -26.325113, mean_eps: 0.147035\n",
            " 174284/200000: episode: 763, duration: 3.948s, episode steps: 153, steps per second:  39, episode reward: -152.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.915 [0.000, 2.000],  loss: 0.633308, mae: 18.002954, mean_q: -26.319745, mean_eps: 0.146386\n",
            " 174371/200000: episode: 764, duration: 2.786s, episode steps:  87, steps per second:  31, episode reward: -86.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.931 [0.000, 2.000],  loss: 0.711291, mae: 17.900352, mean_q: -26.178037, mean_eps: 0.145798\n",
            " 174517/200000: episode: 765, duration: 3.527s, episode steps: 146, steps per second:  41, episode reward: -145.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.959 [0.000, 2.000],  loss: 0.686327, mae: 18.018432, mean_q: -26.354293, mean_eps: 0.145227\n",
            " 174754/200000: episode: 766, duration: 5.549s, episode steps: 237, steps per second:  43, episode reward: -236.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.173 [0.000, 2.000],  loss: 0.706492, mae: 17.993700, mean_q: -26.316561, mean_eps: 0.144289\n",
            " 174860/200000: episode: 767, duration: 3.557s, episode steps: 106, steps per second:  30, episode reward: -105.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.925 [0.000, 2.000],  loss: 0.711342, mae: 18.067539, mean_q: -26.412227, mean_eps: 0.143448\n",
            " 174962/200000: episode: 768, duration: 2.453s, episode steps: 102, steps per second:  42, episode reward: -101.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.892 [0.000, 2.000],  loss: 0.698881, mae: 18.005969, mean_q: -26.323990, mean_eps: 0.142939\n",
            " 175093/200000: episode: 769, duration: 3.010s, episode steps: 131, steps per second:  44, episode reward: -130.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.901 [0.000, 2.000],  loss: 0.702379, mae: 18.049746, mean_q: -26.393607, mean_eps: 0.142368\n",
            " 175189/200000: episode: 770, duration: 2.292s, episode steps:  96, steps per second:  42, episode reward: -95.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.958 [0.000, 2.000],  loss: 0.742527, mae: 18.086675, mean_q: -26.444233, mean_eps: 0.141812\n",
            " 175295/200000: episode: 771, duration: 2.467s, episode steps: 106, steps per second:  43, episode reward: -105.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.896 [0.000, 2.000],  loss: 0.726027, mae: 18.073560, mean_q: -26.407801, mean_eps: 0.141317\n",
            " 175381/200000: episode: 772, duration: 2.812s, episode steps:  86, steps per second:  31, episode reward: -85.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.767 [0.000, 2.000],  loss: 0.684879, mae: 18.091583, mean_q: -26.463062, mean_eps: 0.140846\n",
            " 175511/200000: episode: 773, duration: 3.251s, episode steps: 130, steps per second:  40, episode reward: -129.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.731 [0.000, 2.000],  loss: 0.735284, mae: 18.101065, mean_q: -26.456078, mean_eps: 0.140317\n",
            " 175657/200000: episode: 774, duration: 3.522s, episode steps: 146, steps per second:  41, episode reward: -145.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.021 [0.000, 2.000],  loss: 0.712010, mae: 18.130648, mean_q: -26.514562, mean_eps: 0.139641\n",
            " 175761/200000: episode: 775, duration: 2.571s, episode steps: 104, steps per second:  40, episode reward: -103.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.115 [0.000, 2.000],  loss: 0.703809, mae: 18.071939, mean_q: -26.435434, mean_eps: 0.139028\n",
            " 175901/200000: episode: 776, duration: 4.192s, episode steps: 140, steps per second:  33, episode reward: -139.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.771 [0.000, 2.000],  loss: 0.715801, mae: 18.116007, mean_q: -26.505296, mean_eps: 0.138431\n",
            " 176028/200000: episode: 777, duration: 3.260s, episode steps: 127, steps per second:  39, episode reward: -126.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.016 [0.000, 2.000],  loss: 0.737553, mae: 18.123608, mean_q: -26.509283, mean_eps: 0.137776\n",
            " 176139/200000: episode: 778, duration: 2.594s, episode steps: 111, steps per second:  43, episode reward: -110.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.045 [0.000, 2.000],  loss: 0.686959, mae: 18.169016, mean_q: -26.581947, mean_eps: 0.137193\n",
            " 176263/200000: episode: 779, duration: 2.910s, episode steps: 124, steps per second:  43, episode reward: -123.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.032 [0.000, 2.000],  loss: 0.742104, mae: 18.142575, mean_q: -26.524735, mean_eps: 0.136618\n",
            " 176381/200000: episode: 780, duration: 3.151s, episode steps: 118, steps per second:  37, episode reward: -117.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.992 [0.000, 2.000],  loss: 0.741736, mae: 18.117793, mean_q: -26.484470, mean_eps: 0.136025\n",
            " 176479/200000: episode: 781, duration: 3.151s, episode steps:  98, steps per second:  31, episode reward: -97.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.051 [0.000, 2.000],  loss: 0.733194, mae: 18.110459, mean_q: -26.495619, mean_eps: 0.135495\n",
            " 176645/200000: episode: 782, duration: 4.088s, episode steps: 166, steps per second:  41, episode reward: -165.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.120 [0.000, 2.000],  loss: 0.724392, mae: 18.141151, mean_q: -26.514678, mean_eps: 0.134849\n",
            " 176763/200000: episode: 783, duration: 2.705s, episode steps: 118, steps per second:  44, episode reward: -117.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.119 [0.000, 2.000],  loss: 0.727210, mae: 18.177956, mean_q: -26.572459, mean_eps: 0.134153\n",
            " 176869/200000: episode: 784, duration: 2.544s, episode steps: 106, steps per second:  42, episode reward: -105.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.028 [0.000, 2.000],  loss: 0.676224, mae: 18.151208, mean_q: -26.531806, mean_eps: 0.133604\n",
            " 176986/200000: episode: 785, duration: 3.972s, episode steps: 117, steps per second:  29, episode reward: -116.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.060 [0.000, 2.000],  loss: 0.675400, mae: 18.160289, mean_q: -26.554176, mean_eps: 0.133058\n",
            " 177105/200000: episode: 786, duration: 2.775s, episode steps: 119, steps per second:  43, episode reward: -118.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.034 [0.000, 2.000],  loss: 0.703334, mae: 18.131286, mean_q: -26.498894, mean_eps: 0.132480\n",
            " 177244/200000: episode: 787, duration: 3.327s, episode steps: 139, steps per second:  42, episode reward: -138.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.216 [0.000, 2.000],  loss: 0.695628, mae: 18.143349, mean_q: -26.536986, mean_eps: 0.131847\n",
            " 177389/200000: episode: 788, duration: 3.296s, episode steps: 145, steps per second:  44, episode reward: -144.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.979 [0.000, 2.000],  loss: 0.719143, mae: 18.117814, mean_q: -26.467182, mean_eps: 0.131152\n",
            " 177496/200000: episode: 789, duration: 3.594s, episode steps: 107, steps per second:  30, episode reward: -106.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.888 [0.000, 2.000],  loss: 0.674838, mae: 18.141405, mean_q: -26.549409, mean_eps: 0.130534\n",
            " 177634/200000: episode: 790, duration: 3.417s, episode steps: 138, steps per second:  40, episode reward: -137.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.014 [0.000, 2.000],  loss: 0.642302, mae: 18.144975, mean_q: -26.545291, mean_eps: 0.129934\n",
            " 177761/200000: episode: 791, duration: 2.898s, episode steps: 127, steps per second:  44, episode reward: -126.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.984 [0.000, 2.000],  loss: 0.684110, mae: 18.072550, mean_q: -26.425372, mean_eps: 0.129285\n",
            " 177883/200000: episode: 792, duration: 2.811s, episode steps: 122, steps per second:  43, episode reward: -121.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.033 [0.000, 2.000],  loss: 0.714086, mae: 18.116310, mean_q: -26.490405, mean_eps: 0.128675\n",
            " 177977/200000: episode: 793, duration: 2.604s, episode steps:  94, steps per second:  36, episode reward: -93.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.106 [0.000, 2.000],  loss: 0.700750, mae: 18.025794, mean_q: -26.359721, mean_eps: 0.128145\n",
            " 178113/200000: episode: 794, duration: 3.902s, episode steps: 136, steps per second:  35, episode reward: -135.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.890 [0.000, 2.000],  loss: 0.689350, mae: 18.083404, mean_q: -26.460349, mean_eps: 0.127582\n",
            " 178240/200000: episode: 795, duration: 2.969s, episode steps: 127, steps per second:  43, episode reward: -126.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.929 [0.000, 2.000],  loss: 0.658307, mae: 18.146991, mean_q: -26.568459, mean_eps: 0.126938\n",
            " 178352/200000: episode: 796, duration: 2.558s, episode steps: 112, steps per second:  44, episode reward: -111.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.821 [0.000, 2.000],  loss: 0.699217, mae: 18.130073, mean_q: -26.508880, mean_eps: 0.126352\n",
            " 178483/200000: episode: 797, duration: 3.038s, episode steps: 131, steps per second:  43, episode reward: -130.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.992 [0.000, 2.000],  loss: 0.693801, mae: 18.061899, mean_q: -26.406751, mean_eps: 0.125757\n",
            " 178555/200000: episode: 798, duration: 2.434s, episode steps:  72, steps per second:  30, episode reward: -71.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.889 [0.000, 2.000],  loss: 0.698351, mae: 18.048206, mean_q: -26.412025, mean_eps: 0.125259\n",
            " 178663/200000: episode: 799, duration: 2.806s, episode steps: 108, steps per second:  38, episode reward: -107.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.843 [0.000, 2.000],  loss: 0.712597, mae: 18.037263, mean_q: -26.376069, mean_eps: 0.124818\n",
            " 178782/200000: episode: 800, duration: 2.795s, episode steps: 119, steps per second:  43, episode reward: -118.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.899 [0.000, 2.000],  loss: 0.728363, mae: 18.012009, mean_q: -26.342491, mean_eps: 0.124262\n",
            " 178916/200000: episode: 801, duration: 3.316s, episode steps: 134, steps per second:  40, episode reward: -133.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.896 [0.000, 2.000],  loss: 0.713794, mae: 18.004678, mean_q: -26.328103, mean_eps: 0.123642\n",
            " 179037/200000: episode: 802, duration: 3.084s, episode steps: 121, steps per second:  39, episode reward: -120.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.793 [0.000, 2.000],  loss: 0.712154, mae: 17.963042, mean_q: -26.253626, mean_eps: 0.123018\n",
            " 179153/200000: episode: 803, duration: 3.484s, episode steps: 116, steps per second:  33, episode reward: -115.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.017 [0.000, 2.000],  loss: 0.741293, mae: 17.957863, mean_q: -26.238178, mean_eps: 0.122437\n",
            " 179266/200000: episode: 804, duration: 2.640s, episode steps: 113, steps per second:  43, episode reward: -112.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.743 [0.000, 2.000],  loss: 0.724237, mae: 17.929752, mean_q: -26.203023, mean_eps: 0.121876\n",
            " 179397/200000: episode: 805, duration: 2.946s, episode steps: 131, steps per second:  44, episode reward: -130.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.832 [0.000, 2.000],  loss: 0.718334, mae: 17.929559, mean_q: -26.203152, mean_eps: 0.121278\n",
            " 179579/200000: episode: 806, duration: 4.574s, episode steps: 182, steps per second:  40, episode reward: -181.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.984 [0.000, 2.000],  loss: 0.715469, mae: 17.877708, mean_q: -26.142106, mean_eps: 0.120511\n",
            " 179707/200000: episode: 807, duration: 3.794s, episode steps: 128, steps per second:  34, episode reward: -127.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.938 [0.000, 2.000],  loss: 0.685809, mae: 17.887048, mean_q: -26.139839, mean_eps: 0.119752\n",
            " 179842/200000: episode: 808, duration: 3.118s, episode steps: 135, steps per second:  43, episode reward: -134.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.830 [0.000, 2.000],  loss: 0.740376, mae: 17.886367, mean_q: -26.149677, mean_eps: 0.119107\n",
            " 179935/200000: episode: 809, duration: 2.145s, episode steps:  93, steps per second:  43, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.925 [0.000, 2.000],  loss: 0.679363, mae: 17.933287, mean_q: -26.234073, mean_eps: 0.118549\n",
            " 180041/200000: episode: 810, duration: 2.449s, episode steps: 106, steps per second:  43, episode reward: -105.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.858 [0.000, 2.000],  loss: 0.705825, mae: 17.901772, mean_q: -26.179140, mean_eps: 0.118061\n",
            " 180126/200000: episode: 811, duration: 2.399s, episode steps:  85, steps per second:  35, episode reward: -84.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.871 [0.000, 2.000],  loss: 0.721898, mae: 17.909707, mean_q: -26.186535, mean_eps: 0.117593\n",
            " 180276/200000: episode: 812, duration: 4.039s, episode steps: 150, steps per second:  37, episode reward: -149.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.813 [0.000, 2.000],  loss: 0.689986, mae: 17.907766, mean_q: -26.176502, mean_eps: 0.117018\n",
            " 180385/200000: episode: 813, duration: 2.460s, episode steps: 109, steps per second:  44, episode reward: -108.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.174 [0.000, 2.000],  loss: 0.677786, mae: 17.915973, mean_q: -26.193472, mean_eps: 0.116383\n",
            " 180482/200000: episode: 814, duration: 2.225s, episode steps:  97, steps per second:  44, episode reward: -96.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.897 [0.000, 2.000],  loss: 0.686020, mae: 17.908153, mean_q: -26.167299, mean_eps: 0.115878\n",
            " 180583/200000: episode: 815, duration: 2.291s, episode steps: 101, steps per second:  44, episode reward: -100.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.752 [0.000, 2.000],  loss: 0.688482, mae: 17.909958, mean_q: -26.191713, mean_eps: 0.115393\n",
            " 180708/200000: episode: 816, duration: 3.642s, episode steps: 125, steps per second:  34, episode reward: -124.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.640 [0.000, 2.000],  loss: 0.691880, mae: 17.873252, mean_q: -26.129089, mean_eps: 0.114840\n",
            " 180898/200000: episode: 817, duration: 4.635s, episode steps: 190, steps per second:  41, episode reward: -189.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.663 [0.000, 2.000],  loss: 0.678388, mae: 17.886561, mean_q: -26.162579, mean_eps: 0.114068\n",
            " 180995/200000: episode: 818, duration: 2.156s, episode steps:  97, steps per second:  45, episode reward: -96.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.062 [0.000, 2.000],  loss: 0.653607, mae: 17.915029, mean_q: -26.216582, mean_eps: 0.113365\n",
            " 181105/200000: episode: 819, duration: 2.532s, episode steps: 110, steps per second:  43, episode reward: -109.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.936 [0.000, 2.000],  loss: 0.681042, mae: 17.896407, mean_q: -26.173838, mean_eps: 0.112857\n",
            " 181463/200000: episode: 820, duration: 9.824s, episode steps: 358, steps per second:  36, episode reward: -357.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.366 [0.000, 2.000],  loss: 0.700901, mae: 17.939085, mean_q: -26.232096, mean_eps: 0.111711\n",
            " 181619/200000: episode: 821, duration: 3.596s, episode steps: 156, steps per second:  43, episode reward: -155.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.840 [0.000, 2.000],  loss: 0.688510, mae: 17.946658, mean_q: -26.222693, mean_eps: 0.110452\n",
            " 181713/200000: episode: 822, duration: 2.357s, episode steps:  94, steps per second:  40, episode reward: -93.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.840 [0.000, 2.000],  loss: 0.656331, mae: 17.940281, mean_q: -26.223661, mean_eps: 0.109839\n",
            " 181845/200000: episode: 823, duration: 3.981s, episode steps: 132, steps per second:  33, episode reward: -131.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.826 [0.000, 2.000],  loss: 0.674579, mae: 17.952105, mean_q: -26.234513, mean_eps: 0.109285\n",
            " 181954/200000: episode: 824, duration: 2.482s, episode steps: 109, steps per second:  44, episode reward: -108.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.853 [0.000, 2.000],  loss: 0.660076, mae: 17.982473, mean_q: -26.282136, mean_eps: 0.108695\n",
            " 182066/200000: episode: 825, duration: 2.528s, episode steps: 112, steps per second:  44, episode reward: -111.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.062 [0.000, 2.000],  loss: 0.692059, mae: 17.932123, mean_q: -26.208695, mean_eps: 0.108153\n",
            " 182201/200000: episode: 826, duration: 3.006s, episode steps: 135, steps per second:  45, episode reward: -134.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.896 [0.000, 2.000],  loss: 0.722000, mae: 17.916388, mean_q: -26.201399, mean_eps: 0.107548\n",
            " 182295/200000: episode: 827, duration: 2.749s, episode steps:  94, steps per second:  34, episode reward: -93.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.723 [0.000, 2.000],  loss: 0.708278, mae: 17.957398, mean_q: -26.263760, mean_eps: 0.106987\n",
            " 182393/200000: episode: 828, duration: 2.824s, episode steps:  98, steps per second:  35, episode reward: -97.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.735 [0.000, 2.000],  loss: 0.738549, mae: 17.949874, mean_q: -26.218166, mean_eps: 0.106517\n",
            " 182501/200000: episode: 829, duration: 2.498s, episode steps: 108, steps per second:  43, episode reward: -107.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.907 [0.000, 2.000],  loss: 0.716553, mae: 18.008220, mean_q: -26.329810, mean_eps: 0.106012\n",
            " 182608/200000: episode: 830, duration: 2.564s, episode steps: 107, steps per second:  42, episode reward: -106.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.804 [0.000, 2.000],  loss: 0.676808, mae: 18.006766, mean_q: -26.325888, mean_eps: 0.105485\n",
            " 182829/200000: episode: 831, duration: 5.899s, episode steps: 221, steps per second:  37, episode reward: -220.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.701 [0.000, 2.000],  loss: 0.659835, mae: 18.006713, mean_q: -26.317769, mean_eps: 0.104682\n",
            " 182915/200000: episode: 832, duration: 2.620s, episode steps:  86, steps per second:  33, episode reward: -85.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.872 [0.000, 2.000],  loss: 0.725413, mae: 17.966916, mean_q: -26.252149, mean_eps: 0.103930\n",
            " 183024/200000: episode: 833, duration: 2.526s, episode steps: 109, steps per second:  43, episode reward: -108.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.908 [0.000, 2.000],  loss: 0.692840, mae: 17.945690, mean_q: -26.217697, mean_eps: 0.103452\n",
            " 183193/200000: episode: 834, duration: 3.903s, episode steps: 169, steps per second:  43, episode reward: -168.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.953 [0.000, 2.000],  loss: 0.677099, mae: 17.988695, mean_q: -26.285285, mean_eps: 0.102771\n",
            " 183300/200000: episode: 835, duration: 2.503s, episode steps: 107, steps per second:  43, episode reward: -106.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.860 [0.000, 2.000],  loss: 0.640982, mae: 17.953506, mean_q: -26.242870, mean_eps: 0.102095\n",
            " 183432/200000: episode: 836, duration: 4.383s, episode steps: 132, steps per second:  30, episode reward: -131.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.705 [0.000, 2.000],  loss: 0.648285, mae: 18.003026, mean_q: -26.304324, mean_eps: 0.101509\n",
            " 183529/200000: episode: 837, duration: 2.401s, episode steps:  97, steps per second:  40, episode reward: -96.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.041 [0.000, 2.000],  loss: 0.656663, mae: 17.942259, mean_q: -26.200701, mean_eps: 0.100948\n",
            " 183616/200000: episode: 838, duration: 2.023s, episode steps:  87, steps per second:  43, episode reward: -86.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.851 [0.000, 2.000],  loss: 0.655630, mae: 17.934934, mean_q: -26.214380, mean_eps: 0.100497\n",
            " 183739/200000: episode: 839, duration: 3.262s, episode steps: 123, steps per second:  38, episode reward: -122.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.967 [0.000, 2.000],  loss: 0.624208, mae: 17.954649, mean_q: -26.238026, mean_eps: 0.099983\n",
            " 183861/200000: episode: 840, duration: 3.538s, episode steps: 122, steps per second:  34, episode reward: -121.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.811 [0.000, 2.000],  loss: 0.658517, mae: 17.927720, mean_q: -26.200771, mean_eps: 0.099382\n",
            " 184002/200000: episode: 841, duration: 4.006s, episode steps: 141, steps per second:  35, episode reward: -140.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.766 [0.000, 2.000],  loss: 0.643788, mae: 17.952307, mean_q: -26.249959, mean_eps: 0.098738\n",
            " 184148/200000: episode: 842, duration: 3.366s, episode steps: 146, steps per second:  43, episode reward: -145.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.870 [0.000, 2.000],  loss: 0.664622, mae: 17.981327, mean_q: -26.292587, mean_eps: 0.098035\n",
            " 184254/200000: episode: 843, duration: 2.505s, episode steps: 106, steps per second:  42, episode reward: -105.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.774 [0.000, 2.000],  loss: 0.680486, mae: 17.994033, mean_q: -26.306458, mean_eps: 0.097418\n",
            " 184353/200000: episode: 844, duration: 2.511s, episode steps:  99, steps per second:  39, episode reward: -98.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.778 [0.000, 2.000],  loss: 0.652619, mae: 17.982846, mean_q: -26.294816, mean_eps: 0.096915\n",
            " 184454/200000: episode: 845, duration: 3.246s, episode steps: 101, steps per second:  31, episode reward: -100.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.970 [0.000, 2.000],  loss: 0.631201, mae: 17.959030, mean_q: -26.268233, mean_eps: 0.096425\n",
            " 184622/200000: episode: 846, duration: 3.893s, episode steps: 168, steps per second:  43, episode reward: -167.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.006 [0.000, 2.000],  loss: 0.662321, mae: 17.961561, mean_q: -26.264190, mean_eps: 0.095766\n",
            " 184782/200000: episode: 847, duration: 3.630s, episode steps: 160, steps per second:  44, episode reward: -159.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.531 [0.000, 2.000],  loss: 0.608870, mae: 18.015902, mean_q: -26.378068, mean_eps: 0.094963\n",
            " 185024/200000: episode: 848, duration: 6.798s, episode steps: 242, steps per second:  36, episode reward: -241.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.566 [0.000, 2.000],  loss: 0.621670, mae: 18.011992, mean_q: -26.361022, mean_eps: 0.093978\n",
            " 185200/200000: episode: 849, duration: 4.193s, episode steps: 176, steps per second:  42, episode reward: -175.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.506 [0.000, 2.000],  loss: 0.633321, mae: 18.045837, mean_q: -26.416290, mean_eps: 0.092954\n",
            " 185421/200000: episode: 850, duration: 5.728s, episode steps: 221, steps per second:  39, episode reward: -220.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.602 [0.000, 2.000],  loss: 0.628748, mae: 18.080384, mean_q: -26.463419, mean_eps: 0.091981\n",
            " 185530/200000: episode: 851, duration: 3.549s, episode steps: 109, steps per second:  31, episode reward: -108.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.183 [0.000, 2.000],  loss: 0.645805, mae: 18.052848, mean_q: -26.419761, mean_eps: 0.091173\n",
            " 185686/200000: episode: 852, duration: 3.693s, episode steps: 156, steps per second:  42, episode reward: -155.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.987 [0.000, 2.000],  loss: 0.665281, mae: 18.071557, mean_q: -26.446853, mean_eps: 0.090523\n",
            " 185850/200000: episode: 853, duration: 3.905s, episode steps: 164, steps per second:  42, episode reward: -163.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.860 [0.000, 2.000],  loss: 0.601534, mae: 18.128592, mean_q: -26.544722, mean_eps: 0.089739\n",
            " 186008/200000: episode: 854, duration: 4.808s, episode steps: 158, steps per second:  33, episode reward: -157.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.082 [0.000, 2.000],  loss: 0.645946, mae: 18.040828, mean_q: -26.410546, mean_eps: 0.088950\n",
            " 186125/200000: episode: 855, duration: 3.087s, episode steps: 117, steps per second:  38, episode reward: -116.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.650 [0.000, 2.000],  loss: 0.630082, mae: 18.178503, mean_q: -26.624156, mean_eps: 0.088277\n",
            " 186247/200000: episode: 856, duration: 2.923s, episode steps: 122, steps per second:  42, episode reward: -121.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.779 [0.000, 2.000],  loss: 0.686576, mae: 18.102401, mean_q: -26.462221, mean_eps: 0.087691\n",
            " 186359/200000: episode: 857, duration: 2.660s, episode steps: 112, steps per second:  42, episode reward: -111.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.714 [0.000, 2.000],  loss: 0.615265, mae: 18.153197, mean_q: -26.562993, mean_eps: 0.087118\n",
            " 186456/200000: episode: 858, duration: 2.545s, episode steps:  97, steps per second:  38, episode reward: -96.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.845 [0.000, 2.000],  loss: 0.639834, mae: 18.159201, mean_q: -26.581401, mean_eps: 0.086606\n",
            " 186640/200000: episode: 859, duration: 5.191s, episode steps: 184, steps per second:  35, episode reward: -183.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.788 [0.000, 2.000],  loss: 0.636855, mae: 18.228923, mean_q: -26.698020, mean_eps: 0.085917\n",
            " 186890/200000: episode: 860, duration: 5.655s, episode steps: 250, steps per second:  44, episode reward: -249.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.680 [0.000, 2.000],  loss: 0.651800, mae: 18.298118, mean_q: -26.806135, mean_eps: 0.084854\n",
            " 187003/200000: episode: 861, duration: 2.908s, episode steps: 113, steps per second:  39, episode reward: -112.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.885 [0.000, 2.000],  loss: 0.610516, mae: 18.341580, mean_q: -26.870465, mean_eps: 0.083965\n",
            " 187099/200000: episode: 862, duration: 3.177s, episode steps:  96, steps per second:  30, episode reward: -95.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.708 [0.000, 2.000],  loss: 0.660876, mae: 18.388012, mean_q: -26.935296, mean_eps: 0.083453\n",
            " 187222/200000: episode: 863, duration: 2.944s, episode steps: 123, steps per second:  42, episode reward: -122.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.935 [0.000, 2.000],  loss: 0.665685, mae: 18.386492, mean_q: -26.926582, mean_eps: 0.082916\n",
            " 187359/200000: episode: 864, duration: 3.230s, episode steps: 137, steps per second:  42, episode reward: -136.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.891 [0.000, 2.000],  loss: 0.678899, mae: 18.362262, mean_q: -26.884160, mean_eps: 0.082279\n",
            " 187484/200000: episode: 865, duration: 3.018s, episode steps: 125, steps per second:  41, episode reward: -124.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.080 [0.000, 2.000],  loss: 0.669589, mae: 18.387485, mean_q: -26.932101, mean_eps: 0.081637\n",
            " 187613/200000: episode: 866, duration: 4.230s, episode steps: 129, steps per second:  30, episode reward: -128.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.674 [0.000, 2.000],  loss: 0.659312, mae: 18.374873, mean_q: -26.897932, mean_eps: 0.081015\n",
            " 187726/200000: episode: 867, duration: 2.648s, episode steps: 113, steps per second:  43, episode reward: -112.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.752 [0.000, 2.000],  loss: 0.670269, mae: 18.418333, mean_q: -26.959989, mean_eps: 0.080422\n",
            " 187846/200000: episode: 868, duration: 2.730s, episode steps: 120, steps per second:  44, episode reward: -119.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.017 [0.000, 2.000],  loss: 0.703871, mae: 18.371720, mean_q: -26.874389, mean_eps: 0.079851\n",
            " 188006/200000: episode: 869, duration: 3.594s, episode steps: 160, steps per second:  45, episode reward: -159.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.769 [0.000, 2.000],  loss: 0.648784, mae: 18.402106, mean_q: -26.938555, mean_eps: 0.079165\n",
            " 188114/200000: episode: 870, duration: 3.371s, episode steps: 108, steps per second:  32, episode reward: -107.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.676 [0.000, 2.000],  loss: 0.690383, mae: 18.449928, mean_q: -27.002603, mean_eps: 0.078508\n",
            " 188239/200000: episode: 871, duration: 3.295s, episode steps: 125, steps per second:  38, episode reward: -124.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.168 [0.000, 2.000],  loss: 0.659909, mae: 18.472251, mean_q: -27.049636, mean_eps: 0.077938\n",
            " 188400/200000: episode: 872, duration: 3.755s, episode steps: 161, steps per second:  43, episode reward: -160.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.770 [0.000, 2.000],  loss: 0.717308, mae: 18.488599, mean_q: -27.074048, mean_eps: 0.077237\n",
            " 188501/200000: episode: 873, duration: 2.634s, episode steps: 101, steps per second:  38, episode reward: -100.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.634 [0.000, 2.000],  loss: 0.678589, mae: 18.549625, mean_q: -27.169487, mean_eps: 0.076595\n",
            " 188627/200000: episode: 874, duration: 3.802s, episode steps: 126, steps per second:  33, episode reward: -125.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.929 [0.000, 2.000],  loss: 0.680555, mae: 18.532767, mean_q: -27.144758, mean_eps: 0.076039\n",
            " 188762/200000: episode: 875, duration: 3.451s, episode steps: 135, steps per second:  39, episode reward: -134.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.074 [0.000, 2.000],  loss: 0.665724, mae: 18.601178, mean_q: -27.259753, mean_eps: 0.075399\n",
            " 188877/200000: episode: 876, duration: 2.658s, episode steps: 115, steps per second:  43, episode reward: -114.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.878 [0.000, 2.000],  loss: 0.654918, mae: 18.574361, mean_q: -27.211616, mean_eps: 0.074787\n",
            " 189038/200000: episode: 877, duration: 3.598s, episode steps: 161, steps per second:  45, episode reward: -160.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.857 [0.000, 2.000],  loss: 0.654545, mae: 18.563892, mean_q: -27.185556, mean_eps: 0.074111\n",
            " 189151/200000: episode: 878, duration: 3.049s, episode steps: 113, steps per second:  37, episode reward: -112.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.929 [0.000, 2.000],  loss: 0.715409, mae: 18.587712, mean_q: -27.220786, mean_eps: 0.073439\n",
            " 189313/200000: episode: 879, duration: 4.371s, episode steps: 162, steps per second:  37, episode reward: -161.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.821 [0.000, 2.000],  loss: 0.676962, mae: 18.672916, mean_q: -27.355377, mean_eps: 0.072766\n",
            " 189436/200000: episode: 880, duration: 2.816s, episode steps: 123, steps per second:  44, episode reward: -122.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.707 [0.000, 2.000],  loss: 0.742936, mae: 18.646127, mean_q: -27.300618, mean_eps: 0.072067\n",
            " 189562/200000: episode: 881, duration: 2.929s, episode steps: 126, steps per second:  43, episode reward: -125.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.738 [0.000, 2.000],  loss: 0.674348, mae: 18.651403, mean_q: -27.320027, mean_eps: 0.071457\n",
            " 189697/200000: episode: 882, duration: 3.682s, episode steps: 135, steps per second:  37, episode reward: -134.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.719 [0.000, 2.000],  loss: 0.657593, mae: 18.664115, mean_q: -27.349890, mean_eps: 0.070818\n",
            " 189858/200000: episode: 883, duration: 4.283s, episode steps: 161, steps per second:  38, episode reward: -160.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.596 [0.000, 2.000],  loss: 0.698307, mae: 18.661133, mean_q: -27.340615, mean_eps: 0.070093\n",
            " 190026/200000: episode: 884, duration: 3.892s, episode steps: 168, steps per second:  43, episode reward: -167.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.720 [0.000, 2.000],  loss: 0.696270, mae: 18.689063, mean_q: -27.378277, mean_eps: 0.069287\n",
            " 190141/200000: episode: 885, duration: 2.680s, episode steps: 115, steps per second:  43, episode reward: -114.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.617 [0.000, 2.000],  loss: 0.725143, mae: 18.795184, mean_q: -27.526728, mean_eps: 0.068593\n",
            " 190253/200000: episode: 886, duration: 3.392s, episode steps: 112, steps per second:  33, episode reward: -111.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.795 [0.000, 2.000],  loss: 0.727373, mae: 18.808273, mean_q: -27.532547, mean_eps: 0.068037\n",
            " 190389/200000: episode: 887, duration: 3.458s, episode steps: 136, steps per second:  39, episode reward: -135.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.596 [0.000, 2.000],  loss: 0.727242, mae: 18.858239, mean_q: -27.599857, mean_eps: 0.067430\n",
            " 190488/200000: episode: 888, duration: 2.331s, episode steps:  99, steps per second:  42, episode reward: -98.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.879 [0.000, 2.000],  loss: 0.681983, mae: 18.844928, mean_q: -27.585174, mean_eps: 0.066854\n",
            " 190584/200000: episode: 889, duration: 2.241s, episode steps:  96, steps per second:  43, episode reward: -95.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.708 [0.000, 2.000],  loss: 0.690829, mae: 18.916195, mean_q: -27.686766, mean_eps: 0.066376\n",
            " 190699/200000: episode: 890, duration: 2.655s, episode steps: 115, steps per second:  43, episode reward: -114.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.757 [0.000, 2.000],  loss: 0.709890, mae: 18.946185, mean_q: -27.719726, mean_eps: 0.065859\n",
            " 190805/200000: episode: 891, duration: 3.452s, episode steps: 106, steps per second:  31, episode reward: -105.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.934 [0.000, 2.000],  loss: 0.716739, mae: 18.988709, mean_q: -27.792078, mean_eps: 0.065318\n",
            " 190959/200000: episode: 892, duration: 4.013s, episode steps: 154, steps per second:  38, episode reward: -153.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.610 [0.000, 2.000],  loss: 0.673086, mae: 18.956923, mean_q: -27.772875, mean_eps: 0.064681\n",
            " 191044/200000: episode: 893, duration: 1.957s, episode steps:  85, steps per second:  43, episode reward: -84.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.859 [0.000, 2.000],  loss: 0.644144, mae: 18.961481, mean_q: -27.768214, mean_eps: 0.064095\n",
            " 191234/200000: episode: 894, duration: 4.352s, episode steps: 190, steps per second:  44, episode reward: -189.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.663 [0.000, 2.000],  loss: 0.659557, mae: 18.980165, mean_q: -27.799548, mean_eps: 0.063421\n",
            " 191358/200000: episode: 895, duration: 4.062s, episode steps: 124, steps per second:  31, episode reward: -123.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.984 [0.000, 2.000],  loss: 0.690646, mae: 19.066026, mean_q: -27.924713, mean_eps: 0.062652\n",
            " 191480/200000: episode: 896, duration: 2.739s, episode steps: 122, steps per second:  45, episode reward: -121.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.770 [0.000, 2.000],  loss: 0.625659, mae: 19.011623, mean_q: -27.835845, mean_eps: 0.062049\n",
            " 191649/200000: episode: 897, duration: 3.744s, episode steps: 169, steps per second:  45, episode reward: -168.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.598 [0.000, 2.000],  loss: 0.707935, mae: 19.039725, mean_q: -27.855272, mean_eps: 0.061336\n",
            " 191775/200000: episode: 898, duration: 2.856s, episode steps: 126, steps per second:  44, episode reward: -125.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.929 [0.000, 2.000],  loss: 0.689013, mae: 19.116023, mean_q: -27.983551, mean_eps: 0.060614\n",
            " 191901/200000: episode: 899, duration: 4.084s, episode steps: 126, steps per second:  31, episode reward: -125.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.841 [0.000, 2.000],  loss: 0.657287, mae: 19.060161, mean_q: -27.898572, mean_eps: 0.059996\n",
            " 192045/200000: episode: 900, duration: 3.292s, episode steps: 144, steps per second:  44, episode reward: -143.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.806 [0.000, 2.000],  loss: 0.716131, mae: 19.056597, mean_q: -27.875429, mean_eps: 0.059335\n",
            " 192179/200000: episode: 901, duration: 2.945s, episode steps: 134, steps per second:  46, episode reward: -133.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.948 [0.000, 2.000],  loss: 0.717108, mae: 19.079853, mean_q: -27.916942, mean_eps: 0.058654\n",
            " 192293/200000: episode: 902, duration: 2.667s, episode steps: 114, steps per second:  43, episode reward: -113.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.746 [0.000, 2.000],  loss: 0.713423, mae: 19.119953, mean_q: -27.988122, mean_eps: 0.058046\n",
            " 192418/200000: episode: 903, duration: 3.759s, episode steps: 125, steps per second:  33, episode reward: -124.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.864 [0.000, 2.000],  loss: 0.705502, mae: 19.152277, mean_q: -28.032308, mean_eps: 0.057461\n",
            " 192551/200000: episode: 904, duration: 3.282s, episode steps: 133, steps per second:  41, episode reward: -132.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.910 [0.000, 2.000],  loss: 0.696018, mae: 19.098093, mean_q: -27.947006, mean_eps: 0.056828\n",
            " 192719/200000: episode: 905, duration: 3.772s, episode steps: 168, steps per second:  45, episode reward: -167.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.661 [0.000, 2.000],  loss: 0.679180, mae: 19.149321, mean_q: -28.043492, mean_eps: 0.056091\n",
            " 192927/200000: episode: 906, duration: 5.312s, episode steps: 208, steps per second:  39, episode reward: -207.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.654 [0.000, 2.000],  loss: 0.710256, mae: 19.160121, mean_q: -28.042176, mean_eps: 0.055170\n",
            " 193023/200000: episode: 907, duration: 2.855s, episode steps:  96, steps per second:  34, episode reward: -95.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.740 [0.000, 2.000],  loss: 0.712936, mae: 19.172704, mean_q: -28.049368, mean_eps: 0.054425\n",
            " 193303/200000: episode: 908, duration: 6.404s, episode steps: 280, steps per second:  44, episode reward: -279.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.532 [0.000, 2.000],  loss: 0.716395, mae: 19.186341, mean_q: -28.080572, mean_eps: 0.053504\n",
            " 193401/200000: episode: 909, duration: 2.573s, episode steps:  98, steps per second:  38, episode reward: -97.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.633 [0.000, 2.000],  loss: 0.664681, mae: 19.205158, mean_q: -28.093968, mean_eps: 0.052578\n",
            " 193545/200000: episode: 910, duration: 4.510s, episode steps: 144, steps per second:  32, episode reward: -143.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.701 [0.000, 2.000],  loss: 0.674701, mae: 19.136451, mean_q: -28.008959, mean_eps: 0.051985\n",
            " 193648/200000: episode: 911, duration: 2.388s, episode steps: 103, steps per second:  43, episode reward: -102.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.718 [0.000, 2.000],  loss: 0.728309, mae: 19.139890, mean_q: -27.995351, mean_eps: 0.051380\n",
            " 193776/200000: episode: 912, duration: 2.975s, episode steps: 128, steps per second:  43, episode reward: -127.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.961 [0.000, 2.000],  loss: 0.718718, mae: 19.116120, mean_q: -27.977170, mean_eps: 0.050814\n",
            " 193858/200000: episode: 913, duration: 1.926s, episode steps:  82, steps per second:  43, episode reward: -81.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.963 [0.000, 2.000],  loss: 0.678501, mae: 19.069895, mean_q: -27.897507, mean_eps: 0.050299\n",
            " 193982/200000: episode: 914, duration: 3.382s, episode steps: 124, steps per second:  37, episode reward: -123.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.815 [0.000, 2.000],  loss: 0.681653, mae: 19.126462, mean_q: -27.986700, mean_eps: 0.049794\n",
            " 194095/200000: episode: 915, duration: 3.214s, episode steps: 113, steps per second:  35, episode reward: -112.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.779 [0.000, 2.000],  loss: 0.638575, mae: 19.108436, mean_q: -27.977444, mean_eps: 0.049214\n",
            " 194191/200000: episode: 916, duration: 2.143s, episode steps:  96, steps per second:  45, episode reward: -95.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.792 [0.000, 2.000],  loss: 0.622178, mae: 19.116795, mean_q: -27.975311, mean_eps: 0.048702\n",
            " 194288/200000: episode: 917, duration: 2.162s, episode steps:  97, steps per second:  45, episode reward: -96.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.876 [0.000, 2.000],  loss: 0.628243, mae: 19.095482, mean_q: -27.950277, mean_eps: 0.048229\n",
            " 194399/200000: episode: 918, duration: 2.475s, episode steps: 111, steps per second:  45, episode reward: -110.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.883 [0.000, 2.000],  loss: 0.638036, mae: 19.051028, mean_q: -27.888331, mean_eps: 0.047719\n",
            " 194533/200000: episode: 919, duration: 3.565s, episode steps: 134, steps per second:  38, episode reward: -133.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.813 [0.000, 2.000],  loss: 0.624934, mae: 19.104501, mean_q: -27.975860, mean_eps: 0.047119\n",
            " 194639/200000: episode: 920, duration: 3.154s, episode steps: 106, steps per second:  34, episode reward: -105.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.557 [0.000, 2.000],  loss: 0.651321, mae: 19.077410, mean_q: -27.908642, mean_eps: 0.046531\n",
            " 194726/200000: episode: 921, duration: 1.994s, episode steps:  87, steps per second:  44, episode reward: -86.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.908 [0.000, 2.000],  loss: 0.658427, mae: 19.109464, mean_q: -27.980648, mean_eps: 0.046058\n",
            " 194852/200000: episode: 922, duration: 2.873s, episode steps: 126, steps per second:  44, episode reward: -125.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.921 [0.000, 2.000],  loss: 0.636822, mae: 18.965501, mean_q: -27.747895, mean_eps: 0.045536\n",
            " 194950/200000: episode: 923, duration: 2.236s, episode steps:  98, steps per second:  44, episode reward: -97.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.918 [0.000, 2.000],  loss: 0.653220, mae: 19.051617, mean_q: -27.900382, mean_eps: 0.044988\n",
            " 195160/200000: episode: 924, duration: 5.959s, episode steps: 210, steps per second:  35, episode reward: -209.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.557 [0.000, 2.000],  loss: 0.655910, mae: 19.085835, mean_q: -27.934353, mean_eps: 0.044233\n",
            " 195304/200000: episode: 925, duration: 3.251s, episode steps: 144, steps per second:  44, episode reward: -143.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.861 [0.000, 2.000],  loss: 0.653511, mae: 19.009136, mean_q: -27.808665, mean_eps: 0.043366\n",
            " 195426/200000: episode: 926, duration: 2.785s, episode steps: 122, steps per second:  44, episode reward: -121.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.811 [0.000, 2.000],  loss: 0.659386, mae: 19.118758, mean_q: -27.984238, mean_eps: 0.042714\n",
            " 195585/200000: episode: 927, duration: 3.834s, episode steps: 159, steps per second:  41, episode reward: -158.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.912 [0.000, 2.000],  loss: 0.666877, mae: 19.097239, mean_q: -27.952158, mean_eps: 0.042026\n",
            " 195689/200000: episode: 928, duration: 3.420s, episode steps: 104, steps per second:  30, episode reward: -103.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.865 [0.000, 2.000],  loss: 0.671821, mae: 18.987908, mean_q: -27.790391, mean_eps: 0.041381\n",
            " 195780/200000: episode: 929, duration: 2.464s, episode steps:  91, steps per second:  37, episode reward: -90.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.066 [0.000, 2.000],  loss: 0.681608, mae: 19.026512, mean_q: -27.844429, mean_eps: 0.040903\n",
            " 195908/200000: episode: 930, duration: 2.918s, episode steps: 128, steps per second:  44, episode reward: -127.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.602 [0.000, 2.000],  loss: 0.696430, mae: 18.961828, mean_q: -27.752668, mean_eps: 0.040367\n",
            " 196005/200000: episode: 931, duration: 2.223s, episode steps:  97, steps per second:  44, episode reward: -96.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.938 [0.000, 2.000],  loss: 0.644528, mae: 19.010607, mean_q: -27.822835, mean_eps: 0.039816\n",
            " 196202/200000: episode: 932, duration: 5.596s, episode steps: 197, steps per second:  35, episode reward: -196.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.701 [0.000, 2.000],  loss: 0.689300, mae: 19.041967, mean_q: -27.873774, mean_eps: 0.039095\n",
            " 196299/200000: episode: 933, duration: 2.210s, episode steps:  97, steps per second:  44, episode reward: -96.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.835 [0.000, 2.000],  loss: 0.660640, mae: 18.987679, mean_q: -27.789826, mean_eps: 0.038375\n",
            " 196531/200000: episode: 934, duration: 5.442s, episode steps: 232, steps per second:  43, episode reward: -231.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.836 [0.000, 2.000],  loss: 0.673423, mae: 19.020417, mean_q: -27.852892, mean_eps: 0.037569\n",
            " 196642/200000: episode: 935, duration: 2.828s, episode steps: 111, steps per second:  39, episode reward: -110.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.919 [0.000, 2.000],  loss: 0.713505, mae: 19.021646, mean_q: -27.855928, mean_eps: 0.036729\n",
            " 196753/200000: episode: 936, duration: 3.575s, episode steps: 111, steps per second:  31, episode reward: -110.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.937 [0.000, 2.000],  loss: 0.713754, mae: 19.001991, mean_q: -27.816093, mean_eps: 0.036185\n",
            " 196928/200000: episode: 937, duration: 4.041s, episode steps: 175, steps per second:  43, episode reward: -174.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.634 [0.000, 2.000],  loss: 0.717465, mae: 19.062472, mean_q: -27.898520, mean_eps: 0.035484\n",
            " 197075/200000: episode: 938, duration: 3.393s, episode steps: 147, steps per second:  43, episode reward: -146.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.755 [0.000, 2.000],  loss: 0.684052, mae: 19.021342, mean_q: -27.838060, mean_eps: 0.034695\n",
            " 197185/200000: episode: 939, duration: 2.854s, episode steps: 110, steps per second:  39, episode reward: -109.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.691 [0.000, 2.000],  loss: 0.654592, mae: 19.126224, mean_q: -27.987781, mean_eps: 0.034065\n",
            " 197304/200000: episode: 940, duration: 3.636s, episode steps: 119, steps per second:  33, episode reward: -118.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.807 [0.000, 2.000],  loss: 0.712866, mae: 18.989492, mean_q: -27.780175, mean_eps: 0.033504\n",
            " 197382/200000: episode: 941, duration: 1.757s, episode steps:  78, steps per second:  44, episode reward: -77.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.923 [0.000, 2.000],  loss: 0.628356, mae: 19.044620, mean_q: -27.859507, mean_eps: 0.033022\n",
            " 197598/200000: episode: 942, duration: 4.905s, episode steps: 216, steps per second:  44, episode reward: -215.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.662 [0.000, 2.000],  loss: 0.712575, mae: 19.115983, mean_q: -27.978047, mean_eps: 0.032301\n",
            " 197729/200000: episode: 943, duration: 3.366s, episode steps: 131, steps per second:  39, episode reward: -130.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.695 [0.000, 2.000],  loss: 0.658433, mae: 19.147254, mean_q: -28.009195, mean_eps: 0.031451\n",
            " 197853/200000: episode: 944, duration: 3.795s, episode steps: 124, steps per second:  33, episode reward: -123.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.855 [0.000, 2.000],  loss: 0.686460, mae: 19.117223, mean_q: -27.977004, mean_eps: 0.030827\n",
            " 197963/200000: episode: 945, duration: 2.558s, episode steps: 110, steps per second:  43, episode reward: -109.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.036 [0.000, 2.000],  loss: 0.699290, mae: 19.114804, mean_q: -27.991362, mean_eps: 0.030253\n",
            " 198115/200000: episode: 946, duration: 4.026s, episode steps: 152, steps per second:  38, episode reward: -151.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.697 [0.000, 2.000],  loss: 0.676685, mae: 19.117677, mean_q: -28.003425, mean_eps: 0.029611\n",
            " 198255/200000: episode: 947, duration: 4.140s, episode steps: 140, steps per second:  34, episode reward: -139.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.893 [0.000, 2.000],  loss: 0.674966, mae: 19.110395, mean_q: -27.976233, mean_eps: 0.028896\n",
            " 198366/200000: episode: 948, duration: 3.213s, episode steps: 111, steps per second:  35, episode reward: -110.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.910 [0.000, 2.000],  loss: 0.641142, mae: 19.152720, mean_q: -28.024796, mean_eps: 0.028281\n",
            " 198445/200000: episode: 949, duration: 1.833s, episode steps:  79, steps per second:  43, episode reward: -78.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.886 [0.000, 2.000],  loss: 0.676710, mae: 19.088397, mean_q: -27.949151, mean_eps: 0.027816\n",
            " 198575/200000: episode: 950, duration: 2.902s, episode steps: 130, steps per second:  45, episode reward: -129.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.715 [0.000, 2.000],  loss: 0.663897, mae: 19.137990, mean_q: -28.017663, mean_eps: 0.027303\n",
            " 198662/200000: episode: 951, duration: 1.952s, episode steps:  87, steps per second:  45, episode reward: -86.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.874 [0.000, 2.000],  loss: 0.704805, mae: 19.026894, mean_q: -27.831991, mean_eps: 0.026772\n",
            " 198769/200000: episode: 952, duration: 2.855s, episode steps: 107, steps per second:  37, episode reward: -106.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.879 [0.000, 2.000],  loss: 0.662638, mae: 19.080160, mean_q: -27.919381, mean_eps: 0.026297\n",
            " 198872/200000: episode: 953, duration: 3.298s, episode steps: 103, steps per second:  31, episode reward: -102.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.654820, mae: 19.128141, mean_q: -27.992905, mean_eps: 0.025782\n",
            " 199000/200000: episode: 954, duration: 2.981s, episode steps: 128, steps per second:  43, episode reward: -127.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.773 [0.000, 2.000],  loss: 0.669662, mae: 19.111277, mean_q: -27.963274, mean_eps: 0.025216\n",
            " 199155/200000: episode: 955, duration: 3.688s, episode steps: 155, steps per second:  42, episode reward: -154.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.819 [0.000, 2.000],  loss: 0.687221, mae: 19.124856, mean_q: -27.977800, mean_eps: 0.024523\n",
            " 199237/200000: episode: 956, duration: 1.969s, episode steps:  82, steps per second:  42, episode reward: -81.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.073 [0.000, 2.000],  loss: 0.665466, mae: 19.113756, mean_q: -27.961532, mean_eps: 0.023942\n",
            " 199351/200000: episode: 957, duration: 3.638s, episode steps: 114, steps per second:  31, episode reward: -113.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.719 [0.000, 2.000],  loss: 0.685132, mae: 19.142630, mean_q: -28.007346, mean_eps: 0.023462\n",
            " 199439/200000: episode: 958, duration: 2.334s, episode steps:  88, steps per second:  38, episode reward: -87.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.023 [0.000, 2.000],  loss: 0.663332, mae: 19.138056, mean_q: -27.987917, mean_eps: 0.022967\n",
            " 199534/200000: episode: 959, duration: 2.281s, episode steps:  95, steps per second:  42, episode reward: -94.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.699384, mae: 19.079222, mean_q: -27.893856, mean_eps: 0.022519\n",
            " 199647/200000: episode: 960, duration: 2.642s, episode steps: 113, steps per second:  43, episode reward: -112.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.080 [0.000, 2.000],  loss: 0.691293, mae: 19.057743, mean_q: -27.870025, mean_eps: 0.022009\n",
            " 199756/200000: episode: 961, duration: 2.514s, episode steps: 109, steps per second:  43, episode reward: -108.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.991 [0.000, 2.000],  loss: 0.741470, mae: 19.052474, mean_q: -27.855010, mean_eps: 0.021465\n",
            " 199866/200000: episode: 962, duration: 3.305s, episode steps: 110, steps per second:  33, episode reward: -109.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.182 [0.000, 2.000],  loss: 0.702535, mae: 19.048032, mean_q: -27.867586, mean_eps: 0.020929\n",
            " 199950/200000: episode: 963, duration: 2.421s, episode steps:  84, steps per second:  35, episode reward: -83.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.119 [0.000, 2.000],  loss: 0.662134, mae: 19.070826, mean_q: -27.888694, mean_eps: 0.020453\n",
            "done, took 4756.310 seconds\n",
            "\n",
            "Evaluando Mejora 2...\n",
            "Mejora 2 → Recompensa media: -259.25 ± 187.39\n",
            "\n",
            "Entrenando Mejora 3...\n",
            "Training for 200000 steps ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    500/200000: episode: 1, duration: 1.114s, episode steps: 500, steps per second: 449, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.034 [0.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   1000/200000: episode: 2, duration: 0.752s, episode steps: 500, steps per second: 665, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   1500/200000: episode: 3, duration: 0.812s, episode steps: 500, steps per second: 616, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.984 [0.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   2000/200000: episode: 4, duration: 0.759s, episode steps: 500, steps per second: 659, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.986 [0.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   2500/200000: episode: 5, duration: 0.792s, episode steps: 500, steps per second: 631, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.946 [0.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   3000/200000: episode: 6, duration: 0.935s, episode steps: 500, steps per second: 535, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.952 [0.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   3500/200000: episode: 7, duration: 0.529s, episode steps: 500, steps per second: 946, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.988 [0.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   4000/200000: episode: 8, duration: 0.532s, episode steps: 500, steps per second: 939, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.946 [0.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   4500/200000: episode: 9, duration: 0.514s, episode steps: 500, steps per second: 973, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.018 [0.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   5000/200000: episode: 10, duration: 0.545s, episode steps: 500, steps per second: 918, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.986 [0.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   5500/200000: episode: 11, duration: 9.229s, episode steps: 500, steps per second:  54, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.012 [0.000, 2.000],  loss: 0.122005, mae: 0.638741, mean_q: -0.378183, mean_eps: 0.966750\n",
            "   6000/200000: episode: 12, duration: 6.904s, episode steps: 500, steps per second:  72, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.998 [0.000, 2.000],  loss: 0.044933, mae: 0.625684, mean_q: -0.594474, mean_eps: 0.963587\n",
            "   6500/200000: episode: 13, duration: 6.638s, episode steps: 500, steps per second:  75, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.028 [0.000, 2.000],  loss: 0.053858, mae: 1.142124, mean_q: -1.401425, mean_eps: 0.960420\n",
            "   7000/200000: episode: 14, duration: 6.918s, episode steps: 500, steps per second:  72, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.988 [0.000, 2.000],  loss: 0.031036, mae: 1.145708, mean_q: -1.468358, mean_eps: 0.957253\n",
            "   7500/200000: episode: 15, duration: 6.882s, episode steps: 500, steps per second:  73, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.962 [0.000, 2.000],  loss: 0.044665, mae: 1.647922, mean_q: -2.216798, mean_eps: 0.954086\n",
            "   8000/200000: episode: 16, duration: 6.999s, episode steps: 500, steps per second:  71, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.044 [0.000, 2.000],  loss: 0.030671, mae: 1.648151, mean_q: -2.252209, mean_eps: 0.950920\n",
            "   8500/200000: episode: 17, duration: 6.859s, episode steps: 500, steps per second:  73, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.018 [0.000, 2.000],  loss: 0.045872, mae: 2.139049, mean_q: -2.976869, mean_eps: 0.947753\n",
            "   9000/200000: episode: 18, duration: 7.025s, episode steps: 500, steps per second:  71, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.032 [0.000, 2.000],  loss: 0.035292, mae: 2.145124, mean_q: -3.011042, mean_eps: 0.944587\n",
            "   9500/200000: episode: 19, duration: 6.721s, episode steps: 500, steps per second:  74, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.024 [0.000, 2.000],  loss: 0.051302, mae: 2.638678, mean_q: -3.733628, mean_eps: 0.941420\n",
            "  10000/200000: episode: 20, duration: 6.618s, episode steps: 500, steps per second:  76, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.932 [0.000, 2.000],  loss: 0.041736, mae: 2.632955, mean_q: -3.742378, mean_eps: 0.938253\n",
            "  10500/200000: episode: 21, duration: 6.532s, episode steps: 500, steps per second:  77, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.976 [0.000, 2.000],  loss: 0.056842, mae: 3.084529, mean_q: -4.398277, mean_eps: 0.935086\n",
            "  10973/200000: episode: 22, duration: 6.475s, episode steps: 473, steps per second:  73, episode reward: -472.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.998 [0.000, 2.000],  loss: 0.049605, mae: 3.081375, mean_q: -4.404429, mean_eps: 0.932005\n",
            "  11473/200000: episode: 23, duration: 6.426s, episode steps: 500, steps per second:  78, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.966 [0.000, 2.000],  loss: 0.064920, mae: 3.449815, mean_q: -4.940715, mean_eps: 0.928924\n",
            "  11973/200000: episode: 24, duration: 7.203s, episode steps: 500, steps per second:  69, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.044 [0.000, 2.000],  loss: 0.056797, mae: 3.463941, mean_q: -4.969364, mean_eps: 0.925758\n",
            "  12473/200000: episode: 25, duration: 6.470s, episode steps: 500, steps per second:  77, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.962 [0.000, 2.000],  loss: 0.069153, mae: 3.844812, mean_q: -5.527308, mean_eps: 0.922591\n",
            "  12973/200000: episode: 26, duration: 7.173s, episode steps: 500, steps per second:  70, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.986 [0.000, 2.000],  loss: 0.064650, mae: 3.870777, mean_q: -5.573455, mean_eps: 0.919424\n",
            "  13473/200000: episode: 27, duration: 6.576s, episode steps: 500, steps per second:  76, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.058 [0.000, 2.000],  loss: 0.076997, mae: 4.171998, mean_q: -6.017004, mean_eps: 0.916257\n",
            "  13973/200000: episode: 28, duration: 7.042s, episode steps: 500, steps per second:  71, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.942 [0.000, 2.000],  loss: 0.073956, mae: 4.187350, mean_q: -6.045482, mean_eps: 0.913091\n",
            "  14372/200000: episode: 29, duration: 4.995s, episode steps: 399, steps per second:  80, episode reward: -398.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.972 [0.000, 2.000],  loss: 0.087006, mae: 4.487326, mean_q: -6.488831, mean_eps: 0.910244\n",
            "  14872/200000: episode: 30, duration: 7.389s, episode steps: 500, steps per second:  68, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.002 [0.000, 2.000],  loss: 0.082523, mae: 4.500857, mean_q: -6.510581, mean_eps: 0.907397\n",
            "  15154/200000: episode: 31, duration: 3.608s, episode steps: 282, steps per second:  78, episode reward: -281.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.025 [0.000, 2.000],  loss: 0.087735, mae: 4.668391, mean_q: -6.758298, mean_eps: 0.904921\n",
            "  15654/200000: episode: 32, duration: 7.381s, episode steps: 500, steps per second:  68, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.996 [0.000, 2.000],  loss: 0.092002, mae: 4.808008, mean_q: -6.969403, mean_eps: 0.902445\n",
            "  16154/200000: episode: 33, duration: 6.120s, episode steps: 500, steps per second:  82, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.026 [0.000, 2.000],  loss: 0.093307, mae: 4.894004, mean_q: -7.095800, mean_eps: 0.899278\n",
            "  16556/200000: episode: 34, duration: 5.714s, episode steps: 402, steps per second:  70, episode reward: -401.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.030 [0.000, 2.000],  loss: 0.091669, mae: 5.098295, mean_q: -7.393313, mean_eps: 0.896421\n",
            "  16935/200000: episode: 35, duration: 5.127s, episode steps: 379, steps per second:  74, episode reward: -378.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.029 [0.000, 2.000],  loss: 0.100928, mae: 5.090319, mean_q: -7.384307, mean_eps: 0.893948\n",
            "  17435/200000: episode: 36, duration: 6.755s, episode steps: 500, steps per second:  74, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.004 [0.000, 2.000],  loss: 0.098668, mae: 5.271671, mean_q: -7.644242, mean_eps: 0.891165\n",
            "  17935/200000: episode: 37, duration: 7.410s, episode steps: 500, steps per second:  67, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.095758, mae: 5.291323, mean_q: -7.671733, mean_eps: 0.887998\n",
            "  18435/200000: episode: 38, duration: 6.563s, episode steps: 500, steps per second:  76, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.878 [0.000, 2.000],  loss: 0.100223, mae: 5.424591, mean_q: -7.865432, mean_eps: 0.884831\n",
            "  18935/200000: episode: 39, duration: 7.069s, episode steps: 500, steps per second:  71, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.014 [0.000, 2.000],  loss: 0.097509, mae: 5.446131, mean_q: -7.900861, mean_eps: 0.881665\n",
            "  19181/200000: episode: 40, duration: 3.042s, episode steps: 246, steps per second:  81, episode reward: -245.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.033 [0.000, 2.000],  loss: 0.116228, mae: 5.605612, mean_q: -8.133135, mean_eps: 0.879303\n",
            "  19681/200000: episode: 41, duration: 7.522s, episode steps: 500, steps per second:  66, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.986 [0.000, 2.000],  loss: 0.102783, mae: 5.658360, mean_q: -8.214685, mean_eps: 0.876940\n",
            "  20140/200000: episode: 42, duration: 5.816s, episode steps: 459, steps per second:  79, episode reward: -458.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.950 [0.000, 2.000],  loss: 0.104804, mae: 5.707711, mean_q: -8.288043, mean_eps: 0.873903\n",
            "  20640/200000: episode: 43, duration: 7.525s, episode steps: 500, steps per second:  66, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.022 [0.000, 2.000],  loss: 0.112354, mae: 5.817906, mean_q: -8.450749, mean_eps: 0.870867\n",
            "  20883/200000: episode: 44, duration: 3.035s, episode steps: 243, steps per second:  80, episode reward: -242.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.938 [0.000, 2.000],  loss: 0.112319, mae: 5.818695, mean_q: -8.460792, mean_eps: 0.868514\n",
            "  21320/200000: episode: 45, duration: 5.524s, episode steps: 437, steps per second:  79, episode reward: -436.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.039 [0.000, 2.000],  loss: 0.110892, mae: 5.921331, mean_q: -8.606039, mean_eps: 0.866360\n",
            "  21666/200000: episode: 46, duration: 5.534s, episode steps: 346, steps per second:  63, episode reward: -345.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.974 [0.000, 2.000],  loss: 0.116147, mae: 5.955110, mean_q: -8.653465, mean_eps: 0.863881\n",
            "  22166/200000: episode: 47, duration: 6.387s, episode steps: 500, steps per second:  78, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.122443, mae: 5.998948, mean_q: -8.717566, mean_eps: 0.861202\n",
            "  22503/200000: episode: 48, duration: 5.454s, episode steps: 337, steps per second:  62, episode reward: -336.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.015 [0.000, 2.000],  loss: 0.118989, mae: 6.134531, mean_q: -8.918163, mean_eps: 0.858551\n",
            "  22904/200000: episode: 49, duration: 5.046s, episode steps: 401, steps per second:  79, episode reward: -400.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.052 [0.000, 2.000],  loss: 0.123851, mae: 6.123522, mean_q: -8.900525, mean_eps: 0.856214\n",
            "  23216/200000: episode: 50, duration: 3.960s, episode steps: 312, steps per second:  79, episode reward: -311.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.949 [0.000, 2.000],  loss: 0.128746, mae: 6.201377, mean_q: -9.009579, mean_eps: 0.853957\n",
            "  23495/200000: episode: 51, duration: 4.802s, episode steps: 279, steps per second:  58, episode reward: -278.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.050 [0.000, 2.000],  loss: 0.136382, mae: 6.223638, mean_q: -9.035304, mean_eps: 0.852085\n",
            "  23842/200000: episode: 52, duration: 4.523s, episode steps: 347, steps per second:  77, episode reward: -346.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.032 [0.000, 2.000],  loss: 0.120887, mae: 6.229687, mean_q: -9.047405, mean_eps: 0.850103\n",
            "  24204/200000: episode: 53, duration: 4.792s, episode steps: 362, steps per second:  76, episode reward: -361.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.058 [0.000, 2.000],  loss: 0.147468, mae: 6.323550, mean_q: -9.184340, mean_eps: 0.847858\n",
            "  24643/200000: episode: 54, duration: 6.968s, episode steps: 439, steps per second:  63, episode reward: -438.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.991 [0.000, 2.000],  loss: 0.135151, mae: 6.400196, mean_q: -9.301408, mean_eps: 0.845321\n",
            "  24994/200000: episode: 55, duration: 4.368s, episode steps: 351, steps per second:  80, episode reward: -350.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.989 [0.000, 2.000],  loss: 0.137666, mae: 6.409446, mean_q: -9.321328, mean_eps: 0.842819\n",
            "  25459/200000: episode: 56, duration: 7.314s, episode steps: 465, steps per second:  64, episode reward: -464.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.015 [0.000, 2.000],  loss: 0.137563, mae: 6.564276, mean_q: -9.545233, mean_eps: 0.840235\n",
            "  25770/200000: episode: 57, duration: 4.089s, episode steps: 311, steps per second:  76, episode reward: -310.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.990 [0.000, 2.000],  loss: 0.134568, mae: 6.561680, mean_q: -9.546689, mean_eps: 0.837778\n",
            "  26070/200000: episode: 58, duration: 4.083s, episode steps: 300, steps per second:  73, episode reward: -299.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.030 [0.000, 2.000],  loss: 0.135268, mae: 6.583690, mean_q: -9.579941, mean_eps: 0.835843\n",
            "  26321/200000: episode: 59, duration: 4.555s, episode steps: 251, steps per second:  55, episode reward: -250.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.088 [0.000, 2.000],  loss: 0.145247, mae: 6.720687, mean_q: -9.782227, mean_eps: 0.834098\n",
            "  26658/200000: episode: 60, duration: 4.466s, episode steps: 337, steps per second:  75, episode reward: -336.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.056 [0.000, 2.000],  loss: 0.151564, mae: 6.712690, mean_q: -9.770901, mean_eps: 0.832236\n",
            "  27036/200000: episode: 61, duration: 4.831s, episode steps: 378, steps per second:  78, episode reward: -377.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.958 [0.000, 2.000],  loss: 0.145198, mae: 6.724941, mean_q: -9.790544, mean_eps: 0.829972\n",
            "  27477/200000: episode: 62, duration: 6.818s, episode steps: 441, steps per second:  65, episode reward: -440.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.041 [0.000, 2.000],  loss: 0.147182, mae: 6.866756, mean_q: -9.999593, mean_eps: 0.827379\n",
            "  27902/200000: episode: 63, duration: 5.386s, episode steps: 425, steps per second:  79, episode reward: -424.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.089 [0.000, 2.000],  loss: 0.143192, mae: 6.863261, mean_q: -9.993896, mean_eps: 0.824636\n",
            "  28402/200000: episode: 64, duration: 7.669s, episode steps: 500, steps per second:  65, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.154459, mae: 7.003212, mean_q: -10.200766, mean_eps: 0.821707\n",
            "  28777/200000: episode: 65, duration: 4.924s, episode steps: 375, steps per second:  76, episode reward: -374.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.083 [0.000, 2.000],  loss: 0.155566, mae: 7.013533, mean_q: -10.216791, mean_eps: 0.818936\n",
            "  29114/200000: episode: 66, duration: 5.270s, episode steps: 337, steps per second:  64, episode reward: -336.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.045 [0.000, 2.000],  loss: 0.163901, mae: 7.055474, mean_q: -10.270396, mean_eps: 0.816682\n",
            "  29562/200000: episode: 67, duration: 6.354s, episode steps: 448, steps per second:  71, episode reward: -447.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.958 [0.000, 2.000],  loss: 0.152074, mae: 7.150690, mean_q: -10.408542, mean_eps: 0.814196\n",
            "  30062/200000: episode: 68, duration: 7.477s, episode steps: 500, steps per second:  67, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.984 [0.000, 2.000],  loss: 0.165818, mae: 7.160526, mean_q: -10.425563, mean_eps: 0.811194\n",
            "  30405/200000: episode: 69, duration: 4.929s, episode steps: 343, steps per second:  70, episode reward: -342.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.003 [0.000, 2.000],  loss: 0.159777, mae: 7.245697, mean_q: -10.555615, mean_eps: 0.808524\n",
            "  30717/200000: episode: 70, duration: 4.243s, episode steps: 312, steps per second:  74, episode reward: -311.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.981 [0.000, 2.000],  loss: 0.147424, mae: 7.247235, mean_q: -10.564983, mean_eps: 0.806450\n",
            "  31178/200000: episode: 71, duration: 7.401s, episode steps: 461, steps per second:  62, episode reward: -460.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.026 [0.000, 2.000],  loss: 0.158186, mae: 7.299503, mean_q: -10.644302, mean_eps: 0.804002\n",
            "  31410/200000: episode: 72, duration: 3.042s, episode steps: 232, steps per second:  76, episode reward: -231.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.138 [0.000, 2.000],  loss: 0.161761, mae: 7.396526, mean_q: -10.793193, mean_eps: 0.801808\n",
            "  31714/200000: episode: 73, duration: 4.059s, episode steps: 304, steps per second:  75, episode reward: -303.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.934 [0.000, 2.000],  loss: 0.172619, mae: 7.369369, mean_q: -10.747386, mean_eps: 0.800110\n",
            "  32028/200000: episode: 74, duration: 5.815s, episode steps: 314, steps per second:  54, episode reward: -313.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.035 [0.000, 2.000],  loss: 0.172892, mae: 7.396816, mean_q: -10.783579, mean_eps: 0.798154\n",
            "  32381/200000: episode: 75, duration: 4.603s, episode steps: 353, steps per second:  77, episode reward: -352.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.997 [0.000, 2.000],  loss: 0.170898, mae: 7.531183, mean_q: -10.986673, mean_eps: 0.796041\n",
            "  32617/200000: episode: 76, duration: 3.189s, episode steps: 236, steps per second:  74, episode reward: -235.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.924 [0.000, 2.000],  loss: 0.170635, mae: 7.542135, mean_q: -10.994782, mean_eps: 0.794176\n",
            "  33102/200000: episode: 77, duration: 7.724s, episode steps: 485, steps per second:  63, episode reward: -484.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.037 [0.000, 2.000],  loss: 0.171545, mae: 7.576366, mean_q: -11.041784, mean_eps: 0.791893\n",
            "  33450/200000: episode: 78, duration: 4.600s, episode steps: 348, steps per second:  76, episode reward: -347.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.173306, mae: 7.687455, mean_q: -11.207168, mean_eps: 0.789255\n",
            "  33704/200000: episode: 79, duration: 4.227s, episode steps: 254, steps per second:  60, episode reward: -253.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.909 [0.000, 2.000],  loss: 0.171783, mae: 7.702056, mean_q: -11.231449, mean_eps: 0.787349\n",
            "  34167/200000: episode: 80, duration: 6.801s, episode steps: 463, steps per second:  68, episode reward: -462.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.004 [0.000, 2.000],  loss: 0.185808, mae: 7.741878, mean_q: -11.286073, mean_eps: 0.785078\n",
            "  34455/200000: episode: 81, duration: 4.044s, episode steps: 288, steps per second:  71, episode reward: -287.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.035 [0.000, 2.000],  loss: 0.214513, mae: 7.828383, mean_q: -11.405264, mean_eps: 0.782700\n",
            "  34746/200000: episode: 82, duration: 5.208s, episode steps: 291, steps per second:  56, episode reward: -290.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.928 [0.000, 2.000],  loss: 0.212561, mae: 7.814328, mean_q: -11.380558, mean_eps: 0.780867\n",
            "  35246/200000: episode: 83, duration: 6.650s, episode steps: 500, steps per second:  75, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.884 [0.000, 2.000],  loss: 0.180534, mae: 7.863067, mean_q: -11.458500, mean_eps: 0.778362\n",
            "  35490/200000: episode: 84, duration: 3.966s, episode steps: 244, steps per second:  62, episode reward: -243.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.914 [0.000, 2.000],  loss: 0.213053, mae: 7.891430, mean_q: -11.497953, mean_eps: 0.776006\n",
            "  35715/200000: episode: 85, duration: 3.515s, episode steps: 225, steps per second:  64, episode reward: -224.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.973 [0.000, 2.000],  loss: 0.212835, mae: 7.885464, mean_q: -11.499678, mean_eps: 0.774521\n",
            "  36076/200000: episode: 86, duration: 4.857s, episode steps: 361, steps per second:  74, episode reward: -360.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.069 [0.000, 2.000],  loss: 0.166130, mae: 7.913007, mean_q: -11.541665, mean_eps: 0.772665\n",
            "  36350/200000: episode: 87, duration: 3.901s, episode steps: 274, steps per second:  70, episode reward: -273.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.069 [0.000, 2.000],  loss: 0.192796, mae: 8.009430, mean_q: -11.674530, mean_eps: 0.770654\n",
            "  36638/200000: episode: 88, duration: 4.766s, episode steps: 288, steps per second:  60, episode reward: -287.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.993 [0.000, 2.000],  loss: 0.194285, mae: 8.003227, mean_q: -11.671927, mean_eps: 0.768875\n",
            "  36884/200000: episode: 89, duration: 3.353s, episode steps: 246, steps per second:  73, episode reward: -245.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.057 [0.000, 2.000],  loss: 0.189640, mae: 7.997198, mean_q: -11.674023, mean_eps: 0.767184\n",
            "  37135/200000: episode: 90, duration: 3.487s, episode steps: 251, steps per second:  72, episode reward: -250.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.984 [0.000, 2.000],  loss: 0.212139, mae: 8.059401, mean_q: -11.757044, mean_eps: 0.765610\n",
            "  37388/200000: episode: 91, duration: 4.590s, episode steps: 253, steps per second:  55, episode reward: -252.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.992 [0.000, 2.000],  loss: 0.199361, mae: 8.127429, mean_q: -11.859682, mean_eps: 0.764014\n",
            "  37617/200000: episode: 92, duration: 3.112s, episode steps: 229, steps per second:  74, episode reward: -228.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.013 [0.000, 2.000],  loss: 0.217290, mae: 8.129639, mean_q: -11.861983, mean_eps: 0.762487\n",
            "  37845/200000: episode: 93, duration: 3.128s, episode steps: 228, steps per second:  73, episode reward: -227.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.978 [0.000, 2.000],  loss: 0.186031, mae: 8.122031, mean_q: -11.852197, mean_eps: 0.761040\n",
            "  38093/200000: episode: 94, duration: 3.395s, episode steps: 248, steps per second:  73, episode reward: -247.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.077 [0.000, 2.000],  loss: 0.209277, mae: 8.168312, mean_q: -11.915984, mean_eps: 0.759533\n",
            "  38319/200000: episode: 95, duration: 4.392s, episode steps: 226, steps per second:  51, episode reward: -225.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.053 [0.000, 2.000],  loss: 0.198877, mae: 8.240650, mean_q: -12.028223, mean_eps: 0.758032\n",
            "  38616/200000: episode: 96, duration: 4.116s, episode steps: 297, steps per second:  72, episode reward: -296.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.966 [0.000, 2.000],  loss: 0.190593, mae: 8.241491, mean_q: -12.026780, mean_eps: 0.756376\n",
            "  38849/200000: episode: 97, duration: 3.225s, episode steps: 233, steps per second:  72, episode reward: -232.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.927 [0.000, 2.000],  loss: 0.218640, mae: 8.232393, mean_q: -12.008381, mean_eps: 0.754697\n",
            "  39174/200000: episode: 98, duration: 5.496s, episode steps: 325, steps per second:  59, episode reward: -324.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.114 [0.000, 2.000],  loss: 0.215740, mae: 8.263183, mean_q: -12.059352, mean_eps: 0.752930\n",
            "  39406/200000: episode: 99, duration: 3.313s, episode steps: 232, steps per second:  70, episode reward: -231.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.026 [0.000, 2.000],  loss: 0.206254, mae: 8.318962, mean_q: -12.146431, mean_eps: 0.751166\n",
            "  39660/200000: episode: 100, duration: 3.437s, episode steps: 254, steps per second:  74, episode reward: -253.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.091 [0.000, 2.000],  loss: 0.209678, mae: 8.315750, mean_q: -12.140124, mean_eps: 0.749628\n",
            "  39931/200000: episode: 101, duration: 4.236s, episode steps: 271, steps per second:  64, episode reward: -270.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.967 [0.000, 2.000],  loss: 0.197267, mae: 8.299556, mean_q: -12.113533, mean_eps: 0.747965\n",
            "  40160/200000: episode: 102, duration: 4.179s, episode steps: 229, steps per second:  55, episode reward: -228.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.070 [0.000, 2.000],  loss: 0.208563, mae: 8.347131, mean_q: -12.181868, mean_eps: 0.746382\n",
            "  40386/200000: episode: 103, duration: 3.146s, episode steps: 226, steps per second:  72, episode reward: -225.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.973 [0.000, 2.000],  loss: 0.199056, mae: 8.385383, mean_q: -12.238756, mean_eps: 0.744941\n",
            "  40716/200000: episode: 104, duration: 4.624s, episode steps: 330, steps per second:  71, episode reward: -329.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.130 [0.000, 2.000],  loss: 0.218555, mae: 8.380221, mean_q: -12.231602, mean_eps: 0.743180\n",
            "  41036/200000: episode: 105, duration: 5.619s, episode steps: 320, steps per second:  57, episode reward: -319.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.887 [0.000, 2.000],  loss: 0.205222, mae: 8.377931, mean_q: -12.230966, mean_eps: 0.741122\n",
            "  41275/200000: episode: 106, duration: 3.249s, episode steps: 239, steps per second:  74, episode reward: -238.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.929 [0.000, 2.000],  loss: 0.228805, mae: 8.549976, mean_q: -12.478314, mean_eps: 0.739352\n",
            "  41556/200000: episode: 107, duration: 3.872s, episode steps: 281, steps per second:  73, episode reward: -280.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.918 [0.000, 2.000],  loss: 0.201833, mae: 8.557786, mean_q: -12.496131, mean_eps: 0.737705\n",
            "  41888/200000: episode: 108, duration: 5.704s, episode steps: 332, steps per second:  58, episode reward: -331.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.867 [0.000, 2.000],  loss: 0.230518, mae: 8.549348, mean_q: -12.474810, mean_eps: 0.735764\n",
            "  42134/200000: episode: 109, duration: 3.476s, episode steps: 246, steps per second:  71, episode reward: -245.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.020 [0.000, 2.000],  loss: 0.204864, mae: 8.581456, mean_q: -12.520436, mean_eps: 0.733934\n",
            "  42433/200000: episode: 110, duration: 4.374s, episode steps: 299, steps per second:  68, episode reward: -298.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.960 [0.000, 2.000],  loss: 0.245820, mae: 8.636365, mean_q: -12.598164, mean_eps: 0.732208\n",
            "  42587/200000: episode: 111, duration: 2.544s, episode steps: 154, steps per second:  61, episode reward: -153.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.052 [0.000, 2.000],  loss: 0.211915, mae: 8.619668, mean_q: -12.581501, mean_eps: 0.730773\n",
            "  42868/200000: episode: 112, duration: 4.785s, episode steps: 281, steps per second:  59, episode reward: -280.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.018 [0.000, 2.000],  loss: 0.226875, mae: 8.597029, mean_q: -12.543233, mean_eps: 0.729396\n",
            "  43108/200000: episode: 113, duration: 3.403s, episode steps: 240, steps per second:  71, episode reward: -239.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.942 [0.000, 2.000],  loss: 0.240405, mae: 8.679045, mean_q: -12.651471, mean_eps: 0.727746\n",
            "  43372/200000: episode: 114, duration: 3.750s, episode steps: 264, steps per second:  70, episode reward: -263.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.083 [0.000, 2.000],  loss: 0.224935, mae: 8.746030, mean_q: -12.755226, mean_eps: 0.726150\n",
            "  43720/200000: episode: 115, duration: 5.940s, episode steps: 348, steps per second:  59, episode reward: -347.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.805 [0.000, 2.000],  loss: 0.219330, mae: 8.760814, mean_q: -12.785855, mean_eps: 0.724212\n",
            "  43982/200000: episode: 116, duration: 3.647s, episode steps: 262, steps per second:  72, episode reward: -261.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.981 [0.000, 2.000],  loss: 0.242938, mae: 8.738187, mean_q: -12.746288, mean_eps: 0.722280\n",
            "  44232/200000: episode: 117, duration: 3.476s, episode steps: 250, steps per second:  72, episode reward: -249.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.020 [0.000, 2.000],  loss: 0.234713, mae: 8.781754, mean_q: -12.811086, mean_eps: 0.720659\n",
            "  44480/200000: episode: 118, duration: 4.749s, episode steps: 248, steps per second:  52, episode reward: -247.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.911 [0.000, 2.000],  loss: 0.254805, mae: 8.787383, mean_q: -12.814905, mean_eps: 0.719082\n",
            "  44760/200000: episode: 119, duration: 4.128s, episode steps: 280, steps per second:  68, episode reward: -279.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.911 [0.000, 2.000],  loss: 0.245158, mae: 8.783398, mean_q: -12.814643, mean_eps: 0.717410\n",
            "  44982/200000: episode: 120, duration: 3.253s, episode steps: 222, steps per second:  68, episode reward: -221.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.977 [0.000, 2.000],  loss: 0.202880, mae: 8.776901, mean_q: -12.805890, mean_eps: 0.715820\n",
            "  45251/200000: episode: 121, duration: 4.565s, episode steps: 269, steps per second:  59, episode reward: -268.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.937 [0.000, 2.000],  loss: 0.251122, mae: 8.830461, mean_q: -12.883914, mean_eps: 0.714265\n",
            "  45475/200000: episode: 122, duration: 3.660s, episode steps: 224, steps per second:  61, episode reward: -223.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.080 [0.000, 2.000],  loss: 0.231353, mae: 8.825480, mean_q: -12.882239, mean_eps: 0.712704\n",
            "  45739/200000: episode: 123, duration: 3.682s, episode steps: 264, steps per second:  72, episode reward: -263.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.917 [0.000, 2.000],  loss: 0.246509, mae: 8.822648, mean_q: -12.874467, mean_eps: 0.711159\n",
            "  45932/200000: episode: 124, duration: 2.870s, episode steps: 193, steps per second:  67, episode reward: -192.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.845 [0.000, 2.000],  loss: 0.228511, mae: 8.829401, mean_q: -12.884070, mean_eps: 0.709712\n",
            "  46153/200000: episode: 125, duration: 4.167s, episode steps: 221, steps per second:  53, episode reward: -220.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.032 [0.000, 2.000],  loss: 0.240913, mae: 8.879681, mean_q: -12.960014, mean_eps: 0.708401\n",
            "  46398/200000: episode: 126, duration: 3.768s, episode steps: 245, steps per second:  65, episode reward: -244.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.996 [0.000, 2.000],  loss: 0.215761, mae: 8.909934, mean_q: -13.006292, mean_eps: 0.706925\n",
            "  46654/200000: episode: 127, duration: 3.728s, episode steps: 256, steps per second:  69, episode reward: -255.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.930 [0.000, 2.000],  loss: 0.207506, mae: 8.902338, mean_q: -13.002413, mean_eps: 0.705338\n",
            "  46860/200000: episode: 128, duration: 3.244s, episode steps: 206, steps per second:  64, episode reward: -205.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.995 [0.000, 2.000],  loss: 0.249609, mae: 8.883229, mean_q: -12.970894, mean_eps: 0.703875\n",
            "  47142/200000: episode: 129, duration: 4.990s, episode steps: 282, steps per second:  57, episode reward: -281.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.014 [0.000, 2.000],  loss: 0.260885, mae: 8.920080, mean_q: -13.017628, mean_eps: 0.702330\n",
            "  47461/200000: episode: 130, duration: 4.627s, episode steps: 319, steps per second:  69, episode reward: -318.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.991 [0.000, 2.000],  loss: 0.241885, mae: 8.921995, mean_q: -13.032720, mean_eps: 0.700427\n",
            "  47709/200000: episode: 131, duration: 3.616s, episode steps: 248, steps per second:  69, episode reward: -247.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.236883, mae: 8.950789, mean_q: -13.065320, mean_eps: 0.698632\n",
            "  47905/200000: episode: 132, duration: 3.984s, episode steps: 196, steps per second:  49, episode reward: -195.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.092 [0.000, 2.000],  loss: 0.242160, mae: 8.932440, mean_q: -13.038376, mean_eps: 0.697226\n",
            "  48026/200000: episode: 133, duration: 1.762s, episode steps: 121, steps per second:  69, episode reward: -120.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.083 [0.000, 2.000],  loss: 0.205985, mae: 8.950228, mean_q: -13.072177, mean_eps: 0.696222\n",
            "  48285/200000: episode: 134, duration: 3.714s, episode steps: 259, steps per second:  70, episode reward: -258.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.931 [0.000, 2.000],  loss: 0.241394, mae: 8.971443, mean_q: -13.092897, mean_eps: 0.695018\n",
            "  48579/200000: episode: 135, duration: 4.355s, episode steps: 294, steps per second:  68, episode reward: -293.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.061 [0.000, 2.000],  loss: 0.241122, mae: 8.967115, mean_q: -13.088956, mean_eps: 0.693267\n",
            "  48866/200000: episode: 136, duration: 5.235s, episode steps: 287, steps per second:  55, episode reward: -286.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.056 [0.000, 2.000],  loss: 0.242160, mae: 8.963166, mean_q: -13.080478, mean_eps: 0.691427\n",
            "  49205/200000: episode: 137, duration: 4.935s, episode steps: 339, steps per second:  69, episode reward: -338.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.956 [0.000, 2.000],  loss: 0.241087, mae: 9.013981, mean_q: -13.156831, mean_eps: 0.689445\n",
            "  49347/200000: episode: 138, duration: 2.060s, episode steps: 142, steps per second:  69, episode reward: -141.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.042 [0.000, 2.000],  loss: 0.248697, mae: 9.045370, mean_q: -13.194858, mean_eps: 0.687922\n",
            "  49507/200000: episode: 139, duration: 3.067s, episode steps: 160, steps per second:  52, episode reward: -159.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.006 [0.000, 2.000],  loss: 0.268779, mae: 9.015447, mean_q: -13.147469, mean_eps: 0.686966\n",
            "  49685/200000: episode: 140, duration: 3.183s, episode steps: 178, steps per second:  56, episode reward: -177.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.180 [0.000, 2.000],  loss: 0.282306, mae: 9.023444, mean_q: -13.164258, mean_eps: 0.685895\n",
            "  49902/200000: episode: 141, duration: 3.416s, episode steps: 217, steps per second:  64, episode reward: -216.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.032 [0.000, 2.000],  loss: 0.231309, mae: 9.024848, mean_q: -13.170064, mean_eps: 0.684644\n",
            "  50108/200000: episode: 142, duration: 3.443s, episode steps: 206, steps per second:  60, episode reward: -205.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.121 [0.000, 2.000],  loss: 0.236820, mae: 9.025388, mean_q: -13.179859, mean_eps: 0.683305\n",
            "  50360/200000: episode: 143, duration: 5.147s, episode steps: 252, steps per second:  49, episode reward: -251.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.067 [0.000, 2.000],  loss: 0.256447, mae: 9.020458, mean_q: -13.162349, mean_eps: 0.681855\n",
            "  50709/200000: episode: 144, duration: 5.384s, episode steps: 349, steps per second:  65, episode reward: -348.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.009 [0.000, 2.000],  loss: 0.239069, mae: 8.974989, mean_q: -13.101366, mean_eps: 0.679951\n",
            "  50910/200000: episode: 145, duration: 2.929s, episode steps: 201, steps per second:  69, episode reward: -200.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.960 [0.000, 2.000],  loss: 0.234259, mae: 8.978448, mean_q: -13.108882, mean_eps: 0.678210\n",
            "  51038/200000: episode: 146, duration: 2.101s, episode steps: 128, steps per second:  61, episode reward: -127.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.992 [0.000, 2.000],  loss: 0.249917, mae: 9.010870, mean_q: -13.147539, mean_eps: 0.677168\n",
            "  51262/200000: episode: 147, duration: 4.245s, episode steps: 224, steps per second:  53, episode reward: -223.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.955 [0.000, 2.000],  loss: 0.225069, mae: 9.048877, mean_q: -13.209122, mean_eps: 0.676053\n",
            "  51470/200000: episode: 148, duration: 3.142s, episode steps: 208, steps per second:  66, episode reward: -207.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.875 [0.000, 2.000],  loss: 0.259173, mae: 9.037662, mean_q: -13.191842, mean_eps: 0.674685\n",
            "  51665/200000: episode: 149, duration: 2.882s, episode steps: 195, steps per second:  68, episode reward: -194.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.113 [0.000, 2.000],  loss: 0.254838, mae: 9.061672, mean_q: -13.224148, mean_eps: 0.673409\n",
            "  51900/200000: episode: 150, duration: 3.863s, episode steps: 235, steps per second:  61, episode reward: -234.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.102 [0.000, 2.000],  loss: 0.227264, mae: 9.038319, mean_q: -13.197531, mean_eps: 0.672047\n",
            "  52104/200000: episode: 151, duration: 3.904s, episode steps: 204, steps per second:  52, episode reward: -203.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.980 [0.000, 2.000],  loss: 0.237492, mae: 9.059721, mean_q: -13.227472, mean_eps: 0.670657\n",
            "  52322/200000: episode: 152, duration: 3.331s, episode steps: 218, steps per second:  65, episode reward: -217.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.014 [0.000, 2.000],  loss: 0.247527, mae: 9.074163, mean_q: -13.250931, mean_eps: 0.669321\n",
            "  52498/200000: episode: 153, duration: 2.748s, episode steps: 176, steps per second:  64, episode reward: -175.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.966 [0.000, 2.000],  loss: 0.211580, mae: 9.107163, mean_q: -13.302118, mean_eps: 0.668073\n",
            "  52871/200000: episode: 154, duration: 6.739s, episode steps: 373, steps per second:  55, episode reward: -372.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.984 [0.000, 2.000],  loss: 0.240158, mae: 9.078326, mean_q: -13.256103, mean_eps: 0.666335\n",
            "  53010/200000: episode: 155, duration: 2.129s, episode steps: 139, steps per second:  65, episode reward: -138.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.101 [0.000, 2.000],  loss: 0.239445, mae: 9.069271, mean_q: -13.252697, mean_eps: 0.664713\n",
            "  53214/200000: episode: 156, duration: 3.105s, episode steps: 204, steps per second:  66, episode reward: -203.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.029 [0.000, 2.000],  loss: 0.232748, mae: 9.088785, mean_q: -13.270448, mean_eps: 0.663627\n",
            "  53423/200000: episode: 157, duration: 3.139s, episode steps: 209, steps per second:  67, episode reward: -208.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.938 [0.000, 2.000],  loss: 0.245489, mae: 9.102204, mean_q: -13.288179, mean_eps: 0.662319\n",
            "  53547/200000: episode: 158, duration: 2.433s, episode steps: 124, steps per second:  51, episode reward: -123.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.976 [0.000, 2.000],  loss: 0.271952, mae: 9.083977, mean_q: -13.265955, mean_eps: 0.661265\n",
            "  53889/200000: episode: 159, duration: 5.695s, episode steps: 342, steps per second:  60, episode reward: -341.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.915 [0.000, 2.000],  loss: 0.240197, mae: 9.070408, mean_q: -13.247096, mean_eps: 0.659789\n",
            "  54117/200000: episode: 160, duration: 3.442s, episode steps: 228, steps per second:  66, episode reward: -227.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.868 [0.000, 2.000],  loss: 0.231955, mae: 9.082225, mean_q: -13.261276, mean_eps: 0.657984\n",
            "  54409/200000: episode: 161, duration: 5.104s, episode steps: 292, steps per second:  57, episode reward: -291.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.969 [0.000, 2.000],  loss: 0.241504, mae: 9.110743, mean_q: -13.306047, mean_eps: 0.656338\n",
            "  54615/200000: episode: 162, duration: 3.501s, episode steps: 206, steps per second:  59, episode reward: -205.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.913 [0.000, 2.000],  loss: 0.227198, mae: 9.114168, mean_q: -13.314480, mean_eps: 0.654760\n",
            "  54817/200000: episode: 163, duration: 3.166s, episode steps: 202, steps per second:  64, episode reward: -201.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.257441, mae: 9.083239, mean_q: -13.265569, mean_eps: 0.653468\n",
            "  55210/200000: episode: 164, duration: 6.809s, episode steps: 393, steps per second:  58, episode reward: -392.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.959 [0.000, 2.000],  loss: 0.246671, mae: 9.168182, mean_q: -13.385009, mean_eps: 0.651584\n",
            "  55362/200000: episode: 165, duration: 2.640s, episode steps: 152, steps per second:  58, episode reward: -151.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.987 [0.000, 2.000],  loss: 0.244811, mae: 9.218401, mean_q: -13.449899, mean_eps: 0.649858\n",
            "  55587/200000: episode: 166, duration: 3.461s, episode steps: 225, steps per second:  65, episode reward: -224.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.004 [0.000, 2.000],  loss: 0.248480, mae: 9.208592, mean_q: -13.444415, mean_eps: 0.648665\n",
            "  55700/200000: episode: 167, duration: 1.767s, episode steps: 113, steps per second:  64, episode reward: -112.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.850 [0.000, 2.000],  loss: 0.262186, mae: 9.202650, mean_q: -13.438986, mean_eps: 0.647594\n",
            "  55884/200000: episode: 168, duration: 2.698s, episode steps: 184, steps per second:  68, episode reward: -183.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.054 [0.000, 2.000],  loss: 0.273230, mae: 9.201529, mean_q: -13.428615, mean_eps: 0.646654\n",
            "  56057/200000: episode: 169, duration: 3.674s, episode steps: 173, steps per second:  47, episode reward: -172.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.792 [0.000, 2.000],  loss: 0.262334, mae: 9.201741, mean_q: -13.428357, mean_eps: 0.645523\n",
            "  56280/200000: episode: 170, duration: 3.355s, episode steps: 223, steps per second:  66, episode reward: -222.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.928 [0.000, 2.000],  loss: 0.281830, mae: 9.221548, mean_q: -13.447793, mean_eps: 0.644269\n",
            "  56471/200000: episode: 171, duration: 2.884s, episode steps: 191, steps per second:  66, episode reward: -190.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.026 [0.000, 2.000],  loss: 0.228721, mae: 9.233949, mean_q: -13.470821, mean_eps: 0.642958\n",
            "  56615/200000: episode: 172, duration: 2.230s, episode steps: 144, steps per second:  65, episode reward: -143.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.028 [0.000, 2.000],  loss: 0.259360, mae: 9.225364, mean_q: -13.452852, mean_eps: 0.641898\n",
            "  56782/200000: episode: 173, duration: 2.943s, episode steps: 167, steps per second:  57, episode reward: -166.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.940 [0.000, 2.000],  loss: 0.275562, mae: 9.198512, mean_q: -13.411926, mean_eps: 0.640913\n",
            "  57097/200000: episode: 174, duration: 5.500s, episode steps: 315, steps per second:  57, episode reward: -314.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.987 [0.000, 2.000],  loss: 0.277331, mae: 9.234438, mean_q: -13.469966, mean_eps: 0.639386\n",
            "  57236/200000: episode: 175, duration: 2.227s, episode steps: 139, steps per second:  62, episode reward: -138.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.993 [0.000, 2.000],  loss: 0.256856, mae: 9.247331, mean_q: -13.501445, mean_eps: 0.637949\n",
            "  57381/200000: episode: 176, duration: 2.269s, episode steps: 145, steps per second:  64, episode reward: -144.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.945 [0.000, 2.000],  loss: 0.252887, mae: 9.278759, mean_q: -13.543951, mean_eps: 0.637049\n",
            "  57596/200000: episode: 177, duration: 3.776s, episode steps: 215, steps per second:  57, episode reward: -214.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.791 [0.000, 2.000],  loss: 0.251544, mae: 9.287973, mean_q: -13.551161, mean_eps: 0.635909\n",
            "  57823/200000: episode: 178, duration: 4.000s, episode steps: 227, steps per second:  57, episode reward: -226.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.833 [0.000, 2.000],  loss: 0.263970, mae: 9.280898, mean_q: -13.538180, mean_eps: 0.634510\n",
            "  58004/200000: episode: 179, duration: 2.735s, episode steps: 181, steps per second:  66, episode reward: -180.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.840 [0.000, 2.000],  loss: 0.281216, mae: 9.279314, mean_q: -13.528149, mean_eps: 0.633218\n",
            "  58254/200000: episode: 180, duration: 3.828s, episode steps: 250, steps per second:  65, episode reward: -249.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.008 [0.000, 2.000],  loss: 0.265009, mae: 9.345691, mean_q: -13.639259, mean_eps: 0.631853\n",
            "  58481/200000: episode: 181, duration: 4.458s, episode steps: 227, steps per second:  51, episode reward: -226.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.952 [0.000, 2.000],  loss: 0.260040, mae: 9.352352, mean_q: -13.643544, mean_eps: 0.630342\n",
            "  58738/200000: episode: 182, duration: 4.074s, episode steps: 257, steps per second:  63, episode reward: -256.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.949 [0.000, 2.000],  loss: 0.267699, mae: 9.352076, mean_q: -13.634717, mean_eps: 0.628810\n",
            "  58959/200000: episode: 183, duration: 3.312s, episode steps: 221, steps per second:  67, episode reward: -220.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.045 [0.000, 2.000],  loss: 0.241341, mae: 9.340899, mean_q: -13.623950, mean_eps: 0.627296\n",
            "  59137/200000: episode: 184, duration: 2.699s, episode steps: 178, steps per second:  66, episode reward: -177.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.910 [0.000, 2.000],  loss: 0.284267, mae: 9.313053, mean_q: -13.585117, mean_eps: 0.626032\n",
            "  59323/200000: episode: 185, duration: 3.970s, episode steps: 186, steps per second:  47, episode reward: -185.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.930 [0.000, 2.000],  loss: 0.286655, mae: 9.333522, mean_q: -13.610902, mean_eps: 0.624880\n",
            "  59485/200000: episode: 186, duration: 2.543s, episode steps: 162, steps per second:  64, episode reward: -161.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.988 [0.000, 2.000],  loss: 0.262002, mae: 9.319094, mean_q: -13.594206, mean_eps: 0.623778\n",
            "  59726/200000: episode: 187, duration: 3.727s, episode steps: 241, steps per second:  65, episode reward: -240.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.066 [0.000, 2.000],  loss: 0.283110, mae: 9.296361, mean_q: -13.544184, mean_eps: 0.622502\n",
            "  59923/200000: episode: 188, duration: 2.967s, episode steps: 197, steps per second:  66, episode reward: -196.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.091 [0.000, 2.000],  loss: 0.207663, mae: 9.336278, mean_q: -13.623912, mean_eps: 0.621115\n",
            "  60096/200000: episode: 189, duration: 3.705s, episode steps: 173, steps per second:  47, episode reward: -172.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.133 [0.000, 2.000],  loss: 0.286612, mae: 9.320008, mean_q: -13.599080, mean_eps: 0.619943\n",
            "  60269/200000: episode: 190, duration: 2.882s, episode steps: 173, steps per second:  60, episode reward: -172.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.931 [0.000, 2.000],  loss: 0.298561, mae: 9.339369, mean_q: -13.607377, mean_eps: 0.618847\n",
            "  60482/200000: episode: 191, duration: 3.529s, episode steps: 213, steps per second:  60, episode reward: -212.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.070 [0.000, 2.000],  loss: 0.261339, mae: 9.327910, mean_q: -13.592980, mean_eps: 0.617625\n",
            "  60640/200000: episode: 192, duration: 2.439s, episode steps: 158, steps per second:  65, episode reward: -157.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.854 [0.000, 2.000],  loss: 0.262096, mae: 9.317079, mean_q: -13.579751, mean_eps: 0.616450\n",
            "  60790/200000: episode: 193, duration: 2.794s, episode steps: 150, steps per second:  54, episode reward: -149.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.007 [0.000, 2.000],  loss: 0.250430, mae: 9.316440, mean_q: -13.590017, mean_eps: 0.615475\n",
            "  61030/200000: episode: 194, duration: 4.470s, episode steps: 240, steps per second:  54, episode reward: -239.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.929 [0.000, 2.000],  loss: 0.284473, mae: 9.320094, mean_q: -13.592744, mean_eps: 0.614240\n",
            "  61354/200000: episode: 195, duration: 5.020s, episode steps: 324, steps per second:  65, episode reward: -323.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.910 [0.000, 2.000],  loss: 0.260521, mae: 9.312689, mean_q: -13.588096, mean_eps: 0.612454\n",
            "  61530/200000: episode: 196, duration: 2.861s, episode steps: 176, steps per second:  62, episode reward: -175.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.045 [0.000, 2.000],  loss: 0.269726, mae: 9.311169, mean_q: -13.582882, mean_eps: 0.610870\n",
            "  61729/200000: episode: 197, duration: 4.152s, episode steps: 199, steps per second:  48, episode reward: -198.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.085 [0.000, 2.000],  loss: 0.263770, mae: 9.320309, mean_q: -13.598971, mean_eps: 0.609683\n",
            "  61902/200000: episode: 198, duration: 2.810s, episode steps: 173, steps per second:  62, episode reward: -172.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.150 [0.000, 2.000],  loss: 0.269816, mae: 9.317352, mean_q: -13.593534, mean_eps: 0.608505\n",
            "  62184/200000: episode: 199, duration: 4.593s, episode steps: 282, steps per second:  61, episode reward: -281.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.039 [0.000, 2.000],  loss: 0.291399, mae: 9.376701, mean_q: -13.673951, mean_eps: 0.607064\n",
            "  62357/200000: episode: 200, duration: 3.277s, episode steps: 173, steps per second:  53, episode reward: -172.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.994 [0.000, 2.000],  loss: 0.291985, mae: 9.387950, mean_q: -13.697032, mean_eps: 0.605623\n",
            "  62565/200000: episode: 201, duration: 3.863s, episode steps: 208, steps per second:  54, episode reward: -207.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.990 [0.000, 2.000],  loss: 0.284069, mae: 9.387699, mean_q: -13.692575, mean_eps: 0.604417\n",
            "  62761/200000: episode: 202, duration: 3.260s, episode steps: 196, steps per second:  60, episode reward: -195.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.990 [0.000, 2.000],  loss: 0.275570, mae: 9.377951, mean_q: -13.696438, mean_eps: 0.603137\n",
            "  62945/200000: episode: 203, duration: 3.016s, episode steps: 184, steps per second:  61, episode reward: -183.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.120 [0.000, 2.000],  loss: 0.295625, mae: 9.365338, mean_q: -13.673002, mean_eps: 0.601934\n",
            "  63139/200000: episode: 204, duration: 3.691s, episode steps: 194, steps per second:  53, episode reward: -193.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.046 [0.000, 2.000],  loss: 0.279886, mae: 9.427667, mean_q: -13.756714, mean_eps: 0.600737\n",
            "  63313/200000: episode: 205, duration: 3.280s, episode steps: 174, steps per second:  53, episode reward: -173.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.994 [0.000, 2.000],  loss: 0.270155, mae: 9.413943, mean_q: -13.747412, mean_eps: 0.599572\n",
            "  63516/200000: episode: 206, duration: 3.282s, episode steps: 203, steps per second:  62, episode reward: -202.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.089 [0.000, 2.000],  loss: 0.277199, mae: 9.433397, mean_q: -13.777383, mean_eps: 0.598378\n",
            "  63677/200000: episode: 207, duration: 2.593s, episode steps: 161, steps per second:  62, episode reward: -160.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.093 [0.000, 2.000],  loss: 0.243884, mae: 9.430769, mean_q: -13.773027, mean_eps: 0.597225\n",
            "  63878/200000: episode: 208, duration: 3.642s, episode steps: 201, steps per second:  55, episode reward: -200.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.095 [0.000, 2.000],  loss: 0.281130, mae: 9.439183, mean_q: -13.779361, mean_eps: 0.596079\n",
            "  64027/200000: episode: 209, duration: 3.141s, episode steps: 149, steps per second:  47, episode reward: -148.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.034 [0.000, 2.000],  loss: 0.280598, mae: 9.433969, mean_q: -13.775652, mean_eps: 0.594971\n",
            "  64191/200000: episode: 210, duration: 2.787s, episode steps: 164, steps per second:  59, episode reward: -163.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.134 [0.000, 2.000],  loss: 0.241503, mae: 9.399042, mean_q: -13.726075, mean_eps: 0.593979\n",
            "  64352/200000: episode: 211, duration: 2.649s, episode steps: 161, steps per second:  61, episode reward: -160.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.006 [0.000, 2.000],  loss: 0.254114, mae: 9.384080, mean_q: -13.702186, mean_eps: 0.592950\n",
            "  64542/200000: episode: 212, duration: 3.089s, episode steps: 190, steps per second:  62, episode reward: -189.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.874 [0.000, 2.000],  loss: 0.264225, mae: 9.402484, mean_q: -13.731771, mean_eps: 0.591839\n",
            "  64745/200000: episode: 213, duration: 4.315s, episode steps: 203, steps per second:  47, episode reward: -202.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.059 [0.000, 2.000],  loss: 0.281913, mae: 9.399854, mean_q: -13.717207, mean_eps: 0.590594\n",
            "  64913/200000: episode: 214, duration: 2.573s, episode steps: 168, steps per second:  65, episode reward: -167.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.958 [0.000, 2.000],  loss: 0.264532, mae: 9.371207, mean_q: -13.678836, mean_eps: 0.589420\n",
            "  65070/200000: episode: 215, duration: 2.464s, episode steps: 157, steps per second:  64, episode reward: -156.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.898 [0.000, 2.000],  loss: 0.254586, mae: 9.370476, mean_q: -13.676672, mean_eps: 0.588390\n",
            "  65275/200000: episode: 216, duration: 3.231s, episode steps: 205, steps per second:  63, episode reward: -204.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.907 [0.000, 2.000],  loss: 0.275818, mae: 9.387085, mean_q: -13.706624, mean_eps: 0.587244\n",
            "  65455/200000: episode: 217, duration: 3.546s, episode steps: 180, steps per second:  51, episode reward: -179.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.922 [0.000, 2.000],  loss: 0.255720, mae: 9.401511, mean_q: -13.733469, mean_eps: 0.586025\n",
            "  65841/200000: episode: 218, duration: 6.550s, episode steps: 386, steps per second:  59, episode reward: -385.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.073 [0.000, 2.000],  loss: 0.254883, mae: 9.385276, mean_q: -13.699341, mean_eps: 0.584233\n",
            "  66045/200000: episode: 219, duration: 3.361s, episode steps: 204, steps per second:  61, episode reward: -203.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.882 [0.000, 2.000],  loss: 0.277159, mae: 9.381062, mean_q: -13.698232, mean_eps: 0.582364\n",
            "  66315/200000: episode: 220, duration: 5.524s, episode steps: 270, steps per second:  49, episode reward: -269.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.037 [0.000, 2.000],  loss: 0.262640, mae: 9.408665, mean_q: -13.735097, mean_eps: 0.580863\n",
            "  66555/200000: episode: 221, duration: 3.985s, episode steps: 240, steps per second:  60, episode reward: -239.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.808 [0.000, 2.000],  loss: 0.282302, mae: 9.414476, mean_q: -13.750100, mean_eps: 0.579248\n",
            "  66753/200000: episode: 222, duration: 3.226s, episode steps: 198, steps per second:  61, episode reward: -197.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.035 [0.000, 2.000],  loss: 0.282594, mae: 9.441303, mean_q: -13.776041, mean_eps: 0.577861\n",
            "  66943/200000: episode: 223, duration: 3.685s, episode steps: 190, steps per second:  52, episode reward: -189.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.863 [0.000, 2.000],  loss: 0.261592, mae: 9.414582, mean_q: -13.739668, mean_eps: 0.576632\n",
            "  67173/200000: episode: 224, duration: 4.447s, episode steps: 230, steps per second:  52, episode reward: -229.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.991 [0.000, 2.000],  loss: 0.283322, mae: 9.443283, mean_q: -13.787395, mean_eps: 0.575303\n",
            "  67437/200000: episode: 225, duration: 4.465s, episode steps: 264, steps per second:  59, episode reward: -263.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.117 [0.000, 2.000],  loss: 0.254761, mae: 9.461353, mean_q: -13.821627, mean_eps: 0.573738\n",
            "  67640/200000: episode: 226, duration: 3.497s, episode steps: 203, steps per second:  58, episode reward: -202.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.808 [0.000, 2.000],  loss: 0.258226, mae: 9.419226, mean_q: -13.751161, mean_eps: 0.572259\n",
            "  67856/200000: episode: 227, duration: 4.571s, episode steps: 216, steps per second:  47, episode reward: -215.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.056 [0.000, 2.000],  loss: 0.287008, mae: 9.426950, mean_q: -13.762572, mean_eps: 0.570933\n",
            "  68009/200000: episode: 228, duration: 2.547s, episode steps: 153, steps per second:  60, episode reward: -152.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.993 [0.000, 2.000],  loss: 0.269331, mae: 9.434144, mean_q: -13.777077, mean_eps: 0.569764\n",
            "  68179/200000: episode: 229, duration: 2.913s, episode steps: 170, steps per second:  58, episode reward: -169.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.012 [0.000, 2.000],  loss: 0.267997, mae: 9.447867, mean_q: -13.804902, mean_eps: 0.568741\n",
            "  68340/200000: episode: 230, duration: 2.743s, episode steps: 161, steps per second:  59, episode reward: -160.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.276279, mae: 9.424293, mean_q: -13.770881, mean_eps: 0.567693\n",
            "  68517/200000: episode: 231, duration: 4.097s, episode steps: 177, steps per second:  43, episode reward: -176.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.977 [0.000, 2.000],  loss: 0.268787, mae: 9.455518, mean_q: -13.820186, mean_eps: 0.566623\n",
            "  68700/200000: episode: 232, duration: 3.161s, episode steps: 183, steps per second:  58, episode reward: -182.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.880 [0.000, 2.000],  loss: 0.270621, mae: 9.446692, mean_q: -13.802299, mean_eps: 0.565483\n",
            "  68852/200000: episode: 233, duration: 2.641s, episode steps: 152, steps per second:  58, episode reward: -151.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.901 [0.000, 2.000],  loss: 0.287750, mae: 9.448593, mean_q: -13.814375, mean_eps: 0.564422\n",
            "  69036/200000: episode: 234, duration: 3.171s, episode steps: 184, steps per second:  58, episode reward: -183.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.870 [0.000, 2.000],  loss: 0.256737, mae: 9.429600, mean_q: -13.783501, mean_eps: 0.563358\n",
            "  69187/200000: episode: 235, duration: 3.383s, episode steps: 151, steps per second:  45, episode reward: -150.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.987 [0.000, 2.000],  loss: 0.291890, mae: 9.361600, mean_q: -13.689821, mean_eps: 0.562297\n",
            "  69328/200000: episode: 236, duration: 2.620s, episode steps: 141, steps per second:  54, episode reward: -140.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.021 [0.000, 2.000],  loss: 0.317991, mae: 9.354178, mean_q: -13.661372, mean_eps: 0.561372\n",
            "  69515/200000: episode: 237, duration: 3.153s, episode steps: 187, steps per second:  59, episode reward: -186.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.845 [0.000, 2.000],  loss: 0.261574, mae: 9.353801, mean_q: -13.679749, mean_eps: 0.560334\n",
            "  69725/200000: episode: 238, duration: 3.514s, episode steps: 210, steps per second:  60, episode reward: -209.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.076 [0.000, 2.000],  loss: 0.289387, mae: 9.367051, mean_q: -13.691161, mean_eps: 0.559076\n",
            "  69900/200000: episode: 239, duration: 3.487s, episode steps: 175, steps per second:  50, episode reward: -174.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.874 [0.000, 2.000],  loss: 0.267390, mae: 9.349126, mean_q: -13.666971, mean_eps: 0.557857\n",
            "  70126/200000: episode: 240, duration: 4.202s, episode steps: 226, steps per second:  54, episode reward: -225.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.973 [0.000, 2.000],  loss: 0.252975, mae: 9.347120, mean_q: -13.664353, mean_eps: 0.556588\n",
            "  70289/200000: episode: 241, duration: 2.744s, episode steps: 163, steps per second:  59, episode reward: -162.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.043 [0.000, 2.000],  loss: 0.263062, mae: 9.346530, mean_q: -13.656383, mean_eps: 0.555356\n",
            "  70445/200000: episode: 242, duration: 2.620s, episode steps: 156, steps per second:  60, episode reward: -155.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.160 [0.000, 2.000],  loss: 0.248579, mae: 9.347111, mean_q: -13.660733, mean_eps: 0.554346\n",
            "  70661/200000: episode: 243, duration: 4.466s, episode steps: 216, steps per second:  48, episode reward: -215.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.852 [0.000, 2.000],  loss: 0.285154, mae: 9.335160, mean_q: -13.645518, mean_eps: 0.553167\n",
            "  70969/200000: episode: 244, duration: 5.840s, episode steps: 308, steps per second:  53, episode reward: -307.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.692 [0.000, 2.000],  loss: 0.255809, mae: 9.323050, mean_q: -13.628991, mean_eps: 0.551508\n",
            "  71144/200000: episode: 245, duration: 3.120s, episode steps: 175, steps per second:  56, episode reward: -174.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.272303, mae: 9.411899, mean_q: -13.760446, mean_eps: 0.549979\n",
            "  71327/200000: episode: 246, duration: 3.826s, episode steps: 183, steps per second:  48, episode reward: -182.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.011 [0.000, 2.000],  loss: 0.275432, mae: 9.400908, mean_q: -13.742555, mean_eps: 0.548845\n",
            "  71545/200000: episode: 247, duration: 4.168s, episode steps: 218, steps per second:  52, episode reward: -217.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.244606, mae: 9.407023, mean_q: -13.758762, mean_eps: 0.547575\n",
            "  71698/200000: episode: 248, duration: 2.566s, episode steps: 153, steps per second:  60, episode reward: -152.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.941 [0.000, 2.000],  loss: 0.258729, mae: 9.416199, mean_q: -13.778145, mean_eps: 0.546400\n",
            "  71917/200000: episode: 249, duration: 3.690s, episode steps: 219, steps per second:  59, episode reward: -218.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.114 [0.000, 2.000],  loss: 0.275261, mae: 9.397917, mean_q: -13.734473, mean_eps: 0.545222\n",
            "  72093/200000: episode: 250, duration: 3.660s, episode steps: 176, steps per second:  48, episode reward: -175.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.291868, mae: 9.424582, mean_q: -13.777214, mean_eps: 0.543971\n",
            "  72247/200000: episode: 251, duration: 2.854s, episode steps: 154, steps per second:  54, episode reward: -153.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.032 [0.000, 2.000],  loss: 0.265716, mae: 9.474869, mean_q: -13.852644, mean_eps: 0.542927\n",
            "  72407/200000: episode: 252, duration: 2.616s, episode steps: 160, steps per second:  61, episode reward: -159.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.906 [0.000, 2.000],  loss: 0.287254, mae: 9.461244, mean_q: -13.828826, mean_eps: 0.541932\n",
            "  72608/200000: episode: 253, duration: 3.433s, episode steps: 201, steps per second:  59, episode reward: -200.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.955 [0.000, 2.000],  loss: 0.286523, mae: 9.475616, mean_q: -13.845086, mean_eps: 0.540789\n",
            "  72733/200000: episode: 254, duration: 2.230s, episode steps: 125, steps per second:  56, episode reward: -124.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.032 [0.000, 2.000],  loss: 0.262187, mae: 9.475147, mean_q: -13.839899, mean_eps: 0.539757\n",
            "  72953/200000: episode: 255, duration: 4.787s, episode steps: 220, steps per second:  46, episode reward: -219.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.882 [0.000, 2.000],  loss: 0.276453, mae: 9.490282, mean_q: -13.870945, mean_eps: 0.538664\n",
            "  73107/200000: episode: 256, duration: 2.646s, episode steps: 154, steps per second:  58, episode reward: -153.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.293780, mae: 9.461681, mean_q: -13.823971, mean_eps: 0.537480\n",
            "  73386/200000: episode: 257, duration: 4.669s, episode steps: 279, steps per second:  60, episode reward: -278.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.939 [0.000, 2.000],  loss: 0.273376, mae: 9.456733, mean_q: -13.819403, mean_eps: 0.536109\n",
            "  73578/200000: episode: 258, duration: 4.258s, episode steps: 192, steps per second:  45, episode reward: -191.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.896 [0.000, 2.000],  loss: 0.270096, mae: 9.458536, mean_q: -13.823440, mean_eps: 0.534617\n",
            "  73711/200000: episode: 259, duration: 2.433s, episode steps: 133, steps per second:  55, episode reward: -132.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.970 [0.000, 2.000],  loss: 0.274621, mae: 9.451879, mean_q: -13.817660, mean_eps: 0.533588\n",
            "  73932/200000: episode: 260, duration: 3.895s, episode steps: 221, steps per second:  57, episode reward: -220.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.072 [0.000, 2.000],  loss: 0.259501, mae: 9.456993, mean_q: -13.823626, mean_eps: 0.532467\n",
            "  74201/200000: episode: 261, duration: 4.732s, episode steps: 269, steps per second:  57, episode reward: -268.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.056 [0.000, 2.000],  loss: 0.293351, mae: 9.467945, mean_q: -13.833281, mean_eps: 0.530915\n",
            "  74533/200000: episode: 262, duration: 6.535s, episode steps: 332, steps per second:  51, episode reward: -331.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.003 [0.000, 2.000],  loss: 0.308718, mae: 9.493640, mean_q: -13.879347, mean_eps: 0.529012\n",
            "  74684/200000: episode: 263, duration: 2.684s, episode steps: 151, steps per second:  56, episode reward: -150.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.033 [0.000, 2.000],  loss: 0.255818, mae: 9.461987, mean_q: -13.839056, mean_eps: 0.527483\n",
            "  74896/200000: episode: 264, duration: 3.736s, episode steps: 212, steps per second:  57, episode reward: -211.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.024 [0.000, 2.000],  loss: 0.293114, mae: 9.493466, mean_q: -13.877620, mean_eps: 0.526333\n",
            "  75258/200000: episode: 265, duration: 7.345s, episode steps: 362, steps per second:  49, episode reward: -361.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.064 [0.000, 2.000],  loss: 0.289016, mae: 9.448635, mean_q: -13.818518, mean_eps: 0.524516\n",
            "  75421/200000: episode: 266, duration: 2.901s, episode steps: 163, steps per second:  56, episode reward: -162.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.080 [0.000, 2.000],  loss: 0.303525, mae: 9.439779, mean_q: -13.796839, mean_eps: 0.522853\n",
            "  75582/200000: episode: 267, duration: 2.817s, episode steps: 161, steps per second:  57, episode reward: -160.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.963 [0.000, 2.000],  loss: 0.293176, mae: 9.432130, mean_q: -13.792586, mean_eps: 0.521827\n",
            "  75799/200000: episode: 268, duration: 4.868s, episode steps: 217, steps per second:  45, episode reward: -216.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.032 [0.000, 2.000],  loss: 0.293416, mae: 9.426482, mean_q: -13.788058, mean_eps: 0.520630\n",
            "  76029/200000: episode: 269, duration: 4.065s, episode steps: 230, steps per second:  57, episode reward: -229.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.978 [0.000, 2.000],  loss: 0.291882, mae: 9.430856, mean_q: -13.786165, mean_eps: 0.519215\n",
            "  76247/200000: episode: 270, duration: 3.920s, episode steps: 218, steps per second:  56, episode reward: -217.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.284252, mae: 9.487184, mean_q: -13.876548, mean_eps: 0.517796\n",
            "  76427/200000: episode: 271, duration: 4.267s, episode steps: 180, steps per second:  42, episode reward: -179.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.800 [0.000, 2.000],  loss: 0.254315, mae: 9.468630, mean_q: -13.857770, mean_eps: 0.516535\n",
            "  76605/200000: episode: 272, duration: 3.200s, episode steps: 178, steps per second:  56, episode reward: -177.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.837 [0.000, 2.000],  loss: 0.280900, mae: 9.472442, mean_q: -13.846967, mean_eps: 0.515402\n",
            "  76838/200000: episode: 273, duration: 3.988s, episode steps: 233, steps per second:  58, episode reward: -232.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.837 [0.000, 2.000],  loss: 0.267489, mae: 9.470572, mean_q: -13.852027, mean_eps: 0.514100\n",
            "  77083/200000: episode: 274, duration: 4.913s, episode steps: 245, steps per second:  50, episode reward: -244.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.922 [0.000, 2.000],  loss: 0.326628, mae: 9.474391, mean_q: -13.849604, mean_eps: 0.512587\n",
            "  77237/200000: episode: 275, duration: 3.207s, episode steps: 154, steps per second:  48, episode reward: -153.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.877 [0.000, 2.000],  loss: 0.248506, mae: 9.508443, mean_q: -13.913242, mean_eps: 0.511323\n",
            "  77391/200000: episode: 276, duration: 2.825s, episode steps: 154, steps per second:  55, episode reward: -153.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.019 [0.000, 2.000],  loss: 0.277870, mae: 9.498314, mean_q: -13.886982, mean_eps: 0.510348\n",
            "  77547/200000: episode: 277, duration: 2.752s, episode steps: 156, steps per second:  57, episode reward: -155.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.045 [0.000, 2.000],  loss: 0.290871, mae: 9.545593, mean_q: -13.953053, mean_eps: 0.509366\n",
            "  77749/200000: episode: 278, duration: 4.130s, episode steps: 202, steps per second:  49, episode reward: -201.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.931 [0.000, 2.000],  loss: 0.269876, mae: 9.519733, mean_q: -13.929184, mean_eps: 0.508233\n",
            "  77903/200000: episode: 279, duration: 3.207s, episode steps: 154, steps per second:  48, episode reward: -153.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.006 [0.000, 2.000],  loss: 0.273346, mae: 9.513361, mean_q: -13.915899, mean_eps: 0.507105\n",
            "  78141/200000: episode: 280, duration: 4.126s, episode steps: 238, steps per second:  58, episode reward: -237.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.021 [0.000, 2.000],  loss: 0.290026, mae: 9.488632, mean_q: -13.876873, mean_eps: 0.505864\n",
            "  78339/200000: episode: 281, duration: 3.517s, episode steps: 198, steps per second:  56, episode reward: -197.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.904 [0.000, 2.000],  loss: 0.294039, mae: 9.493388, mean_q: -13.883195, mean_eps: 0.504483\n",
            "  78477/200000: episode: 282, duration: 3.018s, episode steps: 138, steps per second:  46, episode reward: -137.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.993 [0.000, 2.000],  loss: 0.275266, mae: 9.484994, mean_q: -13.866486, mean_eps: 0.503419\n",
            "  78618/200000: episode: 283, duration: 2.916s, episode steps: 141, steps per second:  48, episode reward: -140.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.957 [0.000, 2.000],  loss: 0.286250, mae: 9.489965, mean_q: -13.871340, mean_eps: 0.502536\n",
            "  78768/200000: episode: 284, duration: 2.954s, episode steps: 150, steps per second:  51, episode reward: -149.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.020 [0.000, 2.000],  loss: 0.311121, mae: 9.511986, mean_q: -13.901006, mean_eps: 0.501614\n",
            "  78922/200000: episode: 285, duration: 2.914s, episode steps: 154, steps per second:  53, episode reward: -153.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.058 [0.000, 2.000],  loss: 0.293004, mae: 9.498299, mean_q: -13.889972, mean_eps: 0.500652\n",
            "  79067/200000: episode: 286, duration: 3.058s, episode steps: 145, steps per second:  47, episode reward: -144.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.090 [0.000, 2.000],  loss: 0.311580, mae: 9.461181, mean_q: -13.833754, mean_eps: 0.499705\n",
            "  79268/200000: episode: 287, duration: 4.523s, episode steps: 201, steps per second:  44, episode reward: -200.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.020 [0.000, 2.000],  loss: 0.261747, mae: 9.407149, mean_q: -13.760155, mean_eps: 0.498609\n",
            "  79411/200000: episode: 288, duration: 2.541s, episode steps: 143, steps per second:  56, episode reward: -142.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.119 [0.000, 2.000],  loss: 0.317387, mae: 9.396946, mean_q: -13.736755, mean_eps: 0.497520\n",
            "  79586/200000: episode: 289, duration: 3.140s, episode steps: 175, steps per second:  56, episode reward: -174.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.011 [0.000, 2.000],  loss: 0.293244, mae: 9.407258, mean_q: -13.758619, mean_eps: 0.496513\n",
            "  79763/200000: episode: 290, duration: 3.480s, episode steps: 177, steps per second:  51, episode reward: -176.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.966 [0.000, 2.000],  loss: 0.276558, mae: 9.411421, mean_q: -13.770009, mean_eps: 0.495398\n",
            "  79938/200000: episode: 291, duration: 4.009s, episode steps: 175, steps per second:  44, episode reward: -174.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.777 [0.000, 2.000],  loss: 0.276541, mae: 9.405510, mean_q: -13.762712, mean_eps: 0.494283\n",
            "  80104/200000: episode: 292, duration: 3.073s, episode steps: 166, steps per second:  54, episode reward: -165.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.976 [0.000, 2.000],  loss: 0.300079, mae: 9.390112, mean_q: -13.731116, mean_eps: 0.493204\n",
            "  80280/200000: episode: 293, duration: 3.293s, episode steps: 176, steps per second:  53, episode reward: -175.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.898 [0.000, 2.000],  loss: 0.265197, mae: 9.392736, mean_q: -13.739890, mean_eps: 0.492121\n",
            "  80468/200000: episode: 294, duration: 4.111s, episode steps: 188, steps per second:  46, episode reward: -187.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.968 [0.000, 2.000],  loss: 0.281729, mae: 9.375061, mean_q: -13.711679, mean_eps: 0.490968\n",
            "  80678/200000: episode: 295, duration: 4.221s, episode steps: 210, steps per second:  50, episode reward: -209.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.890 [0.000, 2.000],  loss: 0.271241, mae: 9.392485, mean_q: -13.743087, mean_eps: 0.489707\n",
            "  80787/200000: episode: 296, duration: 1.959s, episode steps: 109, steps per second:  56, episode reward: -108.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.028 [0.000, 2.000],  loss: 0.296182, mae: 9.384147, mean_q: -13.725539, mean_eps: 0.488697\n",
            "  80941/200000: episode: 297, duration: 2.741s, episode steps: 154, steps per second:  56, episode reward: -153.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.929 [0.000, 2.000],  loss: 0.280255, mae: 9.369479, mean_q: -13.708608, mean_eps: 0.487865\n",
            "  81099/200000: episode: 298, duration: 2.819s, episode steps: 158, steps per second:  56, episode reward: -157.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.911 [0.000, 2.000],  loss: 0.306909, mae: 9.365091, mean_q: -13.696531, mean_eps: 0.486877\n",
            "  81323/200000: episode: 299, duration: 4.970s, episode steps: 224, steps per second:  45, episode reward: -223.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.036 [0.000, 2.000],  loss: 0.280728, mae: 9.341186, mean_q: -13.666111, mean_eps: 0.485667\n",
            "  81506/200000: episode: 300, duration: 3.295s, episode steps: 183, steps per second:  56, episode reward: -182.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.033 [0.000, 2.000],  loss: 0.267232, mae: 9.329486, mean_q: -13.656114, mean_eps: 0.484378\n",
            "  81676/200000: episode: 301, duration: 3.007s, episode steps: 170, steps per second:  57, episode reward: -169.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.024 [0.000, 2.000],  loss: 0.278514, mae: 9.339352, mean_q: -13.657833, mean_eps: 0.483260\n",
            "  81835/200000: episode: 302, duration: 3.203s, episode steps: 159, steps per second:  50, episode reward: -158.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.075 [0.000, 2.000],  loss: 0.282009, mae: 9.346909, mean_q: -13.661649, mean_eps: 0.482218\n",
            "  82019/200000: episode: 303, duration: 4.053s, episode steps: 184, steps per second:  45, episode reward: -183.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.973 [0.000, 2.000],  loss: 0.283444, mae: 9.338571, mean_q: -13.661413, mean_eps: 0.481132\n",
            "  82194/200000: episode: 304, duration: 3.177s, episode steps: 175, steps per second:  55, episode reward: -174.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.943 [0.000, 2.000],  loss: 0.283073, mae: 9.309724, mean_q: -13.621552, mean_eps: 0.479995\n",
            "  82342/200000: episode: 305, duration: 2.697s, episode steps: 148, steps per second:  55, episode reward: -147.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.162 [0.000, 2.000],  loss: 0.279787, mae: 9.301511, mean_q: -13.601282, mean_eps: 0.478972\n",
            "  82508/200000: episode: 306, duration: 3.164s, episode steps: 166, steps per second:  52, episode reward: -165.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.078 [0.000, 2.000],  loss: 0.278509, mae: 9.305241, mean_q: -13.613634, mean_eps: 0.477978\n",
            "  82679/200000: episode: 307, duration: 4.028s, episode steps: 171, steps per second:  42, episode reward: -170.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.865 [0.000, 2.000],  loss: 0.247773, mae: 9.267797, mean_q: -13.568228, mean_eps: 0.476911\n",
            "  82882/200000: episode: 308, duration: 3.734s, episode steps: 203, steps per second:  54, episode reward: -202.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.059 [0.000, 2.000],  loss: 0.262640, mae: 9.321249, mean_q: -13.640695, mean_eps: 0.475727\n",
            "  83051/200000: episode: 309, duration: 3.130s, episode steps: 169, steps per second:  54, episode reward: -168.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.213 [0.000, 2.000],  loss: 0.297861, mae: 9.296468, mean_q: -13.594888, mean_eps: 0.474549\n",
            "  83211/200000: episode: 310, duration: 3.329s, episode steps: 160, steps per second:  48, episode reward: -159.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.031 [0.000, 2.000],  loss: 0.258332, mae: 9.309465, mean_q: -13.629761, mean_eps: 0.473507\n",
            "  83377/200000: episode: 311, duration: 3.583s, episode steps: 166, steps per second:  46, episode reward: -165.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.006 [0.000, 2.000],  loss: 0.296884, mae: 9.310417, mean_q: -13.620378, mean_eps: 0.472475\n",
            "  83568/200000: episode: 312, duration: 3.520s, episode steps: 191, steps per second:  54, episode reward: -190.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.120 [0.000, 2.000],  loss: 0.269319, mae: 9.332334, mean_q: -13.661043, mean_eps: 0.471344\n",
            "  83753/200000: episode: 313, duration: 3.370s, episode steps: 185, steps per second:  55, episode reward: -184.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.038 [0.000, 2.000],  loss: 0.300033, mae: 9.297682, mean_q: -13.595744, mean_eps: 0.470153\n",
            "  84000/200000: episode: 314, duration: 5.597s, episode steps: 247, steps per second:  44, episode reward: -246.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.053 [0.000, 2.000],  loss: 0.280447, mae: 9.302931, mean_q: -13.610989, mean_eps: 0.468785\n",
            "  84209/200000: episode: 315, duration: 3.697s, episode steps: 209, steps per second:  57, episode reward: -208.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.856 [0.000, 2.000],  loss: 0.271767, mae: 9.277022, mean_q: -13.577113, mean_eps: 0.467341\n",
            "  84445/200000: episode: 316, duration: 4.258s, episode steps: 236, steps per second:  55, episode reward: -235.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.970 [0.000, 2.000],  loss: 0.254997, mae: 9.279137, mean_q: -13.584624, mean_eps: 0.465932\n",
            "  84651/200000: episode: 317, duration: 4.765s, episode steps: 206, steps per second:  43, episode reward: -205.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.927 [0.000, 2.000],  loss: 0.286176, mae: 9.287339, mean_q: -13.586443, mean_eps: 0.464532\n",
            "  84819/200000: episode: 318, duration: 3.007s, episode steps: 168, steps per second:  56, episode reward: -167.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.095 [0.000, 2.000],  loss: 0.270004, mae: 9.269686, mean_q: -13.562882, mean_eps: 0.463348\n",
            "  84975/200000: episode: 319, duration: 2.923s, episode steps: 156, steps per second:  53, episode reward: -155.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.103 [0.000, 2.000],  loss: 0.277381, mae: 9.278452, mean_q: -13.577100, mean_eps: 0.462322\n",
            "  85126/200000: episode: 320, duration: 2.948s, episode steps: 151, steps per second:  51, episode reward: -150.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.821 [0.000, 2.000],  loss: 0.268192, mae: 9.230086, mean_q: -13.499818, mean_eps: 0.461350\n",
            "  85348/200000: episode: 321, duration: 5.354s, episode steps: 222, steps per second:  41, episode reward: -221.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.946 [0.000, 2.000],  loss: 0.293807, mae: 9.231422, mean_q: -13.500658, mean_eps: 0.460169\n",
            "  85495/200000: episode: 322, duration: 2.701s, episode steps: 147, steps per second:  54, episode reward: -146.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.980 [0.000, 2.000],  loss: 0.275775, mae: 9.200621, mean_q: -13.459670, mean_eps: 0.459000\n",
            "  85805/200000: episode: 323, duration: 5.830s, episode steps: 310, steps per second:  53, episode reward: -309.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.923 [0.000, 2.000],  loss: 0.258309, mae: 9.228804, mean_q: -13.500706, mean_eps: 0.457553\n",
            "  85934/200000: episode: 324, duration: 3.308s, episode steps: 129, steps per second:  39, episode reward: -128.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.031 [0.000, 2.000],  loss: 0.268702, mae: 9.218127, mean_q: -13.480222, mean_eps: 0.456163\n",
            "  86113/200000: episode: 325, duration: 3.762s, episode steps: 179, steps per second:  48, episode reward: -178.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.039 [0.000, 2.000],  loss: 0.260926, mae: 9.163010, mean_q: -13.401131, mean_eps: 0.455188\n",
            "  86297/200000: episode: 326, duration: 3.546s, episode steps: 184, steps per second:  52, episode reward: -183.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.967 [0.000, 2.000],  loss: 0.278838, mae: 9.116143, mean_q: -13.333071, mean_eps: 0.454038\n",
            "  86502/200000: episode: 327, duration: 4.250s, episode steps: 205, steps per second:  48, episode reward: -204.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.015 [0.000, 2.000],  loss: 0.282026, mae: 9.116708, mean_q: -13.332871, mean_eps: 0.452806\n",
            "  86721/200000: episode: 328, duration: 5.050s, episode steps: 219, steps per second:  43, episode reward: -218.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.945 [0.000, 2.000],  loss: 0.268163, mae: 9.129484, mean_q: -13.356310, mean_eps: 0.451464\n",
            "  86924/200000: episode: 329, duration: 3.914s, episode steps: 203, steps per second:  52, episode reward: -202.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.961 [0.000, 2.000],  loss: 0.294402, mae: 9.111947, mean_q: -13.324294, mean_eps: 0.450127\n",
            "  87098/200000: episode: 330, duration: 3.254s, episode steps: 174, steps per second:  53, episode reward: -173.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.833 [0.000, 2.000],  loss: 0.265507, mae: 9.089544, mean_q: -13.302363, mean_eps: 0.448933\n",
            "  87294/200000: episode: 331, duration: 4.834s, episode steps: 196, steps per second:  41, episode reward: -195.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.995 [0.000, 2.000],  loss: 0.268821, mae: 9.070806, mean_q: -13.279016, mean_eps: 0.447762\n",
            "  87510/200000: episode: 332, duration: 4.111s, episode steps: 216, steps per second:  53, episode reward: -215.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.981 [0.000, 2.000],  loss: 0.254150, mae: 9.063472, mean_q: -13.268597, mean_eps: 0.446457\n",
            "  87722/200000: episode: 333, duration: 4.090s, episode steps: 212, steps per second:  52, episode reward: -211.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.972 [0.000, 2.000],  loss: 0.250069, mae: 9.071500, mean_q: -13.284116, mean_eps: 0.445102\n",
            "  87945/200000: episode: 334, duration: 5.362s, episode steps: 223, steps per second:  42, episode reward: -222.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.031 [0.000, 2.000],  loss: 0.266465, mae: 9.056044, mean_q: -13.261177, mean_eps: 0.443724\n",
            "  88087/200000: episode: 335, duration: 2.692s, episode steps: 142, steps per second:  53, episode reward: -141.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.944 [0.000, 2.000],  loss: 0.272772, mae: 8.990583, mean_q: -13.148839, mean_eps: 0.442569\n",
            "  88320/200000: episode: 336, duration: 4.398s, episode steps: 233, steps per second:  53, episode reward: -232.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.828 [0.000, 2.000],  loss: 0.253794, mae: 8.944455, mean_q: -13.084958, mean_eps: 0.441381\n",
            "  88627/200000: episode: 337, duration: 6.757s, episode steps: 307, steps per second:  45, episode reward: -306.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.860 [0.000, 2.000],  loss: 0.255048, mae: 8.952719, mean_q: -13.102115, mean_eps: 0.439671\n",
            "  88814/200000: episode: 338, duration: 3.450s, episode steps: 187, steps per second:  54, episode reward: -186.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.005 [0.000, 2.000],  loss: 0.285360, mae: 8.938145, mean_q: -13.077276, mean_eps: 0.438107\n",
            "  89067/200000: episode: 339, duration: 4.751s, episode steps: 253, steps per second:  53, episode reward: -252.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.897 [0.000, 2.000],  loss: 0.256797, mae: 8.943360, mean_q: -13.092028, mean_eps: 0.436713\n",
            "  89208/200000: episode: 340, duration: 3.676s, episode steps: 141, steps per second:  38, episode reward: -140.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.993 [0.000, 2.000],  loss: 0.236848, mae: 8.941584, mean_q: -13.092794, mean_eps: 0.435466\n",
            "  89408/200000: episode: 341, duration: 3.950s, episode steps: 200, steps per second:  51, episode reward: -199.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.955 [0.000, 2.000],  loss: 0.253315, mae: 8.939192, mean_q: -13.088610, mean_eps: 0.434386\n",
            "  89550/200000: episode: 342, duration: 2.805s, episode steps: 142, steps per second:  51, episode reward: -141.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.993 [0.000, 2.000],  loss: 0.226119, mae: 8.944611, mean_q: -13.102216, mean_eps: 0.433303\n",
            "  89667/200000: episode: 343, duration: 2.297s, episode steps: 117, steps per second:  51, episode reward: -116.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.932 [0.000, 2.000],  loss: 0.241838, mae: 8.936629, mean_q: -13.085213, mean_eps: 0.432483\n",
            "  89807/200000: episode: 344, duration: 3.383s, episode steps: 140, steps per second:  41, episode reward: -139.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.807 [0.000, 2.000],  loss: 0.241592, mae: 8.930402, mean_q: -13.079088, mean_eps: 0.431669\n",
            "  90064/200000: episode: 345, duration: 5.231s, episode steps: 257, steps per second:  49, episode reward: -256.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.008 [0.000, 2.000],  loss: 0.273918, mae: 8.906413, mean_q: -13.040829, mean_eps: 0.430412\n",
            "  90238/200000: episode: 346, duration: 3.205s, episode steps: 174, steps per second:  54, episode reward: -173.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.052 [0.000, 2.000],  loss: 0.257192, mae: 8.887332, mean_q: -13.004096, mean_eps: 0.429047\n",
            "  90422/200000: episode: 347, duration: 3.756s, episode steps: 184, steps per second:  49, episode reward: -183.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.016 [0.000, 2.000],  loss: 0.266678, mae: 8.885699, mean_q: -13.005137, mean_eps: 0.427913\n",
            "  90585/200000: episode: 348, duration: 3.667s, episode steps: 163, steps per second:  44, episode reward: -162.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.141 [0.000, 2.000],  loss: 0.267557, mae: 8.879445, mean_q: -12.995721, mean_eps: 0.426814\n",
            "  90725/200000: episode: 349, duration: 2.498s, episode steps: 140, steps per second:  56, episode reward: -139.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.043 [0.000, 2.000],  loss: 0.267173, mae: 8.885198, mean_q: -13.002985, mean_eps: 0.425855\n",
            "  90976/200000: episode: 350, duration: 4.707s, episode steps: 251, steps per second:  53, episode reward: -250.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.888 [0.000, 2.000],  loss: 0.287631, mae: 8.875566, mean_q: -12.989790, mean_eps: 0.424617\n",
            "  91122/200000: episode: 351, duration: 3.299s, episode steps: 146, steps per second:  44, episode reward: -145.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.884 [0.000, 2.000],  loss: 0.257228, mae: 8.853875, mean_q: -12.962569, mean_eps: 0.423359\n",
            "  91333/200000: episode: 352, duration: 4.441s, episode steps: 211, steps per second:  48, episode reward: -210.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.095 [0.000, 2.000],  loss: 0.254784, mae: 8.864687, mean_q: -12.971639, mean_eps: 0.422229\n",
            "  91484/200000: episode: 353, duration: 2.936s, episode steps: 151, steps per second:  51, episode reward: -150.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.874 [0.000, 2.000],  loss: 0.233980, mae: 8.846747, mean_q: -12.950985, mean_eps: 0.421083\n",
            "  91690/200000: episode: 354, duration: 3.924s, episode steps: 206, steps per second:  52, episode reward: -205.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.966 [0.000, 2.000],  loss: 0.244642, mae: 8.855066, mean_q: -12.967056, mean_eps: 0.419952\n",
            "  91813/200000: episode: 355, duration: 3.139s, episode steps: 123, steps per second:  39, episode reward: -122.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.033 [0.000, 2.000],  loss: 0.257891, mae: 8.844142, mean_q: -12.949451, mean_eps: 0.418910\n",
            "  91977/200000: episode: 356, duration: 3.260s, episode steps: 164, steps per second:  50, episode reward: -163.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.927 [0.000, 2.000],  loss: 0.227466, mae: 8.855880, mean_q: -12.974686, mean_eps: 0.418001\n",
            "  92129/200000: episode: 357, duration: 2.910s, episode steps: 152, steps per second:  52, episode reward: -151.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.993 [0.000, 2.000],  loss: 0.259155, mae: 8.838346, mean_q: -12.946390, mean_eps: 0.417001\n",
            "  92273/200000: episode: 358, duration: 2.672s, episode steps: 144, steps per second:  54, episode reward: -143.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.889 [0.000, 2.000],  loss: 0.227188, mae: 8.834723, mean_q: -12.938362, mean_eps: 0.416063\n",
            "  92465/200000: episode: 359, duration: 4.397s, episode steps: 192, steps per second:  44, episode reward: -191.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.146 [0.000, 2.000],  loss: 0.223479, mae: 8.836310, mean_q: -12.950853, mean_eps: 0.414999\n",
            "  92683/200000: episode: 360, duration: 4.252s, episode steps: 218, steps per second:  51, episode reward: -217.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.917 [0.000, 2.000],  loss: 0.235896, mae: 8.827538, mean_q: -12.931949, mean_eps: 0.413701\n",
            "  92871/200000: episode: 361, duration: 3.458s, episode steps: 188, steps per second:  54, episode reward: -187.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.973 [0.000, 2.000],  loss: 0.256775, mae: 8.814551, mean_q: -12.908388, mean_eps: 0.412415\n",
            "  93047/200000: episode: 362, duration: 3.535s, episode steps: 176, steps per second:  50, episode reward: -175.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.176 [0.000, 2.000],  loss: 0.242900, mae: 8.830216, mean_q: -12.932433, mean_eps: 0.411263\n",
            "  93251/200000: episode: 363, duration: 4.682s, episode steps: 204, steps per second:  44, episode reward: -203.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.779 [0.000, 2.000],  loss: 0.265055, mae: 8.790072, mean_q: -12.875085, mean_eps: 0.410060\n",
            "  93437/200000: episode: 364, duration: 3.470s, episode steps: 186, steps per second:  54, episode reward: -185.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.968 [0.000, 2.000],  loss: 0.275126, mae: 8.801424, mean_q: -12.890984, mean_eps: 0.408825\n",
            "  93641/200000: episode: 365, duration: 3.954s, episode steps: 204, steps per second:  52, episode reward: -203.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.980 [0.000, 2.000],  loss: 0.254995, mae: 8.796549, mean_q: -12.885973, mean_eps: 0.407589\n",
            "  93872/200000: episode: 366, duration: 5.388s, episode steps: 231, steps per second:  43, episode reward: -230.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.004 [0.000, 2.000],  loss: 0.246220, mae: 8.809705, mean_q: -12.911629, mean_eps: 0.406212\n",
            "  94020/200000: episode: 367, duration: 2.844s, episode steps: 148, steps per second:  52, episode reward: -147.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.986 [0.000, 2.000],  loss: 0.257507, mae: 8.801283, mean_q: -12.887593, mean_eps: 0.405012\n",
            "  94237/200000: episode: 368, duration: 4.189s, episode steps: 217, steps per second:  52, episode reward: -216.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.862 [0.000, 2.000],  loss: 0.250870, mae: 8.689940, mean_q: -12.732430, mean_eps: 0.403856\n",
            "  94369/200000: episode: 369, duration: 2.869s, episode steps: 132, steps per second:  46, episode reward: -131.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.023 [0.000, 2.000],  loss: 0.266189, mae: 8.682560, mean_q: -12.716695, mean_eps: 0.402751\n",
            "  94573/200000: episode: 370, duration: 4.440s, episode steps: 204, steps per second:  46, episode reward: -203.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.044 [0.000, 2.000],  loss: 0.243524, mae: 8.694538, mean_q: -12.742006, mean_eps: 0.401687\n",
            "  94757/200000: episode: 371, duration: 3.424s, episode steps: 184, steps per second:  54, episode reward: -183.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.940 [0.000, 2.000],  loss: 0.216062, mae: 8.676075, mean_q: -12.714026, mean_eps: 0.400458\n",
            "  94894/200000: episode: 372, duration: 2.716s, episode steps: 137, steps per second:  50, episode reward: -136.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.197 [0.000, 2.000],  loss: 0.241963, mae: 8.686452, mean_q: -12.725459, mean_eps: 0.399442\n",
            "  95105/200000: episode: 373, duration: 5.250s, episode steps: 211, steps per second:  40, episode reward: -210.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.934 [0.000, 2.000],  loss: 0.238838, mae: 8.691260, mean_q: -12.730948, mean_eps: 0.398340\n",
            "  95435/200000: episode: 374, duration: 6.391s, episode steps: 330, steps per second:  52, episode reward: -329.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.100 [0.000, 2.000],  loss: 0.236401, mae: 8.659852, mean_q: -12.692945, mean_eps: 0.396627\n",
            "  95607/200000: episode: 375, duration: 3.223s, episode steps: 172, steps per second:  53, episode reward: -171.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.983 [0.000, 2.000],  loss: 0.234365, mae: 8.678581, mean_q: -12.719316, mean_eps: 0.395037\n",
            "  95874/200000: episode: 376, duration: 6.123s, episode steps: 267, steps per second:  44, episode reward: -266.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.019 [0.000, 2.000],  loss: 0.233059, mae: 8.677448, mean_q: -12.716697, mean_eps: 0.393647\n",
            "  96081/200000: episode: 377, duration: 3.946s, episode steps: 207, steps per second:  52, episode reward: -206.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.778 [0.000, 2.000],  loss: 0.231882, mae: 8.651120, mean_q: -12.681481, mean_eps: 0.392146\n",
            "  96374/200000: episode: 378, duration: 6.308s, episode steps: 293, steps per second:  46, episode reward: -292.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.908 [0.000, 2.000],  loss: 0.251433, mae: 8.620539, mean_q: -12.627296, mean_eps: 0.390562\n",
            "  96625/200000: episode: 379, duration: 4.843s, episode steps: 251, steps per second:  52, episode reward: -250.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.088 [0.000, 2.000],  loss: 0.234749, mae: 8.631885, mean_q: -12.656997, mean_eps: 0.388840\n",
            "  96946/200000: episode: 380, duration: 6.557s, episode steps: 321, steps per second:  49, episode reward: -320.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.059 [0.000, 2.000],  loss: 0.242813, mae: 8.607608, mean_q: -12.619124, mean_eps: 0.387028\n",
            "  97225/200000: episode: 381, duration: 6.079s, episode steps: 279, steps per second:  46, episode reward: -278.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.853 [0.000, 2.000],  loss: 0.220983, mae: 8.587341, mean_q: -12.590152, mean_eps: 0.385128\n",
            "  97518/200000: episode: 382, duration: 5.762s, episode steps: 293, steps per second:  51, episode reward: -292.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.966 [0.000, 2.000],  loss: 0.222553, mae: 8.583820, mean_q: -12.585423, mean_eps: 0.383317\n",
            "  97663/200000: episode: 383, duration: 3.720s, episode steps: 145, steps per second:  39, episode reward: -144.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.890 [0.000, 2.000],  loss: 0.232915, mae: 8.592414, mean_q: -12.597926, mean_eps: 0.381930\n",
            "  98065/200000: episode: 384, duration: 7.902s, episode steps: 402, steps per second:  51, episode reward: -401.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.953 [0.000, 2.000],  loss: 0.223854, mae: 8.580525, mean_q: -12.584301, mean_eps: 0.380198\n",
            "  98241/200000: episode: 385, duration: 4.011s, episode steps: 176, steps per second:  44, episode reward: -175.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.881 [0.000, 2.000],  loss: 0.244544, mae: 8.531804, mean_q: -12.507461, mean_eps: 0.378368\n",
            "  98461/200000: episode: 386, duration: 4.729s, episode steps: 220, steps per second:  47, episode reward: -219.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.977 [0.000, 2.000],  loss: 0.246569, mae: 8.531039, mean_q: -12.501437, mean_eps: 0.377114\n",
            "  98677/200000: episode: 387, duration: 4.198s, episode steps: 216, steps per second:  51, episode reward: -215.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.944 [0.000, 2.000],  loss: 0.235336, mae: 8.542875, mean_q: -12.519963, mean_eps: 0.375733\n",
            "  99028/200000: episode: 388, duration: 8.145s, episode steps: 351, steps per second:  43, episode reward: -350.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.909 [0.000, 2.000],  loss: 0.230636, mae: 8.531793, mean_q: -12.510447, mean_eps: 0.373937\n",
            "  99162/200000: episode: 389, duration: 2.694s, episode steps: 134, steps per second:  50, episode reward: -133.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.970 [0.000, 2.000],  loss: 0.216716, mae: 8.515470, mean_q: -12.490759, mean_eps: 0.372401\n",
            "  99437/200000: episode: 390, duration: 5.427s, episode steps: 275, steps per second:  51, episode reward: -274.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.804 [0.000, 2.000],  loss: 0.238059, mae: 8.499002, mean_q: -12.456108, mean_eps: 0.371106\n",
            "  99774/200000: episode: 391, duration: 9.198s, episode steps: 337, steps per second:  37, episode reward: -336.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.970 [0.000, 2.000],  loss: 0.215269, mae: 8.503094, mean_q: -12.470223, mean_eps: 0.369168\n",
            "  99994/200000: episode: 392, duration: 5.331s, episode steps: 220, steps per second:  41, episode reward: -219.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.918 [0.000, 2.000],  loss: 0.235554, mae: 8.487520, mean_q: -12.447843, mean_eps: 0.367405\n",
            " 100172/200000: episode: 393, duration: 4.365s, episode steps: 178, steps per second:  41, episode reward: -177.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.051 [0.000, 2.000],  loss: 0.249644, mae: 8.412732, mean_q: -12.327490, mean_eps: 0.366144\n",
            " 100352/200000: episode: 394, duration: 3.552s, episode steps: 180, steps per second:  51, episode reward: -179.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.928 [0.000, 2.000],  loss: 0.229172, mae: 8.425862, mean_q: -12.348017, mean_eps: 0.365011\n",
            " 100536/200000: episode: 395, duration: 3.715s, episode steps: 184, steps per second:  50, episode reward: -183.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.022 [0.000, 2.000],  loss: 0.250867, mae: 8.432204, mean_q: -12.361697, mean_eps: 0.363858\n",
            " 100876/200000: episode: 396, duration: 7.821s, episode steps: 340, steps per second:  43, episode reward: -339.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.035 [0.000, 2.000],  loss: 0.239075, mae: 8.422360, mean_q: -12.346424, mean_eps: 0.362198\n",
            " 101134/200000: episode: 397, duration: 5.113s, episode steps: 258, steps per second:  50, episode reward: -257.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.864 [0.000, 2.000],  loss: 0.237908, mae: 8.388519, mean_q: -12.293778, mean_eps: 0.360305\n",
            " 101363/200000: episode: 398, duration: 5.466s, episode steps: 229, steps per second:  42, episode reward: -228.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.852 [0.000, 2.000],  loss: 0.236166, mae: 8.328379, mean_q: -12.208057, mean_eps: 0.358763\n",
            " 101582/200000: episode: 399, duration: 4.301s, episode steps: 219, steps per second:  51, episode reward: -218.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.087 [0.000, 2.000],  loss: 0.243478, mae: 8.325441, mean_q: -12.202319, mean_eps: 0.357344\n",
            " 101916/200000: episode: 400, duration: 7.038s, episode steps: 334, steps per second:  47, episode reward: -333.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.970 [0.000, 2.000],  loss: 0.224850, mae: 8.329953, mean_q: -12.216689, mean_eps: 0.355593\n",
            " 102098/200000: episode: 401, duration: 3.994s, episode steps: 182, steps per second:  46, episode reward: -181.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.995 [0.000, 2.000],  loss: 0.245331, mae: 8.286442, mean_q: -12.144752, mean_eps: 0.353959\n",
            " 102598/200000: episode: 402, duration: 10.912s, episode steps: 500, steps per second:  46, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 0.227470, mae: 8.245674, mean_q: -12.087827, mean_eps: 0.351799\n",
            " 103098/200000: episode: 403, duration: 9.712s, episode steps: 500, steps per second:  51, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.994 [0.000, 2.000],  loss: 0.221758, mae: 8.236386, mean_q: -12.077208, mean_eps: 0.348633\n",
            " 103278/200000: episode: 404, duration: 4.477s, episode steps: 180, steps per second:  40, episode reward: -179.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.167 [0.000, 2.000],  loss: 0.206063, mae: 8.202063, mean_q: -12.025145, mean_eps: 0.346479\n",
            " 103575/200000: episode: 405, duration: 5.899s, episode steps: 297, steps per second:  50, episode reward: -296.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.003 [0.000, 2.000],  loss: 0.225424, mae: 8.190710, mean_q: -12.010164, mean_eps: 0.344969\n",
            " 103864/200000: episode: 406, duration: 6.704s, episode steps: 289, steps per second:  43, episode reward: -288.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.235461, mae: 8.187127, mean_q: -12.003282, mean_eps: 0.343113\n",
            " 104108/200000: episode: 407, duration: 4.680s, episode steps: 244, steps per second:  52, episode reward: -243.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.902 [0.000, 2.000],  loss: 0.227116, mae: 8.168726, mean_q: -11.976443, mean_eps: 0.341425\n",
            " 104337/200000: episode: 408, duration: 4.456s, episode steps: 229, steps per second:  51, episode reward: -228.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.934 [0.000, 2.000],  loss: 0.203779, mae: 8.128906, mean_q: -11.918335, mean_eps: 0.339927\n",
            " 104478/200000: episode: 409, duration: 3.805s, episode steps: 141, steps per second:  37, episode reward: -140.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.993 [0.000, 2.000],  loss: 0.201727, mae: 8.133275, mean_q: -11.928203, mean_eps: 0.338756\n",
            " 104657/200000: episode: 410, duration: 3.581s, episode steps: 179, steps per second:  50, episode reward: -178.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.860 [0.000, 2.000],  loss: 0.212385, mae: 8.128674, mean_q: -11.920185, mean_eps: 0.337742\n",
            " 104835/200000: episode: 411, duration: 3.520s, episode steps: 178, steps per second:  51, episode reward: -177.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.843 [0.000, 2.000],  loss: 0.195897, mae: 8.122037, mean_q: -11.912607, mean_eps: 0.336612\n",
            " 105028/200000: episode: 412, duration: 4.329s, episode steps: 193, steps per second:  45, episode reward: -192.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.041 [0.000, 2.000],  loss: 0.214668, mae: 8.114115, mean_q: -11.899391, mean_eps: 0.335437\n",
            " 105324/200000: episode: 413, duration: 6.553s, episode steps: 296, steps per second:  45, episode reward: -295.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.976 [0.000, 2.000],  loss: 0.222685, mae: 8.106373, mean_q: -11.889652, mean_eps: 0.333888\n",
            " 105543/200000: episode: 414, duration: 4.290s, episode steps: 219, steps per second:  51, episode reward: -218.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.950 [0.000, 2.000],  loss: 0.201840, mae: 8.113809, mean_q: -11.901518, mean_eps: 0.332258\n",
            " 106043/200000: episode: 415, duration: 10.872s, episode steps: 500, steps per second:  46, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.966 [0.000, 2.000],  loss: 0.204938, mae: 8.092593, mean_q: -11.870640, mean_eps: 0.329981\n",
            " 106273/200000: episode: 416, duration: 5.168s, episode steps: 230, steps per second:  45, episode reward: -229.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.943 [0.000, 2.000],  loss: 0.210891, mae: 8.047606, mean_q: -11.800838, mean_eps: 0.327669\n",
            " 106704/200000: episode: 417, duration: 9.239s, episode steps: 431, steps per second:  47, episode reward: -430.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.014 [0.000, 2.000],  loss: 0.206701, mae: 8.055594, mean_q: -11.816591, mean_eps: 0.325576\n",
            " 107020/200000: episode: 418, duration: 7.216s, episode steps: 316, steps per second:  44, episode reward: -315.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.984 [0.000, 2.000],  loss: 0.220606, mae: 8.046410, mean_q: -11.798632, mean_eps: 0.323211\n",
            " 107520/200000: episode: 419, duration: 10.547s, episode steps: 500, steps per second:  47, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.062 [0.000, 2.000],  loss: 0.211032, mae: 7.991948, mean_q: -11.722185, mean_eps: 0.320627\n",
            " 108020/200000: episode: 420, duration: 10.815s, episode steps: 500, steps per second:  46, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.209871, mae: 7.989849, mean_q: -11.721361, mean_eps: 0.317460\n",
            " 108219/200000: episode: 421, duration: 5.059s, episode steps: 199, steps per second:  39, episode reward: -198.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.015 [0.000, 2.000],  loss: 0.213654, mae: 7.951267, mean_q: -11.659020, mean_eps: 0.315246\n",
            " 108719/200000: episode: 422, duration: 10.480s, episode steps: 500, steps per second:  48, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.936 [0.000, 2.000],  loss: 0.202686, mae: 7.958923, mean_q: -11.676739, mean_eps: 0.313033\n",
            " 108975/200000: episode: 423, duration: 5.668s, episode steps: 256, steps per second:  45, episode reward: -255.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.781 [0.000, 2.000],  loss: 0.220600, mae: 7.951959, mean_q: -11.660172, mean_eps: 0.310639\n",
            " 109209/200000: episode: 424, duration: 4.726s, episode steps: 234, steps per second:  50, episode reward: -233.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.038 [0.000, 2.000],  loss: 0.200034, mae: 7.931553, mean_q: -11.637900, mean_eps: 0.309087\n",
            " 109398/200000: episode: 425, duration: 4.707s, episode steps: 189, steps per second:  40, episode reward: -188.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.947 [0.000, 2.000],  loss: 0.202553, mae: 7.926195, mean_q: -11.623198, mean_eps: 0.307748\n",
            " 109611/200000: episode: 426, duration: 4.259s, episode steps: 213, steps per second:  50, episode reward: -212.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.948 [0.000, 2.000],  loss: 0.193062, mae: 7.934746, mean_q: -11.637120, mean_eps: 0.306475\n",
            " 109877/200000: episode: 427, duration: 5.394s, episode steps: 266, steps per second:  49, episode reward: -265.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.846 [0.000, 2.000],  loss: 0.199721, mae: 7.928456, mean_q: -11.628482, mean_eps: 0.304958\n",
            " 110083/200000: episode: 428, duration: 5.155s, episode steps: 206, steps per second:  40, episode reward: -205.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.777 [0.000, 2.000],  loss: 0.226602, mae: 7.922390, mean_q: -11.615960, mean_eps: 0.303463\n",
            " 110284/200000: episode: 429, duration: 4.054s, episode steps: 201, steps per second:  50, episode reward: -200.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.960 [0.000, 2.000],  loss: 0.219883, mae: 7.918121, mean_q: -11.614812, mean_eps: 0.302174\n",
            " 110577/200000: episode: 430, duration: 6.281s, episode steps: 293, steps per second:  47, episode reward: -292.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.990 [0.000, 2.000],  loss: 0.206899, mae: 7.925848, mean_q: -11.625657, mean_eps: 0.300610\n",
            " 111077/200000: episode: 431, duration: 10.289s, episode steps: 500, steps per second:  49, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.982 [0.000, 2.000],  loss: 0.215389, mae: 7.904215, mean_q: -11.597309, mean_eps: 0.298099\n",
            " 111577/200000: episode: 432, duration: 11.014s, episode steps: 500, steps per second:  45, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.214071, mae: 7.861646, mean_q: -11.535827, mean_eps: 0.294932\n",
            " 112077/200000: episode: 433, duration: 11.034s, episode steps: 500, steps per second:  45, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.054 [0.000, 2.000],  loss: 0.215500, mae: 7.851020, mean_q: -11.519035, mean_eps: 0.291766\n",
            " 112369/200000: episode: 434, duration: 5.890s, episode steps: 292, steps per second:  50, episode reward: -291.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.041 [0.000, 2.000],  loss: 0.190621, mae: 7.830202, mean_q: -11.498826, mean_eps: 0.289258\n",
            " 112844/200000: episode: 435, duration: 10.287s, episode steps: 475, steps per second:  46, episode reward: -474.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.977 [0.000, 2.000],  loss: 0.213520, mae: 7.815019, mean_q: -11.465811, mean_eps: 0.286829\n",
            " 113344/200000: episode: 436, duration: 10.982s, episode steps: 500, steps per second:  46, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.072 [0.000, 2.000],  loss: 0.202359, mae: 7.788961, mean_q: -11.433757, mean_eps: 0.283741\n",
            " 113844/200000: episode: 437, duration: 11.008s, episode steps: 500, steps per second:  45, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.024 [0.000, 2.000],  loss: 0.197736, mae: 7.769053, mean_q: -11.402516, mean_eps: 0.280574\n",
            " 114038/200000: episode: 438, duration: 3.801s, episode steps: 194, steps per second:  51, episode reward: -193.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.943 [0.000, 2.000],  loss: 0.191767, mae: 7.779180, mean_q: -11.421431, mean_eps: 0.278377\n",
            " 114538/200000: episode: 439, duration: 11.240s, episode steps: 500, steps per second:  44, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.191094, mae: 7.758362, mean_q: -11.393119, mean_eps: 0.276179\n",
            " 115038/200000: episode: 440, duration: 11.407s, episode steps: 500, steps per second:  44, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.974 [0.000, 2.000],  loss: 0.198132, mae: 7.762106, mean_q: -11.393874, mean_eps: 0.273012\n",
            " 115474/200000: episode: 441, duration: 9.396s, episode steps: 436, steps per second:  46, episode reward: -435.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.963 [0.000, 2.000],  loss: 0.191445, mae: 7.781033, mean_q: -11.425267, mean_eps: 0.270049\n",
            " 115974/200000: episode: 442, duration: 10.774s, episode steps: 500, steps per second:  46, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.764 [0.000, 2.000],  loss: 0.198822, mae: 7.781512, mean_q: -11.424656, mean_eps: 0.267084\n",
            " 116226/200000: episode: 443, duration: 6.090s, episode steps: 252, steps per second:  41, episode reward: -251.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.948 [0.000, 2.000],  loss: 0.194848, mae: 7.742976, mean_q: -11.365382, mean_eps: 0.264703\n",
            " 116537/200000: episode: 444, duration: 6.218s, episode steps: 311, steps per second:  50, episode reward: -310.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.826 [0.000, 2.000],  loss: 0.183932, mae: 7.741798, mean_q: -11.368398, mean_eps: 0.262920\n",
            " 116813/200000: episode: 445, duration: 6.735s, episode steps: 276, steps per second:  41, episode reward: -275.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.909 [0.000, 2.000],  loss: 0.188425, mae: 7.747314, mean_q: -11.376381, mean_eps: 0.261061\n",
            " 117273/200000: episode: 446, duration: 9.126s, episode steps: 460, steps per second:  50, episode reward: -459.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.915 [0.000, 2.000],  loss: 0.192155, mae: 7.735664, mean_q: -11.363252, mean_eps: 0.258731\n",
            " 117773/200000: episode: 447, duration: 10.992s, episode steps: 500, steps per second:  45, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.038 [0.000, 2.000],  loss: 0.197671, mae: 7.735024, mean_q: -11.360674, mean_eps: 0.255691\n",
            " 118143/200000: episode: 448, duration: 8.164s, episode steps: 370, steps per second:  45, episode reward: -369.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.830 [0.000, 2.000],  loss: 0.197928, mae: 7.729537, mean_q: -11.349926, mean_eps: 0.252936\n",
            " 118643/200000: episode: 449, duration: 10.356s, episode steps: 500, steps per second:  48, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.912 [0.000, 2.000],  loss: 0.205487, mae: 7.727270, mean_q: -11.341171, mean_eps: 0.250181\n",
            " 119143/200000: episode: 450, duration: 10.196s, episode steps: 500, steps per second:  49, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.215348, mae: 7.727087, mean_q: -11.340911, mean_eps: 0.247014\n",
            " 119643/200000: episode: 451, duration: 11.025s, episode steps: 500, steps per second:  45, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.996 [0.000, 2.000],  loss: 0.207850, mae: 7.706020, mean_q: -11.311182, mean_eps: 0.243847\n",
            " 120143/200000: episode: 452, duration: 11.118s, episode steps: 500, steps per second:  45, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.922 [0.000, 2.000],  loss: 0.190006, mae: 7.696845, mean_q: -11.300561, mean_eps: 0.240681\n",
            " 120643/200000: episode: 453, duration: 11.413s, episode steps: 500, steps per second:  44, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.016 [0.000, 2.000],  loss: 0.203513, mae: 7.677496, mean_q: -11.274426, mean_eps: 0.237514\n",
            " 120974/200000: episode: 454, duration: 6.801s, episode steps: 331, steps per second:  49, episode reward: -330.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.009 [0.000, 2.000],  loss: 0.195871, mae: 7.680859, mean_q: -11.283628, mean_eps: 0.234883\n",
            " 121474/200000: episode: 455, duration: 11.965s, episode steps: 500, steps per second:  42, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.946 [0.000, 2.000],  loss: 0.196966, mae: 7.632623, mean_q: -11.209882, mean_eps: 0.232251\n",
            " 121961/200000: episode: 456, duration: 11.392s, episode steps: 487, steps per second:  43, episode reward: -486.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.078 [0.000, 2.000],  loss: 0.194683, mae: 7.626096, mean_q: -11.204419, mean_eps: 0.229126\n",
            " 122168/200000: episode: 457, duration: 4.514s, episode steps: 207, steps per second:  46, episode reward: -206.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.812 [0.000, 2.000],  loss: 0.189712, mae: 7.652222, mean_q: -11.242168, mean_eps: 0.226928\n",
            " 122668/200000: episode: 458, duration: 11.364s, episode steps: 500, steps per second:  44, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.916 [0.000, 2.000],  loss: 0.188946, mae: 7.664494, mean_q: -11.265170, mean_eps: 0.224689\n",
            " 123168/200000: episode: 459, duration: 11.315s, episode steps: 500, steps per second:  44, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.946 [0.000, 2.000],  loss: 0.187893, mae: 7.655098, mean_q: -11.253316, mean_eps: 0.221522\n",
            " 123668/200000: episode: 460, duration: 10.989s, episode steps: 500, steps per second:  46, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.908 [0.000, 2.000],  loss: 0.183291, mae: 7.630458, mean_q: -11.215023, mean_eps: 0.218356\n",
            " 124168/200000: episode: 461, duration: 11.146s, episode steps: 500, steps per second:  45, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.201341, mae: 7.613947, mean_q: -11.187774, mean_eps: 0.215189\n",
            " 124668/200000: episode: 462, duration: 10.943s, episode steps: 500, steps per second:  46, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.191703, mae: 7.588255, mean_q: -11.151420, mean_eps: 0.212023\n",
            " 125168/200000: episode: 463, duration: 10.311s, episode steps: 500, steps per second:  48, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.012 [0.000, 2.000],  loss: 0.201465, mae: 7.587629, mean_q: -11.150066, mean_eps: 0.208856\n",
            " 125668/200000: episode: 464, duration: 11.025s, episode steps: 500, steps per second:  45, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.876 [0.000, 2.000],  loss: 0.197888, mae: 7.583172, mean_q: -11.143418, mean_eps: 0.205689\n",
            " 126168/200000: episode: 465, duration: 10.950s, episode steps: 500, steps per second:  46, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.902 [0.000, 2.000],  loss: 0.189817, mae: 7.583249, mean_q: -11.147896, mean_eps: 0.202522\n",
            " 126668/200000: episode: 466, duration: 11.306s, episode steps: 500, steps per second:  44, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.882 [0.000, 2.000],  loss: 0.195650, mae: 7.579512, mean_q: -11.141964, mean_eps: 0.199356\n",
            " 127168/200000: episode: 467, duration: 11.127s, episode steps: 500, steps per second:  45, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.824 [0.000, 2.000],  loss: 0.202377, mae: 7.572055, mean_q: -11.129705, mean_eps: 0.196189\n",
            " 127668/200000: episode: 468, duration: 10.228s, episode steps: 500, steps per second:  49, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.768 [0.000, 2.000],  loss: 0.183627, mae: 7.578935, mean_q: -11.143333, mean_eps: 0.193022\n",
            " 128168/200000: episode: 469, duration: 11.007s, episode steps: 500, steps per second:  45, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.842 [0.000, 2.000],  loss: 0.192822, mae: 7.562204, mean_q: -11.116969, mean_eps: 0.189856\n",
            " 128668/200000: episode: 470, duration: 11.251s, episode steps: 500, steps per second:  44, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.754 [0.000, 2.000],  loss: 0.185716, mae: 7.544365, mean_q: -11.090869, mean_eps: 0.186689\n",
            " 129168/200000: episode: 471, duration: 11.609s, episode steps: 500, steps per second:  43, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.868 [0.000, 2.000],  loss: 0.171094, mae: 7.543720, mean_q: -11.097380, mean_eps: 0.183522\n",
            " 129668/200000: episode: 472, duration: 11.276s, episode steps: 500, steps per second:  44, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.824 [0.000, 2.000],  loss: 0.181122, mae: 7.523863, mean_q: -11.067933, mean_eps: 0.180356\n",
            " 130168/200000: episode: 473, duration: 11.251s, episode steps: 500, steps per second:  44, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.818 [0.000, 2.000],  loss: 0.175623, mae: 7.518380, mean_q: -11.060560, mean_eps: 0.177189\n",
            " 130495/200000: episode: 474, duration: 6.863s, episode steps: 327, steps per second:  48, episode reward: -326.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.927 [0.000, 2.000],  loss: 0.186781, mae: 7.498672, mean_q: -11.030679, mean_eps: 0.174570\n",
            " 130995/200000: episode: 475, duration: 11.253s, episode steps: 500, steps per second:  44, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.826 [0.000, 2.000],  loss: 0.186637, mae: 7.492906, mean_q: -11.021060, mean_eps: 0.171951\n",
            " 131495/200000: episode: 476, duration: 10.927s, episode steps: 500, steps per second:  46, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.936 [0.000, 2.000],  loss: 0.184508, mae: 7.470240, mean_q: -10.985839, mean_eps: 0.168785\n",
            " 131995/200000: episode: 477, duration: 11.729s, episode steps: 500, steps per second:  43, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.018 [0.000, 2.000],  loss: 0.195366, mae: 7.474284, mean_q: -10.988572, mean_eps: 0.165618\n",
            " 132495/200000: episode: 478, duration: 11.300s, episode steps: 500, steps per second:  44, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.872 [0.000, 2.000],  loss: 0.194294, mae: 7.460775, mean_q: -10.971070, mean_eps: 0.162451\n",
            " 132995/200000: episode: 479, duration: 10.840s, episode steps: 500, steps per second:  46, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.193111, mae: 7.473073, mean_q: -10.989271, mean_eps: 0.159285\n",
            " 133495/200000: episode: 480, duration: 10.951s, episode steps: 500, steps per second:  46, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.052 [0.000, 2.000],  loss: 0.184869, mae: 7.411743, mean_q: -10.903246, mean_eps: 0.156118\n",
            " 133995/200000: episode: 481, duration: 11.268s, episode steps: 500, steps per second:  44, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.992 [0.000, 2.000],  loss: 0.175863, mae: 7.409969, mean_q: -10.904396, mean_eps: 0.152951\n",
            " 134495/200000: episode: 482, duration: 11.124s, episode steps: 500, steps per second:  45, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.956 [0.000, 2.000],  loss: 0.180543, mae: 7.417418, mean_q: -10.916502, mean_eps: 0.149785\n",
            " 134836/200000: episode: 483, duration: 6.689s, episode steps: 341, steps per second:  51, episode reward: -340.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.815 [0.000, 2.000],  loss: 0.164479, mae: 7.423229, mean_q: -10.925532, mean_eps: 0.147122\n",
            " 135336/200000: episode: 484, duration: 11.209s, episode steps: 500, steps per second:  45, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.866 [0.000, 2.000],  loss: 0.177646, mae: 7.415206, mean_q: -10.916365, mean_eps: 0.144459\n",
            " 135836/200000: episode: 485, duration: 11.120s, episode steps: 500, steps per second:  45, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.866 [0.000, 2.000],  loss: 0.172887, mae: 7.411417, mean_q: -10.911994, mean_eps: 0.141292\n",
            " 136336/200000: episode: 486, duration: 11.220s, episode steps: 500, steps per second:  45, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.878 [0.000, 2.000],  loss: 0.160691, mae: 7.364081, mean_q: -10.842491, mean_eps: 0.138125\n",
            " 136836/200000: episode: 487, duration: 10.958s, episode steps: 500, steps per second:  46, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.932 [0.000, 2.000],  loss: 0.167860, mae: 7.331476, mean_q: -10.793483, mean_eps: 0.134959\n",
            " 137336/200000: episode: 488, duration: 10.257s, episode steps: 500, steps per second:  49, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.876 [0.000, 2.000],  loss: 0.166826, mae: 7.343340, mean_q: -10.815625, mean_eps: 0.131792\n",
            " 137836/200000: episode: 489, duration: 10.817s, episode steps: 500, steps per second:  46, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.974 [0.000, 2.000],  loss: 0.163623, mae: 7.351906, mean_q: -10.830591, mean_eps: 0.128625\n",
            " 138336/200000: episode: 490, duration: 11.206s, episode steps: 500, steps per second:  45, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.966 [0.000, 2.000],  loss: 0.153940, mae: 7.320691, mean_q: -10.787133, mean_eps: 0.125459\n",
            " 138836/200000: episode: 491, duration: 11.340s, episode steps: 500, steps per second:  44, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.904 [0.000, 2.000],  loss: 0.169866, mae: 7.303318, mean_q: -10.755157, mean_eps: 0.122292\n",
            " 139336/200000: episode: 492, duration: 11.284s, episode steps: 500, steps per second:  44, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.002 [0.000, 2.000],  loss: 0.171200, mae: 7.299574, mean_q: -10.751689, mean_eps: 0.119125\n",
            " 139836/200000: episode: 493, duration: 10.815s, episode steps: 500, steps per second:  46, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.014 [0.000, 2.000],  loss: 0.177934, mae: 7.296933, mean_q: -10.746681, mean_eps: 0.115958\n",
            " 140336/200000: episode: 494, duration: 10.319s, episode steps: 500, steps per second:  48, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.906 [0.000, 2.000],  loss: 0.165071, mae: 7.326818, mean_q: -10.793746, mean_eps: 0.112792\n",
            " 140836/200000: episode: 495, duration: 11.046s, episode steps: 500, steps per second:  45, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.018 [0.000, 2.000],  loss: 0.175383, mae: 7.345689, mean_q: -10.820520, mean_eps: 0.109625\n",
            " 141336/200000: episode: 496, duration: 11.070s, episode steps: 500, steps per second:  45, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.984 [0.000, 2.000],  loss: 0.164404, mae: 7.354389, mean_q: -10.835573, mean_eps: 0.106458\n",
            " 141836/200000: episode: 497, duration: 11.153s, episode steps: 500, steps per second:  45, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.168 [0.000, 2.000],  loss: 0.159600, mae: 7.369096, mean_q: -10.861164, mean_eps: 0.103292\n",
            " 142336/200000: episode: 498, duration: 10.996s, episode steps: 500, steps per second:  45, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.956 [0.000, 2.000],  loss: 0.170330, mae: 7.394035, mean_q: -10.895983, mean_eps: 0.100125\n",
            " 142836/200000: episode: 499, duration: 10.224s, episode steps: 500, steps per second:  49, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.082 [0.000, 2.000],  loss: 0.160734, mae: 7.407278, mean_q: -10.917628, mean_eps: 0.096958\n",
            " 143336/200000: episode: 500, duration: 11.298s, episode steps: 500, steps per second:  44, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.136 [0.000, 2.000],  loss: 0.160959, mae: 7.394195, mean_q: -10.899031, mean_eps: 0.093792\n",
            " 143836/200000: episode: 501, duration: 11.037s, episode steps: 500, steps per second:  45, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.026 [0.000, 2.000],  loss: 0.172524, mae: 7.382820, mean_q: -10.881019, mean_eps: 0.090625\n",
            " 144336/200000: episode: 502, duration: 11.108s, episode steps: 500, steps per second:  45, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.064 [0.000, 2.000],  loss: 0.163343, mae: 7.413104, mean_q: -10.929024, mean_eps: 0.087458\n",
            " 144836/200000: episode: 503, duration: 11.258s, episode steps: 500, steps per second:  44, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.978 [0.000, 2.000],  loss: 0.166314, mae: 7.430306, mean_q: -10.956133, mean_eps: 0.084292\n",
            " 145336/200000: episode: 504, duration: 11.023s, episode steps: 500, steps per second:  45, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.982 [0.000, 2.000],  loss: 0.159451, mae: 7.400519, mean_q: -10.913335, mean_eps: 0.081125\n",
            " 145836/200000: episode: 505, duration: 10.309s, episode steps: 500, steps per second:  49, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.064 [0.000, 2.000],  loss: 0.154434, mae: 7.389697, mean_q: -10.898252, mean_eps: 0.077958\n",
            " 146336/200000: episode: 506, duration: 11.135s, episode steps: 500, steps per second:  45, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.084 [0.000, 2.000],  loss: 0.162704, mae: 7.386915, mean_q: -10.893228, mean_eps: 0.074792\n",
            " 146836/200000: episode: 507, duration: 11.214s, episode steps: 500, steps per second:  45, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.171209, mae: 7.391033, mean_q: -10.896460, mean_eps: 0.071625\n",
            " 147336/200000: episode: 508, duration: 11.399s, episode steps: 500, steps per second:  44, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.994 [0.000, 2.000],  loss: 0.157146, mae: 7.373373, mean_q: -10.869514, mean_eps: 0.068458\n",
            " 147730/200000: episode: 509, duration: 8.721s, episode steps: 394, steps per second:  45, episode reward: -393.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.830 [0.000, 2.000],  loss: 0.166751, mae: 7.357720, mean_q: -10.848689, mean_eps: 0.065627\n",
            " 148112/200000: episode: 510, duration: 8.064s, episode steps: 382, steps per second:  47, episode reward: -381.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.908 [0.000, 2.000],  loss: 0.162350, mae: 7.359304, mean_q: -10.846243, mean_eps: 0.063170\n",
            " 148612/200000: episode: 511, duration: 11.543s, episode steps: 500, steps per second:  43, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.952 [0.000, 2.000],  loss: 0.161078, mae: 7.378252, mean_q: -10.882500, mean_eps: 0.060377\n",
            " 149087/200000: episode: 512, duration: 10.810s, episode steps: 475, steps per second:  44, episode reward: -474.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.987 [0.000, 2.000],  loss: 0.171662, mae: 7.371611, mean_q: -10.867061, mean_eps: 0.057290\n",
            " 149437/200000: episode: 513, duration: 6.955s, episode steps: 350, steps per second:  50, episode reward: -349.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.026 [0.000, 2.000],  loss: 0.153723, mae: 7.359413, mean_q: -10.854514, mean_eps: 0.054677\n",
            " 149609/200000: episode: 514, duration: 4.527s, episode steps: 172, steps per second:  38, episode reward: -171.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.166131, mae: 7.355257, mean_q: -10.843902, mean_eps: 0.053024\n",
            " 150109/200000: episode: 515, duration: 10.867s, episode steps: 500, steps per second:  46, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.944 [0.000, 2.000],  loss: 0.163237, mae: 7.363528, mean_q: -10.859008, mean_eps: 0.050971\n",
            " 150609/200000: episode: 516, duration: 11.156s, episode steps: 500, steps per second:  45, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.022 [0.000, 2.000],  loss: 0.161402, mae: 7.409274, mean_q: -10.931656, mean_eps: 0.050000\n",
            " 150861/200000: episode: 517, duration: 6.334s, episode steps: 252, steps per second:  40, episode reward: -251.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.079 [0.000, 2.000],  loss: 0.164760, mae: 7.406403, mean_q: -10.923621, mean_eps: 0.050000\n",
            " 151156/200000: episode: 518, duration: 6.002s, episode steps: 295, steps per second:  49, episode reward: -294.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.071 [0.000, 2.000],  loss: 0.167904, mae: 7.388819, mean_q: -10.897468, mean_eps: 0.050000\n",
            " 151576/200000: episode: 519, duration: 9.828s, episode steps: 420, steps per second:  43, episode reward: -419.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.976 [0.000, 2.000],  loss: 0.152423, mae: 7.377209, mean_q: -10.885111, mean_eps: 0.050000\n",
            " 152019/200000: episode: 520, duration: 10.282s, episode steps: 443, steps per second:  43, episode reward: -442.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.106 [0.000, 2.000],  loss: 0.161119, mae: 7.381246, mean_q: -10.890402, mean_eps: 0.050000\n",
            " 152377/200000: episode: 521, duration: 7.659s, episode steps: 358, steps per second:  47, episode reward: -357.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.818 [0.000, 2.000],  loss: 0.143042, mae: 7.397674, mean_q: -10.913711, mean_eps: 0.050000\n",
            " 152567/200000: episode: 522, duration: 4.879s, episode steps: 190, steps per second:  39, episode reward: -189.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.921 [0.000, 2.000],  loss: 0.172131, mae: 7.392432, mean_q: -10.899806, mean_eps: 0.050000\n",
            " 152756/200000: episode: 523, duration: 3.794s, episode steps: 189, steps per second:  50, episode reward: -188.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.884 [0.000, 2.000],  loss: 0.162222, mae: 7.397392, mean_q: -10.912233, mean_eps: 0.050000\n",
            " 153256/200000: episode: 524, duration: 11.231s, episode steps: 500, steps per second:  45, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.158431, mae: 7.393838, mean_q: -10.906787, mean_eps: 0.050000\n",
            " 153701/200000: episode: 525, duration: 9.440s, episode steps: 445, steps per second:  47, episode reward: -444.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.103 [0.000, 2.000],  loss: 0.161067, mae: 7.398762, mean_q: -10.913637, mean_eps: 0.050000\n",
            " 153912/200000: episode: 526, duration: 5.126s, episode steps: 211, steps per second:  41, episode reward: -210.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.986 [0.000, 2.000],  loss: 0.156130, mae: 7.393770, mean_q: -10.906423, mean_eps: 0.050000\n",
            " 154197/200000: episode: 527, duration: 5.818s, episode steps: 285, steps per second:  49, episode reward: -284.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.951 [0.000, 2.000],  loss: 0.158594, mae: 7.420803, mean_q: -10.947896, mean_eps: 0.050000\n",
            " 154397/200000: episode: 528, duration: 5.162s, episode steps: 200, steps per second:  39, episode reward: -199.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.850 [0.000, 2.000],  loss: 0.160464, mae: 7.426206, mean_q: -10.952661, mean_eps: 0.050000\n",
            " 154533/200000: episode: 529, duration: 2.760s, episode steps: 136, steps per second:  49, episode reward: -135.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.919 [0.000, 2.000],  loss: 0.144982, mae: 7.433805, mean_q: -10.967920, mean_eps: 0.050000\n",
            " 155013/200000: episode: 530, duration: 10.782s, episode steps: 480, steps per second:  45, episode reward: -479.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.808 [0.000, 2.000],  loss: 0.158848, mae: 7.432055, mean_q: -10.967109, mean_eps: 0.050000\n",
            " 155422/200000: episode: 531, duration: 8.772s, episode steps: 409, steps per second:  47, episode reward: -408.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.917 [0.000, 2.000],  loss: 0.152298, mae: 7.420973, mean_q: -10.950669, mean_eps: 0.050000\n",
            " 155584/200000: episode: 532, duration: 4.385s, episode steps: 162, steps per second:  37, episode reward: -161.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.173 [0.000, 2.000],  loss: 0.141677, mae: 7.417001, mean_q: -10.943850, mean_eps: 0.050000\n",
            " 155845/200000: episode: 533, duration: 5.374s, episode steps: 261, steps per second:  49, episode reward: -260.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.015 [0.000, 2.000],  loss: 0.141142, mae: 7.420215, mean_q: -10.952987, mean_eps: 0.050000\n",
            " 156014/200000: episode: 534, duration: 3.498s, episode steps: 169, steps per second:  48, episode reward: -168.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.982 [0.000, 2.000],  loss: 0.152054, mae: 7.422309, mean_q: -10.947360, mean_eps: 0.050000\n",
            " 156168/200000: episode: 535, duration: 4.263s, episode steps: 154, steps per second:  36, episode reward: -153.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.877 [0.000, 2.000],  loss: 0.156693, mae: 7.479950, mean_q: -11.037418, mean_eps: 0.050000\n",
            " 156344/200000: episode: 536, duration: 3.605s, episode steps: 176, steps per second:  49, episode reward: -175.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.977 [0.000, 2.000],  loss: 0.135829, mae: 7.478502, mean_q: -11.041982, mean_eps: 0.050000\n",
            " 156463/200000: episode: 537, duration: 2.434s, episode steps: 119, steps per second:  49, episode reward: -118.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.042 [0.000, 2.000],  loss: 0.147936, mae: 7.467077, mean_q: -11.017704, mean_eps: 0.050000\n",
            " 156747/200000: episode: 538, duration: 6.659s, episode steps: 284, steps per second:  43, episode reward: -283.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.113 [0.000, 2.000],  loss: 0.162135, mae: 7.469595, mean_q: -11.023176, mean_eps: 0.050000\n",
            " 156946/200000: episode: 539, duration: 4.295s, episode steps: 199, steps per second:  46, episode reward: -198.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.010 [0.000, 2.000],  loss: 0.155613, mae: 7.472403, mean_q: -11.025841, mean_eps: 0.050000\n",
            " 157098/200000: episode: 540, duration: 3.190s, episode steps: 152, steps per second:  48, episode reward: -151.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.908 [0.000, 2.000],  loss: 0.155836, mae: 7.484240, mean_q: -11.044377, mean_eps: 0.050000\n",
            " 157255/200000: episode: 541, duration: 3.211s, episode steps: 157, steps per second:  49, episode reward: -156.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.873 [0.000, 2.000],  loss: 0.140468, mae: 7.484456, mean_q: -11.045937, mean_eps: 0.050000\n",
            " 157399/200000: episode: 542, duration: 3.853s, episode steps: 144, steps per second:  37, episode reward: -143.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.146 [0.000, 2.000],  loss: 0.163971, mae: 7.477768, mean_q: -11.030621, mean_eps: 0.050000\n",
            " 157871/200000: episode: 543, duration: 9.807s, episode steps: 472, steps per second:  48, episode reward: -471.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.841 [0.000, 2.000],  loss: 0.161148, mae: 7.488834, mean_q: -11.049021, mean_eps: 0.050000\n",
            " 158004/200000: episode: 544, duration: 3.567s, episode steps: 133, steps per second:  37, episode reward: -132.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.947 [0.000, 2.000],  loss: 0.166033, mae: 7.485755, mean_q: -11.045035, mean_eps: 0.050000\n",
            " 158168/200000: episode: 545, duration: 3.324s, episode steps: 164, steps per second:  49, episode reward: -163.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.884 [0.000, 2.000],  loss: 0.144809, mae: 7.520902, mean_q: -11.101097, mean_eps: 0.050000\n",
            " 158268/200000: episode: 546, duration: 2.001s, episode steps: 100, steps per second:  50, episode reward: -99.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.010 [0.000, 2.000],  loss: 0.188205, mae: 7.514158, mean_q: -11.080420, mean_eps: 0.050000\n",
            " 158394/200000: episode: 547, duration: 2.527s, episode steps: 126, steps per second:  50, episode reward: -125.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.817 [0.000, 2.000],  loss: 0.194024, mae: 7.509770, mean_q: -11.075956, mean_eps: 0.050000\n",
            " 158506/200000: episode: 548, duration: 2.597s, episode steps: 112, steps per second:  43, episode reward: -111.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.812 [0.000, 2.000],  loss: 0.145147, mae: 7.525800, mean_q: -11.112766, mean_eps: 0.050000\n",
            " 158672/200000: episode: 549, duration: 4.033s, episode steps: 166, steps per second:  41, episode reward: -165.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.096 [0.000, 2.000],  loss: 0.160269, mae: 7.518098, mean_q: -11.100451, mean_eps: 0.050000\n",
            " 158921/200000: episode: 550, duration: 5.006s, episode steps: 249, steps per second:  50, episode reward: -248.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.084 [0.000, 2.000],  loss: 0.171084, mae: 7.519271, mean_q: -11.101989, mean_eps: 0.050000\n",
            " 159233/200000: episode: 551, duration: 7.400s, episode steps: 312, steps per second:  42, episode reward: -311.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.010 [0.000, 2.000],  loss: 0.161478, mae: 7.551619, mean_q: -11.145720, mean_eps: 0.050000\n",
            " 159523/200000: episode: 552, duration: 6.109s, episode steps: 290, steps per second:  47, episode reward: -289.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.145 [0.000, 2.000],  loss: 0.144029, mae: 7.563348, mean_q: -11.167709, mean_eps: 0.050000\n",
            " 159681/200000: episode: 553, duration: 3.303s, episode steps: 158, steps per second:  48, episode reward: -157.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.082 [0.000, 2.000],  loss: 0.150259, mae: 7.566650, mean_q: -11.173370, mean_eps: 0.050000\n",
            " 159861/200000: episode: 554, duration: 4.528s, episode steps: 180, steps per second:  40, episode reward: -179.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.883 [0.000, 2.000],  loss: 0.156399, mae: 7.565733, mean_q: -11.170306, mean_eps: 0.050000\n",
            " 160030/200000: episode: 555, duration: 3.307s, episode steps: 169, steps per second:  51, episode reward: -168.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.923 [0.000, 2.000],  loss: 0.158258, mae: 7.581758, mean_q: -11.198029, mean_eps: 0.050000\n",
            " 160183/200000: episode: 556, duration: 3.069s, episode steps: 153, steps per second:  50, episode reward: -152.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.013 [0.000, 2.000],  loss: 0.136935, mae: 7.623510, mean_q: -11.259734, mean_eps: 0.050000\n",
            " 160340/200000: episode: 557, duration: 3.483s, episode steps: 157, steps per second:  45, episode reward: -156.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.936 [0.000, 2.000],  loss: 0.163164, mae: 7.621108, mean_q: -11.251234, mean_eps: 0.050000\n",
            " 160437/200000: episode: 558, duration: 2.628s, episode steps:  97, steps per second:  37, episode reward: -96.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.866 [0.000, 2.000],  loss: 0.150653, mae: 7.619421, mean_q: -11.244579, mean_eps: 0.050000\n",
            " 160573/200000: episode: 559, duration: 2.837s, episode steps: 136, steps per second:  48, episode reward: -135.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.154 [0.000, 2.000],  loss: 0.142511, mae: 7.621678, mean_q: -11.254634, mean_eps: 0.050000\n",
            " 160685/200000: episode: 560, duration: 2.229s, episode steps: 112, steps per second:  50, episode reward: -111.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.982 [0.000, 2.000],  loss: 0.146560, mae: 7.623109, mean_q: -11.252185, mean_eps: 0.050000\n",
            " 160782/200000: episode: 561, duration: 1.960s, episode steps:  97, steps per second:  49, episode reward: -96.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.021 [0.000, 2.000],  loss: 0.148744, mae: 7.624902, mean_q: -11.254801, mean_eps: 0.050000\n",
            " 160974/200000: episode: 562, duration: 4.498s, episode steps: 192, steps per second:  43, episode reward: -191.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.010 [0.000, 2.000],  loss: 0.151000, mae: 7.628788, mean_q: -11.259900, mean_eps: 0.050000\n",
            " 161150/200000: episode: 563, duration: 4.018s, episode steps: 176, steps per second:  44, episode reward: -175.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.938 [0.000, 2.000],  loss: 0.161165, mae: 7.608376, mean_q: -11.229641, mean_eps: 0.050000\n",
            " 161336/200000: episode: 564, duration: 3.736s, episode steps: 186, steps per second:  50, episode reward: -185.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.935 [0.000, 2.000],  loss: 0.178618, mae: 7.603148, mean_q: -11.216021, mean_eps: 0.050000\n",
            " 161702/200000: episode: 565, duration: 8.245s, episode steps: 366, steps per second:  44, episode reward: -365.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.863 [0.000, 2.000],  loss: 0.152025, mae: 7.600542, mean_q: -11.220195, mean_eps: 0.050000\n",
            " 161886/200000: episode: 566, duration: 3.708s, episode steps: 184, steps per second:  50, episode reward: -183.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.125 [0.000, 2.000],  loss: 0.165368, mae: 7.594240, mean_q: -11.211156, mean_eps: 0.050000\n",
            " 162064/200000: episode: 567, duration: 3.696s, episode steps: 178, steps per second:  48, episode reward: -177.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.135 [0.000, 2.000],  loss: 0.151800, mae: 7.609695, mean_q: -11.235045, mean_eps: 0.050000\n",
            " 162371/200000: episode: 568, duration: 7.325s, episode steps: 307, steps per second:  42, episode reward: -306.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.925 [0.000, 2.000],  loss: 0.163252, mae: 7.625248, mean_q: -11.255475, mean_eps: 0.050000\n",
            " 162486/200000: episode: 569, duration: 2.323s, episode steps: 115, steps per second:  49, episode reward: -114.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.035 [0.000, 2.000],  loss: 0.133895, mae: 7.623558, mean_q: -11.259446, mean_eps: 0.050000\n",
            " 162631/200000: episode: 570, duration: 2.887s, episode steps: 145, steps per second:  50, episode reward: -144.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.910 [0.000, 2.000],  loss: 0.138616, mae: 7.618357, mean_q: -11.247692, mean_eps: 0.050000\n",
            " 162744/200000: episode: 571, duration: 2.177s, episode steps: 113, steps per second:  52, episode reward: -112.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.027 [0.000, 2.000],  loss: 0.142298, mae: 7.618553, mean_q: -11.247971, mean_eps: 0.050000\n",
            " 162832/200000: episode: 572, duration: 2.398s, episode steps:  88, steps per second:  37, episode reward: -87.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.284 [0.000, 2.000],  loss: 0.182601, mae: 7.619134, mean_q: -11.241457, mean_eps: 0.050000\n",
            " 163054/200000: episode: 573, duration: 4.703s, episode steps: 222, steps per second:  47, episode reward: -221.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.045 [0.000, 2.000],  loss: 0.162993, mae: 7.623034, mean_q: -11.255028, mean_eps: 0.050000\n",
            " 163304/200000: episode: 574, duration: 5.025s, episode steps: 250, steps per second:  50, episode reward: -249.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.828 [0.000, 2.000],  loss: 0.172931, mae: 7.650783, mean_q: -11.299648, mean_eps: 0.050000\n",
            " 163587/200000: episode: 575, duration: 6.779s, episode steps: 283, steps per second:  42, episode reward: -282.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.145 [0.000, 2.000],  loss: 0.174174, mae: 7.644343, mean_q: -11.285955, mean_eps: 0.050000\n",
            " 163819/200000: episode: 576, duration: 4.498s, episode steps: 232, steps per second:  52, episode reward: -231.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.944 [0.000, 2.000],  loss: 0.157215, mae: 7.645712, mean_q: -11.290469, mean_eps: 0.050000\n",
            " 164109/200000: episode: 577, duration: 6.569s, episode steps: 290, steps per second:  44, episode reward: -289.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.117 [0.000, 2.000],  loss: 0.168300, mae: 7.654050, mean_q: -11.302940, mean_eps: 0.050000\n",
            " 164239/200000: episode: 578, duration: 2.621s, episode steps: 130, steps per second:  50, episode reward: -129.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.946 [0.000, 2.000],  loss: 0.180525, mae: 7.665073, mean_q: -11.317706, mean_eps: 0.050000\n",
            " 164450/200000: episode: 579, duration: 4.211s, episode steps: 211, steps per second:  50, episode reward: -210.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.991 [0.000, 2.000],  loss: 0.152209, mae: 7.665501, mean_q: -11.322946, mean_eps: 0.050000\n",
            " 164633/200000: episode: 580, duration: 3.855s, episode steps: 183, steps per second:  47, episode reward: -182.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.126 [0.000, 2.000],  loss: 0.140303, mae: 7.667824, mean_q: -11.327877, mean_eps: 0.050000\n",
            " 164845/200000: episode: 581, duration: 5.145s, episode steps: 212, steps per second:  41, episode reward: -211.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.825 [0.000, 2.000],  loss: 0.160382, mae: 7.671689, mean_q: -11.329380, mean_eps: 0.050000\n",
            " 165007/200000: episode: 582, duration: 3.205s, episode steps: 162, steps per second:  51, episode reward: -161.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.105 [0.000, 2.000],  loss: 0.160689, mae: 7.670448, mean_q: -11.326326, mean_eps: 0.050000\n",
            " 165105/200000: episode: 583, duration: 2.071s, episode steps:  98, steps per second:  47, episode reward: -97.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.163 [0.000, 2.000],  loss: 0.141092, mae: 7.712201, mean_q: -11.386506, mean_eps: 0.050000\n",
            " 165231/200000: episode: 584, duration: 2.671s, episode steps: 126, steps per second:  47, episode reward: -125.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.056 [0.000, 2.000],  loss: 0.151812, mae: 7.710017, mean_q: -11.385086, mean_eps: 0.050000\n",
            " 165361/200000: episode: 585, duration: 3.579s, episode steps: 130, steps per second:  36, episode reward: -129.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.123 [0.000, 2.000],  loss: 0.179812, mae: 7.710383, mean_q: -11.376299, mean_eps: 0.050000\n",
            " 165605/200000: episode: 586, duration: 4.731s, episode steps: 244, steps per second:  52, episode reward: -243.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.090 [0.000, 2.000],  loss: 0.170964, mae: 7.706250, mean_q: -11.372925, mean_eps: 0.050000\n",
            " 165715/200000: episode: 587, duration: 2.235s, episode steps: 110, steps per second:  49, episode reward: -109.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.836 [0.000, 2.000],  loss: 0.163981, mae: 7.711836, mean_q: -11.386528, mean_eps: 0.050000\n",
            " 165986/200000: episode: 588, duration: 6.447s, episode steps: 271, steps per second:  42, episode reward: -270.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.959 [0.000, 2.000],  loss: 0.172805, mae: 7.705446, mean_q: -11.374195, mean_eps: 0.050000\n",
            " 166370/200000: episode: 589, duration: 7.806s, episode steps: 384, steps per second:  49, episode reward: -383.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.938 [0.000, 2.000],  loss: 0.155262, mae: 7.737492, mean_q: -11.429781, mean_eps: 0.050000\n",
            " 166544/200000: episode: 590, duration: 4.181s, episode steps: 174, steps per second:  42, episode reward: -173.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.052 [0.000, 2.000],  loss: 0.159524, mae: 7.745891, mean_q: -11.445487, mean_eps: 0.050000\n",
            " 166710/200000: episode: 591, duration: 3.774s, episode steps: 166, steps per second:  44, episode reward: -165.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.295 [0.000, 2.000],  loss: 0.165850, mae: 7.737167, mean_q: -11.427661, mean_eps: 0.050000\n",
            " 166868/200000: episode: 592, duration: 3.143s, episode steps: 158, steps per second:  50, episode reward: -157.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.044 [0.000, 2.000],  loss: 0.149994, mae: 7.742209, mean_q: -11.437450, mean_eps: 0.050000\n",
            " 167166/200000: episode: 593, duration: 6.785s, episode steps: 298, steps per second:  44, episode reward: -297.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.909 [0.000, 2.000],  loss: 0.168113, mae: 7.737665, mean_q: -11.424174, mean_eps: 0.050000\n",
            " 167331/200000: episode: 594, duration: 3.555s, episode steps: 165, steps per second:  46, episode reward: -164.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.139 [0.000, 2.000],  loss: 0.144033, mae: 7.758080, mean_q: -11.463631, mean_eps: 0.050000\n",
            " 167556/200000: episode: 595, duration: 4.481s, episode steps: 225, steps per second:  50, episode reward: -224.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.849 [0.000, 2.000],  loss: 0.169363, mae: 7.759120, mean_q: -11.460334, mean_eps: 0.050000\n",
            " 167746/200000: episode: 596, duration: 4.389s, episode steps: 190, steps per second:  43, episode reward: -189.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.989 [0.000, 2.000],  loss: 0.183196, mae: 7.754322, mean_q: -11.449895, mean_eps: 0.050000\n",
            " 168064/200000: episode: 597, duration: 6.987s, episode steps: 318, steps per second:  46, episode reward: -317.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.972 [0.000, 2.000],  loss: 0.169295, mae: 7.750190, mean_q: -11.445704, mean_eps: 0.050000\n",
            " 168226/200000: episode: 598, duration: 3.321s, episode steps: 162, steps per second:  49, episode reward: -161.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.864 [0.000, 2.000],  loss: 0.163482, mae: 7.736007, mean_q: -11.423051, mean_eps: 0.050000\n",
            " 168338/200000: episode: 599, duration: 2.684s, episode steps: 112, steps per second:  42, episode reward: -111.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.027 [0.000, 2.000],  loss: 0.146987, mae: 7.739861, mean_q: -11.437953, mean_eps: 0.050000\n",
            " 168469/200000: episode: 600, duration: 3.191s, episode steps: 131, steps per second:  41, episode reward: -130.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.153 [0.000, 2.000],  loss: 0.151858, mae: 7.747905, mean_q: -11.444742, mean_eps: 0.050000\n",
            " 168938/200000: episode: 601, duration: 9.589s, episode steps: 469, steps per second:  49, episode reward: -468.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.158 [0.000, 2.000],  loss: 0.152415, mae: 7.734933, mean_q: -11.430030, mean_eps: 0.050000\n",
            " 169136/200000: episode: 602, duration: 4.783s, episode steps: 198, steps per second:  41, episode reward: -197.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.904 [0.000, 2.000],  loss: 0.172732, mae: 7.772677, mean_q: -11.478721, mean_eps: 0.050000\n",
            " 169313/200000: episode: 603, duration: 3.495s, episode steps: 177, steps per second:  51, episode reward: -176.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.254 [0.000, 2.000],  loss: 0.148241, mae: 7.786468, mean_q: -11.504662, mean_eps: 0.050000\n",
            " 169681/200000: episode: 604, duration: 8.418s, episode steps: 368, steps per second:  44, episode reward: -367.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.033 [0.000, 2.000],  loss: 0.175049, mae: 7.792280, mean_q: -11.509996, mean_eps: 0.050000\n",
            " 169895/200000: episode: 605, duration: 4.237s, episode steps: 214, steps per second:  51, episode reward: -213.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.168184, mae: 7.783083, mean_q: -11.495152, mean_eps: 0.050000\n",
            " 170040/200000: episode: 606, duration: 2.906s, episode steps: 145, steps per second:  50, episode reward: -144.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.952 [0.000, 2.000],  loss: 0.155074, mae: 7.796185, mean_q: -11.520456, mean_eps: 0.050000\n",
            " 170151/200000: episode: 607, duration: 2.418s, episode steps: 111, steps per second:  46, episode reward: -110.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.099 [0.000, 2.000],  loss: 0.194079, mae: 7.777756, mean_q: -11.480865, mean_eps: 0.050000\n",
            " 170283/200000: episode: 608, duration: 3.577s, episode steps: 132, steps per second:  37, episode reward: -131.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.879 [0.000, 2.000],  loss: 0.156859, mae: 7.781959, mean_q: -11.496479, mean_eps: 0.050000\n",
            " 170393/200000: episode: 609, duration: 2.167s, episode steps: 110, steps per second:  51, episode reward: -109.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.982 [0.000, 2.000],  loss: 0.163752, mae: 7.773145, mean_q: -11.480140, mean_eps: 0.050000\n",
            " 170521/200000: episode: 610, duration: 2.495s, episode steps: 128, steps per second:  51, episode reward: -127.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.938 [0.000, 2.000],  loss: 0.155801, mae: 7.775523, mean_q: -11.482553, mean_eps: 0.050000\n",
            " 170634/200000: episode: 611, duration: 2.241s, episode steps: 113, steps per second:  50, episode reward: -112.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.956 [0.000, 2.000],  loss: 0.151939, mae: 7.775579, mean_q: -11.482441, mean_eps: 0.050000\n",
            " 170862/200000: episode: 612, duration: 5.320s, episode steps: 228, steps per second:  43, episode reward: -227.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.035 [0.000, 2.000],  loss: 0.165958, mae: 7.781343, mean_q: -11.494808, mean_eps: 0.050000\n",
            " 170972/200000: episode: 613, duration: 2.449s, episode steps: 110, steps per second:  45, episode reward: -109.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.264 [0.000, 2.000],  loss: 0.134800, mae: 7.778698, mean_q: -11.493226, mean_eps: 0.050000\n",
            " 171183/200000: episode: 614, duration: 4.317s, episode steps: 211, steps per second:  49, episode reward: -210.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.161 [0.000, 2.000],  loss: 0.166494, mae: 7.781654, mean_q: -11.499789, mean_eps: 0.050000\n",
            " 171625/200000: episode: 615, duration: 9.877s, episode steps: 442, steps per second:  45, episode reward: -441.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.864 [0.000, 2.000],  loss: 0.172078, mae: 7.772868, mean_q: -11.483800, mean_eps: 0.050000\n",
            " 171806/200000: episode: 616, duration: 3.737s, episode steps: 181, steps per second:  48, episode reward: -180.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.155 [0.000, 2.000],  loss: 0.174914, mae: 7.779962, mean_q: -11.493452, mean_eps: 0.050000\n",
            " 172088/200000: episode: 617, duration: 6.492s, episode steps: 282, steps per second:  43, episode reward: -281.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.064 [0.000, 2.000],  loss: 0.161491, mae: 7.788646, mean_q: -11.511210, mean_eps: 0.050000\n",
            " 172588/200000: episode: 618, duration: 9.997s, episode steps: 500, steps per second:  50, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.436 [0.000, 2.000],  loss: 0.154114, mae: 7.816159, mean_q: -11.549628, mean_eps: 0.050000\n",
            " 173088/200000: episode: 619, duration: 11.062s, episode steps: 500, steps per second:  45, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.630 [0.000, 2.000],  loss: 0.152447, mae: 7.818287, mean_q: -11.556356, mean_eps: 0.050000\n",
            " 173588/200000: episode: 620, duration: 11.038s, episode steps: 500, steps per second:  45, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.796 [0.000, 2.000],  loss: 0.158009, mae: 7.820919, mean_q: -11.558853, mean_eps: 0.050000\n",
            " 174088/200000: episode: 621, duration: 11.106s, episode steps: 500, steps per second:  45, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.618 [0.000, 2.000],  loss: 0.150646, mae: 7.821804, mean_q: -11.562410, mean_eps: 0.050000\n",
            " 174432/200000: episode: 622, duration: 6.725s, episode steps: 344, steps per second:  51, episode reward: -343.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.619 [0.000, 2.000],  loss: 0.155505, mae: 7.852753, mean_q: -11.605389, mean_eps: 0.050000\n",
            " 174653/200000: episode: 623, duration: 5.363s, episode steps: 221, steps per second:  41, episode reward: -220.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.801 [0.000, 2.000],  loss: 0.172401, mae: 7.860996, mean_q: -11.620217, mean_eps: 0.050000\n",
            " 175044/200000: episode: 624, duration: 8.076s, episode steps: 391, steps per second:  48, episode reward: -390.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.870 [0.000, 2.000],  loss: 0.159725, mae: 7.862613, mean_q: -11.626287, mean_eps: 0.050000\n",
            " 175544/200000: episode: 625, duration: 11.383s, episode steps: 500, steps per second:  44, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.804 [0.000, 2.000],  loss: 0.154206, mae: 7.885091, mean_q: -11.657064, mean_eps: 0.050000\n",
            " 176044/200000: episode: 626, duration: 11.690s, episode steps: 500, steps per second:  43, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.866 [0.000, 2.000],  loss: 0.154837, mae: 7.884691, mean_q: -11.658243, mean_eps: 0.050000\n",
            " 176274/200000: episode: 627, duration: 5.064s, episode steps: 230, steps per second:  45, episode reward: -229.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.043 [0.000, 2.000],  loss: 0.186655, mae: 7.875219, mean_q: -11.637242, mean_eps: 0.050000\n",
            " 176756/200000: episode: 628, duration: 10.423s, episode steps: 482, steps per second:  46, episode reward: -481.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.826 [0.000, 2.000],  loss: 0.154987, mae: 7.880566, mean_q: -11.651522, mean_eps: 0.050000\n",
            " 177134/200000: episode: 629, duration: 8.799s, episode steps: 378, steps per second:  43, episode reward: -377.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.944 [0.000, 2.000],  loss: 0.170659, mae: 7.891037, mean_q: -11.665832, mean_eps: 0.050000\n",
            " 177276/200000: episode: 630, duration: 2.939s, episode steps: 142, steps per second:  48, episode reward: -141.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.014 [0.000, 2.000],  loss: 0.152341, mae: 7.912149, mean_q: -11.695614, mean_eps: 0.050000\n",
            " 177738/200000: episode: 631, duration: 10.575s, episode steps: 462, steps per second:  44, episode reward: -461.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.054 [0.000, 2.000],  loss: 0.171771, mae: 7.909363, mean_q: -11.689894, mean_eps: 0.050000\n",
            " 177910/200000: episode: 632, duration: 3.434s, episode steps: 172, steps per second:  50, episode reward: -171.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.093 [0.000, 2.000],  loss: 0.147593, mae: 7.916282, mean_q: -11.708612, mean_eps: 0.050000\n",
            " 178146/200000: episode: 633, duration: 5.145s, episode steps: 236, steps per second:  46, episode reward: -235.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.941 [0.000, 2.000],  loss: 0.153356, mae: 7.906765, mean_q: -11.691801, mean_eps: 0.050000\n",
            " 178252/200000: episode: 634, duration: 2.613s, episode steps: 106, steps per second:  41, episode reward: -105.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.170 [0.000, 2.000],  loss: 0.131100, mae: 7.895712, mean_q: -11.676814, mean_eps: 0.050000\n",
            " 178390/200000: episode: 635, duration: 2.827s, episode steps: 138, steps per second:  49, episode reward: -137.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.149883, mae: 7.908749, mean_q: -11.691627, mean_eps: 0.050000\n",
            " 178631/200000: episode: 636, duration: 5.055s, episode steps: 241, steps per second:  48, episode reward: -240.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.871 [0.000, 2.000],  loss: 0.142753, mae: 7.895182, mean_q: -11.671952, mean_eps: 0.050000\n",
            " 178809/200000: episode: 637, duration: 4.821s, episode steps: 178, steps per second:  37, episode reward: -177.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.230 [0.000, 2.000],  loss: 0.177032, mae: 7.890043, mean_q: -11.660389, mean_eps: 0.050000\n",
            " 178986/200000: episode: 638, duration: 3.743s, episode steps: 177, steps per second:  47, episode reward: -176.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.989 [0.000, 2.000],  loss: 0.153536, mae: 7.899191, mean_q: -11.681580, mean_eps: 0.050000\n",
            " 179121/200000: episode: 639, duration: 2.777s, episode steps: 135, steps per second:  49, episode reward: -134.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.141 [0.000, 2.000],  loss: 0.135633, mae: 7.908897, mean_q: -11.695904, mean_eps: 0.050000\n",
            " 179245/200000: episode: 640, duration: 2.546s, episode steps: 124, steps per second:  49, episode reward: -123.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.129 [0.000, 2.000],  loss: 0.180933, mae: 7.905491, mean_q: -11.686105, mean_eps: 0.050000\n",
            " 179378/200000: episode: 641, duration: 3.792s, episode steps: 133, steps per second:  35, episode reward: -132.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.744 [0.000, 2.000],  loss: 0.143412, mae: 7.892431, mean_q: -11.666688, mean_eps: 0.050000\n",
            " 179496/200000: episode: 642, duration: 2.502s, episode steps: 118, steps per second:  47, episode reward: -117.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.076 [0.000, 2.000],  loss: 0.146330, mae: 7.905484, mean_q: -11.689802, mean_eps: 0.050000\n",
            " 179632/200000: episode: 643, duration: 2.847s, episode steps: 136, steps per second:  48, episode reward: -135.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.037 [0.000, 2.000],  loss: 0.160362, mae: 7.906029, mean_q: -11.688146, mean_eps: 0.050000\n",
            " 179752/200000: episode: 644, duration: 2.412s, episode steps: 120, steps per second:  50, episode reward: -119.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.033 [0.000, 2.000],  loss: 0.156279, mae: 7.907388, mean_q: -11.688423, mean_eps: 0.050000\n",
            " 179872/200000: episode: 645, duration: 2.611s, episode steps: 120, steps per second:  46, episode reward: -119.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.958 [0.000, 2.000],  loss: 0.146497, mae: 7.912341, mean_q: -11.701197, mean_eps: 0.050000\n",
            " 180042/200000: episode: 646, duration: 4.646s, episode steps: 170, steps per second:  37, episode reward: -169.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.047 [0.000, 2.000],  loss: 0.153930, mae: 7.911390, mean_q: -11.700193, mean_eps: 0.050000\n",
            " 180232/200000: episode: 647, duration: 3.933s, episode steps: 190, steps per second:  48, episode reward: -189.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.942 [0.000, 2.000],  loss: 0.177122, mae: 7.907566, mean_q: -11.689740, mean_eps: 0.050000\n",
            " 180386/200000: episode: 648, duration: 3.256s, episode steps: 154, steps per second:  47, episode reward: -153.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.948 [0.000, 2.000],  loss: 0.171951, mae: 7.905477, mean_q: -11.687768, mean_eps: 0.050000\n",
            " 180655/200000: episode: 649, duration: 6.475s, episode steps: 269, steps per second:  42, episode reward: -268.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.130 [0.000, 2.000],  loss: 0.152281, mae: 7.908704, mean_q: -11.696741, mean_eps: 0.050000\n",
            " 180792/200000: episode: 650, duration: 2.881s, episode steps: 137, steps per second:  48, episode reward: -136.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.095 [0.000, 2.000],  loss: 0.146070, mae: 7.915025, mean_q: -11.703825, mean_eps: 0.050000\n",
            " 180939/200000: episode: 651, duration: 3.000s, episode steps: 147, steps per second:  49, episode reward: -146.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.109 [0.000, 2.000],  loss: 0.160385, mae: 7.903186, mean_q: -11.682802, mean_eps: 0.050000\n",
            " 181168/200000: episode: 652, duration: 5.624s, episode steps: 229, steps per second:  41, episode reward: -228.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.956 [0.000, 2.000],  loss: 0.130337, mae: 7.890359, mean_q: -11.667438, mean_eps: 0.050000\n",
            " 181339/200000: episode: 653, duration: 3.491s, episode steps: 171, steps per second:  49, episode reward: -170.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.971 [0.000, 2.000],  loss: 0.129933, mae: 7.884116, mean_q: -11.661434, mean_eps: 0.050000\n",
            " 181712/200000: episode: 654, duration: 7.584s, episode steps: 373, steps per second:  49, episode reward: -372.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.772 [0.000, 2.000],  loss: 0.144195, mae: 7.886619, mean_q: -11.666501, mean_eps: 0.050000\n",
            " 181820/200000: episode: 655, duration: 2.927s, episode steps: 108, steps per second:  37, episode reward: -107.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.167 [0.000, 2.000],  loss: 0.138220, mae: 7.890100, mean_q: -11.672463, mean_eps: 0.050000\n",
            " 182117/200000: episode: 656, duration: 5.928s, episode steps: 297, steps per second:  50, episode reward: -296.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.630 [0.000, 2.000],  loss: 0.140884, mae: 7.889146, mean_q: -11.668742, mean_eps: 0.050000\n",
            " 182270/200000: episode: 657, duration: 3.036s, episode steps: 153, steps per second:  50, episode reward: -152.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.176 [0.000, 2.000],  loss: 0.160042, mae: 7.897344, mean_q: -11.679067, mean_eps: 0.050000\n",
            " 182429/200000: episode: 658, duration: 4.310s, episode steps: 159, steps per second:  37, episode reward: -158.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.931 [0.000, 2.000],  loss: 0.136169, mae: 7.909971, mean_q: -11.701930, mean_eps: 0.050000\n",
            " 182660/200000: episode: 659, duration: 4.723s, episode steps: 231, steps per second:  49, episode reward: -230.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.571 [0.000, 2.000],  loss: 0.134571, mae: 7.904447, mean_q: -11.694279, mean_eps: 0.050000\n",
            " 182831/200000: episode: 660, duration: 3.608s, episode steps: 171, steps per second:  47, episode reward: -170.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.871 [0.000, 2.000],  loss: 0.139599, mae: 7.892242, mean_q: -11.673142, mean_eps: 0.050000\n",
            " 182951/200000: episode: 661, duration: 2.876s, episode steps: 120, steps per second:  42, episode reward: -119.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.675 [0.000, 2.000],  loss: 0.149093, mae: 7.897567, mean_q: -11.677572, mean_eps: 0.050000\n",
            " 183056/200000: episode: 662, duration: 2.756s, episode steps: 105, steps per second:  38, episode reward: -104.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.124 [0.000, 2.000],  loss: 0.146873, mae: 7.901782, mean_q: -11.682674, mean_eps: 0.050000\n",
            " 183151/200000: episode: 663, duration: 1.938s, episode steps:  95, steps per second:  49, episode reward: -94.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.863 [0.000, 2.000],  loss: 0.125682, mae: 7.905083, mean_q: -11.691531, mean_eps: 0.050000\n",
            " 183303/200000: episode: 664, duration: 3.109s, episode steps: 152, steps per second:  49, episode reward: -151.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.691 [0.000, 2.000],  loss: 0.150238, mae: 7.903713, mean_q: -11.685484, mean_eps: 0.050000\n",
            " 183412/200000: episode: 665, duration: 2.257s, episode steps: 109, steps per second:  48, episode reward: -108.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.670 [0.000, 2.000],  loss: 0.144918, mae: 7.913020, mean_q: -11.698833, mean_eps: 0.050000\n",
            " 183518/200000: episode: 666, duration: 2.326s, episode steps: 106, steps per second:  46, episode reward: -105.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.915 [0.000, 2.000],  loss: 0.143417, mae: 7.911009, mean_q: -11.702996, mean_eps: 0.050000\n",
            " 183632/200000: episode: 667, duration: 3.144s, episode steps: 114, steps per second:  36, episode reward: -113.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.956 [0.000, 2.000],  loss: 0.167780, mae: 7.899494, mean_q: -11.674081, mean_eps: 0.050000\n",
            " 183799/200000: episode: 668, duration: 3.364s, episode steps: 167, steps per second:  50, episode reward: -166.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.874 [0.000, 2.000],  loss: 0.123697, mae: 7.913495, mean_q: -11.703133, mean_eps: 0.050000\n",
            " 183929/200000: episode: 669, duration: 2.689s, episode steps: 130, steps per second:  48, episode reward: -129.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.815 [0.000, 2.000],  loss: 0.170396, mae: 7.910453, mean_q: -11.689553, mean_eps: 0.050000\n",
            " 184018/200000: episode: 670, duration: 1.814s, episode steps:  89, steps per second:  49, episode reward: -88.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.247 [0.000, 2.000],  loss: 0.146217, mae: 7.915488, mean_q: -11.709702, mean_eps: 0.050000\n",
            " 184174/200000: episode: 671, duration: 3.682s, episode steps: 156, steps per second:  42, episode reward: -155.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.929 [0.000, 2.000],  loss: 0.153702, mae: 7.924453, mean_q: -11.715515, mean_eps: 0.050000\n",
            " 184355/200000: episode: 672, duration: 4.165s, episode steps: 181, steps per second:  43, episode reward: -180.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.834 [0.000, 2.000],  loss: 0.149136, mae: 7.924775, mean_q: -11.711988, mean_eps: 0.050000\n",
            " 184451/200000: episode: 673, duration: 2.024s, episode steps:  96, steps per second:  47, episode reward: -95.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.979 [0.000, 2.000],  loss: 0.146504, mae: 7.910630, mean_q: -11.691420, mean_eps: 0.050000\n",
            " 184598/200000: episode: 674, duration: 2.891s, episode steps: 147, steps per second:  51, episode reward: -146.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.395 [0.000, 2.000],  loss: 0.161522, mae: 7.926490, mean_q: -11.712680, mean_eps: 0.050000\n",
            " 184699/200000: episode: 675, duration: 2.076s, episode steps: 101, steps per second:  49, episode reward: -100.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.960 [0.000, 2.000],  loss: 0.158164, mae: 7.930144, mean_q: -11.725705, mean_eps: 0.050000\n",
            " 184831/200000: episode: 676, duration: 3.526s, episode steps: 132, steps per second:  37, episode reward: -131.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.076 [0.000, 2.000],  loss: 0.139807, mae: 7.934816, mean_q: -11.730597, mean_eps: 0.050000\n",
            " 184970/200000: episode: 677, duration: 2.922s, episode steps: 139, steps per second:  48, episode reward: -138.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.165 [0.000, 2.000],  loss: 0.145051, mae: 7.922194, mean_q: -11.706498, mean_eps: 0.050000\n",
            " 185091/200000: episode: 678, duration: 2.479s, episode steps: 121, steps per second:  49, episode reward: -120.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.074 [0.000, 2.000],  loss: 0.139035, mae: 7.935755, mean_q: -11.732399, mean_eps: 0.050000\n",
            " 185219/200000: episode: 679, duration: 2.668s, episode steps: 128, steps per second:  48, episode reward: -127.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.938 [0.000, 2.000],  loss: 0.146640, mae: 7.937082, mean_q: -11.735878, mean_eps: 0.050000\n",
            " 185423/200000: episode: 680, duration: 5.172s, episode steps: 204, steps per second:  39, episode reward: -203.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.078 [0.000, 2.000],  loss: 0.189026, mae: 7.932588, mean_q: -11.724655, mean_eps: 0.050000\n",
            " 185601/200000: episode: 681, duration: 3.825s, episode steps: 178, steps per second:  47, episode reward: -177.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.904 [0.000, 2.000],  loss: 0.180699, mae: 7.928617, mean_q: -11.718826, mean_eps: 0.050000\n",
            " 185770/200000: episode: 682, duration: 3.528s, episode steps: 169, steps per second:  48, episode reward: -168.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.609 [0.000, 2.000],  loss: 0.131134, mae: 7.940785, mean_q: -11.736973, mean_eps: 0.050000\n",
            " 185865/200000: episode: 683, duration: 2.018s, episode steps:  95, steps per second:  47, episode reward: -94.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.863 [0.000, 2.000],  loss: 0.115989, mae: 7.946659, mean_q: -11.755487, mean_eps: 0.050000\n",
            " 185986/200000: episode: 684, duration: 3.084s, episode steps: 121, steps per second:  39, episode reward: -120.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.934 [0.000, 2.000],  loss: 0.158022, mae: 7.940555, mean_q: -11.739644, mean_eps: 0.050000\n",
            " 186176/200000: episode: 685, duration: 4.412s, episode steps: 190, steps per second:  43, episode reward: -189.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.821 [0.000, 2.000],  loss: 0.146892, mae: 7.960764, mean_q: -11.768819, mean_eps: 0.050000\n",
            " 186316/200000: episode: 686, duration: 3.258s, episode steps: 140, steps per second:  43, episode reward: -139.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.029 [0.000, 2.000],  loss: 0.177520, mae: 7.963336, mean_q: -11.766365, mean_eps: 0.050000\n",
            " 186459/200000: episode: 687, duration: 2.961s, episode steps: 143, steps per second:  48, episode reward: -142.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.699 [0.000, 2.000],  loss: 0.189473, mae: 7.972540, mean_q: -11.780285, mean_eps: 0.050000\n",
            " 186644/200000: episode: 688, duration: 4.848s, episode steps: 185, steps per second:  38, episode reward: -184.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.827 [0.000, 2.000],  loss: 0.157795, mae: 7.972495, mean_q: -11.785745, mean_eps: 0.050000\n",
            " 186850/200000: episode: 689, duration: 4.321s, episode steps: 206, steps per second:  48, episode reward: -205.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.796 [0.000, 2.000],  loss: 0.142203, mae: 7.970776, mean_q: -11.785741, mean_eps: 0.050000\n",
            " 186985/200000: episode: 690, duration: 2.816s, episode steps: 135, steps per second:  48, episode reward: -134.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.644 [0.000, 2.000],  loss: 0.150840, mae: 7.974737, mean_q: -11.792909, mean_eps: 0.050000\n",
            " 187138/200000: episode: 691, duration: 3.956s, episode steps: 153, steps per second:  39, episode reward: -152.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.895 [0.000, 2.000],  loss: 0.137760, mae: 8.050581, mean_q: -11.900750, mean_eps: 0.050000\n",
            " 187245/200000: episode: 692, duration: 2.652s, episode steps: 107, steps per second:  40, episode reward: -106.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.925 [0.000, 2.000],  loss: 0.174055, mae: 8.063110, mean_q: -11.921657, mean_eps: 0.050000\n",
            " 187407/200000: episode: 693, duration: 3.321s, episode steps: 162, steps per second:  49, episode reward: -161.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.735 [0.000, 2.000],  loss: 0.175279, mae: 8.062104, mean_q: -11.913779, mean_eps: 0.050000\n",
            " 187713/200000: episode: 694, duration: 6.855s, episode steps: 306, steps per second:  45, episode reward: -305.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.680 [0.000, 2.000],  loss: 0.154598, mae: 8.056723, mean_q: -11.912398, mean_eps: 0.050000\n",
            " 187888/200000: episode: 695, duration: 4.290s, episode steps: 175, steps per second:  41, episode reward: -174.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.914 [0.000, 2.000],  loss: 0.176186, mae: 8.058431, mean_q: -11.913325, mean_eps: 0.050000\n",
            " 188004/200000: episode: 696, duration: 2.562s, episode steps: 116, steps per second:  45, episode reward: -115.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.362 [0.000, 2.000],  loss: 0.162817, mae: 8.065444, mean_q: -11.923737, mean_eps: 0.050000\n",
            " 188139/200000: episode: 697, duration: 3.017s, episode steps: 135, steps per second:  45, episode reward: -134.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.274 [0.000, 2.000],  loss: 0.170563, mae: 8.045052, mean_q: -11.893477, mean_eps: 0.050000\n",
            " 188285/200000: episode: 698, duration: 3.432s, episode steps: 146, steps per second:  43, episode reward: -145.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.884 [0.000, 2.000],  loss: 0.174573, mae: 8.049860, mean_q: -11.897874, mean_eps: 0.050000\n",
            " 188430/200000: episode: 699, duration: 3.705s, episode steps: 145, steps per second:  39, episode reward: -144.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.662 [0.000, 2.000],  loss: 0.162561, mae: 8.037233, mean_q: -11.876848, mean_eps: 0.050000\n",
            " 188581/200000: episode: 700, duration: 3.209s, episode steps: 151, steps per second:  47, episode reward: -150.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.172 [0.000, 2.000],  loss: 0.146102, mae: 8.058954, mean_q: -11.914570, mean_eps: 0.050000\n",
            " 188709/200000: episode: 701, duration: 2.682s, episode steps: 128, steps per second:  48, episode reward: -127.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.102 [0.000, 2.000],  loss: 0.193238, mae: 8.036825, mean_q: -11.877699, mean_eps: 0.050000\n",
            " 188894/200000: episode: 702, duration: 4.371s, episode steps: 185, steps per second:  42, episode reward: -184.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.903 [0.000, 2.000],  loss: 0.147050, mae: 8.044486, mean_q: -11.895169, mean_eps: 0.050000\n",
            " 189041/200000: episode: 703, duration: 3.497s, episode steps: 147, steps per second:  42, episode reward: -146.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.986 [0.000, 2.000],  loss: 0.169056, mae: 8.054813, mean_q: -11.909162, mean_eps: 0.050000\n",
            " 189203/200000: episode: 704, duration: 3.395s, episode steps: 162, steps per second:  48, episode reward: -161.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.031 [0.000, 2.000],  loss: 0.140415, mae: 8.072762, mean_q: -11.941267, mean_eps: 0.050000\n",
            " 189495/200000: episode: 705, duration: 7.002s, episode steps: 292, steps per second:  42, episode reward: -291.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.788 [0.000, 2.000],  loss: 0.166628, mae: 8.065748, mean_q: -11.923683, mean_eps: 0.050000\n",
            " 189743/200000: episode: 706, duration: 5.597s, episode steps: 248, steps per second:  44, episode reward: -247.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.625 [0.000, 2.000],  loss: 0.164545, mae: 8.073546, mean_q: -11.935101, mean_eps: 0.050000\n",
            " 189908/200000: episode: 707, duration: 3.425s, episode steps: 165, steps per second:  48, episode reward: -164.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.824 [0.000, 2.000],  loss: 0.184615, mae: 8.073698, mean_q: -11.929674, mean_eps: 0.050000\n",
            " 190065/200000: episode: 708, duration: 3.845s, episode steps: 157, steps per second:  41, episode reward: -156.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.210 [0.000, 2.000],  loss: 0.167426, mae: 8.076828, mean_q: -11.936619, mean_eps: 0.050000\n",
            " 190240/200000: episode: 709, duration: 4.235s, episode steps: 175, steps per second:  41, episode reward: -174.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.537 [0.000, 2.000],  loss: 0.157184, mae: 8.091033, mean_q: -11.953063, mean_eps: 0.050000\n",
            " 190351/200000: episode: 710, duration: 2.425s, episode steps: 111, steps per second:  46, episode reward: -110.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.172479, mae: 8.091764, mean_q: -11.962428, mean_eps: 0.050000\n",
            " 190457/200000: episode: 711, duration: 2.187s, episode steps: 106, steps per second:  48, episode reward: -105.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.689 [0.000, 2.000],  loss: 0.176117, mae: 8.075515, mean_q: -11.921179, mean_eps: 0.050000\n",
            " 190604/200000: episode: 712, duration: 3.385s, episode steps: 147, steps per second:  43, episode reward: -146.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.687 [0.000, 2.000],  loss: 0.178210, mae: 8.082868, mean_q: -11.944704, mean_eps: 0.050000\n",
            " 190720/200000: episode: 713, duration: 3.346s, episode steps: 116, steps per second:  35, episode reward: -115.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.181 [0.000, 2.000],  loss: 0.134142, mae: 8.091543, mean_q: -11.964144, mean_eps: 0.050000\n",
            " 190818/200000: episode: 714, duration: 2.009s, episode steps:  98, steps per second:  49, episode reward: -97.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.969 [0.000, 2.000],  loss: 0.161291, mae: 8.082747, mean_q: -11.943924, mean_eps: 0.050000\n",
            " 190935/200000: episode: 715, duration: 2.307s, episode steps: 117, steps per second:  51, episode reward: -116.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.718 [0.000, 2.000],  loss: 0.135773, mae: 8.091432, mean_q: -11.963986, mean_eps: 0.050000\n",
            " 191065/200000: episode: 716, duration: 2.648s, episode steps: 130, steps per second:  49, episode reward: -129.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.785 [0.000, 2.000],  loss: 0.160592, mae: 8.112396, mean_q: -11.991733, mean_eps: 0.050000\n",
            " 191214/200000: episode: 717, duration: 3.071s, episode steps: 149, steps per second:  49, episode reward: -148.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.859 [0.000, 2.000],  loss: 0.154167, mae: 8.123628, mean_q: -12.009870, mean_eps: 0.050000\n",
            " 191370/200000: episode: 718, duration: 4.314s, episode steps: 156, steps per second:  36, episode reward: -155.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.699 [0.000, 2.000],  loss: 0.145933, mae: 8.129175, mean_q: -12.019679, mean_eps: 0.050000\n",
            " 191446/200000: episode: 719, duration: 1.743s, episode steps:  76, steps per second:  44, episode reward: -75.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.092 [0.000, 2.000],  loss: 0.151118, mae: 8.137405, mean_q: -12.020316, mean_eps: 0.050000\n",
            " 191570/200000: episode: 720, duration: 2.637s, episode steps: 124, steps per second:  47, episode reward: -123.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.887 [0.000, 2.000],  loss: 0.153570, mae: 8.137104, mean_q: -12.028491, mean_eps: 0.050000\n",
            " 191696/200000: episode: 721, duration: 2.683s, episode steps: 126, steps per second:  47, episode reward: -125.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.810 [0.000, 2.000],  loss: 0.179638, mae: 8.132623, mean_q: -12.018493, mean_eps: 0.050000\n",
            " 191844/200000: episode: 722, duration: 3.775s, episode steps: 148, steps per second:  39, episode reward: -147.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.750 [0.000, 2.000],  loss: 0.169912, mae: 8.133973, mean_q: -12.018388, mean_eps: 0.050000\n",
            " 191948/200000: episode: 723, duration: 2.638s, episode steps: 104, steps per second:  39, episode reward: -103.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.663 [0.000, 2.000],  loss: 0.137995, mae: 8.129579, mean_q: -12.019157, mean_eps: 0.050000\n",
            " 192100/200000: episode: 724, duration: 3.560s, episode steps: 152, steps per second:  43, episode reward: -151.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.401 [0.000, 2.000],  loss: 0.161886, mae: 8.137594, mean_q: -12.024969, mean_eps: 0.050000\n",
            " 192217/200000: episode: 725, duration: 2.645s, episode steps: 117, steps per second:  44, episode reward: -116.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.009 [0.000, 2.000],  loss: 0.146146, mae: 8.152011, mean_q: -12.048320, mean_eps: 0.050000\n",
            " 192329/200000: episode: 726, duration: 2.532s, episode steps: 112, steps per second:  44, episode reward: -111.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.205 [0.000, 2.000],  loss: 0.145878, mae: 8.151175, mean_q: -12.047393, mean_eps: 0.050000\n",
            " 192449/200000: episode: 727, duration: 3.531s, episode steps: 120, steps per second:  34, episode reward: -119.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.975 [0.000, 2.000],  loss: 0.130205, mae: 8.163046, mean_q: -12.074402, mean_eps: 0.050000\n",
            " 192615/200000: episode: 728, duration: 3.445s, episode steps: 166, steps per second:  48, episode reward: -165.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.892 [0.000, 2.000],  loss: 0.156841, mae: 8.145888, mean_q: -12.041132, mean_eps: 0.050000\n",
            " 192730/200000: episode: 729, duration: 2.753s, episode steps: 115, steps per second:  42, episode reward: -114.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.017 [0.000, 2.000],  loss: 0.165696, mae: 8.150069, mean_q: -12.046172, mean_eps: 0.050000\n",
            " 192819/200000: episode: 730, duration: 2.003s, episode steps:  89, steps per second:  44, episode reward: -88.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.101 [0.000, 2.000],  loss: 0.185890, mae: 8.151343, mean_q: -12.047740, mean_eps: 0.050000\n",
            " 192961/200000: episode: 731, duration: 3.702s, episode steps: 142, steps per second:  38, episode reward: -141.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.711 [0.000, 2.000],  loss: 0.142823, mae: 8.159137, mean_q: -12.065935, mean_eps: 0.050000\n",
            " 193045/200000: episode: 732, duration: 2.273s, episode steps:  84, steps per second:  37, episode reward: -83.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.012 [0.000, 2.000],  loss: 0.156866, mae: 8.170314, mean_q: -12.081878, mean_eps: 0.050000\n",
            " 193143/200000: episode: 733, duration: 2.293s, episode steps:  98, steps per second:  43, episode reward: -97.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.929 [0.000, 2.000],  loss: 0.151772, mae: 8.195624, mean_q: -12.121336, mean_eps: 0.050000\n",
            " 193238/200000: episode: 734, duration: 2.132s, episode steps:  95, steps per second:  45, episode reward: -94.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.295 [0.000, 2.000],  loss: 0.192036, mae: 8.182625, mean_q: -12.091503, mean_eps: 0.050000\n",
            " 193357/200000: episode: 735, duration: 2.487s, episode steps: 119, steps per second:  48, episode reward: -118.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.882 [0.000, 2.000],  loss: 0.138998, mae: 8.193729, mean_q: -12.113565, mean_eps: 0.050000\n",
            " 193492/200000: episode: 736, duration: 2.941s, episode steps: 135, steps per second:  46, episode reward: -134.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.963 [0.000, 2.000],  loss: 0.112100, mae: 8.187138, mean_q: -12.109475, mean_eps: 0.050000\n",
            " 193598/200000: episode: 737, duration: 2.970s, episode steps: 106, steps per second:  36, episode reward: -105.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.057 [0.000, 2.000],  loss: 0.133385, mae: 8.192819, mean_q: -12.116962, mean_eps: 0.050000\n",
            " 193764/200000: episode: 738, duration: 3.350s, episode steps: 166, steps per second:  50, episode reward: -165.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.952 [0.000, 2.000],  loss: 0.134069, mae: 8.190338, mean_q: -12.105834, mean_eps: 0.050000\n",
            " 193864/200000: episode: 739, duration: 2.101s, episode steps: 100, steps per second:  48, episode reward: -99.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.110 [0.000, 2.000],  loss: 0.155204, mae: 8.190110, mean_q: -12.106111, mean_eps: 0.050000\n",
            " 193977/200000: episode: 740, duration: 2.544s, episode steps: 113, steps per second:  44, episode reward: -112.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.894 [0.000, 2.000],  loss: 0.158265, mae: 8.193210, mean_q: -12.115114, mean_eps: 0.050000\n",
            " 194075/200000: episode: 741, duration: 2.352s, episode steps:  98, steps per second:  42, episode reward: -97.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.847 [0.000, 2.000],  loss: 0.174556, mae: 8.221688, mean_q: -12.156619, mean_eps: 0.050000\n",
            " 194256/200000: episode: 742, duration: 4.529s, episode steps: 181, steps per second:  40, episode reward: -180.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.459 [0.000, 2.000],  loss: 0.139554, mae: 8.238541, mean_q: -12.179101, mean_eps: 0.050000\n",
            " 194397/200000: episode: 743, duration: 3.030s, episode steps: 141, steps per second:  47, episode reward: -140.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.234 [0.000, 2.000],  loss: 0.126970, mae: 8.242579, mean_q: -12.194466, mean_eps: 0.050000\n",
            " 194507/200000: episode: 744, duration: 2.227s, episode steps: 110, steps per second:  49, episode reward: -109.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.882 [0.000, 2.000],  loss: 0.137091, mae: 8.230287, mean_q: -12.172802, mean_eps: 0.050000\n",
            " 194604/200000: episode: 745, duration: 2.001s, episode steps:  97, steps per second:  48, episode reward: -96.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.082 [0.000, 2.000],  loss: 0.122324, mae: 8.226038, mean_q: -12.168475, mean_eps: 0.050000\n",
            " 194700/200000: episode: 746, duration: 2.526s, episode steps:  96, steps per second:  38, episode reward: -95.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.167 [0.000, 2.000],  loss: 0.149985, mae: 8.246067, mean_q: -12.193177, mean_eps: 0.050000\n",
            " 194992/200000: episode: 747, duration: 6.520s, episode steps: 292, steps per second:  45, episode reward: -291.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.349 [0.000, 2.000],  loss: 0.154249, mae: 8.231379, mean_q: -12.173662, mean_eps: 0.050000\n",
            " 195156/200000: episode: 748, duration: 3.324s, episode steps: 164, steps per second:  49, episode reward: -163.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.024 [0.000, 2.000],  loss: 0.153058, mae: 8.293518, mean_q: -12.271681, mean_eps: 0.050000\n",
            " 195306/200000: episode: 749, duration: 3.347s, episode steps: 150, steps per second:  45, episode reward: -149.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.220 [0.000, 2.000],  loss: 0.146799, mae: 8.284878, mean_q: -12.256352, mean_eps: 0.050000\n",
            " 195419/200000: episode: 750, duration: 3.020s, episode steps: 113, steps per second:  37, episode reward: -112.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.195 [0.000, 2.000],  loss: 0.130443, mae: 8.296191, mean_q: -12.274305, mean_eps: 0.050000\n",
            " 195545/200000: episode: 751, duration: 2.710s, episode steps: 126, steps per second:  46, episode reward: -125.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.159996, mae: 8.286085, mean_q: -12.255521, mean_eps: 0.050000\n",
            " 195718/200000: episode: 752, duration: 3.645s, episode steps: 173, steps per second:  47, episode reward: -172.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.133 [0.000, 2.000],  loss: 0.153933, mae: 8.280871, mean_q: -12.249004, mean_eps: 0.050000\n",
            " 195910/200000: episode: 753, duration: 4.814s, episode steps: 192, steps per second:  40, episode reward: -191.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.375 [0.000, 2.000],  loss: 0.125151, mae: 8.281710, mean_q: -12.254551, mean_eps: 0.050000\n",
            " 196027/200000: episode: 754, duration: 2.869s, episode steps: 117, steps per second:  41, episode reward: -116.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.162 [0.000, 2.000],  loss: 0.165819, mae: 8.274654, mean_q: -12.233822, mean_eps: 0.050000\n",
            " 196147/200000: episode: 755, duration: 2.475s, episode steps: 120, steps per second:  48, episode reward: -119.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.233 [0.000, 2.000],  loss: 0.176647, mae: 8.248578, mean_q: -12.199442, mean_eps: 0.050000\n",
            " 196294/200000: episode: 756, duration: 3.116s, episode steps: 147, steps per second:  47, episode reward: -146.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.884 [0.000, 2.000],  loss: 0.158049, mae: 8.245253, mean_q: -12.193713, mean_eps: 0.050000\n",
            " 196430/200000: episode: 757, duration: 2.741s, episode steps: 136, steps per second:  50, episode reward: -135.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.074 [0.000, 2.000],  loss: 0.164741, mae: 8.247372, mean_q: -12.197703, mean_eps: 0.050000\n",
            " 196593/200000: episode: 758, duration: 4.328s, episode steps: 163, steps per second:  38, episode reward: -162.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.006 [0.000, 2.000],  loss: 0.136877, mae: 8.253246, mean_q: -12.212110, mean_eps: 0.050000\n",
            " 196770/200000: episode: 759, duration: 3.517s, episode steps: 177, steps per second:  50, episode reward: -176.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.994 [0.000, 2.000],  loss: 0.152890, mae: 8.251763, mean_q: -12.204394, mean_eps: 0.050000\n",
            " 196893/200000: episode: 760, duration: 2.505s, episode steps: 123, steps per second:  49, episode reward: -122.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.106 [0.000, 2.000],  loss: 0.151643, mae: 8.242040, mean_q: -12.192368, mean_eps: 0.050000\n",
            " 197006/200000: episode: 761, duration: 2.263s, episode steps: 113, steps per second:  50, episode reward: -112.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.159 [0.000, 2.000],  loss: 0.143938, mae: 8.256399, mean_q: -12.207383, mean_eps: 0.050000\n",
            " 197179/200000: episode: 762, duration: 4.585s, episode steps: 173, steps per second:  38, episode reward: -172.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.780 [0.000, 2.000],  loss: 0.177578, mae: 8.325383, mean_q: -12.309531, mean_eps: 0.050000\n",
            " 197312/200000: episode: 763, duration: 2.776s, episode steps: 133, steps per second:  48, episode reward: -132.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.165 [0.000, 2.000],  loss: 0.196940, mae: 8.305929, mean_q: -12.277037, mean_eps: 0.050000\n",
            " 197450/200000: episode: 764, duration: 2.783s, episode steps: 138, steps per second:  50, episode reward: -137.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.217 [0.000, 2.000],  loss: 0.171012, mae: 8.309800, mean_q: -12.288872, mean_eps: 0.050000\n",
            " 197569/200000: episode: 765, duration: 2.453s, episode steps: 119, steps per second:  49, episode reward: -118.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.017 [0.000, 2.000],  loss: 0.166566, mae: 8.304958, mean_q: -12.280717, mean_eps: 0.050000\n",
            " 197686/200000: episode: 766, duration: 2.608s, episode steps: 117, steps per second:  45, episode reward: -116.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.923 [0.000, 2.000],  loss: 0.156535, mae: 8.326380, mean_q: -12.318696, mean_eps: 0.050000\n",
            " 197837/200000: episode: 767, duration: 3.912s, episode steps: 151, steps per second:  39, episode reward: -150.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.980 [0.000, 2.000],  loss: 0.139523, mae: 8.313999, mean_q: -12.295921, mean_eps: 0.050000\n",
            " 197986/200000: episode: 768, duration: 3.080s, episode steps: 149, steps per second:  48, episode reward: -148.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.987 [0.000, 2.000],  loss: 0.194355, mae: 8.311997, mean_q: -12.286483, mean_eps: 0.050000\n",
            " 198106/200000: episode: 769, duration: 2.523s, episode steps: 120, steps per second:  48, episode reward: -119.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.083 [0.000, 2.000],  loss: 0.210978, mae: 8.307828, mean_q: -12.272863, mean_eps: 0.050000\n",
            " 198228/200000: episode: 770, duration: 2.672s, episode steps: 122, steps per second:  46, episode reward: -121.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.156 [0.000, 2.000],  loss: 0.179484, mae: 8.314497, mean_q: -12.294166, mean_eps: 0.050000\n",
            " 198358/200000: episode: 771, duration: 3.552s, episode steps: 130, steps per second:  37, episode reward: -129.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.900 [0.000, 2.000],  loss: 0.161448, mae: 8.310610, mean_q: -12.293624, mean_eps: 0.050000\n",
            " 198488/200000: episode: 772, duration: 2.789s, episode steps: 130, steps per second:  47, episode reward: -129.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.977 [0.000, 2.000],  loss: 0.148661, mae: 8.306040, mean_q: -12.280486, mean_eps: 0.050000\n",
            " 198655/200000: episode: 773, duration: 3.621s, episode steps: 167, steps per second:  46, episode reward: -166.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.737 [0.000, 2.000],  loss: 0.174252, mae: 8.301516, mean_q: -12.270457, mean_eps: 0.050000\n",
            " 198871/200000: episode: 774, duration: 4.749s, episode steps: 216, steps per second:  45, episode reward: -215.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.912 [0.000, 2.000],  loss: 0.158770, mae: 8.314610, mean_q: -12.291198, mean_eps: 0.050000\n",
            " 199001/200000: episode: 775, duration: 3.447s, episode steps: 130, steps per second:  38, episode reward: -129.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.915 [0.000, 2.000],  loss: 0.172123, mae: 8.303810, mean_q: -12.272303, mean_eps: 0.050000\n",
            " 199098/200000: episode: 776, duration: 1.957s, episode steps:  97, steps per second:  50, episode reward: -96.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.979 [0.000, 2.000],  loss: 0.156946, mae: 8.378608, mean_q: -12.386921, mean_eps: 0.050000\n",
            " 199333/200000: episode: 777, duration: 5.058s, episode steps: 235, steps per second:  46, episode reward: -234.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.009 [0.000, 2.000],  loss: 0.189388, mae: 8.381902, mean_q: -12.388558, mean_eps: 0.050000\n",
            " 199429/200000: episode: 778, duration: 2.137s, episode steps:  96, steps per second:  45, episode reward: -95.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.146 [0.000, 2.000],  loss: 0.147847, mae: 8.390010, mean_q: -12.400065, mean_eps: 0.050000\n",
            " 199536/200000: episode: 779, duration: 3.161s, episode steps: 107, steps per second:  34, episode reward: -106.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.215 [0.000, 2.000],  loss: 0.172389, mae: 8.389028, mean_q: -12.395804, mean_eps: 0.050000\n",
            " 199624/200000: episode: 780, duration: 2.036s, episode steps:  88, steps per second:  43, episode reward: -87.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.125 [0.000, 2.000],  loss: 0.163788, mae: 8.375518, mean_q: -12.377754, mean_eps: 0.050000\n",
            " 199766/200000: episode: 781, duration: 2.968s, episode steps: 142, steps per second:  48, episode reward: -141.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.838 [0.000, 2.000],  loss: 0.213454, mae: 8.373247, mean_q: -12.366404, mean_eps: 0.050000\n",
            " 199934/200000: episode: 782, duration: 3.435s, episode steps: 168, steps per second:  49, episode reward: -167.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.833 [0.000, 2.000],  loss: 0.168985, mae: 8.371882, mean_q: -12.373800, mean_eps: 0.050000\n",
            "done, took 3817.038 seconds\n",
            "\n",
            "Evaluando Mejora 3...\n",
            "Mejora 3 → Recompensa media: -110.90 ± 15.85\n",
            "\n",
            "Entrenando Base...\n",
            "Training for 50000 steps ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n",
            "/usr/local/lib/python3.11/dist-packages/rl/memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
            "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   500/50000: episode: 1, duration: 6.944s, episode steps: 500, steps per second:  72, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.938 [0.000, 2.000],  loss: 0.039460, mae: 0.558532, mean_q: -0.678986, mean_eps: 0.989200\n",
            "  1000/50000: episode: 2, duration: 5.509s, episode steps: 500, steps per second:  91, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.001984, mae: 0.565045, mean_q: -0.793312, mean_eps: 0.973018\n",
            "  1500/50000: episode: 3, duration: 4.564s, episode steps: 500, steps per second: 110, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.038 [0.000, 2.000],  loss: 0.009374, mae: 1.223209, mean_q: -1.765256, mean_eps: 0.955018\n",
            "  2000/50000: episode: 4, duration: 4.463s, episode steps: 500, steps per second: 112, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.952 [0.000, 2.000],  loss: 0.001604, mae: 1.216785, mean_q: -1.781800, mean_eps: 0.937018\n",
            "  2500/50000: episode: 5, duration: 5.488s, episode steps: 500, steps per second:  91, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.968 [0.000, 2.000],  loss: 0.009640, mae: 1.901566, mean_q: -2.796497, mean_eps: 0.919018\n",
            "  3000/50000: episode: 6, duration: 4.468s, episode steps: 500, steps per second: 112, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.003824, mae: 1.901114, mean_q: -2.815105, mean_eps: 0.901018\n",
            "  3500/50000: episode: 7, duration: 4.316s, episode steps: 500, steps per second: 116, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.974 [0.000, 2.000],  loss: 0.013131, mae: 2.590528, mean_q: -3.831956, mean_eps: 0.883018\n",
            "  4000/50000: episode: 8, duration: 5.495s, episode steps: 500, steps per second:  91, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.068 [0.000, 2.000],  loss: 0.007530, mae: 2.584048, mean_q: -3.837156, mean_eps: 0.865018\n",
            "  4500/50000: episode: 9, duration: 4.397s, episode steps: 500, steps per second: 114, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.984 [0.000, 2.000],  loss: 0.019684, mae: 3.267273, mean_q: -4.841223, mean_eps: 0.847018\n",
            "  5000/50000: episode: 10, duration: 4.830s, episode steps: 500, steps per second: 104, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.028 [0.000, 2.000],  loss: 0.014169, mae: 3.261831, mean_q: -4.843684, mean_eps: 0.829018\n",
            "  5500/50000: episode: 11, duration: 5.559s, episode steps: 500, steps per second:  90, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.998 [0.000, 2.000],  loss: 0.028061, mae: 3.955430, mean_q: -5.863342, mean_eps: 0.811018\n",
            "  6000/50000: episode: 12, duration: 4.496s, episode steps: 500, steps per second: 111, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.992 [0.000, 2.000],  loss: 0.024105, mae: 3.950452, mean_q: -5.864051, mean_eps: 0.793018\n",
            "  6500/50000: episode: 13, duration: 5.137s, episode steps: 500, steps per second:  97, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.038 [0.000, 2.000],  loss: 0.035869, mae: 4.621949, mean_q: -6.853890, mean_eps: 0.775018\n",
            "  7000/50000: episode: 14, duration: 5.075s, episode steps: 500, steps per second:  99, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.036 [0.000, 2.000],  loss: 0.038878, mae: 4.608833, mean_q: -6.835914, mean_eps: 0.757018\n",
            "  7500/50000: episode: 15, duration: 4.489s, episode steps: 500, steps per second: 111, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.902 [0.000, 2.000],  loss: 0.057557, mae: 5.283053, mean_q: -7.831528, mean_eps: 0.739018\n",
            "  8000/50000: episode: 16, duration: 5.681s, episode steps: 500, steps per second:  88, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.862 [0.000, 2.000],  loss: 0.050597, mae: 5.273757, mean_q: -7.826160, mean_eps: 0.721018\n",
            "  8500/50000: episode: 17, duration: 4.544s, episode steps: 500, steps per second: 110, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.838 [0.000, 2.000],  loss: 0.059621, mae: 5.933817, mean_q: -8.803498, mean_eps: 0.703018\n",
            "  9000/50000: episode: 18, duration: 4.468s, episode steps: 500, steps per second: 112, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.830 [0.000, 2.000],  loss: 0.060484, mae: 5.928953, mean_q: -8.798882, mean_eps: 0.685018\n",
            "  9451/50000: episode: 19, duration: 5.114s, episode steps: 451, steps per second:  88, episode reward: -450.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.925 [0.000, 2.000],  loss: 0.073941, mae: 6.584730, mean_q: -9.770110, mean_eps: 0.667900\n",
            "  9951/50000: episode: 20, duration: 4.420s, episode steps: 500, steps per second: 113, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.872 [0.000, 2.000],  loss: 0.077405, mae: 6.568429, mean_q: -9.751729, mean_eps: 0.650782\n",
            " 10451/50000: episode: 21, duration: 4.643s, episode steps: 500, steps per second: 108, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.884 [0.000, 2.000],  loss: 0.097910, mae: 7.057082, mean_q: -10.469626, mean_eps: 0.632782\n",
            " 10892/50000: episode: 22, duration: 5.365s, episode steps: 441, steps per second:  82, episode reward: -440.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.097782, mae: 7.098325, mean_q: -10.534981, mean_eps: 0.615844\n",
            " 11392/50000: episode: 23, duration: 4.680s, episode steps: 500, steps per second: 107, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.974 [0.000, 2.000],  loss: 0.102288, mae: 7.527416, mean_q: -11.168000, mean_eps: 0.598906\n",
            " 11892/50000: episode: 24, duration: 4.717s, episode steps: 500, steps per second: 106, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.938 [0.000, 2.000],  loss: 0.122819, mae: 7.635188, mean_q: -11.321364, mean_eps: 0.580906\n",
            " 12294/50000: episode: 25, duration: 4.303s, episode steps: 402, steps per second:  93, episode reward: -401.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.995 [0.000, 2.000],  loss: 0.142245, mae: 8.023815, mean_q: -11.885592, mean_eps: 0.564670\n",
            " 12724/50000: episode: 26, duration: 3.862s, episode steps: 430, steps per second: 111, episode reward: -429.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.949 [0.000, 2.000],  loss: 0.138179, mae: 8.155698, mean_q: -12.088863, mean_eps: 0.549694\n",
            " 13119/50000: episode: 27, duration: 3.517s, episode steps: 395, steps per second: 112, episode reward: -394.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.863 [0.000, 2.000],  loss: 0.157058, mae: 8.310636, mean_q: -12.312854, mean_eps: 0.534844\n",
            " 13537/50000: episode: 28, duration: 4.808s, episode steps: 418, steps per second:  87, episode reward: -417.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.890 [0.000, 2.000],  loss: 0.149489, mae: 8.700589, mean_q: -12.894347, mean_eps: 0.520210\n",
            " 14037/50000: episode: 29, duration: 4.597s, episode steps: 500, steps per second: 109, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.884 [0.000, 2.000],  loss: 0.141146, mae: 8.723108, mean_q: -12.926059, mean_eps: 0.503686\n",
            " 14409/50000: episode: 30, duration: 3.477s, episode steps: 372, steps per second: 107, episode reward: -371.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.895 [0.000, 2.000],  loss: 0.156140, mae: 9.242080, mean_q: -13.699853, mean_eps: 0.487990\n",
            " 14629/50000: episode: 31, duration: 2.310s, episode steps: 220, steps per second:  95, episode reward: -219.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.764 [0.000, 2.000],  loss: 0.183073, mae: 9.225104, mean_q: -13.668296, mean_eps: 0.477334\n",
            " 15129/50000: episode: 32, duration: 5.443s, episode steps: 500, steps per second:  92, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.660 [0.000, 2.000],  loss: 0.196370, mae: 9.336819, mean_q: -13.821670, mean_eps: 0.464374\n",
            " 15356/50000: episode: 33, duration: 2.004s, episode steps: 227, steps per second: 113, episode reward: -226.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.872 [0.000, 2.000],  loss: 0.213448, mae: 9.674805, mean_q: -14.317594, mean_eps: 0.451288\n",
            " 15636/50000: episode: 34, duration: 2.446s, episode steps: 280, steps per second: 114, episode reward: -279.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.843 [0.000, 2.000],  loss: 0.166306, mae: 9.666959, mean_q: -14.316987, mean_eps: 0.442162\n",
            " 15870/50000: episode: 35, duration: 2.089s, episode steps: 234, steps per second: 112, episode reward: -233.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.897 [0.000, 2.000],  loss: 0.197990, mae: 9.651135, mean_q: -14.286518, mean_eps: 0.432910\n",
            " 16162/50000: episode: 36, duration: 3.279s, episode steps: 292, steps per second:  89, episode reward: -291.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.880 [0.000, 2.000],  loss: 0.202843, mae: 9.924693, mean_q: -14.678529, mean_eps: 0.423442\n",
            " 16449/50000: episode: 37, duration: 3.062s, episode steps: 287, steps per second:  94, episode reward: -286.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.944 [0.000, 2.000],  loss: 0.227797, mae: 10.129910, mean_q: -14.982126, mean_eps: 0.413020\n",
            " 16677/50000: episode: 38, duration: 2.059s, episode steps: 228, steps per second: 111, episode reward: -227.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.855 [0.000, 2.000],  loss: 0.237418, mae: 10.134878, mean_q: -14.990758, mean_eps: 0.403750\n",
            " 16916/50000: episode: 39, duration: 2.171s, episode steps: 239, steps per second: 110, episode reward: -238.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.895 [0.000, 2.000],  loss: 0.209357, mae: 10.129574, mean_q: -14.986566, mean_eps: 0.395344\n",
            " 17249/50000: episode: 40, duration: 3.076s, episode steps: 333, steps per second: 108, episode reward: -332.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.850 [0.000, 2.000],  loss: 0.254765, mae: 10.529532, mean_q: -15.555499, mean_eps: 0.385048\n",
            " 17449/50000: episode: 41, duration: 2.306s, episode steps: 200, steps per second:  87, episode reward: -199.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.735 [0.000, 2.000],  loss: 0.174571, mae: 10.659327, mean_q: -15.771853, mean_eps: 0.375454\n",
            " 17608/50000: episode: 42, duration: 2.226s, episode steps: 159, steps per second:  71, episode reward: -158.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.881 [0.000, 2.000],  loss: 0.205299, mae: 10.646386, mean_q: -15.748246, mean_eps: 0.368992\n",
            " 17741/50000: episode: 43, duration: 1.280s, episode steps: 133, steps per second: 104, episode reward: -132.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.910 [0.000, 2.000],  loss: 0.223386, mae: 10.643103, mean_q: -15.737077, mean_eps: 0.363736\n",
            " 17981/50000: episode: 44, duration: 2.300s, episode steps: 240, steps per second: 104, episode reward: -239.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.967 [0.000, 2.000],  loss: 0.204150, mae: 10.642773, mean_q: -15.738762, mean_eps: 0.357022\n",
            " 18194/50000: episode: 45, duration: 1.965s, episode steps: 213, steps per second: 108, episode reward: -212.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.901 [0.000, 2.000],  loss: 0.261798, mae: 11.086064, mean_q: -16.367883, mean_eps: 0.348868\n",
            " 18475/50000: episode: 46, duration: 2.647s, episode steps: 281, steps per second: 106, episode reward: -280.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.715 [0.000, 2.000],  loss: 0.253158, mae: 11.123278, mean_q: -16.429267, mean_eps: 0.339976\n",
            " 18632/50000: episode: 47, duration: 1.397s, episode steps: 157, steps per second: 112, episode reward: -156.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.057 [0.000, 2.000],  loss: 0.219149, mae: 11.107544, mean_q: -16.417010, mean_eps: 0.332092\n",
            " 18828/50000: episode: 48, duration: 2.382s, episode steps: 196, steps per second:  82, episode reward: -195.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.995 [0.000, 2.000],  loss: 0.231985, mae: 11.098153, mean_q: -16.405488, mean_eps: 0.325738\n",
            " 19043/50000: episode: 49, duration: 2.388s, episode steps: 215, steps per second:  90, episode reward: -214.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.963 [0.000, 2.000],  loss: 0.231879, mae: 11.188053, mean_q: -16.529544, mean_eps: 0.318340\n",
            " 19285/50000: episode: 50, duration: 2.103s, episode steps: 242, steps per second: 115, episode reward: -241.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.888 [0.000, 2.000],  loss: 0.250174, mae: 11.545749, mean_q: -17.050569, mean_eps: 0.310114\n",
            " 19493/50000: episode: 51, duration: 1.877s, episode steps: 208, steps per second: 111, episode reward: -207.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.053 [0.000, 2.000],  loss: 0.286617, mae: 11.547570, mean_q: -17.049189, mean_eps: 0.302014\n",
            " 19645/50000: episode: 52, duration: 1.421s, episode steps: 152, steps per second: 107, episode reward: -151.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.901 [0.000, 2.000],  loss: 0.240136, mae: 11.541877, mean_q: -17.049425, mean_eps: 0.295534\n",
            " 19850/50000: episode: 53, duration: 1.884s, episode steps: 205, steps per second: 109, episode reward: -204.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.815 [0.000, 2.000],  loss: 0.235654, mae: 11.550913, mean_q: -17.065622, mean_eps: 0.289108\n",
            " 19998/50000: episode: 54, duration: 1.426s, episode steps: 148, steps per second: 104, episode reward: -147.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.946 [0.000, 2.000],  loss: 0.284275, mae: 11.536542, mean_q: -17.023527, mean_eps: 0.282754\n",
            " 20153/50000: episode: 55, duration: 1.938s, episode steps: 155, steps per second:  80, episode reward: -154.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.852 [0.000, 2.000],  loss: 0.313081, mae: 11.970101, mean_q: -17.656081, mean_eps: 0.277300\n",
            " 20313/50000: episode: 56, duration: 2.246s, episode steps: 160, steps per second:  71, episode reward: -159.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.038 [0.000, 2.000],  loss: 0.263614, mae: 11.959592, mean_q: -17.656084, mean_eps: 0.271630\n",
            " 20562/50000: episode: 57, duration: 2.448s, episode steps: 249, steps per second: 102, episode reward: -248.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.992 [0.000, 2.000],  loss: 0.286553, mae: 11.966536, mean_q: -17.655308, mean_eps: 0.264268\n",
            " 20768/50000: episode: 58, duration: 1.937s, episode steps: 206, steps per second: 106, episode reward: -205.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.956 [0.000, 2.000],  loss: 0.263720, mae: 11.958445, mean_q: -17.637580, mean_eps: 0.256078\n",
            " 21037/50000: episode: 59, duration: 2.603s, episode steps: 269, steps per second: 103, episode reward: -268.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.866 [0.000, 2.000],  loss: 0.311204, mae: 12.032788, mean_q: -17.738520, mean_eps: 0.247528\n",
            " 21249/50000: episode: 60, duration: 2.042s, episode steps: 212, steps per second: 104, episode reward: -211.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.863 [0.000, 2.000],  loss: 0.348643, mae: 12.440140, mean_q: -18.320077, mean_eps: 0.238870\n",
            " 21397/50000: episode: 61, duration: 1.557s, episode steps: 148, steps per second:  95, episode reward: -147.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.959 [0.000, 2.000],  loss: 0.367385, mae: 12.440526, mean_q: -18.321243, mean_eps: 0.232390\n",
            " 21650/50000: episode: 62, duration: 3.473s, episode steps: 253, steps per second:  73, episode reward: -252.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.830 [0.000, 2.000],  loss: 0.315229, mae: 12.444744, mean_q: -18.332186, mean_eps: 0.225172\n",
            " 21821/50000: episode: 63, duration: 1.560s, episode steps: 171, steps per second: 110, episode reward: -170.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.994 [0.000, 2.000],  loss: 0.377085, mae: 12.469064, mean_q: -18.369952, mean_eps: 0.217540\n",
            " 22023/50000: episode: 64, duration: 1.878s, episode steps: 202, steps per second: 108, episode reward: -201.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.832 [0.000, 2.000],  loss: 0.306186, mae: 12.517391, mean_q: -18.451489, mean_eps: 0.210826\n",
            " 22285/50000: episode: 65, duration: 2.423s, episode steps: 262, steps per second: 108, episode reward: -261.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.714 [0.000, 2.000],  loss: 0.373957, mae: 12.955620, mean_q: -19.074755, mean_eps: 0.202474\n",
            " 22424/50000: episode: 66, duration: 1.360s, episode steps: 139, steps per second: 102, episode reward: -138.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.698 [0.000, 2.000],  loss: 0.413119, mae: 12.924298, mean_q: -19.004367, mean_eps: 0.195256\n",
            " 22632/50000: episode: 67, duration: 2.010s, episode steps: 208, steps per second: 103, episode reward: -207.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.995 [0.000, 2.000],  loss: 0.385893, mae: 12.923206, mean_q: -19.004763, mean_eps: 0.189010\n",
            " 22746/50000: episode: 68, duration: 1.373s, episode steps: 114, steps per second:  83, episode reward: -113.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.930 [0.000, 2.000],  loss: 0.358505, mae: 12.936554, mean_q: -19.036487, mean_eps: 0.183214\n",
            " 22899/50000: episode: 69, duration: 2.280s, episode steps: 153, steps per second:  67, episode reward: -152.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.987 [0.000, 2.000],  loss: 0.387074, mae: 12.903755, mean_q: -18.978368, mean_eps: 0.178408\n",
            " 23017/50000: episode: 70, duration: 1.486s, episode steps: 118, steps per second:  79, episode reward: -117.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.797 [0.000, 2.000],  loss: 0.396444, mae: 12.969911, mean_q: -19.064457, mean_eps: 0.173530\n",
            " 23152/50000: episode: 71, duration: 1.305s, episode steps: 135, steps per second: 103, episode reward: -134.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.822 [0.000, 2.000],  loss: 0.477864, mae: 13.382151, mean_q: -19.657255, mean_eps: 0.168976\n",
            " 23281/50000: episode: 72, duration: 1.205s, episode steps: 129, steps per second: 107, episode reward: -128.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.109 [0.000, 2.000],  loss: 0.473875, mae: 13.370836, mean_q: -19.633939, mean_eps: 0.164224\n",
            " 23491/50000: episode: 73, duration: 1.988s, episode steps: 210, steps per second: 106, episode reward: -209.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.110 [0.000, 2.000],  loss: 0.470031, mae: 13.358673, mean_q: -19.616091, mean_eps: 0.158122\n",
            " 23626/50000: episode: 74, duration: 1.287s, episode steps: 135, steps per second: 105, episode reward: -134.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.252 [0.000, 2.000],  loss: 0.415079, mae: 13.371359, mean_q: -19.644815, mean_eps: 0.151912\n",
            " 23765/50000: episode: 75, duration: 1.339s, episode steps: 139, steps per second: 104, episode reward: -138.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.878 [0.000, 2.000],  loss: 0.453859, mae: 13.368579, mean_q: -19.635578, mean_eps: 0.146980\n",
            " 23918/50000: episode: 76, duration: 1.472s, episode steps: 153, steps per second: 104, episode reward: -152.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.850 [0.000, 2.000],  loss: 0.441463, mae: 13.359974, mean_q: -19.627948, mean_eps: 0.141724\n",
            " 24098/50000: episode: 77, duration: 2.028s, episode steps: 180, steps per second:  89, episode reward: -179.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.256 [0.000, 2.000],  loss: 0.482429, mae: 13.586425, mean_q: -19.944825, mean_eps: 0.135730\n",
            " 24270/50000: episode: 78, duration: 2.410s, episode steps: 172, steps per second:  71, episode reward: -171.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.521363, mae: 13.755976, mean_q: -20.180445, mean_eps: 0.129394\n",
            " 24381/50000: episode: 79, duration: 1.058s, episode steps: 111, steps per second: 105, episode reward: -110.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.189 [0.000, 2.000],  loss: 0.524535, mae: 13.734878, mean_q: -20.142273, mean_eps: 0.124300\n",
            " 24548/50000: episode: 80, duration: 1.508s, episode steps: 167, steps per second: 111, episode reward: -166.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.970 [0.000, 2.000],  loss: 0.484738, mae: 13.765688, mean_q: -20.194369, mean_eps: 0.119296\n",
            " 24712/50000: episode: 81, duration: 1.531s, episode steps: 164, steps per second: 107, episode reward: -163.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.104 [0.000, 2.000],  loss: 0.493892, mae: 13.745192, mean_q: -20.164368, mean_eps: 0.113338\n",
            " 24830/50000: episode: 82, duration: 1.080s, episode steps: 118, steps per second: 109, episode reward: -117.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.178 [0.000, 2.000],  loss: 0.502202, mae: 13.722906, mean_q: -20.147876, mean_eps: 0.108262\n",
            " 24995/50000: episode: 83, duration: 1.474s, episode steps: 165, steps per second: 112, episode reward: -164.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.055 [0.000, 2.000],  loss: 0.505366, mae: 13.752728, mean_q: -20.183673, mean_eps: 0.103168\n",
            " 25153/50000: episode: 84, duration: 1.410s, episode steps: 158, steps per second: 112, episode reward: -157.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.867 [0.000, 2.000],  loss: 0.609771, mae: 14.087063, mean_q: -20.660998, mean_eps: 0.100003\n",
            " 25309/50000: episode: 85, duration: 1.446s, episode steps: 156, steps per second: 108, episode reward: -155.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.128 [0.000, 2.000],  loss: 0.546903, mae: 14.069891, mean_q: -20.642650, mean_eps: 0.100000\n",
            " 25438/50000: episode: 86, duration: 1.387s, episode steps: 129, steps per second:  93, episode reward: -128.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.047 [0.000, 2.000],  loss: 0.540528, mae: 14.091597, mean_q: -20.688943, mean_eps: 0.100000\n",
            " 25597/50000: episode: 87, duration: 2.068s, episode steps: 159, steps per second:  77, episode reward: -158.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.025 [0.000, 2.000],  loss: 0.536421, mae: 14.093440, mean_q: -20.688037, mean_eps: 0.100000\n",
            " 25744/50000: episode: 88, duration: 1.598s, episode steps: 147, steps per second:  92, episode reward: -146.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.061 [0.000, 2.000],  loss: 0.505712, mae: 14.100215, mean_q: -20.686794, mean_eps: 0.100000\n",
            " 25907/50000: episode: 89, duration: 1.524s, episode steps: 163, steps per second: 107, episode reward: -162.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.012 [0.000, 2.000],  loss: 0.517003, mae: 14.077801, mean_q: -20.666380, mean_eps: 0.100000\n",
            " 26028/50000: episode: 90, duration: 1.106s, episode steps: 121, steps per second: 109, episode reward: -120.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.926 [0.000, 2.000],  loss: 0.512958, mae: 14.158770, mean_q: -20.789191, mean_eps: 0.100000\n",
            " 26185/50000: episode: 91, duration: 1.525s, episode steps: 157, steps per second: 103, episode reward: -156.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.064 [0.000, 2.000],  loss: 0.575366, mae: 14.478952, mean_q: -21.249372, mean_eps: 0.100000\n",
            " 26364/50000: episode: 92, duration: 1.624s, episode steps: 179, steps per second: 110, episode reward: -178.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.994 [0.000, 2.000],  loss: 0.604020, mae: 14.499728, mean_q: -21.273642, mean_eps: 0.100000\n",
            " 26520/50000: episode: 93, duration: 1.429s, episode steps: 156, steps per second: 109, episode reward: -155.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.019 [0.000, 2.000],  loss: 0.605617, mae: 14.484113, mean_q: -21.242244, mean_eps: 0.100000\n",
            " 26629/50000: episode: 94, duration: 0.986s, episode steps: 109, steps per second: 111, episode reward: -108.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.899 [0.000, 2.000],  loss: 0.591466, mae: 14.462538, mean_q: -21.200497, mean_eps: 0.100000\n",
            " 26786/50000: episode: 95, duration: 1.574s, episode steps: 157, steps per second: 100, episode reward: -156.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.064 [0.000, 2.000],  loss: 0.591024, mae: 14.469562, mean_q: -21.216271, mean_eps: 0.100000\n",
            " 26912/50000: episode: 96, duration: 1.678s, episode steps: 126, steps per second:  75, episode reward: -125.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.341 [0.000, 2.000],  loss: 0.583440, mae: 14.456779, mean_q: -21.201625, mean_eps: 0.100000\n",
            " 27075/50000: episode: 97, duration: 1.883s, episode steps: 163, steps per second:  87, episode reward: -162.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.785 [0.000, 2.000],  loss: 0.597882, mae: 14.735650, mean_q: -21.614101, mean_eps: 0.100000\n",
            " 27276/50000: episode: 98, duration: 1.755s, episode steps: 201, steps per second: 115, episode reward: -200.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.821 [0.000, 2.000],  loss: 0.611957, mae: 15.035303, mean_q: -22.057251, mean_eps: 0.100000\n",
            " 27445/50000: episode: 99, duration: 1.498s, episode steps: 169, steps per second: 113, episode reward: -168.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.006 [0.000, 2.000],  loss: 0.686259, mae: 15.016749, mean_q: -21.996238, mean_eps: 0.100000\n",
            " 27628/50000: episode: 100, duration: 1.620s, episode steps: 183, steps per second: 113, episode reward: -182.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.852 [0.000, 2.000],  loss: 0.723033, mae: 15.000566, mean_q: -21.974654, mean_eps: 0.100000\n",
            " 27820/50000: episode: 101, duration: 1.668s, episode steps: 192, steps per second: 115, episode reward: -191.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.062 [0.000, 2.000],  loss: 0.664592, mae: 15.006204, mean_q: -21.989062, mean_eps: 0.100000\n",
            " 27941/50000: episode: 102, duration: 1.076s, episode steps: 121, steps per second: 112, episode reward: -120.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.909 [0.000, 2.000],  loss: 0.649768, mae: 15.003874, mean_q: -21.996058, mean_eps: 0.100000\n",
            " 28080/50000: episode: 103, duration: 1.175s, episode steps: 139, steps per second: 118, episode reward: -138.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.144 [0.000, 2.000],  loss: 0.729606, mae: 15.231642, mean_q: -22.309348, mean_eps: 0.100000\n",
            " 28241/50000: episode: 104, duration: 1.743s, episode steps: 161, steps per second:  92, episode reward: -160.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.876 [0.000, 2.000],  loss: 0.795216, mae: 15.426571, mean_q: -22.552522, mean_eps: 0.100000\n",
            " 28489/50000: episode: 105, duration: 2.983s, episode steps: 248, steps per second:  83, episode reward: -247.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.984 [0.000, 2.000],  loss: 0.727289, mae: 15.409270, mean_q: -22.537201, mean_eps: 0.100000\n",
            " 28642/50000: episode: 106, duration: 1.429s, episode steps: 153, steps per second: 107, episode reward: -152.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.987 [0.000, 2.000],  loss: 0.664002, mae: 15.401812, mean_q: -22.545068, mean_eps: 0.100000\n",
            " 28832/50000: episode: 107, duration: 1.705s, episode steps: 190, steps per second: 111, episode reward: -189.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.989 [0.000, 2.000],  loss: 0.679692, mae: 15.418039, mean_q: -22.555209, mean_eps: 0.100000\n",
            " 29019/50000: episode: 108, duration: 1.661s, episode steps: 187, steps per second: 113, episode reward: -186.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.048 [0.000, 2.000],  loss: 0.702818, mae: 15.439263, mean_q: -22.584397, mean_eps: 0.100000\n",
            " 29213/50000: episode: 109, duration: 1.796s, episode steps: 194, steps per second: 108, episode reward: -193.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.928 [0.000, 2.000],  loss: 0.819300, mae: 15.790067, mean_q: -23.064015, mean_eps: 0.100000\n",
            " 29434/50000: episode: 110, duration: 1.989s, episode steps: 221, steps per second: 111, episode reward: -220.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.964 [0.000, 2.000],  loss: 0.769937, mae: 15.803112, mean_q: -23.095885, mean_eps: 0.100000\n",
            " 29603/50000: episode: 111, duration: 1.786s, episode steps: 169, steps per second:  95, episode reward: -168.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.012 [0.000, 2.000],  loss: 0.798711, mae: 15.808025, mean_q: -23.092065, mean_eps: 0.100000\n",
            " 29858/50000: episode: 112, duration: 3.113s, episode steps: 255, steps per second:  82, episode reward: -254.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.992 [0.000, 2.000],  loss: 0.739078, mae: 15.832921, mean_q: -23.138150, mean_eps: 0.100000\n",
            " 30053/50000: episode: 113, duration: 1.695s, episode steps: 195, steps per second: 115, episode reward: -194.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.979 [0.000, 2.000],  loss: 0.771602, mae: 15.999517, mean_q: -23.376511, mean_eps: 0.100000\n",
            " 30274/50000: episode: 114, duration: 1.930s, episode steps: 221, steps per second: 115, episode reward: -220.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.873 [0.000, 2.000],  loss: 0.886139, mae: 16.314180, mean_q: -23.826208, mean_eps: 0.100000\n",
            " 30499/50000: episode: 115, duration: 2.026s, episode steps: 225, steps per second: 111, episode reward: -224.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.036 [0.000, 2.000],  loss: 0.834415, mae: 16.302018, mean_q: -23.790680, mean_eps: 0.100000\n",
            " 30706/50000: episode: 116, duration: 1.926s, episode steps: 207, steps per second: 107, episode reward: -206.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.063 [0.000, 2.000],  loss: 0.863676, mae: 16.328020, mean_q: -23.835207, mean_eps: 0.100000\n",
            " 30955/50000: episode: 117, duration: 2.516s, episode steps: 249, steps per second:  99, episode reward: -248.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.835 [0.000, 2.000],  loss: 0.869743, mae: 16.306845, mean_q: -23.790193, mean_eps: 0.100000\n",
            " 31133/50000: episode: 118, duration: 2.422s, episode steps: 178, steps per second:  73, episode reward: -177.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.096 [0.000, 2.000],  loss: 0.842729, mae: 16.629769, mean_q: -24.263345, mean_eps: 0.100000\n",
            " 31386/50000: episode: 119, duration: 2.616s, episode steps: 253, steps per second:  97, episode reward: -252.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.123 [0.000, 2.000],  loss: 0.871593, mae: 16.730460, mean_q: -24.418673, mean_eps: 0.100000\n",
            " 31600/50000: episode: 120, duration: 1.979s, episode steps: 214, steps per second: 108, episode reward: -213.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.140 [0.000, 2.000],  loss: 0.780804, mae: 16.755859, mean_q: -24.454234, mean_eps: 0.100000\n",
            " 31748/50000: episode: 121, duration: 1.380s, episode steps: 148, steps per second: 107, episode reward: -147.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.169 [0.000, 2.000],  loss: 0.835674, mae: 16.747590, mean_q: -24.444988, mean_eps: 0.100000\n",
            " 31957/50000: episode: 122, duration: 1.961s, episode steps: 209, steps per second: 107, episode reward: -208.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.105 [0.000, 2.000],  loss: 0.781896, mae: 16.744687, mean_q: -24.428761, mean_eps: 0.100000\n",
            " 32148/50000: episode: 123, duration: 1.634s, episode steps: 191, steps per second: 117, episode reward: -190.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.314 [0.000, 2.000],  loss: 0.830160, mae: 17.056904, mean_q: -24.883792, mean_eps: 0.100000\n",
            " 32329/50000: episode: 124, duration: 1.881s, episode steps: 181, steps per second:  96, episode reward: -180.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.320 [0.000, 2.000],  loss: 0.758781, mae: 17.228149, mean_q: -25.162930, mean_eps: 0.100000\n",
            " 32498/50000: episode: 125, duration: 2.179s, episode steps: 169, steps per second:  78, episode reward: -168.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.331 [0.000, 2.000],  loss: 0.779699, mae: 17.215416, mean_q: -25.127092, mean_eps: 0.100000\n",
            " 32695/50000: episode: 126, duration: 1.961s, episode steps: 197, steps per second: 100, episode reward: -196.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.127 [0.000, 2.000],  loss: 0.716144, mae: 17.275654, mean_q: -25.235051, mean_eps: 0.100000\n",
            " 32847/50000: episode: 127, duration: 1.368s, episode steps: 152, steps per second: 111, episode reward: -151.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.066 [0.000, 2.000],  loss: 0.768880, mae: 17.257686, mean_q: -25.211322, mean_eps: 0.100000\n",
            " 33039/50000: episode: 128, duration: 1.724s, episode steps: 192, steps per second: 111, episode reward: -191.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.984 [0.000, 2.000],  loss: 0.782581, mae: 17.316566, mean_q: -25.267249, mean_eps: 0.100000\n",
            " 33212/50000: episode: 129, duration: 1.537s, episode steps: 173, steps per second: 113, episode reward: -172.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.329 [0.000, 2.000],  loss: 0.778410, mae: 17.611113, mean_q: -25.677989, mean_eps: 0.100000\n",
            " 33402/50000: episode: 130, duration: 1.679s, episode steps: 190, steps per second: 113, episode reward: -189.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.342 [0.000, 2.000],  loss: 0.837489, mae: 17.613150, mean_q: -25.677977, mean_eps: 0.100000\n",
            " 33584/50000: episode: 131, duration: 1.611s, episode steps: 182, steps per second: 113, episode reward: -181.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.242 [0.000, 2.000],  loss: 0.847725, mae: 17.599401, mean_q: -25.639790, mean_eps: 0.100000\n",
            " 33822/50000: episode: 132, duration: 2.748s, episode steps: 238, steps per second:  87, episode reward: -237.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.391 [0.000, 2.000],  loss: 0.771125, mae: 17.576841, mean_q: -25.601230, mean_eps: 0.100000\n",
            " 34049/50000: episode: 133, duration: 2.561s, episode steps: 227, steps per second:  89, episode reward: -226.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.480 [0.000, 2.000],  loss: 0.829183, mae: 17.672508, mean_q: -25.731531, mean_eps: 0.100000\n",
            " 34222/50000: episode: 134, duration: 1.561s, episode steps: 173, steps per second: 111, episode reward: -172.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.283 [0.000, 2.000],  loss: 0.797472, mae: 17.968384, mean_q: -26.169135, mean_eps: 0.100000\n",
            " 34464/50000: episode: 135, duration: 2.184s, episode steps: 242, steps per second: 111, episode reward: -241.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.450 [0.000, 2.000],  loss: 0.754860, mae: 18.019433, mean_q: -26.255988, mean_eps: 0.100000\n",
            " 34586/50000: episode: 136, duration: 1.106s, episode steps: 122, steps per second: 110, episode reward: -121.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.131 [0.000, 2.000],  loss: 0.745399, mae: 17.947742, mean_q: -26.133690, mean_eps: 0.100000\n",
            " 34783/50000: episode: 137, duration: 1.872s, episode steps: 197, steps per second: 105, episode reward: -196.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.168 [0.000, 2.000],  loss: 0.741897, mae: 17.996676, mean_q: -26.222691, mean_eps: 0.100000\n",
            " 34964/50000: episode: 138, duration: 1.858s, episode steps: 181, steps per second:  97, episode reward: -180.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.994 [0.000, 2.000],  loss: 0.772301, mae: 17.974012, mean_q: -26.207335, mean_eps: 0.100000\n",
            " 35205/50000: episode: 139, duration: 3.191s, episode steps: 241, steps per second:  76, episode reward: -240.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.228 [0.000, 2.000],  loss: 0.846215, mae: 18.306451, mean_q: -26.652638, mean_eps: 0.100000\n",
            " 35335/50000: episode: 140, duration: 1.559s, episode steps: 130, steps per second:  83, episode reward: -129.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.223 [0.000, 2.000],  loss: 0.737123, mae: 18.414106, mean_q: -26.836620, mean_eps: 0.100000\n",
            " 35511/50000: episode: 141, duration: 1.599s, episode steps: 176, steps per second: 110, episode reward: -175.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.210 [0.000, 2.000],  loss: 0.742788, mae: 18.396477, mean_q: -26.819309, mean_eps: 0.100000\n",
            " 35737/50000: episode: 142, duration: 2.068s, episode steps: 226, steps per second: 109, episode reward: -225.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.951 [0.000, 2.000],  loss: 0.749015, mae: 18.394760, mean_q: -26.810649, mean_eps: 0.100000\n",
            " 35931/50000: episode: 143, duration: 1.744s, episode steps: 194, steps per second: 111, episode reward: -193.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.088 [0.000, 2.000],  loss: 0.791436, mae: 18.320486, mean_q: -26.670582, mean_eps: 0.100000\n",
            " 36088/50000: episode: 144, duration: 1.475s, episode steps: 157, steps per second: 106, episode reward: -156.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.051 [0.000, 2.000],  loss: 0.865623, mae: 18.460947, mean_q: -26.859120, mean_eps: 0.100000\n",
            " 36287/50000: episode: 145, duration: 1.801s, episode steps: 199, steps per second: 110, episode reward: -198.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.151 [0.000, 2.000],  loss: 0.870246, mae: 18.561768, mean_q: -27.024353, mean_eps: 0.100000\n",
            " 36431/50000: episode: 146, duration: 1.596s, episode steps: 144, steps per second:  90, episode reward: -143.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.104 [0.000, 2.000],  loss: 0.953249, mae: 18.573015, mean_q: -27.053842, mean_eps: 0.100000\n",
            " 36654/50000: episode: 147, duration: 2.885s, episode steps: 223, steps per second:  77, episode reward: -222.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.964 [0.000, 2.000],  loss: 0.852365, mae: 18.582549, mean_q: -27.060653, mean_eps: 0.100000\n",
            " 36881/50000: episode: 148, duration: 2.127s, episode steps: 227, steps per second: 107, episode reward: -226.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.066 [0.000, 2.000],  loss: 0.832858, mae: 18.584517, mean_q: -27.081704, mean_eps: 0.100000\n",
            " 37058/50000: episode: 149, duration: 1.684s, episode steps: 177, steps per second: 105, episode reward: -176.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.927 [0.000, 2.000],  loss: 0.792957, mae: 18.680720, mean_q: -27.235039, mean_eps: 0.100000\n",
            " 37239/50000: episode: 150, duration: 1.538s, episode steps: 181, steps per second: 118, episode reward: -180.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.204 [0.000, 2.000],  loss: 0.814947, mae: 18.853789, mean_q: -27.515053, mean_eps: 0.100000\n",
            " 37513/50000: episode: 151, duration: 2.595s, episode steps: 274, steps per second: 106, episode reward: -273.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.033 [0.000, 2.000],  loss: 0.750479, mae: 18.901283, mean_q: -27.604444, mean_eps: 0.100000\n",
            " 37684/50000: episode: 152, duration: 1.506s, episode steps: 171, steps per second: 114, episode reward: -170.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.047 [0.000, 2.000],  loss: 0.757557, mae: 18.869940, mean_q: -27.559840, mean_eps: 0.100000\n",
            " 37870/50000: episode: 153, duration: 2.263s, episode steps: 186, steps per second:  82, episode reward: -185.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.177 [0.000, 2.000],  loss: 0.712332, mae: 18.853893, mean_q: -27.534211, mean_eps: 0.100000\n",
            " 38062/50000: episode: 154, duration: 2.197s, episode steps: 192, steps per second:  87, episode reward: -191.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.094 [0.000, 2.000],  loss: 0.745398, mae: 18.941406, mean_q: -27.663602, mean_eps: 0.100000\n",
            " 38253/50000: episode: 155, duration: 1.741s, episode steps: 191, steps per second: 110, episode reward: -190.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.089 [0.000, 2.000],  loss: 0.712315, mae: 19.167812, mean_q: -28.018097, mean_eps: 0.100000\n",
            " 38432/50000: episode: 156, duration: 1.550s, episode steps: 179, steps per second: 115, episode reward: -178.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.145 [0.000, 2.000],  loss: 0.781187, mae: 19.150801, mean_q: -27.989402, mean_eps: 0.100000\n",
            " 38685/50000: episode: 157, duration: 2.289s, episode steps: 253, steps per second: 111, episode reward: -252.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.119 [0.000, 2.000],  loss: 0.704445, mae: 19.182546, mean_q: -28.040950, mean_eps: 0.100000\n",
            " 38860/50000: episode: 158, duration: 1.728s, episode steps: 175, steps per second: 101, episode reward: -174.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.069 [0.000, 2.000],  loss: 0.734404, mae: 19.205478, mean_q: -28.081238, mean_eps: 0.100000\n",
            " 39078/50000: episode: 159, duration: 2.073s, episode steps: 218, steps per second: 105, episode reward: -217.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.147 [0.000, 2.000],  loss: 0.694152, mae: 19.252136, mean_q: -28.140617, mean_eps: 0.100000\n",
            " 39223/50000: episode: 160, duration: 1.982s, episode steps: 145, steps per second:  73, episode reward: -144.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.841 [0.000, 2.000],  loss: 0.669807, mae: 19.384881, mean_q: -28.338047, mean_eps: 0.100000\n",
            " 39360/50000: episode: 161, duration: 1.783s, episode steps: 137, steps per second:  77, episode reward: -136.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.044 [0.000, 2.000],  loss: 0.767253, mae: 19.428740, mean_q: -28.382075, mean_eps: 0.100000\n",
            " 39519/50000: episode: 162, duration: 1.401s, episode steps: 159, steps per second: 113, episode reward: -158.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.101 [0.000, 2.000],  loss: 0.721369, mae: 19.430728, mean_q: -28.373856, mean_eps: 0.100000\n",
            " 39670/50000: episode: 163, duration: 1.417s, episode steps: 151, steps per second: 107, episode reward: -150.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.020 [0.000, 2.000],  loss: 0.696801, mae: 19.388631, mean_q: -28.322698, mean_eps: 0.100000\n",
            " 39824/50000: episode: 164, duration: 1.397s, episode steps: 154, steps per second: 110, episode reward: -153.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.156 [0.000, 2.000],  loss: 0.643450, mae: 19.471746, mean_q: -28.469029, mean_eps: 0.100000\n",
            " 39999/50000: episode: 165, duration: 1.620s, episode steps: 175, steps per second: 108, episode reward: -174.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.286 [0.000, 2.000],  loss: 0.705275, mae: 19.457215, mean_q: -28.428297, mean_eps: 0.100000\n",
            " 40110/50000: episode: 166, duration: 0.992s, episode steps: 111, steps per second: 112, episode reward: -110.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.910 [0.000, 2.000],  loss: 0.709291, mae: 19.562466, mean_q: -28.570101, mean_eps: 0.100000\n",
            " 40259/50000: episode: 167, duration: 1.305s, episode steps: 149, steps per second: 114, episode reward: -148.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.047 [0.000, 2.000],  loss: 0.677350, mae: 19.583643, mean_q: -28.609951, mean_eps: 0.100000\n",
            " 40465/50000: episode: 168, duration: 1.883s, episode steps: 206, steps per second: 109, episode reward: -205.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.947 [0.000, 2.000],  loss: 0.681171, mae: 19.615237, mean_q: -28.663848, mean_eps: 0.100000\n",
            " 40612/50000: episode: 169, duration: 1.880s, episode steps: 147, steps per second:  78, episode reward: -146.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.048 [0.000, 2.000],  loss: 0.664740, mae: 19.581100, mean_q: -28.606537, mean_eps: 0.100000\n",
            " 40750/50000: episode: 170, duration: 1.762s, episode steps: 138, steps per second:  78, episode reward: -137.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.964 [0.000, 2.000],  loss: 0.650702, mae: 19.504070, mean_q: -28.465304, mean_eps: 0.100000\n",
            " 40955/50000: episode: 171, duration: 1.802s, episode steps: 205, steps per second: 114, episode reward: -204.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.093 [0.000, 2.000],  loss: 0.631078, mae: 19.563446, mean_q: -28.569582, mean_eps: 0.100000\n",
            " 41123/50000: episode: 172, duration: 1.476s, episode steps: 168, steps per second: 114, episode reward: -167.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.036 [0.000, 2.000],  loss: 0.693439, mae: 19.727853, mean_q: -28.801030, mean_eps: 0.100000\n",
            " 41274/50000: episode: 173, duration: 1.371s, episode steps: 151, steps per second: 110, episode reward: -150.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.146 [0.000, 2.000],  loss: 0.705925, mae: 19.835742, mean_q: -28.950843, mean_eps: 0.100000\n",
            " 41410/50000: episode: 174, duration: 1.290s, episode steps: 136, steps per second: 105, episode reward: -135.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.110 [0.000, 2.000],  loss: 0.674481, mae: 19.898771, mean_q: -29.053045, mean_eps: 0.100000\n",
            " 41606/50000: episode: 175, duration: 1.870s, episode steps: 196, steps per second: 105, episode reward: -195.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.128 [0.000, 2.000],  loss: 0.664980, mae: 19.822075, mean_q: -28.917389, mean_eps: 0.100000\n",
            " 41755/50000: episode: 176, duration: 1.374s, episode steps: 149, steps per second: 108, episode reward: -148.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.027 [0.000, 2.000],  loss: 0.695314, mae: 19.808173, mean_q: -28.893954, mean_eps: 0.100000\n",
            " 41899/50000: episode: 177, duration: 1.596s, episode steps: 144, steps per second:  90, episode reward: -143.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.986 [0.000, 2.000],  loss: 0.679837, mae: 19.795206, mean_q: -28.867677, mean_eps: 0.100000\n",
            " 42043/50000: episode: 178, duration: 1.867s, episode steps: 144, steps per second:  77, episode reward: -143.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.924 [0.000, 2.000],  loss: 0.698424, mae: 19.846293, mean_q: -28.914001, mean_eps: 0.100000\n",
            " 42231/50000: episode: 179, duration: 1.953s, episode steps: 188, steps per second:  96, episode reward: -187.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.995 [0.000, 2.000],  loss: 0.685417, mae: 20.152976, mean_q: -29.403487, mean_eps: 0.100000\n",
            " 42346/50000: episode: 180, duration: 1.017s, episode steps: 115, steps per second: 113, episode reward: -114.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.817 [0.000, 2.000],  loss: 0.748653, mae: 20.124760, mean_q: -29.327098, mean_eps: 0.100000\n",
            " 42491/50000: episode: 181, duration: 1.322s, episode steps: 145, steps per second: 110, episode reward: -144.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.979 [0.000, 2.000],  loss: 0.702027, mae: 20.042786, mean_q: -29.215242, mean_eps: 0.100000\n",
            " 42674/50000: episode: 182, duration: 1.739s, episode steps: 183, steps per second: 105, episode reward: -182.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.820 [0.000, 2.000],  loss: 0.646396, mae: 20.058818, mean_q: -29.257533, mean_eps: 0.100000\n",
            " 42821/50000: episode: 183, duration: 1.471s, episode steps: 147, steps per second: 100, episode reward: -146.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.810 [0.000, 2.000],  loss: 0.692891, mae: 20.103052, mean_q: -29.315033, mean_eps: 0.100000\n",
            " 42995/50000: episode: 184, duration: 1.746s, episode steps: 174, steps per second: 100, episode reward: -173.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.874 [0.000, 2.000],  loss: 0.672417, mae: 20.069740, mean_q: -29.262980, mean_eps: 0.100000\n",
            " 43138/50000: episode: 185, duration: 1.297s, episode steps: 143, steps per second: 110, episode reward: -142.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.084 [0.000, 2.000],  loss: 0.778655, mae: 20.292818, mean_q: -29.563162, mean_eps: 0.100000\n",
            " 43281/50000: episode: 186, duration: 1.852s, episode steps: 143, steps per second:  77, episode reward: -142.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.958 [0.000, 2.000],  loss: 0.700192, mae: 20.332813, mean_q: -29.661896, mean_eps: 0.100000\n",
            " 43465/50000: episode: 187, duration: 2.381s, episode steps: 184, steps per second:  77, episode reward: -183.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.935 [0.000, 2.000],  loss: 0.699491, mae: 20.394363, mean_q: -29.764663, mean_eps: 0.100000\n",
            " 43649/50000: episode: 188, duration: 1.717s, episode steps: 184, steps per second: 107, episode reward: -183.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.870 [0.000, 2.000],  loss: 0.728399, mae: 20.377209, mean_q: -29.724046, mean_eps: 0.100000\n",
            " 43830/50000: episode: 189, duration: 1.777s, episode steps: 181, steps per second: 102, episode reward: -180.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.762 [0.000, 2.000],  loss: 0.657070, mae: 20.350814, mean_q: -29.710833, mean_eps: 0.100000\n",
            " 43985/50000: episode: 190, duration: 1.560s, episode steps: 155, steps per second:  99, episode reward: -154.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.884 [0.000, 2.000],  loss: 0.700589, mae: 20.304712, mean_q: -29.640113, mean_eps: 0.100000\n",
            " 44138/50000: episode: 191, duration: 1.449s, episode steps: 153, steps per second: 106, episode reward: -152.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.817 [0.000, 2.000],  loss: 0.755407, mae: 20.660782, mean_q: -30.139166, mean_eps: 0.100000\n",
            " 44290/50000: episode: 192, duration: 1.401s, episode steps: 152, steps per second: 108, episode reward: -151.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.967 [0.000, 2.000],  loss: 0.787539, mae: 20.682947, mean_q: -30.165566, mean_eps: 0.100000\n",
            " 44429/50000: episode: 193, duration: 1.262s, episode steps: 139, steps per second: 110, episode reward: -138.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.899 [0.000, 2.000],  loss: 0.735554, mae: 20.675420, mean_q: -30.188968, mean_eps: 0.100000\n",
            " 44587/50000: episode: 194, duration: 1.761s, episode steps: 158, steps per second:  90, episode reward: -157.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.930 [0.000, 2.000],  loss: 0.743144, mae: 20.731353, mean_q: -30.285003, mean_eps: 0.100000\n",
            " 44742/50000: episode: 195, duration: 1.974s, episode steps: 155, steps per second:  79, episode reward: -154.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.761 [0.000, 2.000],  loss: 0.713355, mae: 20.651924, mean_q: -30.163350, mean_eps: 0.100000\n",
            " 44892/50000: episode: 196, duration: 1.479s, episode steps: 150, steps per second: 101, episode reward: -149.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.947 [0.000, 2.000],  loss: 0.787527, mae: 20.682858, mean_q: -30.198376, mean_eps: 0.100000\n",
            " 45034/50000: episode: 197, duration: 1.282s, episode steps: 142, steps per second: 111, episode reward: -141.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.951 [0.000, 2.000],  loss: 0.691820, mae: 20.788415, mean_q: -30.361641, mean_eps: 0.100000\n",
            " 45230/50000: episode: 198, duration: 1.821s, episode steps: 196, steps per second: 108, episode reward: -195.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.031 [0.000, 2.000],  loss: 0.754976, mae: 20.941572, mean_q: -30.544513, mean_eps: 0.100000\n",
            " 45369/50000: episode: 199, duration: 1.209s, episode steps: 139, steps per second: 115, episode reward: -138.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.029 [0.000, 2.000],  loss: 0.777316, mae: 20.916985, mean_q: -30.492710, mean_eps: 0.100000\n",
            " 45478/50000: episode: 200, duration: 1.025s, episode steps: 109, steps per second: 106, episode reward: -108.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.908 [0.000, 2.000],  loss: 0.673701, mae: 20.964970, mean_q: -30.587795, mean_eps: 0.100000\n",
            " 45661/50000: episode: 201, duration: 1.615s, episode steps: 183, steps per second: 113, episode reward: -182.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.224 [0.000, 2.000],  loss: 0.760800, mae: 20.878711, mean_q: -30.453883, mean_eps: 0.100000\n",
            " 45792/50000: episode: 202, duration: 1.170s, episode steps: 131, steps per second: 112, episode reward: -130.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.076 [0.000, 2.000],  loss: 0.737668, mae: 20.917641, mean_q: -30.507712, mean_eps: 0.100000\n",
            " 45912/50000: episode: 203, duration: 1.253s, episode steps: 120, steps per second:  96, episode reward: -119.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.058 [0.000, 2.000],  loss: 0.803350, mae: 20.883510, mean_q: -30.450259, mean_eps: 0.100000\n",
            " 46048/50000: episode: 204, duration: 1.734s, episode steps: 136, steps per second:  78, episode reward: -135.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.963 [0.000, 2.000],  loss: 0.748573, mae: 21.045500, mean_q: -30.689111, mean_eps: 0.100000\n",
            " 46177/50000: episode: 205, duration: 1.580s, episode steps: 129, steps per second:  82, episode reward: -128.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.884 [0.000, 2.000],  loss: 0.805408, mae: 21.174797, mean_q: -30.867553, mean_eps: 0.100000\n",
            " 46274/50000: episode: 206, duration: 0.896s, episode steps:  97, steps per second: 108, episode reward: -96.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.918 [0.000, 2.000],  loss: 0.777009, mae: 21.144538, mean_q: -30.801560, mean_eps: 0.100000\n",
            " 46382/50000: episode: 207, duration: 1.017s, episode steps: 108, steps per second: 106, episode reward: -107.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.963 [0.000, 2.000],  loss: 0.698729, mae: 21.207134, mean_q: -30.928940, mean_eps: 0.100000\n",
            " 46507/50000: episode: 208, duration: 1.104s, episode steps: 125, steps per second: 113, episode reward: -124.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.072 [0.000, 2.000],  loss: 0.730792, mae: 21.181850, mean_q: -30.878837, mean_eps: 0.100000\n",
            " 46656/50000: episode: 209, duration: 1.357s, episode steps: 149, steps per second: 110, episode reward: -148.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.859 [0.000, 2.000],  loss: 0.732335, mae: 21.123437, mean_q: -30.794783, mean_eps: 0.100000\n",
            " 46774/50000: episode: 210, duration: 1.125s, episode steps: 118, steps per second: 105, episode reward: -117.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.017 [0.000, 2.000],  loss: 0.733069, mae: 21.102579, mean_q: -30.776409, mean_eps: 0.100000\n",
            " 46903/50000: episode: 211, duration: 1.204s, episode steps: 129, steps per second: 107, episode reward: -128.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.946 [0.000, 2.000],  loss: 0.715813, mae: 21.167303, mean_q: -30.876940, mean_eps: 0.100000\n",
            " 47079/50000: episode: 212, duration: 1.561s, episode steps: 176, steps per second: 113, episode reward: -175.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.892 [0.000, 2.000],  loss: 0.794787, mae: 21.100811, mean_q: -30.741577, mean_eps: 0.100000\n",
            " 47579/50000: episode: 213, duration: 5.605s, episode steps: 500, steps per second:  89, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.788 [0.000, 2.000],  loss: 0.825243, mae: 21.009283, mean_q: -30.614219, mean_eps: 0.100000\n",
            " 47751/50000: episode: 214, duration: 1.514s, episode steps: 172, steps per second: 114, episode reward: -171.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.012 [0.000, 2.000],  loss: 0.840752, mae: 20.790934, mean_q: -30.283437, mean_eps: 0.100000\n",
            " 47885/50000: episode: 215, duration: 1.237s, episode steps: 134, steps per second: 108, episode reward: -133.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.828 [0.000, 2.000],  loss: 0.799494, mae: 20.725286, mean_q: -30.168614, mean_eps: 0.100000\n",
            " 47988/50000: episode: 216, duration: 0.957s, episode steps: 103, steps per second: 108, episode reward: -102.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.816 [0.000, 2.000],  loss: 0.871199, mae: 20.815253, mean_q: -30.303202, mean_eps: 0.100000\n",
            " 48112/50000: episode: 217, duration: 1.068s, episode steps: 124, steps per second: 116, episode reward: -123.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.089 [0.000, 2.000],  loss: 0.903651, mae: 21.069075, mean_q: -30.683497, mean_eps: 0.100000\n",
            " 48250/50000: episode: 218, duration: 1.196s, episode steps: 138, steps per second: 115, episode reward: -137.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.051 [0.000, 2.000],  loss: 0.867688, mae: 21.007140, mean_q: -30.562794, mean_eps: 0.100000\n",
            " 48370/50000: episode: 219, duration: 1.037s, episode steps: 120, steps per second: 116, episode reward: -119.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.975 [0.000, 2.000],  loss: 0.888326, mae: 21.029220, mean_q: -30.600865, mean_eps: 0.100000\n",
            " 48483/50000: episode: 220, duration: 0.975s, episode steps: 113, steps per second: 116, episode reward: -112.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.080 [0.000, 2.000],  loss: 0.859440, mae: 21.162496, mean_q: -30.822410, mean_eps: 0.100000\n",
            " 48622/50000: episode: 221, duration: 1.160s, episode steps: 139, steps per second: 120, episode reward: -138.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.101 [0.000, 2.000],  loss: 0.876265, mae: 20.997391, mean_q: -30.572639, mean_eps: 0.100000\n",
            " 48766/50000: episode: 222, duration: 1.715s, episode steps: 144, steps per second:  84, episode reward: -143.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.014 [0.000, 2.000],  loss: 0.898668, mae: 20.930702, mean_q: -30.480899, mean_eps: 0.100000\n",
            " 48873/50000: episode: 223, duration: 1.341s, episode steps: 107, steps per second:  80, episode reward: -106.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.888 [0.000, 2.000],  loss: 0.881967, mae: 20.969034, mean_q: -30.536677, mean_eps: 0.100000\n",
            " 49008/50000: episode: 224, duration: 1.483s, episode steps: 135, steps per second:  91, episode reward: -134.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.852 [0.000, 2.000],  loss: 0.872042, mae: 20.947392, mean_q: -30.488807, mean_eps: 0.100000\n",
            " 49116/50000: episode: 225, duration: 0.939s, episode steps: 108, steps per second: 115, episode reward: -107.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.926 [0.000, 2.000],  loss: 0.948477, mae: 21.145090, mean_q: -30.748298, mean_eps: 0.100000\n",
            " 49250/50000: episode: 226, duration: 1.183s, episode steps: 134, steps per second: 113, episode reward: -133.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.896 [0.000, 2.000],  loss: 0.960836, mae: 21.197491, mean_q: -30.852992, mean_eps: 0.100000\n",
            " 49388/50000: episode: 227, duration: 1.162s, episode steps: 138, steps per second: 119, episode reward: -137.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.920 [0.000, 2.000],  loss: 0.912243, mae: 21.113172, mean_q: -30.733448, mean_eps: 0.100000\n",
            " 49604/50000: episode: 228, duration: 1.904s, episode steps: 216, steps per second: 113, episode reward: -215.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.111 [0.000, 2.000],  loss: 0.874573, mae: 21.123054, mean_q: -30.751794, mean_eps: 0.100000\n",
            " 49719/50000: episode: 229, duration: 1.025s, episode steps: 115, steps per second: 112, episode reward: -114.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.974 [0.000, 2.000],  loss: 0.928579, mae: 21.078161, mean_q: -30.664467, mean_eps: 0.100000\n",
            " 49813/50000: episode: 230, duration: 0.868s, episode steps:  94, steps per second: 108, episode reward: -93.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.957 [0.000, 2.000],  loss: 0.928025, mae: 21.012893, mean_q: -30.530301, mean_eps: 0.100000\n",
            " 49946/50000: episode: 231, duration: 1.225s, episode steps: 133, steps per second: 109, episode reward: -132.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.939629, mae: 20.974299, mean_q: -30.480003, mean_eps: 0.100000\n",
            "done, took 499.152 seconds\n",
            "\n",
            "Evaluando Base...\n",
            "Base → Recompensa media: -500.00 ± 0.00\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Resumen comparativo de todas las mejoras:\n",
            "\n",
            "           Media  Desviación estándar\n",
            "Mejora 1  -94.95            14.058056\n",
            "Mejora 2 -259.25           192.256921\n",
            "Mejora 3 -110.90            16.257468\n",
            "Base     -500.00             0.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importación de bibliotecas necesarias para el entorno, procesamiento y visualización\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import random\n",
        "import gym\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Componentes para la construcción de modelos de redes neuronales con Keras\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense, Activation, Flatten, BatchNormalization, Dropout, Input, Lambda\n",
        "from tensorflow.keras.optimizers.legacy import Adam\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "# Importación de módulos de Keras-RL necesarios para agentes DQN y gestión de políticas y memoria\n",
        "from rl.agents.dqn import DQNAgent\n",
        "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy, BoltzmannQPolicy\n",
        "from rl.memory import SequentialMemory\n",
        "\n",
        "# Establecimiento de semillas para asegurar la reproducibilidad de los resultados\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "# Inicialización del entorno Acrobot-v1 y fijación de las semillas para consistencia experimental\n",
        "env = gym.make('Acrobot-v1')\n",
        "env.seed(SEED)\n",
        "if hasattr(env.action_space, 'seed'):\n",
        "    env.action_space.seed(SEED)\n",
        "if hasattr(env.observation_space, 'seed'):\n",
        "    env.observation_space.seed(SEED)\n",
        "\n",
        "# Extracción de dimensiones del espacio de observación y número de acciones disponibles\n",
        "nb_actions = env.action_space.n\n",
        "obs_shape = env.observation_space.shape\n",
        "\n",
        "# Definición de modelo base para DQN: red neuronal multicapa densa con técnicas de regularización\n",
        "def build_base_model():\n",
        "    model = Sequential()\n",
        "    model.add(Flatten(input_shape=(1,) + obs_shape))\n",
        "    model.add(Dense(128))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(128))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(64))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dense(nb_actions))\n",
        "    model.add(Activation('linear'))  # Salida lineal para estimar Q-valores\n",
        "    return model\n",
        "\n",
        "# Definición del modelo Dueling DQN que separa el valor del estado y la ventaja de cada acción\n",
        "def build_dueling_model():\n",
        "    input_layer = Input(shape=(1,) + obs_shape)\n",
        "    x = Flatten()(input_layer)\n",
        "    x = Dense(128, activation='relu')(x)\n",
        "    x = Dense(128, activation='relu')(x)\n",
        "\n",
        "    value = Dense(1, activation='linear')(x)  # V(s)\n",
        "    advantage = Dense(nb_actions, activation='linear')(x)  # A(s,a)\n",
        "\n",
        "    # Combinación de V y A para calcular Q(s,a) de forma dueling\n",
        "    def dueling_output(inputs):\n",
        "        v, a = inputs\n",
        "        return v + (a - K.mean(a, axis=1, keepdims=True))\n",
        "\n",
        "    q_values = Lambda(dueling_output)([value, advantage])\n",
        "    return Model(inputs=input_layer, outputs=q_values)\n",
        "\n",
        "# Función generalizada para entrenar y evaluar un agente DQN configurable\n",
        "def train_agent(model, double_dqn=False, name='dqn_model', policy_type='epsgreedy'):\n",
        "    \"\"\"\n",
        "    Entrena un agente DQN con configuración flexible, incluyendo opción de política ε-greedy o Boltzmann,\n",
        "    así como activación de Double DQN.\n",
        "\n",
        "    Args:\n",
        "        model: modelo Keras de red neuronal.\n",
        "        double_dqn: activa Double DQN si es True.\n",
        "        name: nombre base para guardar los resultados.\n",
        "        policy_type: tipo de política de exploración (ε-greedy o Boltzmann).\n",
        "    Returns:\n",
        "        Array de recompensas obtenidas durante la evaluación.\n",
        "    \"\"\"\n",
        "    memory = SequentialMemory(limit=100000, window_length=1)\n",
        "\n",
        "    # Definición de política de acción\n",
        "    if policy_type == 'boltzmann':\n",
        "        policy = BoltzmannQPolicy()  # Política suave basada en temperatura\n",
        "    else:\n",
        "        policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps',\n",
        "                                      value_max=1.0, value_min=0.05,\n",
        "                                      value_test=0.01, nb_steps=150000)\n",
        "\n",
        "    # Construcción del agente DQN\n",
        "    agent = DQNAgent(model=model,\n",
        "                     nb_actions=nb_actions,\n",
        "                     memory=memory,\n",
        "                     policy=policy,\n",
        "                     nb_steps_warmup=5000,\n",
        "                     gamma=0.99,\n",
        "                     target_model_update=1000,\n",
        "                     train_interval=4,\n",
        "                     batch_size=128,\n",
        "                     enable_double_dqn=double_dqn)\n",
        "\n",
        "    # Compilación del agente con optimizador Adam y métrica de error medio absoluto\n",
        "    agent.compile(Adam(learning_rate=1e-4), metrics=['mae'])\n",
        "\n",
        "    print(f\"\\nEntrenando {name.upper()}...\")\n",
        "    agent.fit(env, nb_steps=200000, visualize=False, verbose=2)\n",
        "    agent.save_weights(f'{name}_weights.h5f', overwrite=True)\n",
        "\n",
        "    print(f\"\\nEvaluando {name.upper()}...\")\n",
        "    agent.load_weights(f'{name}_weights.h5f')\n",
        "    history = agent.test(env, nb_episodes=20, visualize=False, verbose=0)\n",
        "\n",
        "    rewards = np.array(history.history['episode_reward'])\n",
        "    print(f\"{name.upper()} → Recompensa media: {rewards.mean():.2f} ± {rewards.std():.2f}\")\n",
        "    return rewards\n",
        "\n",
        "# Entrenamiento y evaluación de los distintos agentes con diferentes configuraciones\n",
        "rewards_dqn = train_agent(build_base_model(), double_dqn=False, name='dqn', policy_type='epsgreedy')\n",
        "rewards_ddqn = train_agent(build_base_model(), double_dqn=True, name='ddqn', policy_type='epsgreedy')\n",
        "rewards_dueling = train_agent(build_dueling_model(), double_dqn=True, name='dueling_dqn', policy_type='epsgreedy')\n",
        "rewards_boltzmann = train_agent(build_base_model(), double_dqn=False, name='boltzmann_dqn', policy_type='boltzmann')\n",
        "\n",
        "# ---- VISUALIZACIÓN DE RESULTADOS ----\n",
        "\n",
        "# Histograma comparativo de recompensas obtenidas\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.hist(rewards_dqn, bins=10, alpha=0.5, label='DQN')\n",
        "plt.hist(rewards_ddqn, bins=10, alpha=0.5, label='Double DQN')\n",
        "plt.hist(rewards_dueling, bins=10, alpha=0.5, label='Dueling DQN')\n",
        "plt.hist(rewards_boltzmann, bins=10, alpha=0.5, label='Boltzmann DQN')\n",
        "plt.title('Distribución de Recompensas - Evaluación en 20 Episodios')\n",
        "plt.xlabel('Recompensa total por episodio')\n",
        "plt.ylabel('Frecuencia')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Boxplot para observar variabilidad, medianas y posibles valores atípicos\n",
        "df_results = pd.DataFrame({\n",
        "    'DQN': rewards_dqn,\n",
        "    'Double DQN': rewards_ddqn,\n",
        "    'Dueling DQN': rewards_dueling,\n",
        "    'Boltzmann DQN': rewards_boltzmann\n",
        "})\n",
        "plt.figure(figsize=(10, 6))\n",
        "df_results.boxplot()\n",
        "plt.title('Boxplot de Recompensas por Algoritmo')\n",
        "plt.ylabel('Recompensa total por episodio')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Representación de la evolución de las recompensas a lo largo de los episodios\n",
        "plt.figure(figsize=(10, 6))\n",
        "for col in df_results.columns:\n",
        "    plt.plot(df_results.index, df_results[col], label=col)\n",
        "plt.title('Evolución de Recompensas por Episodio')\n",
        "plt.xlabel('Episodio')\n",
        "plt.ylabel('Recompensa')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Generación de tabla resumen con estadísticos descriptivos por agente\n",
        "summary_table = df_results.describe().loc[['mean', 'std']].T\n",
        "summary_table.columns = ['Media', 'Desviación estándar']\n",
        "print(\"\\nResumen estadístico por algoritmo:\\n\")\n",
        "print(summary_table)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "WbES8C6V9ZHn",
        "outputId": "7830b409-7c33-4779-cd48-ef035ffcaef0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Entrenando DQN...\n",
            "Training for 200000 steps ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    500/200000: episode: 1, duration: 1.509s, episode steps: 500, steps per second: 331, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.948 [0.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   1000/200000: episode: 2, duration: 0.624s, episode steps: 500, steps per second: 801, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.908 [0.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   1500/200000: episode: 3, duration: 0.620s, episode steps: 500, steps per second: 807, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.958 [0.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   2000/200000: episode: 4, duration: 0.588s, episode steps: 500, steps per second: 851, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.026 [0.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   2500/200000: episode: 5, duration: 0.594s, episode steps: 500, steps per second: 842, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   3000/200000: episode: 6, duration: 0.567s, episode steps: 500, steps per second: 882, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.058 [0.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   3500/200000: episode: 7, duration: 0.607s, episode steps: 500, steps per second: 824, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.974 [0.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   4000/200000: episode: 8, duration: 0.604s, episode steps: 500, steps per second: 827, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.984 [0.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   4500/200000: episode: 9, duration: 0.588s, episode steps: 500, steps per second: 850, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.968 [0.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   5000/200000: episode: 10, duration: 0.568s, episode steps: 500, steps per second: 880, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   5500/200000: episode: 11, duration: 6.800s, episode steps: 500, steps per second:  74, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.992 [0.000, 2.000],  loss: 0.261666, mae: 0.615926, mean_q: -0.050390, mean_eps: 0.966750\n",
            "   6000/200000: episode: 12, duration: 2.070s, episode steps: 500, steps per second: 242, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.100860, mae: 0.671568, mean_q: -0.491591, mean_eps: 0.963596\n",
            "   6500/200000: episode: 13, duration: 2.105s, episode steps: 500, steps per second: 238, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.038 [0.000, 2.000],  loss: 0.136170, mae: 1.038541, mean_q: -1.059999, mean_eps: 0.960429\n",
            "   7000/200000: episode: 14, duration: 2.024s, episode steps: 500, steps per second: 247, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.044 [0.000, 2.000],  loss: 0.078036, mae: 1.102054, mean_q: -1.236162, mean_eps: 0.957263\n",
            "   7500/200000: episode: 15, duration: 2.082s, episode steps: 500, steps per second: 240, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.008 [0.000, 2.000],  loss: 0.118384, mae: 1.489191, mean_q: -1.794069, mean_eps: 0.954096\n",
            "   8000/200000: episode: 16, duration: 2.328s, episode steps: 500, steps per second: 215, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.076640, mae: 1.532134, mean_q: -1.917834, mean_eps: 0.950929\n",
            "   8500/200000: episode: 17, duration: 3.081s, episode steps: 500, steps per second: 162, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.984 [0.000, 2.000],  loss: 0.111806, mae: 1.899344, mean_q: -2.442510, mean_eps: 0.947763\n",
            "   9000/200000: episode: 18, duration: 2.074s, episode steps: 500, steps per second: 241, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.976 [0.000, 2.000],  loss: 0.080324, mae: 1.937148, mean_q: -2.540211, mean_eps: 0.944596\n",
            "   9500/200000: episode: 19, duration: 2.125s, episode steps: 500, steps per second: 235, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.946 [0.000, 2.000],  loss: 0.106443, mae: 2.285689, mean_q: -3.039742, mean_eps: 0.941429\n",
            "  10000/200000: episode: 20, duration: 2.048s, episode steps: 500, steps per second: 244, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.996 [0.000, 2.000],  loss: 0.086051, mae: 2.308377, mean_q: -3.105153, mean_eps: 0.938263\n",
            "  10500/200000: episode: 21, duration: 2.183s, episode steps: 500, steps per second: 229, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.058 [0.000, 2.000],  loss: 0.111657, mae: 2.640796, mean_q: -3.580574, mean_eps: 0.935096\n",
            "  10896/200000: episode: 22, duration: 1.993s, episode steps: 396, steps per second: 199, episode reward: -395.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.957 [0.000, 2.000],  loss: 0.091753, mae: 2.669208, mean_q: -3.647762, mean_eps: 0.932259\n",
            "  11396/200000: episode: 23, duration: 3.189s, episode steps: 500, steps per second: 157, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.004 [0.000, 2.000],  loss: 0.117718, mae: 2.906709, mean_q: -3.983409, mean_eps: 0.929421\n",
            "  11896/200000: episode: 24, duration: 2.089s, episode steps: 500, steps per second: 239, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.102927, mae: 3.005148, mean_q: -4.150360, mean_eps: 0.926255\n",
            "  12396/200000: episode: 25, duration: 2.206s, episode steps: 500, steps per second: 227, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.042 [0.000, 2.000],  loss: 0.120444, mae: 3.222992, mean_q: -4.467141, mean_eps: 0.923088\n",
            "  12896/200000: episode: 26, duration: 2.245s, episode steps: 500, steps per second: 223, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.946 [0.000, 2.000],  loss: 0.103745, mae: 3.307576, mean_q: -4.610565, mean_eps: 0.919921\n",
            "  13396/200000: episode: 27, duration: 2.252s, episode steps: 500, steps per second: 222, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.904 [0.000, 2.000],  loss: 0.129138, mae: 3.503076, mean_q: -4.885645, mean_eps: 0.916755\n",
            "  13896/200000: episode: 28, duration: 2.878s, episode steps: 500, steps per second: 174, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.006 [0.000, 2.000],  loss: 0.115501, mae: 3.579611, mean_q: -5.014559, mean_eps: 0.913588\n",
            "  14396/200000: episode: 29, duration: 2.727s, episode steps: 500, steps per second: 183, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.129693, mae: 3.803568, mean_q: -5.338944, mean_eps: 0.910421\n",
            "  14896/200000: episode: 30, duration: 2.133s, episode steps: 500, steps per second: 234, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.134166, mae: 3.873008, mean_q: -5.453575, mean_eps: 0.907255\n",
            "  15396/200000: episode: 31, duration: 2.370s, episode steps: 500, steps per second: 211, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.136034, mae: 4.041778, mean_q: -5.700981, mean_eps: 0.904088\n",
            "  15860/200000: episode: 32, duration: 2.095s, episode steps: 464, steps per second: 221, episode reward: -463.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.022 [0.000, 2.000],  loss: 0.134201, mae: 4.082232, mean_q: -5.765523, mean_eps: 0.901035\n",
            "  16360/200000: episode: 33, duration: 2.243s, episode steps: 500, steps per second: 223, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.054 [0.000, 2.000],  loss: 0.146551, mae: 4.265231, mean_q: -6.025898, mean_eps: 0.897983\n",
            "  16860/200000: episode: 34, duration: 3.332s, episode steps: 500, steps per second: 150, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.964 [0.000, 2.000],  loss: 0.142305, mae: 4.347205, mean_q: -6.152293, mean_eps: 0.894816\n",
            "  17360/200000: episode: 35, duration: 2.407s, episode steps: 500, steps per second: 208, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.147164, mae: 4.505660, mean_q: -6.381529, mean_eps: 0.891649\n",
            "  17860/200000: episode: 36, duration: 2.347s, episode steps: 500, steps per second: 213, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.024 [0.000, 2.000],  loss: 0.149919, mae: 4.575441, mean_q: -6.485224, mean_eps: 0.888483\n",
            "  18360/200000: episode: 37, duration: 2.545s, episode steps: 500, steps per second: 196, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.068 [0.000, 2.000],  loss: 0.170965, mae: 4.759981, mean_q: -6.747764, mean_eps: 0.885316\n",
            "  18699/200000: episode: 38, duration: 1.700s, episode steps: 339, steps per second: 199, episode reward: -338.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.929 [0.000, 2.000],  loss: 0.180856, mae: 4.859329, mean_q: -6.897941, mean_eps: 0.882656\n",
            "  19199/200000: episode: 39, duration: 2.965s, episode steps: 500, steps per second: 169, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.004 [0.000, 2.000],  loss: 0.189338, mae: 4.930488, mean_q: -6.998524, mean_eps: 0.879996\n",
            "  19671/200000: episode: 40, duration: 2.787s, episode steps: 472, steps per second: 169, episode reward: -471.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.998 [0.000, 2.000],  loss: 0.178451, mae: 5.097354, mean_q: -7.256645, mean_eps: 0.876918\n",
            "  20171/200000: episode: 41, duration: 2.205s, episode steps: 500, steps per second: 227, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.096 [0.000, 2.000],  loss: 0.195917, mae: 5.174320, mean_q: -7.363619, mean_eps: 0.873840\n",
            "  20671/200000: episode: 42, duration: 2.175s, episode steps: 500, steps per second: 230, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.185465, mae: 5.372243, mean_q: -7.660187, mean_eps: 0.870673\n",
            "  21171/200000: episode: 43, duration: 2.166s, episode steps: 500, steps per second: 231, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.066 [0.000, 2.000],  loss: 0.201105, mae: 5.416258, mean_q: -7.725148, mean_eps: 0.867507\n",
            "  21671/200000: episode: 44, duration: 2.211s, episode steps: 500, steps per second: 226, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.088 [0.000, 2.000],  loss: 0.196497, mae: 5.547941, mean_q: -7.909741, mean_eps: 0.864340\n",
            "  22171/200000: episode: 45, duration: 2.988s, episode steps: 500, steps per second: 167, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.986 [0.000, 2.000],  loss: 0.204837, mae: 5.561655, mean_q: -7.924711, mean_eps: 0.861173\n",
            "  22671/200000: episode: 46, duration: 2.523s, episode steps: 500, steps per second: 198, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.996 [0.000, 2.000],  loss: 0.201485, mae: 5.672197, mean_q: -8.083571, mean_eps: 0.858007\n",
            "  23045/200000: episode: 47, duration: 1.716s, episode steps: 374, steps per second: 218, episode reward: -373.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.024 [0.000, 2.000],  loss: 0.210966, mae: 5.692324, mean_q: -8.108305, mean_eps: 0.855233\n",
            "  23474/200000: episode: 48, duration: 1.874s, episode steps: 429, steps per second: 229, episode reward: -428.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.974 [0.000, 2.000],  loss: 0.211353, mae: 5.779662, mean_q: -8.239282, mean_eps: 0.852687\n",
            "  23882/200000: episode: 49, duration: 1.873s, episode steps: 408, steps per second: 218, episode reward: -407.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.042 [0.000, 2.000],  loss: 0.222232, mae: 5.781543, mean_q: -8.233523, mean_eps: 0.850039\n",
            "  24375/200000: episode: 50, duration: 2.167s, episode steps: 493, steps per second: 227, episode reward: -492.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.018 [0.000, 2.000],  loss: 0.228136, mae: 5.893485, mean_q: -8.396282, mean_eps: 0.847189\n",
            "  24875/200000: episode: 51, duration: 2.876s, episode steps: 500, steps per second: 174, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.978 [0.000, 2.000],  loss: 0.206729, mae: 5.922468, mean_q: -8.450948, mean_eps: 0.844048\n",
            "  25375/200000: episode: 52, duration: 2.827s, episode steps: 500, steps per second: 177, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.066 [0.000, 2.000],  loss: 0.233896, mae: 6.155530, mean_q: -8.787563, mean_eps: 0.840881\n",
            "  25724/200000: episode: 53, duration: 1.664s, episode steps: 349, steps per second: 210, episode reward: -348.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.974 [0.000, 2.000],  loss: 0.218659, mae: 6.160392, mean_q: -8.800320, mean_eps: 0.838196\n",
            "  25957/200000: episode: 54, duration: 1.093s, episode steps: 233, steps per second: 213, episode reward: -232.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.979 [0.000, 2.000],  loss: 0.222995, mae: 6.156858, mean_q: -8.805096, mean_eps: 0.836347\n",
            "  26329/200000: episode: 55, duration: 1.641s, episode steps: 372, steps per second: 227, episode reward: -371.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.078 [0.000, 2.000],  loss: 0.237647, mae: 6.296366, mean_q: -8.995676, mean_eps: 0.834421\n",
            "  26829/200000: episode: 56, duration: 2.230s, episode steps: 500, steps per second: 224, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.968 [0.000, 2.000],  loss: 0.264974, mae: 6.307800, mean_q: -9.005210, mean_eps: 0.831660\n",
            "  27143/200000: episode: 57, duration: 1.444s, episode steps: 314, steps per second: 217, episode reward: -313.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.035 [0.000, 2.000],  loss: 0.238831, mae: 6.378328, mean_q: -9.112827, mean_eps: 0.829089\n",
            "  27643/200000: episode: 58, duration: 2.998s, episode steps: 500, steps per second: 167, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.008 [0.000, 2.000],  loss: 0.260246, mae: 6.497993, mean_q: -9.286124, mean_eps: 0.826517\n",
            "  27840/200000: episode: 59, duration: 1.327s, episode steps: 197, steps per second: 148, episode reward: -196.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.832 [0.000, 2.000],  loss: 0.242510, mae: 6.470656, mean_q: -9.251158, mean_eps: 0.824313\n",
            "  28294/200000: episode: 60, duration: 1.997s, episode steps: 454, steps per second: 227, episode reward: -453.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.945 [0.000, 2.000],  loss: 0.265829, mae: 6.532076, mean_q: -9.327623, mean_eps: 0.822249\n",
            "  28541/200000: episode: 61, duration: 1.108s, episode steps: 247, steps per second: 223, episode reward: -246.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.866 [0.000, 2.000],  loss: 0.244791, mae: 6.594137, mean_q: -9.433730, mean_eps: 0.820019\n",
            "  28933/200000: episode: 62, duration: 1.631s, episode steps: 392, steps per second: 240, episode reward: -391.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.020 [0.000, 2.000],  loss: 0.278832, mae: 6.553389, mean_q: -9.362932, mean_eps: 0.817993\n",
            "  29341/200000: episode: 63, duration: 1.813s, episode steps: 408, steps per second: 225, episode reward: -407.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.998 [0.000, 2.000],  loss: 0.268432, mae: 6.668357, mean_q: -9.533067, mean_eps: 0.815459\n",
            "  29591/200000: episode: 64, duration: 1.056s, episode steps: 250, steps per second: 237, episode reward: -249.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.996 [0.000, 2.000],  loss: 0.251984, mae: 6.709217, mean_q: -9.599106, mean_eps: 0.813382\n",
            "  29882/200000: episode: 65, duration: 1.260s, episode steps: 291, steps per second: 231, episode reward: -290.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.034 [0.000, 2.000],  loss: 0.264954, mae: 6.728525, mean_q: -9.620351, mean_eps: 0.811672\n",
            "  30119/200000: episode: 66, duration: 0.998s, episode steps: 237, steps per second: 238, episode reward: -236.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.301911, mae: 6.761101, mean_q: -9.662471, mean_eps: 0.810000\n",
            "  30449/200000: episode: 67, duration: 2.060s, episode steps: 330, steps per second: 160, episode reward: -329.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.039 [0.000, 2.000],  loss: 0.237122, mae: 6.800661, mean_q: -9.743500, mean_eps: 0.808201\n",
            "  30767/200000: episode: 68, duration: 1.865s, episode steps: 318, steps per second: 170, episode reward: -317.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.937 [0.000, 2.000],  loss: 0.248063, mae: 6.803908, mean_q: -9.743269, mean_eps: 0.806149\n",
            "  31185/200000: episode: 69, duration: 1.789s, episode steps: 418, steps per second: 234, episode reward: -417.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.005 [0.000, 2.000],  loss: 0.243322, mae: 6.882023, mean_q: -9.858324, mean_eps: 0.803819\n",
            "  31610/200000: episode: 70, duration: 1.858s, episode steps: 425, steps per second: 229, episode reward: -424.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.007 [0.000, 2.000],  loss: 0.264402, mae: 6.968247, mean_q: -9.975852, mean_eps: 0.801146\n",
            "  32016/200000: episode: 71, duration: 1.771s, episode steps: 406, steps per second: 229, episode reward: -405.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.017 [0.000, 2.000],  loss: 0.282616, mae: 6.941705, mean_q: -9.938586, mean_eps: 0.798524\n",
            "  32208/200000: episode: 72, duration: 0.856s, episode steps: 192, steps per second: 224, episode reward: -191.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.906 [0.000, 2.000],  loss: 0.297611, mae: 7.063281, mean_q: -10.101137, mean_eps: 0.796637\n",
            "  32444/200000: episode: 73, duration: 1.061s, episode steps: 236, steps per second: 222, episode reward: -235.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.962 [0.000, 2.000],  loss: 0.271744, mae: 7.067831, mean_q: -10.117571, mean_eps: 0.795281\n",
            "  32754/200000: episode: 74, duration: 1.342s, episode steps: 310, steps per second: 231, episode reward: -309.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.026 [0.000, 2.000],  loss: 0.255527, mae: 7.024297, mean_q: -10.062241, mean_eps: 0.793546\n",
            "  33254/200000: episode: 75, duration: 2.876s, episode steps: 500, steps per second: 174, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.002 [0.000, 2.000],  loss: 0.288779, mae: 7.119860, mean_q: -10.199617, mean_eps: 0.790975\n",
            "  33661/200000: episode: 76, duration: 2.336s, episode steps: 407, steps per second: 174, episode reward: -406.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.049 [0.000, 2.000],  loss: 0.298416, mae: 7.145489, mean_q: -10.241013, mean_eps: 0.788099\n",
            "  33941/200000: episode: 77, duration: 1.284s, episode steps: 280, steps per second: 218, episode reward: -279.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.950 [0.000, 2.000],  loss: 0.271408, mae: 7.150777, mean_q: -10.248730, mean_eps: 0.785921\n",
            "  34190/200000: episode: 78, duration: 1.109s, episode steps: 249, steps per second: 224, episode reward: -248.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.988 [0.000, 2.000],  loss: 0.276617, mae: 7.160935, mean_q: -10.257601, mean_eps: 0.784249\n",
            "  34574/200000: episode: 79, duration: 1.718s, episode steps: 384, steps per second: 223, episode reward: -383.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.044 [0.000, 2.000],  loss: 0.312023, mae: 7.214317, mean_q: -10.332682, mean_eps: 0.782247\n",
            "  34928/200000: episode: 80, duration: 1.660s, episode steps: 354, steps per second: 213, episode reward: -353.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.305950, mae: 7.216512, mean_q: -10.334543, mean_eps: 0.779917\n",
            "  35213/200000: episode: 81, duration: 1.306s, episode steps: 285, steps per second: 218, episode reward: -284.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.972 [0.000, 2.000],  loss: 0.298465, mae: 7.240800, mean_q: -10.378278, mean_eps: 0.777890\n",
            "  35468/200000: episode: 82, duration: 1.194s, episode steps: 255, steps per second: 214, episode reward: -254.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.965 [0.000, 2.000],  loss: 0.295811, mae: 7.291719, mean_q: -10.455066, mean_eps: 0.776180\n",
            "  35712/200000: episode: 83, duration: 1.212s, episode steps: 244, steps per second: 201, episode reward: -243.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.029 [0.000, 2.000],  loss: 0.274152, mae: 7.295482, mean_q: -10.455167, mean_eps: 0.774609\n",
            "  36058/200000: episode: 84, duration: 2.283s, episode steps: 346, steps per second: 152, episode reward: -345.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.012 [0.000, 2.000],  loss: 0.262047, mae: 7.252121, mean_q: -10.396033, mean_eps: 0.772735\n",
            "  36315/200000: episode: 85, duration: 1.485s, episode steps: 257, steps per second: 173, episode reward: -256.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.019 [0.000, 2.000],  loss: 0.340595, mae: 7.403892, mean_q: -10.594710, mean_eps: 0.770822\n",
            "  36657/200000: episode: 86, duration: 1.542s, episode steps: 342, steps per second: 222, episode reward: -341.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.977 [0.000, 2.000],  loss: 0.327997, mae: 7.442427, mean_q: -10.641793, mean_eps: 0.768922\n",
            "  36917/200000: episode: 87, duration: 1.170s, episode steps: 260, steps per second: 222, episode reward: -259.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.015 [0.000, 2.000],  loss: 0.306807, mae: 7.451854, mean_q: -10.670206, mean_eps: 0.767009\n",
            "  37218/200000: episode: 88, duration: 1.381s, episode steps: 301, steps per second: 218, episode reward: -300.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.010 [0.000, 2.000],  loss: 0.308658, mae: 7.460394, mean_q: -10.681318, mean_eps: 0.765236\n",
            "  37593/200000: episode: 89, duration: 1.645s, episode steps: 375, steps per second: 228, episode reward: -374.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.037 [0.000, 2.000],  loss: 0.305443, mae: 7.461958, mean_q: -10.697523, mean_eps: 0.763095\n",
            "  37901/200000: episode: 90, duration: 1.457s, episode steps: 308, steps per second: 211, episode reward: -307.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.078 [0.000, 2.000],  loss: 0.325286, mae: 7.494589, mean_q: -10.728719, mean_eps: 0.760929\n",
            "  38202/200000: episode: 91, duration: 1.462s, episode steps: 301, steps per second: 206, episode reward: -300.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.934 [0.000, 2.000],  loss: 0.314996, mae: 7.459821, mean_q: -10.695071, mean_eps: 0.759004\n",
            "  38584/200000: episode: 92, duration: 2.254s, episode steps: 382, steps per second: 169, episode reward: -381.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.334471, mae: 7.532121, mean_q: -10.782563, mean_eps: 0.756851\n",
            "  38853/200000: episode: 93, duration: 1.875s, episode steps: 269, steps per second: 143, episode reward: -268.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.941 [0.000, 2.000],  loss: 0.302044, mae: 7.490728, mean_q: -10.735556, mean_eps: 0.754786\n",
            "  39073/200000: episode: 94, duration: 1.074s, episode steps: 220, steps per second: 205, episode reward: -219.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.005 [0.000, 2.000],  loss: 0.344404, mae: 7.546462, mean_q: -10.803404, mean_eps: 0.753228\n",
            "  39321/200000: episode: 95, duration: 1.108s, episode steps: 248, steps per second: 224, episode reward: -247.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.133 [0.000, 2.000],  loss: 0.314962, mae: 7.623310, mean_q: -10.939714, mean_eps: 0.751746\n",
            "  39540/200000: episode: 96, duration: 0.987s, episode steps: 219, steps per second: 222, episode reward: -218.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.064 [0.000, 2.000],  loss: 0.307780, mae: 7.623883, mean_q: -10.927221, mean_eps: 0.750277\n",
            "  39729/200000: episode: 97, duration: 0.889s, episode steps: 189, steps per second: 213, episode reward: -188.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.915 [0.000, 2.000],  loss: 0.319515, mae: 7.572439, mean_q: -10.862662, mean_eps: 0.748985\n",
            "  40017/200000: episode: 98, duration: 1.372s, episode steps: 288, steps per second: 210, episode reward: -287.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.021 [0.000, 2.000],  loss: 0.324949, mae: 7.590186, mean_q: -10.878832, mean_eps: 0.747465\n",
            "  40305/200000: episode: 99, duration: 1.309s, episode steps: 288, steps per second: 220, episode reward: -287.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.017 [0.000, 2.000],  loss: 0.322998, mae: 7.744019, mean_q: -11.111266, mean_eps: 0.745641\n",
            "  40720/200000: episode: 100, duration: 1.827s, episode steps: 415, steps per second: 227, episode reward: -414.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.990 [0.000, 2.000],  loss: 0.340048, mae: 7.743382, mean_q: -11.095326, mean_eps: 0.743424\n",
            "  40932/200000: episode: 101, duration: 1.001s, episode steps: 212, steps per second: 212, episode reward: -211.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.991 [0.000, 2.000],  loss: 0.327732, mae: 7.720207, mean_q: -11.067517, mean_eps: 0.741448\n",
            "  41159/200000: episode: 102, duration: 1.232s, episode steps: 227, steps per second: 184, episode reward: -226.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.334934, mae: 7.740136, mean_q: -11.080923, mean_eps: 0.740055\n",
            "  41309/200000: episode: 103, duration: 1.039s, episode steps: 150, steps per second: 144, episode reward: -149.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.980 [0.000, 2.000],  loss: 0.304805, mae: 7.796066, mean_q: -11.181003, mean_eps: 0.738851\n",
            "  41505/200000: episode: 104, duration: 1.321s, episode steps: 196, steps per second: 148, episode reward: -195.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.082 [0.000, 2.000],  loss: 0.296140, mae: 7.823090, mean_q: -11.218555, mean_eps: 0.737749\n",
            "  41762/200000: episode: 105, duration: 1.388s, episode steps: 257, steps per second: 185, episode reward: -256.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.039 [0.000, 2.000],  loss: 0.347313, mae: 7.787858, mean_q: -11.145893, mean_eps: 0.736318\n",
            "  41959/200000: episode: 106, duration: 0.983s, episode steps: 197, steps per second: 200, episode reward: -196.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.071 [0.000, 2.000],  loss: 0.345164, mae: 7.792143, mean_q: -11.170876, mean_eps: 0.734887\n",
            "  42289/200000: episode: 107, duration: 1.521s, episode steps: 330, steps per second: 217, episode reward: -329.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.948 [0.000, 2.000],  loss: 0.336416, mae: 7.882375, mean_q: -11.294253, mean_eps: 0.733215\n",
            "  42653/200000: episode: 108, duration: 1.725s, episode steps: 364, steps per second: 211, episode reward: -363.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.920 [0.000, 2.000],  loss: 0.357870, mae: 7.943778, mean_q: -11.370479, mean_eps: 0.731011\n",
            "  42859/200000: episode: 109, duration: 1.001s, episode steps: 206, steps per second: 206, episode reward: -205.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.922 [0.000, 2.000],  loss: 0.327443, mae: 7.870693, mean_q: -11.267472, mean_eps: 0.729212\n",
            "  43103/200000: episode: 110, duration: 1.152s, episode steps: 244, steps per second: 212, episode reward: -243.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.996 [0.000, 2.000],  loss: 0.356613, mae: 7.914721, mean_q: -11.323386, mean_eps: 0.727793\n",
            "  43388/200000: episode: 111, duration: 1.504s, episode steps: 285, steps per second: 190, episode reward: -284.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.081 [0.000, 2.000],  loss: 0.343937, mae: 8.014051, mean_q: -11.485182, mean_eps: 0.726121\n",
            "  43605/200000: episode: 112, duration: 1.219s, episode steps: 217, steps per second: 178, episode reward: -216.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.101 [0.000, 2.000],  loss: 0.362754, mae: 7.990609, mean_q: -11.430436, mean_eps: 0.724525\n",
            "  44035/200000: episode: 113, duration: 3.195s, episode steps: 430, steps per second: 135, episode reward: -429.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.965 [0.000, 2.000],  loss: 0.336498, mae: 7.991861, mean_q: -11.458898, mean_eps: 0.722473\n",
            "  44275/200000: episode: 114, duration: 1.388s, episode steps: 240, steps per second: 173, episode reward: -239.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.133 [0.000, 2.000],  loss: 0.338169, mae: 8.044969, mean_q: -11.536899, mean_eps: 0.720358\n",
            "  44469/200000: episode: 115, duration: 0.950s, episode steps: 194, steps per second: 204, episode reward: -193.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.077 [0.000, 2.000],  loss: 0.327355, mae: 8.051762, mean_q: -11.551499, mean_eps: 0.718977\n",
            "  44718/200000: episode: 116, duration: 1.280s, episode steps: 249, steps per second: 195, episode reward: -248.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.968 [0.000, 2.000],  loss: 0.335590, mae: 8.009851, mean_q: -11.496155, mean_eps: 0.717571\n",
            "  44910/200000: episode: 117, duration: 1.009s, episode steps: 192, steps per second: 190, episode reward: -191.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.120 [0.000, 2.000],  loss: 0.373490, mae: 8.029026, mean_q: -11.510243, mean_eps: 0.716178\n",
            "  45152/200000: episode: 118, duration: 1.228s, episode steps: 242, steps per second: 197, episode reward: -241.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.132 [0.000, 2.000],  loss: 0.379227, mae: 8.036468, mean_q: -11.517448, mean_eps: 0.714810\n",
            "  45384/200000: episode: 119, duration: 1.263s, episode steps: 232, steps per second: 184, episode reward: -231.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.918 [0.000, 2.000],  loss: 0.405186, mae: 8.058729, mean_q: -11.541839, mean_eps: 0.713315\n",
            "  45499/200000: episode: 120, duration: 0.598s, episode steps: 115, steps per second: 192, episode reward: -114.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.139 [0.000, 2.000],  loss: 0.344309, mae: 7.995124, mean_q: -11.477293, mean_eps: 0.712213\n",
            "  45669/200000: episode: 121, duration: 0.905s, episode steps: 170, steps per second: 188, episode reward: -169.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.947 [0.000, 2.000],  loss: 0.399566, mae: 8.027922, mean_q: -11.498712, mean_eps: 0.711301\n",
            "  45968/200000: episode: 122, duration: 1.458s, episode steps: 299, steps per second: 205, episode reward: -298.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.970 [0.000, 2.000],  loss: 0.394144, mae: 8.058954, mean_q: -11.541232, mean_eps: 0.709819\n",
            "  46142/200000: episode: 123, duration: 1.119s, episode steps: 174, steps per second: 155, episode reward: -173.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.046 [0.000, 2.000],  loss: 0.346572, mae: 8.037348, mean_q: -11.523938, mean_eps: 0.708325\n",
            "  46364/200000: episode: 124, duration: 1.713s, episode steps: 222, steps per second: 130, episode reward: -221.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.932 [0.000, 2.000],  loss: 0.313930, mae: 8.081979, mean_q: -11.615321, mean_eps: 0.707071\n",
            "  46679/200000: episode: 125, duration: 1.955s, episode steps: 315, steps per second: 161, episode reward: -314.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.051 [0.000, 2.000],  loss: 0.334298, mae: 8.077671, mean_q: -11.594506, mean_eps: 0.705373\n",
            "  46887/200000: episode: 126, duration: 1.051s, episode steps: 208, steps per second: 198, episode reward: -207.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.111 [0.000, 2.000],  loss: 0.331232, mae: 8.101083, mean_q: -11.639155, mean_eps: 0.703714\n",
            "  47123/200000: episode: 127, duration: 1.186s, episode steps: 236, steps per second: 199, episode reward: -235.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.970 [0.000, 2.000],  loss: 0.353918, mae: 8.077158, mean_q: -11.584505, mean_eps: 0.702308\n",
            "  47405/200000: episode: 128, duration: 1.404s, episode steps: 282, steps per second: 201, episode reward: -281.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.014 [0.000, 2.000],  loss: 0.381909, mae: 8.128317, mean_q: -11.650428, mean_eps: 0.700661\n",
            "  47634/200000: episode: 129, duration: 1.090s, episode steps: 229, steps per second: 210, episode reward: -228.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.991 [0.000, 2.000],  loss: 0.380348, mae: 8.085590, mean_q: -11.587627, mean_eps: 0.699040\n",
            "  47792/200000: episode: 130, duration: 0.828s, episode steps: 158, steps per second: 191, episode reward: -157.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.013 [0.000, 2.000],  loss: 0.349733, mae: 8.151681, mean_q: -11.705832, mean_eps: 0.697824\n",
            "  48114/200000: episode: 131, duration: 1.692s, episode steps: 322, steps per second: 190, episode reward: -321.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.966 [0.000, 2.000],  loss: 0.374377, mae: 8.132388, mean_q: -11.642820, mean_eps: 0.696304\n",
            "  48536/200000: episode: 132, duration: 2.332s, episode steps: 422, steps per second: 181, episode reward: -421.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.976 [0.000, 2.000],  loss: 0.379141, mae: 8.173207, mean_q: -11.712875, mean_eps: 0.693948\n",
            "  48732/200000: episode: 133, duration: 1.398s, episode steps: 196, steps per second: 140, episode reward: -195.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.980 [0.000, 2.000],  loss: 0.347297, mae: 8.172041, mean_q: -11.745109, mean_eps: 0.691997\n",
            "  48935/200000: episode: 134, duration: 1.533s, episode steps: 203, steps per second: 132, episode reward: -202.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.916 [0.000, 2.000],  loss: 0.363627, mae: 8.183163, mean_q: -11.742595, mean_eps: 0.690731\n",
            "  49221/200000: episode: 135, duration: 1.552s, episode steps: 286, steps per second: 184, episode reward: -285.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.909 [0.000, 2.000],  loss: 0.381768, mae: 8.201182, mean_q: -11.744005, mean_eps: 0.689173\n",
            "  49408/200000: episode: 136, duration: 0.969s, episode steps: 187, steps per second: 193, episode reward: -186.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.032 [0.000, 2.000],  loss: 0.363709, mae: 8.250424, mean_q: -11.820524, mean_eps: 0.687678\n",
            "  49594/200000: episode: 137, duration: 0.992s, episode steps: 186, steps per second: 188, episode reward: -185.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.855 [0.000, 2.000],  loss: 0.315353, mae: 8.199267, mean_q: -11.774547, mean_eps: 0.686500\n",
            "  49721/200000: episode: 138, duration: 0.688s, episode steps: 127, steps per second: 185, episode reward: -126.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.953 [0.000, 2.000],  loss: 0.359014, mae: 8.241341, mean_q: -11.824869, mean_eps: 0.685499\n",
            "  49922/200000: episode: 139, duration: 0.991s, episode steps: 201, steps per second: 203, episode reward: -200.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.055 [0.000, 2.000],  loss: 0.337541, mae: 8.238739, mean_q: -11.825339, mean_eps: 0.684461\n",
            "  50188/200000: episode: 140, duration: 1.306s, episode steps: 266, steps per second: 204, episode reward: -265.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.970 [0.000, 2.000],  loss: 0.378394, mae: 8.298765, mean_q: -11.883982, mean_eps: 0.682991\n",
            "  50535/200000: episode: 141, duration: 1.752s, episode steps: 347, steps per second: 198, episode reward: -346.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.977 [0.000, 2.000],  loss: 0.374743, mae: 8.337852, mean_q: -11.953221, mean_eps: 0.681053\n",
            "  50728/200000: episode: 142, duration: 1.082s, episode steps: 193, steps per second: 178, episode reward: -192.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.073 [0.000, 2.000],  loss: 0.396408, mae: 8.285220, mean_q: -11.883686, mean_eps: 0.679343\n",
            "  50893/200000: episode: 143, duration: 1.008s, episode steps: 165, steps per second: 164, episode reward: -164.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.018 [0.000, 2.000],  loss: 0.335045, mae: 8.286844, mean_q: -11.890733, mean_eps: 0.678203\n",
            "  51129/200000: episode: 144, duration: 1.845s, episode steps: 236, steps per second: 128, episode reward: -235.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.979 [0.000, 2.000],  loss: 0.394188, mae: 8.387745, mean_q: -12.025429, mean_eps: 0.676924\n",
            "  51421/200000: episode: 145, duration: 2.070s, episode steps: 292, steps per second: 141, episode reward: -291.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.890 [0.000, 2.000],  loss: 0.372273, mae: 8.430259, mean_q: -12.108939, mean_eps: 0.675252\n",
            "  51609/200000: episode: 146, duration: 0.993s, episode steps: 188, steps per second: 189, episode reward: -187.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.995 [0.000, 2.000],  loss: 0.398663, mae: 8.490269, mean_q: -12.169832, mean_eps: 0.673732\n",
            "  51786/200000: episode: 147, duration: 0.973s, episode steps: 177, steps per second: 182, episode reward: -176.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.977 [0.000, 2.000],  loss: 0.395685, mae: 8.476188, mean_q: -12.173292, mean_eps: 0.672579\n",
            "  52049/200000: episode: 148, duration: 1.430s, episode steps: 263, steps per second: 184, episode reward: -262.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.989 [0.000, 2.000],  loss: 0.384332, mae: 8.478410, mean_q: -12.157321, mean_eps: 0.671186\n",
            "  52330/200000: episode: 149, duration: 1.477s, episode steps: 281, steps per second: 190, episode reward: -280.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.043 [0.000, 2.000],  loss: 0.423744, mae: 8.590869, mean_q: -12.326959, mean_eps: 0.669463\n",
            "  52540/200000: episode: 150, duration: 1.085s, episode steps: 210, steps per second: 194, episode reward: -209.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.086 [0.000, 2.000],  loss: 0.423035, mae: 8.568565, mean_q: -12.269971, mean_eps: 0.667918\n",
            "  52732/200000: episode: 151, duration: 1.018s, episode steps: 192, steps per second: 189, episode reward: -191.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.047 [0.000, 2.000],  loss: 0.393676, mae: 8.574729, mean_q: -12.304560, mean_eps: 0.666651\n",
            "  52897/200000: episode: 152, duration: 0.809s, episode steps: 165, steps per second: 204, episode reward: -164.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.042 [0.000, 2.000],  loss: 0.344079, mae: 8.579935, mean_q: -12.324664, mean_eps: 0.665511\n",
            "  53078/200000: episode: 153, duration: 0.872s, episode steps: 181, steps per second: 207, episode reward: -180.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.967 [0.000, 2.000],  loss: 0.424772, mae: 8.550678, mean_q: -12.265723, mean_eps: 0.664409\n",
            "  53245/200000: episode: 154, duration: 0.798s, episode steps: 167, steps per second: 209, episode reward: -166.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.018 [0.000, 2.000],  loss: 0.467236, mae: 8.662465, mean_q: -12.418273, mean_eps: 0.663307\n",
            "  53479/200000: episode: 155, duration: 1.693s, episode steps: 234, steps per second: 138, episode reward: -233.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.068 [0.000, 2.000],  loss: 0.416958, mae: 8.655390, mean_q: -12.424582, mean_eps: 0.662041\n",
            "  53659/200000: episode: 156, duration: 1.307s, episode steps: 180, steps per second: 138, episode reward: -179.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.844 [0.000, 2.000],  loss: 0.415547, mae: 8.689106, mean_q: -12.457805, mean_eps: 0.660736\n",
            "  54048/200000: episode: 157, duration: 2.261s, episode steps: 389, steps per second: 172, episode reward: -388.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.111 [0.000, 2.000],  loss: 0.343302, mae: 8.621206, mean_q: -12.374707, mean_eps: 0.658937\n",
            "  54273/200000: episode: 158, duration: 1.202s, episode steps: 225, steps per second: 187, episode reward: -224.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.827 [0.000, 2.000],  loss: 0.387895, mae: 8.709751, mean_q: -12.514073, mean_eps: 0.656987\n",
            "  54503/200000: episode: 159, duration: 1.261s, episode steps: 230, steps per second: 182, episode reward: -229.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.957 [0.000, 2.000],  loss: 0.446684, mae: 8.701654, mean_q: -12.471731, mean_eps: 0.655543\n",
            "  54769/200000: episode: 160, duration: 1.417s, episode steps: 266, steps per second: 188, episode reward: -265.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.162 [0.000, 2.000],  loss: 0.413356, mae: 8.649874, mean_q: -12.390477, mean_eps: 0.653972\n",
            "  54958/200000: episode: 161, duration: 1.020s, episode steps: 189, steps per second: 185, episode reward: -188.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.005 [0.000, 2.000],  loss: 0.393401, mae: 8.588898, mean_q: -12.323318, mean_eps: 0.652528\n",
            "  55232/200000: episode: 162, duration: 1.416s, episode steps: 274, steps per second: 194, episode reward: -273.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.026 [0.000, 2.000],  loss: 0.390587, mae: 8.711372, mean_q: -12.488467, mean_eps: 0.651071\n",
            "  55451/200000: episode: 163, duration: 1.211s, episode steps: 219, steps per second: 181, episode reward: -218.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.110 [0.000, 2.000],  loss: 0.376244, mae: 8.722716, mean_q: -12.505189, mean_eps: 0.649513\n",
            "  55600/200000: episode: 164, duration: 0.753s, episode steps: 149, steps per second: 198, episode reward: -148.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.919 [0.000, 2.000],  loss: 0.392334, mae: 8.713847, mean_q: -12.494275, mean_eps: 0.648348\n",
            "  55808/200000: episode: 165, duration: 1.524s, episode steps: 208, steps per second: 136, episode reward: -207.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.058 [0.000, 2.000],  loss: 0.414550, mae: 8.647324, mean_q: -12.399593, mean_eps: 0.647221\n",
            "  56203/200000: episode: 166, duration: 2.777s, episode steps: 395, steps per second: 142, episode reward: -394.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.894 [0.000, 2.000],  loss: 0.400515, mae: 8.742128, mean_q: -12.543407, mean_eps: 0.645308\n",
            "  56440/200000: episode: 167, duration: 1.225s, episode steps: 237, steps per second: 193, episode reward: -236.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.975 [0.000, 2.000],  loss: 0.434298, mae: 8.789353, mean_q: -12.595067, mean_eps: 0.643307\n",
            "  56605/200000: episode: 168, duration: 0.854s, episode steps: 165, steps per second: 193, episode reward: -164.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.061 [0.000, 2.000],  loss: 0.412200, mae: 8.828404, mean_q: -12.660872, mean_eps: 0.642027\n",
            "  56866/200000: episode: 169, duration: 1.304s, episode steps: 261, steps per second: 200, episode reward: -260.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.004 [0.000, 2.000],  loss: 0.374158, mae: 8.771331, mean_q: -12.600136, mean_eps: 0.640672\n",
            "  57060/200000: episode: 170, duration: 1.042s, episode steps: 194, steps per second: 186, episode reward: -193.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.072 [0.000, 2.000],  loss: 0.390787, mae: 8.759016, mean_q: -12.569411, mean_eps: 0.639241\n",
            "  57348/200000: episode: 171, duration: 1.470s, episode steps: 288, steps per second: 196, episode reward: -287.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.934 [0.000, 2.000],  loss: 0.363210, mae: 8.881935, mean_q: -12.764486, mean_eps: 0.637721\n",
            "  57471/200000: episode: 172, duration: 0.674s, episode steps: 123, steps per second: 183, episode reward: -122.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.911 [0.000, 2.000],  loss: 0.326093, mae: 8.855244, mean_q: -12.728795, mean_eps: 0.636416\n",
            "  57719/200000: episode: 173, duration: 1.318s, episode steps: 248, steps per second: 188, episode reward: -247.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.944 [0.000, 2.000],  loss: 0.409706, mae: 8.786778, mean_q: -12.607090, mean_eps: 0.635238\n",
            "  57918/200000: episode: 174, duration: 1.092s, episode steps: 199, steps per second: 182, episode reward: -198.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.025 [0.000, 2.000],  loss: 0.365910, mae: 8.827258, mean_q: -12.676674, mean_eps: 0.633819\n",
            "  58051/200000: episode: 175, duration: 0.925s, episode steps: 133, steps per second: 144, episode reward: -132.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.083 [0.000, 2.000],  loss: 0.458948, mae: 8.839846, mean_q: -12.654924, mean_eps: 0.632768\n",
            "  58365/200000: episode: 176, duration: 2.297s, episode steps: 314, steps per second: 137, episode reward: -313.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.025 [0.000, 2.000],  loss: 0.386145, mae: 8.789264, mean_q: -12.615140, mean_eps: 0.631349\n",
            "  58518/200000: episode: 177, duration: 1.062s, episode steps: 153, steps per second: 144, episode reward: -152.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.941 [0.000, 2.000],  loss: 0.491944, mae: 8.913154, mean_q: -12.775305, mean_eps: 0.629867\n",
            "  58768/200000: episode: 178, duration: 1.318s, episode steps: 250, steps per second: 190, episode reward: -249.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.028 [0.000, 2.000],  loss: 0.424968, mae: 8.850388, mean_q: -12.706458, mean_eps: 0.628601\n",
            "  58970/200000: episode: 179, duration: 1.018s, episode steps: 202, steps per second: 199, episode reward: -201.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.040 [0.000, 2.000],  loss: 0.373754, mae: 8.805131, mean_q: -12.653384, mean_eps: 0.627169\n",
            "  59146/200000: episode: 180, duration: 0.930s, episode steps: 176, steps per second: 189, episode reward: -175.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.938 [0.000, 2.000],  loss: 0.372450, mae: 8.915733, mean_q: -12.818845, mean_eps: 0.625966\n",
            "  59323/200000: episode: 181, duration: 0.953s, episode steps: 177, steps per second: 186, episode reward: -176.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.414909, mae: 9.023340, mean_q: -12.952623, mean_eps: 0.624851\n",
            "  59527/200000: episode: 182, duration: 1.101s, episode steps: 204, steps per second: 185, episode reward: -203.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.922 [0.000, 2.000],  loss: 0.402896, mae: 8.967643, mean_q: -12.860063, mean_eps: 0.623648\n",
            "  59754/200000: episode: 183, duration: 1.272s, episode steps: 227, steps per second: 178, episode reward: -226.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.093 [0.000, 2.000],  loss: 0.425633, mae: 8.931667, mean_q: -12.828856, mean_eps: 0.622280\n",
            "  59948/200000: episode: 184, duration: 1.013s, episode steps: 194, steps per second: 192, episode reward: -193.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.005 [0.000, 2.000],  loss: 0.422216, mae: 8.968526, mean_q: -12.862748, mean_eps: 0.620950\n",
            "  60173/200000: episode: 185, duration: 1.342s, episode steps: 225, steps per second: 168, episode reward: -224.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.929 [0.000, 2.000],  loss: 0.481630, mae: 8.920666, mean_q: -12.781943, mean_eps: 0.619620\n",
            "  60407/200000: episode: 186, duration: 1.458s, episode steps: 234, steps per second: 160, episode reward: -233.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.983 [0.000, 2.000],  loss: 0.429193, mae: 8.953405, mean_q: -12.847810, mean_eps: 0.618163\n",
            "  60664/200000: episode: 187, duration: 1.927s, episode steps: 257, steps per second: 133, episode reward: -256.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.016 [0.000, 2.000],  loss: 0.435662, mae: 8.997383, mean_q: -12.927290, mean_eps: 0.616618\n",
            "  60910/200000: episode: 188, duration: 1.611s, episode steps: 246, steps per second: 153, episode reward: -245.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.065 [0.000, 2.000],  loss: 0.374268, mae: 8.948033, mean_q: -12.872688, mean_eps: 0.615022\n",
            "  61081/200000: episode: 189, duration: 0.920s, episode steps: 171, steps per second: 186, episode reward: -170.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.431261, mae: 8.946060, mean_q: -12.849434, mean_eps: 0.613692\n",
            "  61267/200000: episode: 190, duration: 0.996s, episode steps: 186, steps per second: 187, episode reward: -185.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.941 [0.000, 2.000],  loss: 0.398030, mae: 9.016842, mean_q: -12.956644, mean_eps: 0.612565\n",
            "  61454/200000: episode: 191, duration: 1.014s, episode steps: 187, steps per second: 184, episode reward: -186.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.016 [0.000, 2.000],  loss: 0.396359, mae: 9.016960, mean_q: -12.958181, mean_eps: 0.611387\n",
            "  61681/200000: episode: 192, duration: 1.231s, episode steps: 227, steps per second: 184, episode reward: -226.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.093 [0.000, 2.000],  loss: 0.419868, mae: 8.993902, mean_q: -12.927291, mean_eps: 0.610069\n",
            "  61879/200000: episode: 193, duration: 1.041s, episode steps: 198, steps per second: 190, episode reward: -197.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.030 [0.000, 2.000],  loss: 0.438666, mae: 8.990317, mean_q: -12.914798, mean_eps: 0.608727\n",
            "  62073/200000: episode: 194, duration: 1.079s, episode steps: 194, steps per second: 180, episode reward: -193.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.067 [0.000, 2.000],  loss: 0.446547, mae: 9.009618, mean_q: -12.940516, mean_eps: 0.607485\n",
            "  62224/200000: episode: 195, duration: 0.841s, episode steps: 151, steps per second: 179, episode reward: -150.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.993 [0.000, 2.000],  loss: 0.441077, mae: 9.125789, mean_q: -13.115788, mean_eps: 0.606396\n",
            "  62469/200000: episode: 196, duration: 1.257s, episode steps: 245, steps per second: 195, episode reward: -244.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.057 [0.000, 2.000],  loss: 0.404871, mae: 9.089212, mean_q: -13.074309, mean_eps: 0.605142\n",
            "  62657/200000: episode: 197, duration: 1.151s, episode steps: 188, steps per second: 163, episode reward: -187.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.112 [0.000, 2.000],  loss: 0.387818, mae: 9.063507, mean_q: -13.028504, mean_eps: 0.603761\n",
            "  62820/200000: episode: 198, duration: 1.203s, episode steps: 163, steps per second: 136, episode reward: -162.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.335126, mae: 9.058206, mean_q: -13.039241, mean_eps: 0.602659\n",
            "  63004/200000: episode: 199, duration: 1.464s, episode steps: 184, steps per second: 126, episode reward: -183.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.875 [0.000, 2.000],  loss: 0.407792, mae: 9.107983, mean_q: -13.085791, mean_eps: 0.601570\n",
            "  63193/200000: episode: 200, duration: 1.208s, episode steps: 189, steps per second: 156, episode reward: -188.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.989 [0.000, 2.000],  loss: 0.480403, mae: 9.174158, mean_q: -13.189352, mean_eps: 0.600379\n",
            "  63392/200000: episode: 201, duration: 1.024s, episode steps: 199, steps per second: 194, episode reward: -198.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.111 [0.000, 2.000],  loss: 0.489561, mae: 9.125412, mean_q: -13.115717, mean_eps: 0.599151\n",
            "  63531/200000: episode: 202, duration: 0.759s, episode steps: 139, steps per second: 183, episode reward: -138.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.964 [0.000, 2.000],  loss: 0.405389, mae: 9.130603, mean_q: -13.146253, mean_eps: 0.598087\n",
            "  63703/200000: episode: 203, duration: 0.945s, episode steps: 172, steps per second: 182, episode reward: -171.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.808 [0.000, 2.000],  loss: 0.440865, mae: 9.152894, mean_q: -13.151020, mean_eps: 0.597099\n",
            "  63874/200000: episode: 204, duration: 0.956s, episode steps: 171, steps per second: 179, episode reward: -170.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.029 [0.000, 2.000],  loss: 0.438124, mae: 9.169126, mean_q: -13.175427, mean_eps: 0.596009\n",
            "  64050/200000: episode: 205, duration: 0.967s, episode steps: 176, steps per second: 182, episode reward: -175.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.114 [0.000, 2.000],  loss: 0.525018, mae: 9.205874, mean_q: -13.205379, mean_eps: 0.594907\n",
            "  64223/200000: episode: 206, duration: 0.930s, episode steps: 173, steps per second: 186, episode reward: -172.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.012 [0.000, 2.000],  loss: 0.414272, mae: 9.303877, mean_q: -13.370293, mean_eps: 0.593805\n",
            "  64398/200000: episode: 207, duration: 0.903s, episode steps: 175, steps per second: 194, episode reward: -174.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.966 [0.000, 2.000],  loss: 0.519818, mae: 9.222485, mean_q: -13.248634, mean_eps: 0.592703\n",
            "  64613/200000: episode: 208, duration: 1.199s, episode steps: 215, steps per second: 179, episode reward: -214.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.860 [0.000, 2.000],  loss: 0.398401, mae: 9.233428, mean_q: -13.269876, mean_eps: 0.591462\n",
            "  64830/200000: episode: 209, duration: 1.159s, episode steps: 217, steps per second: 187, episode reward: -216.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.949 [0.000, 2.000],  loss: 0.371674, mae: 9.227292, mean_q: -13.276040, mean_eps: 0.590094\n",
            "  65027/200000: episode: 210, duration: 1.250s, episode steps: 197, steps per second: 158, episode reward: -196.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.081 [0.000, 2.000],  loss: 0.426669, mae: 9.261006, mean_q: -13.320041, mean_eps: 0.588789\n",
            "  65220/200000: episode: 211, duration: 1.510s, episode steps: 193, steps per second: 128, episode reward: -192.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.067 [0.000, 2.000],  loss: 0.461344, mae: 9.259062, mean_q: -13.303986, mean_eps: 0.587561\n",
            "  65381/200000: episode: 212, duration: 1.298s, episode steps: 161, steps per second: 124, episode reward: -160.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.913 [0.000, 2.000],  loss: 0.427088, mae: 9.219032, mean_q: -13.253560, mean_eps: 0.586433\n",
            "  65538/200000: episode: 213, duration: 0.873s, episode steps: 157, steps per second: 180, episode reward: -156.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.962 [0.000, 2.000],  loss: 0.421879, mae: 9.332705, mean_q: -13.423522, mean_eps: 0.585420\n",
            "  65726/200000: episode: 214, duration: 1.051s, episode steps: 188, steps per second: 179, episode reward: -187.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.213 [0.000, 2.000],  loss: 0.477130, mae: 9.198679, mean_q: -13.207446, mean_eps: 0.584331\n",
            "  65944/200000: episode: 215, duration: 1.231s, episode steps: 218, steps per second: 177, episode reward: -217.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.908 [0.000, 2.000],  loss: 0.405901, mae: 9.268077, mean_q: -13.330893, mean_eps: 0.583051\n",
            "  66064/200000: episode: 216, duration: 0.674s, episode steps: 120, steps per second: 178, episode reward: -119.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.100 [0.000, 2.000],  loss: 0.468829, mae: 9.212727, mean_q: -13.233915, mean_eps: 0.581987\n",
            "  66251/200000: episode: 217, duration: 1.028s, episode steps: 187, steps per second: 182, episode reward: -186.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.059 [0.000, 2.000],  loss: 0.408933, mae: 9.244473, mean_q: -13.290817, mean_eps: 0.581012\n",
            "  66406/200000: episode: 218, duration: 0.861s, episode steps: 155, steps per second: 180, episode reward: -154.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.226 [0.000, 2.000],  loss: 0.432137, mae: 9.295912, mean_q: -13.346456, mean_eps: 0.579923\n",
            "  66585/200000: episode: 219, duration: 0.930s, episode steps: 179, steps per second: 192, episode reward: -178.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.994 [0.000, 2.000],  loss: 0.486905, mae: 9.206718, mean_q: -13.214730, mean_eps: 0.578859\n",
            "  66814/200000: episode: 220, duration: 1.198s, episode steps: 229, steps per second: 191, episode reward: -228.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.974 [0.000, 2.000],  loss: 0.455407, mae: 9.302241, mean_q: -13.371781, mean_eps: 0.577567\n",
            "  66997/200000: episode: 221, duration: 0.924s, episode steps: 183, steps per second: 198, episode reward: -182.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.442023, mae: 9.266837, mean_q: -13.328369, mean_eps: 0.576262\n",
            "  67172/200000: episode: 222, duration: 0.937s, episode steps: 175, steps per second: 187, episode reward: -174.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.240 [0.000, 2.000],  loss: 0.458712, mae: 9.317988, mean_q: -13.368081, mean_eps: 0.575135\n",
            "  67371/200000: episode: 223, duration: 1.486s, episode steps: 199, steps per second: 134, episode reward: -198.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.095 [0.000, 2.000],  loss: 0.487718, mae: 9.213330, mean_q: -13.222089, mean_eps: 0.573957\n",
            "  67528/200000: episode: 224, duration: 1.256s, episode steps: 157, steps per second: 125, episode reward: -156.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.076 [0.000, 2.000],  loss: 0.402417, mae: 9.313781, mean_q: -13.401961, mean_eps: 0.572829\n",
            "  67789/200000: episode: 225, duration: 1.872s, episode steps: 261, steps per second: 139, episode reward: -260.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.008 [0.000, 2.000],  loss: 0.431715, mae: 9.308378, mean_q: -13.377132, mean_eps: 0.571499\n",
            "  68035/200000: episode: 226, duration: 1.475s, episode steps: 246, steps per second: 167, episode reward: -245.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.919 [0.000, 2.000],  loss: 0.454578, mae: 9.316621, mean_q: -13.389600, mean_eps: 0.569891\n",
            "  68208/200000: episode: 227, duration: 0.969s, episode steps: 173, steps per second: 179, episode reward: -172.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.104 [0.000, 2.000],  loss: 0.469538, mae: 9.474033, mean_q: -13.600012, mean_eps: 0.568573\n",
            "  68375/200000: episode: 228, duration: 0.922s, episode steps: 167, steps per second: 181, episode reward: -166.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.216 [0.000, 2.000],  loss: 0.524984, mae: 9.454107, mean_q: -13.548882, mean_eps: 0.567497\n",
            "  68547/200000: episode: 229, duration: 0.914s, episode steps: 172, steps per second: 188, episode reward: -171.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.959 [0.000, 2.000],  loss: 0.393773, mae: 9.401476, mean_q: -13.516147, mean_eps: 0.566420\n",
            "  68886/200000: episode: 230, duration: 1.866s, episode steps: 339, steps per second: 182, episode reward: -338.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.879 [0.000, 2.000],  loss: 0.453395, mae: 9.466808, mean_q: -13.599439, mean_eps: 0.564799\n",
            "  69065/200000: episode: 231, duration: 0.961s, episode steps: 179, steps per second: 186, episode reward: -178.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.939 [0.000, 2.000],  loss: 0.458068, mae: 9.435972, mean_q: -13.558949, mean_eps: 0.563152\n",
            "  69251/200000: episode: 232, duration: 0.989s, episode steps: 186, steps per second: 188, episode reward: -185.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.935 [0.000, 2.000],  loss: 0.469736, mae: 9.488804, mean_q: -13.642707, mean_eps: 0.561999\n",
            "  69443/200000: episode: 233, duration: 1.020s, episode steps: 192, steps per second: 188, episode reward: -191.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.130 [0.000, 2.000],  loss: 0.478636, mae: 9.495386, mean_q: -13.616060, mean_eps: 0.560809\n",
            "  69572/200000: episode: 234, duration: 1.013s, episode steps: 129, steps per second: 127, episode reward: -128.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.155 [0.000, 2.000],  loss: 0.412628, mae: 9.511899, mean_q: -13.671437, mean_eps: 0.559795\n",
            "  69764/200000: episode: 235, duration: 1.555s, episode steps: 192, steps per second: 123, episode reward: -191.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.995 [0.000, 2.000],  loss: 0.478906, mae: 9.447426, mean_q: -13.575023, mean_eps: 0.558782\n",
            "  69898/200000: episode: 236, duration: 1.102s, episode steps: 134, steps per second: 122, episode reward: -133.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.007 [0.000, 2.000],  loss: 0.489103, mae: 9.522014, mean_q: -13.676878, mean_eps: 0.557743\n",
            "  70083/200000: episode: 237, duration: 1.115s, episode steps: 185, steps per second: 166, episode reward: -184.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.070 [0.000, 2.000],  loss: 0.539414, mae: 9.465064, mean_q: -13.568343, mean_eps: 0.556730\n",
            "  70318/200000: episode: 238, duration: 1.415s, episode steps: 235, steps per second: 166, episode reward: -234.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.889 [0.000, 2.000],  loss: 0.414341, mae: 9.493285, mean_q: -13.653785, mean_eps: 0.555400\n",
            "  70462/200000: episode: 239, duration: 0.847s, episode steps: 144, steps per second: 170, episode reward: -143.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.014 [0.000, 2.000],  loss: 0.429462, mae: 9.525656, mean_q: -13.702451, mean_eps: 0.554197\n",
            "  70610/200000: episode: 240, duration: 0.838s, episode steps: 148, steps per second: 177, episode reward: -147.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.041 [0.000, 2.000],  loss: 0.399679, mae: 9.479411, mean_q: -13.627580, mean_eps: 0.553272\n",
            "  70765/200000: episode: 241, duration: 0.889s, episode steps: 155, steps per second: 174, episode reward: -154.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.981 [0.000, 2.000],  loss: 0.456456, mae: 9.425465, mean_q: -13.515571, mean_eps: 0.552309\n",
            "  70987/200000: episode: 242, duration: 1.220s, episode steps: 222, steps per second: 182, episode reward: -221.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.117 [0.000, 2.000],  loss: 0.471848, mae: 9.398103, mean_q: -13.488832, mean_eps: 0.551119\n",
            "  71159/200000: episode: 243, duration: 0.956s, episode steps: 172, steps per second: 180, episode reward: -171.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.953 [0.000, 2.000],  loss: 0.480336, mae: 9.498713, mean_q: -13.630216, mean_eps: 0.549877\n",
            "  71308/200000: episode: 244, duration: 0.815s, episode steps: 149, steps per second: 183, episode reward: -148.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.872 [0.000, 2.000],  loss: 0.404728, mae: 9.500502, mean_q: -13.641644, mean_eps: 0.548864\n",
            "  71808/200000: episode: 245, duration: 3.264s, episode steps: 500, steps per second: 153, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.344 [0.000, 2.000],  loss: 0.451419, mae: 9.488918, mean_q: -13.627931, mean_eps: 0.546812\n",
            "  71954/200000: episode: 246, duration: 1.055s, episode steps: 146, steps per second: 138, episode reward: -145.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.966 [0.000, 2.000],  loss: 0.405364, mae: 9.409540, mean_q: -13.534156, mean_eps: 0.544760\n",
            "  72116/200000: episode: 247, duration: 1.248s, episode steps: 162, steps per second: 130, episode reward: -161.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.988 [0.000, 2.000],  loss: 0.514468, mae: 9.513272, mean_q: -13.657953, mean_eps: 0.543785\n",
            "  72295/200000: episode: 248, duration: 0.968s, episode steps: 179, steps per second: 185, episode reward: -178.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.927 [0.000, 2.000],  loss: 0.531991, mae: 9.545914, mean_q: -13.695212, mean_eps: 0.542708\n",
            "  72460/200000: episode: 249, duration: 0.904s, episode steps: 165, steps per second: 183, episode reward: -164.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.055 [0.000, 2.000],  loss: 0.432278, mae: 9.547040, mean_q: -13.713756, mean_eps: 0.541619\n",
            "  72614/200000: episode: 250, duration: 0.846s, episode steps: 154, steps per second: 182, episode reward: -153.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.922 [0.000, 2.000],  loss: 0.479236, mae: 9.573416, mean_q: -13.728750, mean_eps: 0.540605\n",
            "  72753/200000: episode: 251, duration: 0.790s, episode steps: 139, steps per second: 176, episode reward: -138.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.007 [0.000, 2.000],  loss: 0.516217, mae: 9.652748, mean_q: -13.859121, mean_eps: 0.539668\n",
            "  72912/200000: episode: 252, duration: 0.912s, episode steps: 159, steps per second: 174, episode reward: -158.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.119 [0.000, 2.000],  loss: 0.445812, mae: 9.542212, mean_q: -13.701015, mean_eps: 0.538731\n",
            "  73412/200000: episode: 253, duration: 2.856s, episode steps: 500, steps per second: 175, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.294 [0.000, 2.000],  loss: 0.473013, mae: 9.505082, mean_q: -13.638402, mean_eps: 0.536653\n",
            "  73547/200000: episode: 254, duration: 0.795s, episode steps: 135, steps per second: 170, episode reward: -134.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.007 [0.000, 2.000],  loss: 0.467812, mae: 9.621907, mean_q: -13.822908, mean_eps: 0.534639\n",
            "  73707/200000: episode: 255, duration: 0.939s, episode steps: 160, steps per second: 170, episode reward: -159.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.850 [0.000, 2.000],  loss: 0.484922, mae: 9.584673, mean_q: -13.751303, mean_eps: 0.533702\n",
            "  73874/200000: episode: 256, duration: 1.039s, episode steps: 167, steps per second: 161, episode reward: -166.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.982 [0.000, 2.000],  loss: 0.540090, mae: 9.530098, mean_q: -13.669463, mean_eps: 0.532663\n",
            "  74041/200000: episode: 257, duration: 1.266s, episode steps: 167, steps per second: 132, episode reward: -166.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.976 [0.000, 2.000],  loss: 0.450752, mae: 9.558287, mean_q: -13.728552, mean_eps: 0.531599\n",
            "  74178/200000: episode: 258, duration: 1.015s, episode steps: 137, steps per second: 135, episode reward: -136.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.051 [0.000, 2.000],  loss: 0.442764, mae: 9.501800, mean_q: -13.648565, mean_eps: 0.530637\n",
            "  74306/200000: episode: 259, duration: 0.999s, episode steps: 128, steps per second: 128, episode reward: -127.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.047 [0.000, 2.000],  loss: 0.501923, mae: 9.488923, mean_q: -13.614252, mean_eps: 0.529801\n",
            "  74490/200000: episode: 260, duration: 0.997s, episode steps: 184, steps per second: 185, episode reward: -183.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.033 [0.000, 2.000],  loss: 0.471152, mae: 9.541633, mean_q: -13.688451, mean_eps: 0.528813\n",
            "  74641/200000: episode: 261, duration: 0.872s, episode steps: 151, steps per second: 173, episode reward: -150.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.974 [0.000, 2.000],  loss: 0.465050, mae: 9.549686, mean_q: -13.719255, mean_eps: 0.527749\n",
            "  74814/200000: episode: 262, duration: 0.972s, episode steps: 173, steps per second: 178, episode reward: -172.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.191 [0.000, 2.000],  loss: 0.431528, mae: 9.510327, mean_q: -13.676769, mean_eps: 0.526723\n",
            "  74975/200000: episode: 263, duration: 0.947s, episode steps: 161, steps per second: 170, episode reward: -160.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.031 [0.000, 2.000],  loss: 0.543609, mae: 9.593236, mean_q: -13.764495, mean_eps: 0.525671\n",
            "  75153/200000: episode: 264, duration: 1.062s, episode steps: 178, steps per second: 168, episode reward: -177.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.949 [0.000, 2.000],  loss: 0.433841, mae: 9.616740, mean_q: -13.826808, mean_eps: 0.524595\n",
            "  75303/200000: episode: 265, duration: 0.864s, episode steps: 150, steps per second: 174, episode reward: -149.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.120 [0.000, 2.000],  loss: 0.521546, mae: 9.703655, mean_q: -13.925689, mean_eps: 0.523556\n",
            "  75493/200000: episode: 266, duration: 1.140s, episode steps: 190, steps per second: 167, episode reward: -189.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.492258, mae: 9.692167, mean_q: -13.910878, mean_eps: 0.522479\n",
            "  75727/200000: episode: 267, duration: 1.260s, episode steps: 234, steps per second: 186, episode reward: -233.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.094 [0.000, 2.000],  loss: 0.508178, mae: 9.707784, mean_q: -13.934910, mean_eps: 0.521137\n",
            "  75871/200000: episode: 268, duration: 0.790s, episode steps: 144, steps per second: 182, episode reward: -143.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.014 [0.000, 2.000],  loss: 0.429615, mae: 9.636782, mean_q: -13.853264, mean_eps: 0.519946\n",
            "  76041/200000: episode: 269, duration: 0.959s, episode steps: 170, steps per second: 177, episode reward: -169.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.894 [0.000, 2.000],  loss: 0.555810, mae: 9.614418, mean_q: -13.787089, mean_eps: 0.518945\n",
            "  76168/200000: episode: 270, duration: 1.061s, episode steps: 127, steps per second: 120, episode reward: -126.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.110 [0.000, 2.000],  loss: 0.445972, mae: 9.630946, mean_q: -13.826995, mean_eps: 0.518008\n",
            "  76357/200000: episode: 271, duration: 1.488s, episode steps: 189, steps per second: 127, episode reward: -188.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.751 [0.000, 2.000],  loss: 0.470063, mae: 9.648354, mean_q: -13.846454, mean_eps: 0.517007\n",
            "  76533/200000: episode: 272, duration: 1.289s, episode steps: 176, steps per second: 137, episode reward: -175.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.943 [0.000, 2.000],  loss: 0.456657, mae: 9.561962, mean_q: -13.752175, mean_eps: 0.515842\n",
            "  76697/200000: episode: 273, duration: 0.901s, episode steps: 164, steps per second: 182, episode reward: -163.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.012 [0.000, 2.000],  loss: 0.398351, mae: 9.560628, mean_q: -13.747441, mean_eps: 0.514765\n",
            "  76843/200000: episode: 274, duration: 0.814s, episode steps: 146, steps per second: 179, episode reward: -145.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.993 [0.000, 2.000],  loss: 0.424847, mae: 9.640244, mean_q: -13.857455, mean_eps: 0.513790\n",
            "  77001/200000: episode: 275, duration: 0.920s, episode steps: 158, steps per second: 172, episode reward: -157.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.823 [0.000, 2.000],  loss: 0.458392, mae: 9.578355, mean_q: -13.760505, mean_eps: 0.512827\n",
            "  77171/200000: episode: 276, duration: 0.966s, episode steps: 170, steps per second: 176, episode reward: -169.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.894 [0.000, 2.000],  loss: 0.490778, mae: 9.765002, mean_q: -14.023088, mean_eps: 0.511789\n",
            "  77370/200000: episode: 277, duration: 1.105s, episode steps: 199, steps per second: 180, episode reward: -198.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.884 [0.000, 2.000],  loss: 0.487929, mae: 9.724404, mean_q: -13.969966, mean_eps: 0.510623\n",
            "  77518/200000: episode: 278, duration: 0.916s, episode steps: 148, steps per second: 162, episode reward: -147.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.899 [0.000, 2.000],  loss: 0.430494, mae: 9.738438, mean_q: -13.992679, mean_eps: 0.509521\n",
            "  77731/200000: episode: 279, duration: 1.154s, episode steps: 213, steps per second: 185, episode reward: -212.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.529253, mae: 9.773443, mean_q: -14.017039, mean_eps: 0.508381\n",
            "  77895/200000: episode: 280, duration: 1.039s, episode steps: 164, steps per second: 158, episode reward: -163.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.146 [0.000, 2.000],  loss: 0.537964, mae: 9.704272, mean_q: -13.896100, mean_eps: 0.507191\n",
            "  78023/200000: episode: 281, duration: 0.760s, episode steps: 128, steps per second: 169, episode reward: -127.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.812 [0.000, 2.000],  loss: 0.449480, mae: 9.725136, mean_q: -13.989752, mean_eps: 0.506266\n",
            "  78180/200000: episode: 282, duration: 0.927s, episode steps: 157, steps per second: 169, episode reward: -156.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.115 [0.000, 2.000],  loss: 0.523737, mae: 9.769888, mean_q: -14.026434, mean_eps: 0.505367\n",
            "  78347/200000: episode: 283, duration: 1.364s, episode steps: 167, steps per second: 122, episode reward: -166.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.156 [0.000, 2.000],  loss: 0.562662, mae: 9.859222, mean_q: -14.152667, mean_eps: 0.504341\n",
            "  78511/200000: episode: 284, duration: 1.330s, episode steps: 164, steps per second: 123, episode reward: -163.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.976 [0.000, 2.000],  loss: 0.506067, mae: 9.812510, mean_q: -14.089799, mean_eps: 0.503289\n",
            "  78647/200000: episode: 285, duration: 1.097s, episode steps: 136, steps per second: 124, episode reward: -135.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.118 [0.000, 2.000],  loss: 0.565314, mae: 9.796816, mean_q: -14.035134, mean_eps: 0.502339\n",
            "  78790/200000: episode: 286, duration: 0.817s, episode steps: 143, steps per second: 175, episode reward: -142.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.916 [0.000, 2.000],  loss: 0.568860, mae: 9.898021, mean_q: -14.203336, mean_eps: 0.501453\n",
            "  78952/200000: episode: 287, duration: 0.906s, episode steps: 162, steps per second: 179, episode reward: -161.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.037 [0.000, 2.000],  loss: 0.452241, mae: 9.832568, mean_q: -14.120488, mean_eps: 0.500490\n",
            "  79100/200000: episode: 288, duration: 0.883s, episode steps: 148, steps per second: 168, episode reward: -147.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.007 [0.000, 2.000],  loss: 0.492315, mae: 9.863753, mean_q: -14.179271, mean_eps: 0.499515\n",
            "  79235/200000: episode: 289, duration: 0.765s, episode steps: 135, steps per second: 176, episode reward: -134.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.059 [0.000, 2.000],  loss: 0.529987, mae: 9.870415, mean_q: -14.181292, mean_eps: 0.498615\n",
            "  79331/200000: episode: 290, duration: 0.552s, episode steps:  96, steps per second: 174, episode reward: -95.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.073 [0.000, 2.000],  loss: 0.599528, mae: 9.870712, mean_q: -14.171832, mean_eps: 0.497881\n",
            "  79447/200000: episode: 291, duration: 0.682s, episode steps: 116, steps per second: 170, episode reward: -115.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.129 [0.000, 2.000],  loss: 0.579996, mae: 9.894275, mean_q: -14.159045, mean_eps: 0.497209\n",
            "  79615/200000: episode: 292, duration: 0.927s, episode steps: 168, steps per second: 181, episode reward: -167.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.054 [0.000, 2.000],  loss: 0.541660, mae: 9.926524, mean_q: -14.230965, mean_eps: 0.496310\n",
            "  79745/200000: episode: 293, duration: 0.748s, episode steps: 130, steps per second: 174, episode reward: -129.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.877 [0.000, 2.000],  loss: 0.445185, mae: 9.802407, mean_q: -14.084127, mean_eps: 0.495360\n",
            "  79906/200000: episode: 294, duration: 0.929s, episode steps: 161, steps per second: 173, episode reward: -160.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.807 [0.000, 2.000],  loss: 0.645118, mae: 9.836133, mean_q: -14.075456, mean_eps: 0.494435\n",
            "  80099/200000: episode: 295, duration: 1.083s, episode steps: 193, steps per second: 178, episode reward: -192.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.150 [0.000, 2.000],  loss: 0.507701, mae: 9.874791, mean_q: -14.189203, mean_eps: 0.493321\n",
            "  80247/200000: episode: 296, duration: 0.819s, episode steps: 148, steps per second: 181, episode reward: -147.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.919 [0.000, 2.000],  loss: 0.480250, mae: 9.841199, mean_q: -14.143804, mean_eps: 0.492244\n",
            "  80414/200000: episode: 297, duration: 1.122s, episode steps: 167, steps per second: 149, episode reward: -166.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.994 [0.000, 2.000],  loss: 0.491422, mae: 9.918439, mean_q: -14.253783, mean_eps: 0.491243\n",
            "  80582/200000: episode: 298, duration: 1.383s, episode steps: 168, steps per second: 121, episode reward: -167.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.875 [0.000, 2.000],  loss: 0.493182, mae: 9.962365, mean_q: -14.314441, mean_eps: 0.490179\n",
            "  80765/200000: episode: 299, duration: 1.518s, episode steps: 183, steps per second: 121, episode reward: -182.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.852 [0.000, 2.000],  loss: 0.483328, mae: 9.850061, mean_q: -14.165735, mean_eps: 0.489065\n",
            "  80892/200000: episode: 300, duration: 0.783s, episode steps: 127, steps per second: 162, episode reward: -126.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.016 [0.000, 2.000],  loss: 0.582958, mae: 9.933641, mean_q: -14.273209, mean_eps: 0.488089\n",
            "  81044/200000: episode: 301, duration: 0.893s, episode steps: 152, steps per second: 170, episode reward: -151.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.237 [0.000, 2.000],  loss: 0.629913, mae: 9.957640, mean_q: -14.295540, mean_eps: 0.487215\n",
            "  81171/200000: episode: 302, duration: 0.701s, episode steps: 127, steps per second: 181, episode reward: -126.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.031 [0.000, 2.000],  loss: 0.520858, mae: 10.087088, mean_q: -14.496738, mean_eps: 0.486329\n",
            "  81271/200000: episode: 303, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -99.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.140 [0.000, 2.000],  loss: 0.465420, mae: 10.002342, mean_q: -14.375384, mean_eps: 0.485607\n",
            "  81434/200000: episode: 304, duration: 0.920s, episode steps: 163, steps per second: 177, episode reward: -162.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.061 [0.000, 2.000],  loss: 0.693891, mae: 9.991750, mean_q: -14.307808, mean_eps: 0.484771\n",
            "  81635/200000: episode: 305, duration: 1.091s, episode steps: 201, steps per second: 184, episode reward: -200.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.786 [0.000, 2.000],  loss: 0.575713, mae: 9.987428, mean_q: -14.300547, mean_eps: 0.483618\n",
            "  81789/200000: episode: 306, duration: 0.902s, episode steps: 154, steps per second: 171, episode reward: -153.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.935 [0.000, 2.000],  loss: 0.471187, mae: 10.015690, mean_q: -14.413278, mean_eps: 0.482491\n",
            "  81987/200000: episode: 307, duration: 1.117s, episode steps: 198, steps per second: 177, episode reward: -197.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.035 [0.000, 2.000],  loss: 0.538010, mae: 10.072291, mean_q: -14.468724, mean_eps: 0.481376\n",
            "  82172/200000: episode: 308, duration: 1.069s, episode steps: 185, steps per second: 173, episode reward: -184.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.092 [0.000, 2.000],  loss: 0.462059, mae: 10.142841, mean_q: -14.614187, mean_eps: 0.480173\n",
            "  82320/200000: episode: 309, duration: 0.896s, episode steps: 148, steps per second: 165, episode reward: -147.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.919 [0.000, 2.000],  loss: 0.572146, mae: 10.085825, mean_q: -14.500127, mean_eps: 0.479121\n",
            "  82488/200000: episode: 310, duration: 1.001s, episode steps: 168, steps per second: 168, episode reward: -167.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.030 [0.000, 2.000],  loss: 0.531743, mae: 10.140457, mean_q: -14.565575, mean_eps: 0.478121\n",
            "  82675/200000: episode: 311, duration: 1.475s, episode steps: 187, steps per second: 127, episode reward: -186.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.134 [0.000, 2.000],  loss: 0.514078, mae: 10.145530, mean_q: -14.578980, mean_eps: 0.476993\n",
            "  82820/200000: episode: 312, duration: 1.192s, episode steps: 145, steps per second: 122, episode reward: -144.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.924 [0.000, 2.000],  loss: 0.611826, mae: 10.087402, mean_q: -14.500266, mean_eps: 0.475942\n",
            "  82947/200000: episode: 313, duration: 1.094s, episode steps: 127, steps per second: 116, episode reward: -126.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.984 [0.000, 2.000],  loss: 0.658968, mae: 10.121432, mean_q: -14.525061, mean_eps: 0.475081\n",
            "  83152/200000: episode: 314, duration: 1.213s, episode steps: 205, steps per second: 169, episode reward: -204.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.049 [0.000, 2.000],  loss: 0.616988, mae: 10.208034, mean_q: -14.643664, mean_eps: 0.474029\n",
            "  83299/200000: episode: 315, duration: 0.898s, episode steps: 147, steps per second: 164, episode reward: -146.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.020 [0.000, 2.000],  loss: 0.686543, mae: 10.335669, mean_q: -14.809388, mean_eps: 0.472915\n",
            "  83479/200000: episode: 316, duration: 1.009s, episode steps: 180, steps per second: 178, episode reward: -179.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.039 [0.000, 2.000],  loss: 0.546946, mae: 10.319818, mean_q: -14.847322, mean_eps: 0.471876\n",
            "  83638/200000: episode: 317, duration: 0.897s, episode steps: 159, steps per second: 177, episode reward: -158.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.925 [0.000, 2.000],  loss: 0.481313, mae: 10.269406, mean_q: -14.782741, mean_eps: 0.470799\n",
            "  83784/200000: episode: 318, duration: 0.863s, episode steps: 146, steps per second: 169, episode reward: -145.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.986 [0.000, 2.000],  loss: 0.560724, mae: 10.309016, mean_q: -14.801113, mean_eps: 0.469837\n",
            "  83911/200000: episode: 319, duration: 0.812s, episode steps: 127, steps per second: 156, episode reward: -126.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.087 [0.000, 2.000],  loss: 0.579363, mae: 10.255851, mean_q: -14.735065, mean_eps: 0.468975\n",
            "  84029/200000: episode: 320, duration: 0.715s, episode steps: 118, steps per second: 165, episode reward: -117.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.915 [0.000, 2.000],  loss: 0.669433, mae: 10.276531, mean_q: -14.716807, mean_eps: 0.468190\n",
            "  84161/200000: episode: 321, duration: 0.773s, episode steps: 132, steps per second: 171, episode reward: -131.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.932 [0.000, 2.000],  loss: 0.591515, mae: 10.234867, mean_q: -14.677253, mean_eps: 0.467392\n",
            "  84661/200000: episode: 322, duration: 3.000s, episode steps: 500, steps per second: 167, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.350 [0.000, 2.000],  loss: 0.524831, mae: 10.309895, mean_q: -14.815205, mean_eps: 0.465391\n",
            "  84801/200000: episode: 323, duration: 1.146s, episode steps: 140, steps per second: 122, episode reward: -139.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.143 [0.000, 2.000],  loss: 0.586946, mae: 10.223640, mean_q: -14.679694, mean_eps: 0.463364\n",
            "  84965/200000: episode: 324, duration: 1.322s, episode steps: 164, steps per second: 124, episode reward: -163.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.872 [0.000, 2.000],  loss: 0.519751, mae: 10.226185, mean_q: -14.683242, mean_eps: 0.462401\n",
            "  85108/200000: episode: 325, duration: 1.082s, episode steps: 143, steps per second: 132, episode reward: -142.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.916 [0.000, 2.000],  loss: 0.579398, mae: 10.193139, mean_q: -14.631313, mean_eps: 0.461439\n",
            "  85267/200000: episode: 326, duration: 0.935s, episode steps: 159, steps per second: 170, episode reward: -158.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.956 [0.000, 2.000],  loss: 0.679691, mae: 10.236945, mean_q: -14.663728, mean_eps: 0.460489\n",
            "  85417/200000: episode: 327, duration: 0.879s, episode steps: 150, steps per second: 171, episode reward: -149.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.073 [0.000, 2.000],  loss: 0.520597, mae: 10.242745, mean_q: -14.707532, mean_eps: 0.459501\n",
            "  85520/200000: episode: 328, duration: 0.604s, episode steps: 103, steps per second: 171, episode reward: -102.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.874 [0.000, 2.000],  loss: 0.490542, mae: 10.167562, mean_q: -14.593145, mean_eps: 0.458703\n",
            "  85666/200000: episode: 329, duration: 0.864s, episode steps: 146, steps per second: 169, episode reward: -145.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.760 [0.000, 2.000],  loss: 0.607261, mae: 10.245545, mean_q: -14.713546, mean_eps: 0.457917\n",
            "  85781/200000: episode: 330, duration: 0.656s, episode steps: 115, steps per second: 175, episode reward: -114.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.026 [0.000, 2.000],  loss: 0.610654, mae: 10.180201, mean_q: -14.609380, mean_eps: 0.457081\n",
            "  85938/200000: episode: 331, duration: 0.845s, episode steps: 157, steps per second: 186, episode reward: -156.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.981 [0.000, 2.000],  loss: 0.500964, mae: 10.195005, mean_q: -14.652960, mean_eps: 0.456220\n",
            "  86083/200000: episode: 332, duration: 0.796s, episode steps: 145, steps per second: 182, episode reward: -144.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.131 [0.000, 2.000],  loss: 0.549594, mae: 10.213792, mean_q: -14.669586, mean_eps: 0.455270\n",
            "  86234/200000: episode: 333, duration: 0.834s, episode steps: 151, steps per second: 181, episode reward: -150.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.980 [0.000, 2.000],  loss: 0.612757, mae: 10.253899, mean_q: -14.719774, mean_eps: 0.454333\n",
            "  86359/200000: episode: 334, duration: 0.700s, episode steps: 125, steps per second: 179, episode reward: -124.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.888 [0.000, 2.000],  loss: 0.455773, mae: 10.142103, mean_q: -14.599563, mean_eps: 0.453459\n",
            "  86545/200000: episode: 335, duration: 1.050s, episode steps: 186, steps per second: 177, episode reward: -185.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.269 [0.000, 2.000],  loss: 0.657377, mae: 10.259639, mean_q: -14.723132, mean_eps: 0.452471\n",
            "  86675/200000: episode: 336, duration: 0.750s, episode steps: 130, steps per second: 173, episode reward: -129.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.923 [0.000, 2.000],  loss: 0.540461, mae: 10.235711, mean_q: -14.724303, mean_eps: 0.451470\n",
            "  86856/200000: episode: 337, duration: 1.193s, episode steps: 181, steps per second: 152, episode reward: -180.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.796 [0.000, 2.000],  loss: 0.549734, mae: 10.183463, mean_q: -14.632749, mean_eps: 0.450495\n",
            "  86971/200000: episode: 338, duration: 0.957s, episode steps: 115, steps per second: 120, episode reward: -114.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.061 [0.000, 2.000],  loss: 0.548549, mae: 10.245621, mean_q: -14.702213, mean_eps: 0.449557\n",
            "  87115/200000: episode: 339, duration: 1.173s, episode steps: 144, steps per second: 123, episode reward: -143.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.840 [0.000, 2.000],  loss: 0.532209, mae: 10.326989, mean_q: -14.847983, mean_eps: 0.448734\n",
            "  87229/200000: episode: 340, duration: 0.915s, episode steps: 114, steps per second: 125, episode reward: -113.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.639866, mae: 10.334290, mean_q: -14.849816, mean_eps: 0.447911\n",
            "  87384/200000: episode: 341, duration: 0.909s, episode steps: 155, steps per second: 171, episode reward: -154.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.981 [0.000, 2.000],  loss: 0.563060, mae: 10.349617, mean_q: -14.891613, mean_eps: 0.447062\n",
            "  87522/200000: episode: 342, duration: 0.773s, episode steps: 138, steps per second: 179, episode reward: -137.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.964 [0.000, 2.000],  loss: 0.579827, mae: 10.380446, mean_q: -14.930490, mean_eps: 0.446137\n",
            "  87704/200000: episode: 343, duration: 1.043s, episode steps: 182, steps per second: 174, episode reward: -181.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.945 [0.000, 2.000],  loss: 0.488171, mae: 10.370221, mean_q: -14.925838, mean_eps: 0.445124\n",
            "  87870/200000: episode: 344, duration: 0.906s, episode steps: 166, steps per second: 183, episode reward: -165.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.807 [0.000, 2.000],  loss: 0.550221, mae: 10.275036, mean_q: -14.787599, mean_eps: 0.444022\n",
            "  88016/200000: episode: 345, duration: 0.848s, episode steps: 146, steps per second: 172, episode reward: -145.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.829 [0.000, 2.000],  loss: 0.517560, mae: 10.286273, mean_q: -14.804710, mean_eps: 0.443034\n",
            "  88162/200000: episode: 346, duration: 0.911s, episode steps: 146, steps per second: 160, episode reward: -145.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.966 [0.000, 2.000],  loss: 0.568202, mae: 10.520473, mean_q: -15.150442, mean_eps: 0.442109\n",
            "  88310/200000: episode: 347, duration: 0.845s, episode steps: 148, steps per second: 175, episode reward: -147.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.658017, mae: 10.460519, mean_q: -15.048439, mean_eps: 0.441172\n",
            "  88429/200000: episode: 348, duration: 0.685s, episode steps: 119, steps per second: 174, episode reward: -118.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.034 [0.000, 2.000],  loss: 0.568926, mae: 10.548584, mean_q: -15.177963, mean_eps: 0.440323\n",
            "  88598/200000: episode: 349, duration: 0.976s, episode steps: 169, steps per second: 173, episode reward: -168.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.059 [0.000, 2.000],  loss: 0.637486, mae: 10.412904, mean_q: -14.953914, mean_eps: 0.439411\n",
            "  88720/200000: episode: 350, duration: 0.730s, episode steps: 122, steps per second: 167, episode reward: -121.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.074 [0.000, 2.000],  loss: 0.573131, mae: 10.525311, mean_q: -15.135893, mean_eps: 0.438499\n",
            "  88853/200000: episode: 351, duration: 0.813s, episode steps: 133, steps per second: 164, episode reward: -132.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.060 [0.000, 2.000],  loss: 0.527738, mae: 10.387213, mean_q: -14.924146, mean_eps: 0.437689\n",
            "  88986/200000: episode: 352, duration: 0.941s, episode steps: 133, steps per second: 141, episode reward: -132.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.113 [0.000, 2.000],  loss: 0.520644, mae: 10.443005, mean_q: -15.023356, mean_eps: 0.436840\n",
            "  89120/200000: episode: 353, duration: 1.159s, episode steps: 134, steps per second: 116, episode reward: -133.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.015 [0.000, 2.000],  loss: 0.611381, mae: 10.618780, mean_q: -15.242384, mean_eps: 0.436004\n",
            "  89246/200000: episode: 354, duration: 1.004s, episode steps: 126, steps per second: 125, episode reward: -125.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.143 [0.000, 2.000],  loss: 0.603717, mae: 10.606802, mean_q: -15.210108, mean_eps: 0.435181\n",
            "  89435/200000: episode: 355, duration: 1.360s, episode steps: 189, steps per second: 139, episode reward: -188.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.143 [0.000, 2.000],  loss: 0.638956, mae: 10.637017, mean_q: -15.270260, mean_eps: 0.434180\n",
            "  89596/200000: episode: 356, duration: 0.969s, episode steps: 161, steps per second: 166, episode reward: -160.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.043 [0.000, 2.000],  loss: 0.607243, mae: 10.600033, mean_q: -15.232381, mean_eps: 0.433078\n",
            "  89767/200000: episode: 357, duration: 1.015s, episode steps: 171, steps per second: 168, episode reward: -170.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.924 [0.000, 2.000],  loss: 0.700883, mae: 10.538443, mean_q: -15.109548, mean_eps: 0.432027\n",
            "  89889/200000: episode: 358, duration: 0.697s, episode steps: 122, steps per second: 175, episode reward: -121.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.869 [0.000, 2.000],  loss: 0.614752, mae: 10.620171, mean_q: -15.249138, mean_eps: 0.431089\n",
            "  90027/200000: episode: 359, duration: 0.790s, episode steps: 138, steps per second: 175, episode reward: -137.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.138 [0.000, 2.000],  loss: 0.575044, mae: 10.560504, mean_q: -15.168186, mean_eps: 0.430266\n",
            "  90170/200000: episode: 360, duration: 0.816s, episode steps: 143, steps per second: 175, episode reward: -142.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.049 [0.000, 2.000],  loss: 0.602901, mae: 10.651784, mean_q: -15.281953, mean_eps: 0.429379\n",
            "  90263/200000: episode: 361, duration: 0.501s, episode steps:  93, steps per second: 186, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.290 [0.000, 2.000],  loss: 0.625066, mae: 10.584112, mean_q: -15.170416, mean_eps: 0.428632\n",
            "  90460/200000: episode: 362, duration: 1.117s, episode steps: 197, steps per second: 176, episode reward: -196.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.203 [0.000, 2.000],  loss: 0.570870, mae: 10.639129, mean_q: -15.282187, mean_eps: 0.427720\n",
            "  90575/200000: episode: 363, duration: 0.645s, episode steps: 115, steps per second: 178, episode reward: -114.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.104 [0.000, 2.000],  loss: 0.644503, mae: 10.663128, mean_q: -15.287509, mean_eps: 0.426732\n",
            "  90686/200000: episode: 364, duration: 0.702s, episode steps: 111, steps per second: 158, episode reward: -110.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.018 [0.000, 2.000],  loss: 0.521804, mae: 10.549708, mean_q: -15.157405, mean_eps: 0.426010\n",
            "  90840/200000: episode: 365, duration: 0.864s, episode steps: 154, steps per second: 178, episode reward: -153.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.143 [0.000, 2.000],  loss: 0.618521, mae: 10.603161, mean_q: -15.221460, mean_eps: 0.425174\n",
            "  91025/200000: episode: 366, duration: 1.060s, episode steps: 185, steps per second: 175, episode reward: -184.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.184 [0.000, 2.000],  loss: 0.520000, mae: 10.680747, mean_q: -15.373669, mean_eps: 0.424097\n",
            "  91169/200000: episode: 367, duration: 1.103s, episode steps: 144, steps per second: 131, episode reward: -143.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.965 [0.000, 2.000],  loss: 0.696740, mae: 10.784967, mean_q: -15.456977, mean_eps: 0.423046\n",
            "  91298/200000: episode: 368, duration: 1.072s, episode steps: 129, steps per second: 120, episode reward: -128.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.031 [0.000, 2.000],  loss: 0.591721, mae: 10.754468, mean_q: -15.454457, mean_eps: 0.422185\n",
            "  91433/200000: episode: 369, duration: 1.128s, episode steps: 135, steps per second: 120, episode reward: -134.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.200 [0.000, 2.000],  loss: 0.668648, mae: 10.719140, mean_q: -15.368125, mean_eps: 0.421349\n",
            "  91590/200000: episode: 370, duration: 1.049s, episode steps: 157, steps per second: 150, episode reward: -156.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.318 [0.000, 2.000],  loss: 0.573840, mae: 10.715230, mean_q: -15.402637, mean_eps: 0.420424\n",
            "  91719/200000: episode: 371, duration: 0.770s, episode steps: 129, steps per second: 167, episode reward: -128.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.295 [0.000, 2.000],  loss: 0.549831, mae: 10.697218, mean_q: -15.357082, mean_eps: 0.419525\n",
            "  91868/200000: episode: 372, duration: 0.875s, episode steps: 149, steps per second: 170, episode reward: -148.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.107 [0.000, 2.000],  loss: 0.565105, mae: 10.750188, mean_q: -15.440657, mean_eps: 0.418651\n",
            "  92010/200000: episode: 373, duration: 0.830s, episode steps: 142, steps per second: 171, episode reward: -141.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.979 [0.000, 2.000],  loss: 0.533368, mae: 10.672983, mean_q: -15.322073, mean_eps: 0.417726\n",
            "  92135/200000: episode: 374, duration: 0.760s, episode steps: 125, steps per second: 165, episode reward: -124.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.208 [0.000, 2.000],  loss: 0.641686, mae: 10.676260, mean_q: -15.316082, mean_eps: 0.416877\n",
            "  92294/200000: episode: 375, duration: 0.940s, episode steps: 159, steps per second: 169, episode reward: -158.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.245 [0.000, 2.000],  loss: 0.681866, mae: 10.646841, mean_q: -15.266903, mean_eps: 0.415978\n",
            "  92438/200000: episode: 376, duration: 0.820s, episode steps: 144, steps per second: 176, episode reward: -143.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.021 [0.000, 2.000],  loss: 0.634483, mae: 10.677757, mean_q: -15.347369, mean_eps: 0.415015\n",
            "  92578/200000: episode: 377, duration: 0.859s, episode steps: 140, steps per second: 163, episode reward: -139.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.093 [0.000, 2.000],  loss: 0.562780, mae: 10.637943, mean_q: -15.253526, mean_eps: 0.414116\n",
            "  92766/200000: episode: 378, duration: 1.102s, episode steps: 188, steps per second: 171, episode reward: -187.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.245 [0.000, 2.000],  loss: 0.534509, mae: 10.606986, mean_q: -15.233047, mean_eps: 0.413077\n",
            "  92894/200000: episode: 379, duration: 0.783s, episode steps: 128, steps per second: 163, episode reward: -127.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.495996, mae: 10.634232, mean_q: -15.290191, mean_eps: 0.412077\n",
            "  93065/200000: episode: 380, duration: 1.014s, episode steps: 171, steps per second: 169, episode reward: -170.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.082 [0.000, 2.000],  loss: 0.649400, mae: 10.616754, mean_q: -15.204400, mean_eps: 0.411127\n",
            "  93253/200000: episode: 381, duration: 1.317s, episode steps: 188, steps per second: 143, episode reward: -187.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.106 [0.000, 2.000],  loss: 0.635595, mae: 10.631261, mean_q: -15.257516, mean_eps: 0.409987\n",
            "  93388/200000: episode: 382, duration: 1.187s, episode steps: 135, steps per second: 114, episode reward: -134.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.096 [0.000, 2.000],  loss: 0.608890, mae: 10.692582, mean_q: -15.340263, mean_eps: 0.408973\n",
            "  93527/200000: episode: 383, duration: 1.272s, episode steps: 139, steps per second: 109, episode reward: -138.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.014 [0.000, 2.000],  loss: 0.654938, mae: 10.626680, mean_q: -15.245193, mean_eps: 0.408112\n",
            "  93638/200000: episode: 384, duration: 0.822s, episode steps: 111, steps per second: 135, episode reward: -110.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.162 [0.000, 2.000],  loss: 0.689385, mae: 10.737446, mean_q: -15.397690, mean_eps: 0.407314\n",
            "  93818/200000: episode: 385, duration: 1.085s, episode steps: 180, steps per second: 166, episode reward: -179.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.772 [0.000, 2.000],  loss: 0.589320, mae: 10.667065, mean_q: -15.313337, mean_eps: 0.406389\n",
            "  93964/200000: episode: 386, duration: 0.901s, episode steps: 146, steps per second: 162, episode reward: -145.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.103 [0.000, 2.000],  loss: 0.529204, mae: 10.609808, mean_q: -15.260820, mean_eps: 0.405363\n",
            "  94074/200000: episode: 387, duration: 0.711s, episode steps: 110, steps per second: 155, episode reward: -109.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.927 [0.000, 2.000],  loss: 0.560887, mae: 10.704613, mean_q: -15.403938, mean_eps: 0.404553\n",
            "  94156/200000: episode: 388, duration: 0.521s, episode steps:  82, steps per second: 157, episode reward: -81.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.049 [0.000, 2.000],  loss: 0.690632, mae: 10.712164, mean_q: -15.369382, mean_eps: 0.403945\n",
            "  94257/200000: episode: 389, duration: 0.651s, episode steps: 101, steps per second: 155, episode reward: -100.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.010 [0.000, 2.000],  loss: 0.839989, mae: 10.702682, mean_q: -15.335594, mean_eps: 0.403362\n",
            "  94418/200000: episode: 390, duration: 1.003s, episode steps: 161, steps per second: 160, episode reward: -160.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.882 [0.000, 2.000],  loss: 0.568167, mae: 10.716617, mean_q: -15.430980, mean_eps: 0.402526\n",
            "  94530/200000: episode: 391, duration: 0.693s, episode steps: 112, steps per second: 162, episode reward: -111.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.964 [0.000, 2.000],  loss: 0.629238, mae: 10.781209, mean_q: -15.480944, mean_eps: 0.401665\n",
            "  94680/200000: episode: 392, duration: 0.931s, episode steps: 150, steps per second: 161, episode reward: -149.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.920 [0.000, 2.000],  loss: 0.684537, mae: 10.694270, mean_q: -15.330976, mean_eps: 0.400841\n",
            "  94859/200000: episode: 393, duration: 1.095s, episode steps: 179, steps per second: 163, episode reward: -178.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.804 [0.000, 2.000],  loss: 0.611024, mae: 10.755560, mean_q: -15.442242, mean_eps: 0.399803\n",
            "  94968/200000: episode: 394, duration: 0.685s, episode steps: 109, steps per second: 159, episode reward: -108.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.908 [0.000, 2.000],  loss: 0.688075, mae: 10.801707, mean_q: -15.478233, mean_eps: 0.398891\n",
            "  95104/200000: episode: 395, duration: 0.897s, episode steps: 136, steps per second: 152, episode reward: -135.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.051 [0.000, 2.000],  loss: 0.757123, mae: 10.736595, mean_q: -15.373673, mean_eps: 0.398118\n",
            "  95238/200000: episode: 396, duration: 1.119s, episode steps: 134, steps per second: 120, episode reward: -133.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.866 [0.000, 2.000],  loss: 0.636907, mae: 10.692332, mean_q: -15.329265, mean_eps: 0.397257\n",
            "  95395/200000: episode: 397, duration: 1.398s, episode steps: 157, steps per second: 112, episode reward: -156.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.962 [0.000, 2.000],  loss: 0.613970, mae: 10.753944, mean_q: -15.444457, mean_eps: 0.396332\n",
            "  95534/200000: episode: 398, duration: 1.258s, episode steps: 139, steps per second: 110, episode reward: -138.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.978 [0.000, 2.000],  loss: 0.614028, mae: 10.803312, mean_q: -15.543678, mean_eps: 0.395395\n",
            "  95649/200000: episode: 399, duration: 0.745s, episode steps: 115, steps per second: 154, episode reward: -114.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.043 [0.000, 2.000],  loss: 0.613295, mae: 10.725765, mean_q: -15.392025, mean_eps: 0.394584\n",
            "  95748/200000: episode: 400, duration: 0.663s, episode steps:  99, steps per second: 149, episode reward: -98.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.970 [0.000, 2.000],  loss: 0.600118, mae: 10.762013, mean_q: -15.455903, mean_eps: 0.393913\n",
            "  95842/200000: episode: 401, duration: 0.627s, episode steps:  94, steps per second: 150, episode reward: -93.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.032 [0.000, 2.000],  loss: 0.578181, mae: 10.782463, mean_q: -15.483954, mean_eps: 0.393305\n",
            "  95963/200000: episode: 402, duration: 0.738s, episode steps: 121, steps per second: 164, episode reward: -120.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.942 [0.000, 2.000],  loss: 0.557976, mae: 10.772855, mean_q: -15.485381, mean_eps: 0.392621\n",
            "  96081/200000: episode: 403, duration: 0.733s, episode steps: 118, steps per second: 161, episode reward: -117.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.025 [0.000, 2.000],  loss: 0.652389, mae: 10.754342, mean_q: -15.414685, mean_eps: 0.391861\n",
            "  96236/200000: episode: 404, duration: 0.939s, episode steps: 155, steps per second: 165, episode reward: -154.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.923 [0.000, 2.000],  loss: 0.539014, mae: 10.783848, mean_q: -15.512741, mean_eps: 0.390999\n",
            "  96359/200000: episode: 405, duration: 0.740s, episode steps: 123, steps per second: 166, episode reward: -122.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.846 [0.000, 2.000],  loss: 0.639963, mae: 10.767774, mean_q: -15.454189, mean_eps: 0.390125\n",
            "  96525/200000: episode: 406, duration: 1.037s, episode steps: 166, steps per second: 160, episode reward: -165.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.024 [0.000, 2.000],  loss: 0.557392, mae: 10.793350, mean_q: -15.507524, mean_eps: 0.389201\n",
            "  96658/200000: episode: 407, duration: 0.867s, episode steps: 133, steps per second: 153, episode reward: -132.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.955 [0.000, 2.000],  loss: 0.639994, mae: 10.628253, mean_q: -15.260409, mean_eps: 0.388251\n",
            "  96797/200000: episode: 408, duration: 0.819s, episode steps: 139, steps per second: 170, episode reward: -138.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.978 [0.000, 2.000],  loss: 0.580982, mae: 10.618354, mean_q: -15.267787, mean_eps: 0.387389\n",
            "  96926/200000: episode: 409, duration: 0.779s, episode steps: 129, steps per second: 166, episode reward: -128.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.969 [0.000, 2.000],  loss: 0.618224, mae: 10.675032, mean_q: -15.327268, mean_eps: 0.386541\n",
            "  97067/200000: episode: 410, duration: 0.869s, episode steps: 141, steps per second: 162, episode reward: -140.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.837 [0.000, 2.000],  loss: 0.680672, mae: 10.791492, mean_q: -15.487271, mean_eps: 0.385692\n",
            "  97189/200000: episode: 411, duration: 0.950s, episode steps: 122, steps per second: 128, episode reward: -121.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.008 [0.000, 2.000],  loss: 0.557449, mae: 10.829530, mean_q: -15.534407, mean_eps: 0.384856\n",
            "  97335/200000: episode: 412, duration: 1.267s, episode steps: 146, steps per second: 115, episode reward: -145.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.007 [0.000, 2.000],  loss: 0.615571, mae: 10.842969, mean_q: -15.543516, mean_eps: 0.384007\n",
            "  97445/200000: episode: 413, duration: 0.984s, episode steps: 110, steps per second: 112, episode reward: -109.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.818 [0.000, 2.000],  loss: 0.685602, mae: 10.841626, mean_q: -15.514148, mean_eps: 0.383197\n",
            "  97584/200000: episode: 414, duration: 1.068s, episode steps: 139, steps per second: 130, episode reward: -138.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.971 [0.000, 2.000],  loss: 0.581469, mae: 10.775776, mean_q: -15.489127, mean_eps: 0.382411\n",
            "  97701/200000: episode: 415, duration: 0.742s, episode steps: 117, steps per second: 158, episode reward: -116.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.915 [0.000, 2.000],  loss: 0.527260, mae: 10.814980, mean_q: -15.556601, mean_eps: 0.381601\n",
            "  97871/200000: episode: 416, duration: 1.105s, episode steps: 170, steps per second: 154, episode reward: -169.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.971 [0.000, 2.000],  loss: 0.606526, mae: 10.848871, mean_q: -15.581851, mean_eps: 0.380689\n",
            "  98013/200000: episode: 417, duration: 0.937s, episode steps: 142, steps per second: 152, episode reward: -141.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.979 [0.000, 2.000],  loss: 0.701030, mae: 10.858883, mean_q: -15.549684, mean_eps: 0.379701\n",
            "  98156/200000: episode: 418, duration: 0.997s, episode steps: 143, steps per second: 143, episode reward: -142.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.028 [0.000, 2.000],  loss: 0.692298, mae: 10.898111, mean_q: -15.591826, mean_eps: 0.378801\n",
            "  98260/200000: episode: 419, duration: 0.688s, episode steps: 104, steps per second: 151, episode reward: -103.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.769 [0.000, 2.000],  loss: 0.547880, mae: 10.932709, mean_q: -15.705992, mean_eps: 0.378029\n",
            "  98384/200000: episode: 420, duration: 0.862s, episode steps: 124, steps per second: 144, episode reward: -123.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.040 [0.000, 2.000],  loss: 0.570780, mae: 10.929842, mean_q: -15.705459, mean_eps: 0.377307\n",
            "  98554/200000: episode: 421, duration: 1.089s, episode steps: 170, steps per second: 156, episode reward: -169.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.959 [0.000, 2.000],  loss: 0.572819, mae: 10.882270, mean_q: -15.635807, mean_eps: 0.376369\n",
            "  98658/200000: episode: 422, duration: 0.640s, episode steps: 104, steps per second: 162, episode reward: -103.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.817 [0.000, 2.000],  loss: 0.690898, mae: 10.802611, mean_q: -15.482778, mean_eps: 0.375495\n",
            "  98767/200000: episode: 423, duration: 0.650s, episode steps: 109, steps per second: 168, episode reward: -108.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.954 [0.000, 2.000],  loss: 0.731566, mae: 10.909676, mean_q: -15.664155, mean_eps: 0.374824\n",
            "  99022/200000: episode: 424, duration: 1.644s, episode steps: 255, steps per second: 155, episode reward: -254.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.894 [0.000, 2.000],  loss: 0.612677, mae: 10.878878, mean_q: -15.642585, mean_eps: 0.373671\n",
            "  99126/200000: episode: 425, duration: 0.874s, episode steps: 104, steps per second: 119, episode reward: -103.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.106 [0.000, 2.000],  loss: 0.636507, mae: 10.830780, mean_q: -15.563970, mean_eps: 0.372531\n",
            "  99249/200000: episode: 426, duration: 1.138s, episode steps: 123, steps per second: 108, episode reward: -122.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.171 [0.000, 2.000],  loss: 0.578442, mae: 10.825312, mean_q: -15.572400, mean_eps: 0.371809\n",
            "  99431/200000: episode: 427, duration: 1.552s, episode steps: 182, steps per second: 117, episode reward: -181.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.989 [0.000, 2.000],  loss: 0.580451, mae: 10.846513, mean_q: -15.582449, mean_eps: 0.370847\n",
            "  99551/200000: episode: 428, duration: 0.764s, episode steps: 120, steps per second: 157, episode reward: -119.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.025 [0.000, 2.000],  loss: 0.715584, mae: 10.782396, mean_q: -15.481820, mean_eps: 0.369897\n",
            "  99676/200000: episode: 429, duration: 0.788s, episode steps: 125, steps per second: 159, episode reward: -124.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.912 [0.000, 2.000],  loss: 0.591723, mae: 10.806997, mean_q: -15.545481, mean_eps: 0.369124\n",
            "  99781/200000: episode: 430, duration: 0.654s, episode steps: 105, steps per second: 160, episode reward: -104.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.048 [0.000, 2.000],  loss: 0.587824, mae: 10.817612, mean_q: -15.551058, mean_eps: 0.368389\n",
            "  99956/200000: episode: 431, duration: 1.029s, episode steps: 175, steps per second: 170, episode reward: -174.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.897 [0.000, 2.000],  loss: 0.645958, mae: 10.832397, mean_q: -15.568453, mean_eps: 0.367503\n",
            " 100080/200000: episode: 432, duration: 0.762s, episode steps: 124, steps per second: 163, episode reward: -123.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.056 [0.000, 2.000],  loss: 0.571243, mae: 10.868449, mean_q: -15.643782, mean_eps: 0.366565\n",
            " 100211/200000: episode: 433, duration: 0.801s, episode steps: 131, steps per second: 164, episode reward: -130.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.107 [0.000, 2.000],  loss: 0.600669, mae: 10.793311, mean_q: -15.506267, mean_eps: 0.365755\n",
            " 100307/200000: episode: 434, duration: 0.630s, episode steps:  96, steps per second: 152, episode reward: -95.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.135 [0.000, 2.000],  loss: 0.592495, mae: 10.824199, mean_q: -15.524933, mean_eps: 0.365033\n",
            " 100455/200000: episode: 435, duration: 0.888s, episode steps: 148, steps per second: 167, episode reward: -147.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.101 [0.000, 2.000],  loss: 0.828194, mae: 10.800332, mean_q: -15.482689, mean_eps: 0.364260\n",
            " 100571/200000: episode: 436, duration: 0.733s, episode steps: 116, steps per second: 158, episode reward: -115.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.129 [0.000, 2.000],  loss: 0.501602, mae: 10.824691, mean_q: -15.585337, mean_eps: 0.363424\n",
            " 100682/200000: episode: 437, duration: 0.735s, episode steps: 111, steps per second: 151, episode reward: -110.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.180 [0.000, 2.000],  loss: 0.665145, mae: 10.777309, mean_q: -15.455700, mean_eps: 0.362702\n",
            " 100807/200000: episode: 438, duration: 0.783s, episode steps: 125, steps per second: 160, episode reward: -124.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.784 [0.000, 2.000],  loss: 0.580127, mae: 10.814668, mean_q: -15.552076, mean_eps: 0.361955\n",
            " 100910/200000: episode: 439, duration: 0.656s, episode steps: 103, steps per second: 157, episode reward: -102.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.165 [0.000, 2.000],  loss: 0.559233, mae: 10.852894, mean_q: -15.590362, mean_eps: 0.361233\n",
            " 101019/200000: episode: 440, duration: 0.720s, episode steps: 109, steps per second: 151, episode reward: -108.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.156 [0.000, 2.000],  loss: 0.644956, mae: 10.894126, mean_q: -15.659393, mean_eps: 0.360561\n",
            " 101143/200000: episode: 441, duration: 1.153s, episode steps: 124, steps per second: 108, episode reward: -123.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.960 [0.000, 2.000],  loss: 0.674246, mae: 10.988856, mean_q: -15.768454, mean_eps: 0.359827\n",
            " 101271/200000: episode: 442, duration: 1.269s, episode steps: 128, steps per second: 101, episode reward: -127.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.172 [0.000, 2.000],  loss: 0.713194, mae: 10.846937, mean_q: -15.545744, mean_eps: 0.359029\n",
            " 101421/200000: episode: 443, duration: 1.379s, episode steps: 150, steps per second: 109, episode reward: -149.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.007 [0.000, 2.000],  loss: 0.751870, mae: 11.002152, mean_q: -15.774189, mean_eps: 0.358142\n",
            " 101547/200000: episode: 444, duration: 0.825s, episode steps: 126, steps per second: 153, episode reward: -125.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.040 [0.000, 2.000],  loss: 0.544416, mae: 10.921032, mean_q: -15.716988, mean_eps: 0.357268\n",
            " 101668/200000: episode: 445, duration: 0.830s, episode steps: 121, steps per second: 146, episode reward: -120.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.017 [0.000, 2.000],  loss: 0.637863, mae: 10.989158, mean_q: -15.776656, mean_eps: 0.356495\n",
            " 101821/200000: episode: 446, duration: 1.000s, episode steps: 153, steps per second: 153, episode reward: -152.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.033 [0.000, 2.000],  loss: 0.664120, mae: 10.910052, mean_q: -15.673839, mean_eps: 0.355621\n",
            " 101944/200000: episode: 447, duration: 0.839s, episode steps: 123, steps per second: 147, episode reward: -122.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.154 [0.000, 2.000],  loss: 0.603247, mae: 10.890530, mean_q: -15.624060, mean_eps: 0.354747\n",
            " 102047/200000: episode: 448, duration: 0.659s, episode steps: 103, steps per second: 156, episode reward: -102.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.204 [0.000, 2.000],  loss: 0.762561, mae: 10.889376, mean_q: -15.597503, mean_eps: 0.354038\n",
            " 102167/200000: episode: 449, duration: 0.814s, episode steps: 120, steps per second: 147, episode reward: -119.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.033 [0.000, 2.000],  loss: 0.703982, mae: 10.964412, mean_q: -15.700547, mean_eps: 0.353329\n",
            " 102319/200000: episode: 450, duration: 0.973s, episode steps: 152, steps per second: 156, episode reward: -151.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.974 [0.000, 2.000],  loss: 0.709010, mae: 10.861435, mean_q: -15.560213, mean_eps: 0.352467\n",
            " 102450/200000: episode: 451, duration: 0.846s, episode steps: 131, steps per second: 155, episode reward: -130.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.168 [0.000, 2.000],  loss: 0.631948, mae: 10.876773, mean_q: -15.630299, mean_eps: 0.351568\n",
            " 102560/200000: episode: 452, duration: 0.686s, episode steps: 110, steps per second: 160, episode reward: -109.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.891 [0.000, 2.000],  loss: 0.632897, mae: 10.849856, mean_q: -15.579093, mean_eps: 0.350808\n",
            " 102695/200000: episode: 453, duration: 0.819s, episode steps: 135, steps per second: 165, episode reward: -134.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.119 [0.000, 2.000],  loss: 0.603342, mae: 10.750424, mean_q: -15.424460, mean_eps: 0.350035\n",
            " 102833/200000: episode: 454, duration: 0.874s, episode steps: 138, steps per second: 158, episode reward: -137.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.957 [0.000, 2.000],  loss: 0.651999, mae: 10.856408, mean_q: -15.581609, mean_eps: 0.349161\n",
            " 102977/200000: episode: 455, duration: 1.046s, episode steps: 144, steps per second: 138, episode reward: -143.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.167 [0.000, 2.000],  loss: 0.654793, mae: 10.857670, mean_q: -15.568110, mean_eps: 0.348262\n",
            " 103111/200000: episode: 456, duration: 1.179s, episode steps: 134, steps per second: 114, episode reward: -133.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.149 [0.000, 2.000],  loss: 0.641003, mae: 10.874089, mean_q: -15.608592, mean_eps: 0.347388\n",
            " 103250/200000: episode: 457, duration: 1.203s, episode steps: 139, steps per second: 116, episode reward: -138.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.029 [0.000, 2.000],  loss: 0.731304, mae: 10.807342, mean_q: -15.486524, mean_eps: 0.346527\n",
            " 103356/200000: episode: 458, duration: 0.853s, episode steps: 106, steps per second: 124, episode reward: -105.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.123 [0.000, 2.000],  loss: 0.644118, mae: 10.793547, mean_q: -15.464674, mean_eps: 0.345754\n",
            " 103494/200000: episode: 459, duration: 0.828s, episode steps: 138, steps per second: 167, episode reward: -137.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.928 [0.000, 2.000],  loss: 0.531227, mae: 10.787178, mean_q: -15.480268, mean_eps: 0.344981\n",
            " 103642/200000: episode: 460, duration: 0.925s, episode steps: 148, steps per second: 160, episode reward: -147.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.007 [0.000, 2.000],  loss: 0.571124, mae: 10.711890, mean_q: -15.360776, mean_eps: 0.344069\n",
            " 103781/200000: episode: 461, duration: 0.870s, episode steps: 139, steps per second: 160, episode reward: -138.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.029 [0.000, 2.000],  loss: 0.575265, mae: 10.729657, mean_q: -15.400444, mean_eps: 0.343157\n",
            " 103899/200000: episode: 462, duration: 0.746s, episode steps: 118, steps per second: 158, episode reward: -117.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.068 [0.000, 2.000],  loss: 0.719243, mae: 10.727237, mean_q: -15.367121, mean_eps: 0.342347\n",
            " 104036/200000: episode: 463, duration: 0.844s, episode steps: 137, steps per second: 162, episode reward: -136.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.109 [0.000, 2.000],  loss: 0.669963, mae: 10.822885, mean_q: -15.511705, mean_eps: 0.341549\n",
            " 104226/200000: episode: 464, duration: 1.206s, episode steps: 190, steps per second: 158, episode reward: -189.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.216 [0.000, 2.000],  loss: 0.563225, mae: 10.893178, mean_q: -15.649274, mean_eps: 0.340510\n",
            " 104364/200000: episode: 465, duration: 0.846s, episode steps: 138, steps per second: 163, episode reward: -137.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.043 [0.000, 2.000],  loss: 0.630365, mae: 10.902612, mean_q: -15.653877, mean_eps: 0.339471\n",
            " 104475/200000: episode: 466, duration: 0.685s, episode steps: 111, steps per second: 162, episode reward: -110.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.063 [0.000, 2.000],  loss: 0.653518, mae: 10.805272, mean_q: -15.480038, mean_eps: 0.338686\n",
            " 104592/200000: episode: 467, duration: 0.733s, episode steps: 117, steps per second: 160, episode reward: -116.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.137 [0.000, 2.000],  loss: 0.566271, mae: 10.873859, mean_q: -15.641313, mean_eps: 0.337964\n",
            " 104709/200000: episode: 468, duration: 0.757s, episode steps: 117, steps per second: 155, episode reward: -116.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.094 [0.000, 2.000],  loss: 0.621392, mae: 10.824061, mean_q: -15.546048, mean_eps: 0.337217\n",
            " 104841/200000: episode: 469, duration: 0.818s, episode steps: 132, steps per second: 161, episode reward: -131.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.703118, mae: 10.890633, mean_q: -15.611876, mean_eps: 0.336419\n",
            " 105000/200000: episode: 470, duration: 1.227s, episode steps: 159, steps per second: 130, episode reward: -158.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.937 [0.000, 2.000],  loss: 0.557639, mae: 10.806642, mean_q: -15.541086, mean_eps: 0.335507\n",
            " 105165/200000: episode: 471, duration: 1.456s, episode steps: 165, steps per second: 113, episode reward: -164.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.988 [0.000, 2.000],  loss: 0.588911, mae: 10.817886, mean_q: -15.540888, mean_eps: 0.334481\n",
            " 105297/200000: episode: 472, duration: 1.154s, episode steps: 132, steps per second: 114, episode reward: -131.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.061 [0.000, 2.000],  loss: 0.720472, mae: 10.838326, mean_q: -15.559532, mean_eps: 0.333531\n",
            " 105443/200000: episode: 473, duration: 0.864s, episode steps: 146, steps per second: 169, episode reward: -145.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.870 [0.000, 2.000],  loss: 0.591576, mae: 10.780727, mean_q: -15.488907, mean_eps: 0.332657\n",
            " 105558/200000: episode: 474, duration: 0.706s, episode steps: 115, steps per second: 163, episode reward: -114.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.096 [0.000, 2.000],  loss: 0.539445, mae: 10.810305, mean_q: -15.553013, mean_eps: 0.331833\n",
            " 105680/200000: episode: 475, duration: 0.775s, episode steps: 122, steps per second: 157, episode reward: -121.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.885 [0.000, 2.000],  loss: 0.531434, mae: 10.822775, mean_q: -15.579093, mean_eps: 0.331086\n",
            " 105835/200000: episode: 476, duration: 0.975s, episode steps: 155, steps per second: 159, episode reward: -154.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.155 [0.000, 2.000],  loss: 0.641579, mae: 10.840076, mean_q: -15.574913, mean_eps: 0.330212\n",
            " 105969/200000: episode: 477, duration: 0.842s, episode steps: 134, steps per second: 159, episode reward: -133.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.910 [0.000, 2.000],  loss: 0.645080, mae: 10.774659, mean_q: -15.479903, mean_eps: 0.329287\n",
            " 106116/200000: episode: 478, duration: 0.896s, episode steps: 147, steps per second: 164, episode reward: -146.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.075 [0.000, 2.000],  loss: 0.691167, mae: 10.735851, mean_q: -15.390541, mean_eps: 0.328401\n",
            " 106255/200000: episode: 479, duration: 0.841s, episode steps: 139, steps per second: 165, episode reward: -138.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.115 [0.000, 2.000],  loss: 0.674447, mae: 10.799389, mean_q: -15.498490, mean_eps: 0.327501\n",
            " 106402/200000: episode: 480, duration: 0.886s, episode steps: 147, steps per second: 166, episode reward: -146.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.211 [0.000, 2.000],  loss: 0.613803, mae: 10.666203, mean_q: -15.321519, mean_eps: 0.326589\n",
            " 106537/200000: episode: 481, duration: 0.844s, episode steps: 135, steps per second: 160, episode reward: -134.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.052 [0.000, 2.000],  loss: 0.651244, mae: 10.713323, mean_q: -15.373123, mean_eps: 0.325690\n",
            " 106643/200000: episode: 482, duration: 0.658s, episode steps: 106, steps per second: 161, episode reward: -105.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.047 [0.000, 2.000],  loss: 0.622771, mae: 10.731243, mean_q: -15.427007, mean_eps: 0.324930\n",
            " 106758/200000: episode: 483, duration: 0.738s, episode steps: 115, steps per second: 156, episode reward: -114.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.991 [0.000, 2.000],  loss: 0.630863, mae: 10.757305, mean_q: -15.465703, mean_eps: 0.324233\n",
            " 106863/200000: episode: 484, duration: 0.642s, episode steps: 105, steps per second: 164, episode reward: -104.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.143 [0.000, 2.000],  loss: 0.558449, mae: 10.708282, mean_q: -15.382065, mean_eps: 0.323537\n",
            " 107000/200000: episode: 485, duration: 1.115s, episode steps: 137, steps per second: 123, episode reward: -136.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.226 [0.000, 2.000],  loss: 0.589959, mae: 10.646226, mean_q: -15.274397, mean_eps: 0.322777\n",
            " 107219/200000: episode: 486, duration: 1.953s, episode steps: 219, steps per second: 112, episode reward: -218.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.237 [0.000, 2.000],  loss: 0.629446, mae: 10.745861, mean_q: -15.426122, mean_eps: 0.321649\n",
            " 107343/200000: episode: 487, duration: 1.032s, episode steps: 124, steps per second: 120, episode reward: -123.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.056 [0.000, 2.000],  loss: 0.589331, mae: 10.767527, mean_q: -15.487902, mean_eps: 0.320560\n",
            " 107458/200000: episode: 488, duration: 0.734s, episode steps: 115, steps per second: 157, episode reward: -114.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.078 [0.000, 2.000],  loss: 0.604187, mae: 10.786138, mean_q: -15.481815, mean_eps: 0.319800\n",
            " 107571/200000: episode: 489, duration: 0.698s, episode steps: 113, steps per second: 162, episode reward: -112.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.823 [0.000, 2.000],  loss: 0.661796, mae: 10.775310, mean_q: -15.473326, mean_eps: 0.319078\n",
            " 107672/200000: episode: 490, duration: 0.685s, episode steps: 101, steps per second: 148, episode reward: -100.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.921 [0.000, 2.000],  loss: 0.536082, mae: 10.691106, mean_q: -15.373583, mean_eps: 0.318407\n",
            " 107815/200000: episode: 491, duration: 0.919s, episode steps: 143, steps per second: 156, episode reward: -142.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.713 [0.000, 2.000],  loss: 0.601388, mae: 10.625020, mean_q: -15.254202, mean_eps: 0.317634\n",
            " 107910/200000: episode: 492, duration: 0.632s, episode steps:  95, steps per second: 150, episode reward: -94.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.768 [0.000, 2.000],  loss: 0.549155, mae: 10.632449, mean_q: -15.287992, mean_eps: 0.316874\n",
            " 108001/200000: episode: 493, duration: 0.604s, episode steps:  91, steps per second: 151, episode reward: -90.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.945 [0.000, 2.000],  loss: 0.524901, mae: 10.758096, mean_q: -15.485360, mean_eps: 0.316279\n",
            " 108132/200000: episode: 494, duration: 0.853s, episode steps: 131, steps per second: 154, episode reward: -130.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.038 [0.000, 2.000],  loss: 0.658446, mae: 10.719172, mean_q: -15.365076, mean_eps: 0.315582\n",
            " 108257/200000: episode: 495, duration: 0.795s, episode steps: 125, steps per second: 157, episode reward: -124.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.040 [0.000, 2.000],  loss: 0.632388, mae: 10.811045, mean_q: -15.510399, mean_eps: 0.314771\n",
            " 108385/200000: episode: 496, duration: 0.834s, episode steps: 128, steps per second: 154, episode reward: -127.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.945 [0.000, 2.000],  loss: 0.670985, mae: 10.808550, mean_q: -15.518626, mean_eps: 0.313961\n",
            " 108520/200000: episode: 497, duration: 0.828s, episode steps: 135, steps per second: 163, episode reward: -134.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.963 [0.000, 2.000],  loss: 0.543576, mae: 10.831229, mean_q: -15.567875, mean_eps: 0.313137\n",
            " 108633/200000: episode: 498, duration: 0.705s, episode steps: 113, steps per second: 160, episode reward: -112.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.867 [0.000, 2.000],  loss: 0.571897, mae: 10.786443, mean_q: -15.481938, mean_eps: 0.312352\n",
            " 108785/200000: episode: 499, duration: 0.912s, episode steps: 152, steps per second: 167, episode reward: -151.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.993 [0.000, 2.000],  loss: 0.618120, mae: 10.720902, mean_q: -15.370972, mean_eps: 0.311503\n",
            " 108886/200000: episode: 500, duration: 0.744s, episode steps: 101, steps per second: 136, episode reward: -100.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.050 [0.000, 2.000],  loss: 0.543581, mae: 10.776480, mean_q: -15.515460, mean_eps: 0.310705\n",
            " 109037/200000: episode: 501, duration: 1.330s, episode steps: 151, steps per second: 114, episode reward: -150.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.974 [0.000, 2.000],  loss: 0.635642, mae: 10.797022, mean_q: -15.493373, mean_eps: 0.309907\n",
            " 109170/200000: episode: 502, duration: 1.136s, episode steps: 133, steps per second: 117, episode reward: -132.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.955 [0.000, 2.000],  loss: 0.600799, mae: 10.816141, mean_q: -15.534590, mean_eps: 0.309008\n",
            " 109276/200000: episode: 503, duration: 0.892s, episode steps: 106, steps per second: 119, episode reward: -105.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.009 [0.000, 2.000],  loss: 0.666837, mae: 10.727851, mean_q: -15.390265, mean_eps: 0.308261\n",
            " 109706/200000: episode: 504, duration: 2.560s, episode steps: 430, steps per second: 168, episode reward: -429.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.867 [0.000, 2.000],  loss: 0.635361, mae: 10.774581, mean_q: -15.442399, mean_eps: 0.306563\n",
            " 109887/200000: episode: 505, duration: 1.093s, episode steps: 181, steps per second: 166, episode reward: -180.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.956 [0.000, 2.000],  loss: 0.666201, mae: 10.806070, mean_q: -15.487283, mean_eps: 0.304625\n",
            " 110021/200000: episode: 506, duration: 0.815s, episode steps: 134, steps per second: 164, episode reward: -133.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.888 [0.000, 2.000],  loss: 0.617422, mae: 10.767746, mean_q: -15.431221, mean_eps: 0.303625\n",
            " 110197/200000: episode: 507, duration: 1.060s, episode steps: 176, steps per second: 166, episode reward: -175.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.784 [0.000, 2.000],  loss: 0.593252, mae: 10.752890, mean_q: -15.439042, mean_eps: 0.302637\n",
            " 110344/200000: episode: 508, duration: 0.933s, episode steps: 147, steps per second: 158, episode reward: -146.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.939 [0.000, 2.000],  loss: 0.629035, mae: 10.684460, mean_q: -15.322281, mean_eps: 0.301623\n",
            " 110466/200000: episode: 509, duration: 0.793s, episode steps: 122, steps per second: 154, episode reward: -121.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.836 [0.000, 2.000],  loss: 0.661033, mae: 10.759736, mean_q: -15.431137, mean_eps: 0.300775\n",
            " 110560/200000: episode: 510, duration: 0.587s, episode steps:  94, steps per second: 160, episode reward: -93.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.021 [0.000, 2.000],  loss: 0.586295, mae: 10.846042, mean_q: -15.555474, mean_eps: 0.300091\n",
            " 110668/200000: episode: 511, duration: 0.680s, episode steps: 108, steps per second: 159, episode reward: -107.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.898 [0.000, 2.000],  loss: 0.523668, mae: 10.753792, mean_q: -15.444179, mean_eps: 0.299457\n",
            " 110786/200000: episode: 512, duration: 0.745s, episode steps: 118, steps per second: 158, episode reward: -117.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.051 [0.000, 2.000],  loss: 0.579452, mae: 10.704099, mean_q: -15.374600, mean_eps: 0.298735\n",
            " 110924/200000: episode: 513, duration: 1.067s, episode steps: 138, steps per second: 129, episode reward: -137.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.087 [0.000, 2.000],  loss: 0.635001, mae: 10.786575, mean_q: -15.477170, mean_eps: 0.297925\n",
            " 111034/200000: episode: 514, duration: 1.016s, episode steps: 110, steps per second: 108, episode reward: -109.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.982 [0.000, 2.000],  loss: 0.488280, mae: 10.740844, mean_q: -15.458931, mean_eps: 0.297139\n",
            " 111154/200000: episode: 515, duration: 1.042s, episode steps: 120, steps per second: 115, episode reward: -119.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.075 [0.000, 2.000],  loss: 0.570515, mae: 10.753445, mean_q: -15.460671, mean_eps: 0.296405\n",
            " 111303/200000: episode: 516, duration: 1.157s, episode steps: 149, steps per second: 129, episode reward: -148.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.926 [0.000, 2.000],  loss: 0.577636, mae: 10.753154, mean_q: -15.458058, mean_eps: 0.295556\n",
            " 111425/200000: episode: 517, duration: 0.797s, episode steps: 122, steps per second: 153, episode reward: -121.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.992 [0.000, 2.000],  loss: 0.561035, mae: 10.728248, mean_q: -15.417911, mean_eps: 0.294695\n",
            " 111588/200000: episode: 518, duration: 1.117s, episode steps: 163, steps per second: 146, episode reward: -162.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.877 [0.000, 2.000],  loss: 0.563765, mae: 10.774537, mean_q: -15.493776, mean_eps: 0.293795\n",
            " 111727/200000: episode: 519, duration: 0.928s, episode steps: 139, steps per second: 150, episode reward: -138.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.935 [0.000, 2.000],  loss: 0.615525, mae: 10.752122, mean_q: -15.454215, mean_eps: 0.292845\n",
            " 111837/200000: episode: 520, duration: 0.714s, episode steps: 110, steps per second: 154, episode reward: -109.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.873 [0.000, 2.000],  loss: 0.556456, mae: 10.757918, mean_q: -15.476008, mean_eps: 0.292047\n",
            " 111961/200000: episode: 521, duration: 0.827s, episode steps: 124, steps per second: 150, episode reward: -123.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.879 [0.000, 2.000],  loss: 0.645097, mae: 10.878470, mean_q: -15.633736, mean_eps: 0.291300\n",
            " 112107/200000: episode: 522, duration: 0.923s, episode steps: 146, steps per second: 158, episode reward: -145.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.151 [0.000, 2.000],  loss: 0.683581, mae: 10.770407, mean_q: -15.451189, mean_eps: 0.290451\n",
            " 112269/200000: episode: 523, duration: 1.005s, episode steps: 162, steps per second: 161, episode reward: -161.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.173 [0.000, 2.000],  loss: 0.653613, mae: 10.775158, mean_q: -15.486117, mean_eps: 0.289476\n",
            " 112372/200000: episode: 524, duration: 0.628s, episode steps: 103, steps per second: 164, episode reward: -102.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.117 [0.000, 2.000],  loss: 0.609328, mae: 10.801638, mean_q: -15.523684, mean_eps: 0.288640\n",
            " 112469/200000: episode: 525, duration: 0.665s, episode steps:  97, steps per second: 146, episode reward: -96.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.928 [0.000, 2.000],  loss: 0.605414, mae: 10.776459, mean_q: -15.467918, mean_eps: 0.288007\n",
            " 112586/200000: episode: 526, duration: 0.770s, episode steps: 117, steps per second: 152, episode reward: -116.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.718 [0.000, 2.000],  loss: 0.573521, mae: 10.795250, mean_q: -15.529206, mean_eps: 0.287323\n",
            " 113086/200000: episode: 527, duration: 3.936s, episode steps: 500, steps per second: 127, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.250 [0.000, 2.000],  loss: 0.590761, mae: 10.761032, mean_q: -15.488413, mean_eps: 0.285372\n",
            " 113169/200000: episode: 528, duration: 0.742s, episode steps:  83, steps per second: 112, episode reward: -82.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.120 [0.000, 2.000],  loss: 0.695533, mae: 10.756597, mean_q: -15.451545, mean_eps: 0.283523\n",
            " 113274/200000: episode: 529, duration: 0.683s, episode steps: 105, steps per second: 154, episode reward: -104.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.171 [0.000, 2.000],  loss: 0.611701, mae: 10.666416, mean_q: -15.334082, mean_eps: 0.282927\n",
            " 113406/200000: episode: 530, duration: 0.856s, episode steps: 132, steps per second: 154, episode reward: -131.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.235 [0.000, 2.000],  loss: 0.637008, mae: 10.850145, mean_q: -15.598025, mean_eps: 0.282180\n",
            " 113528/200000: episode: 531, duration: 0.826s, episode steps: 122, steps per second: 148, episode reward: -121.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.975 [0.000, 2.000],  loss: 0.623642, mae: 10.801375, mean_q: -15.510364, mean_eps: 0.281382\n",
            " 113655/200000: episode: 532, duration: 0.804s, episode steps: 127, steps per second: 158, episode reward: -126.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.961 [0.000, 2.000],  loss: 0.465890, mae: 10.696295, mean_q: -15.433044, mean_eps: 0.280597\n",
            " 113774/200000: episode: 533, duration: 0.757s, episode steps: 119, steps per second: 157, episode reward: -118.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.193 [0.000, 2.000],  loss: 0.669044, mae: 10.805210, mean_q: -15.538863, mean_eps: 0.279811\n",
            " 113920/200000: episode: 534, duration: 0.963s, episode steps: 146, steps per second: 152, episode reward: -145.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.082 [0.000, 2.000],  loss: 0.572838, mae: 10.748381, mean_q: -15.478129, mean_eps: 0.278975\n",
            " 114011/200000: episode: 535, duration: 0.591s, episode steps:  91, steps per second: 154, episode reward: -90.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.154 [0.000, 2.000],  loss: 0.676849, mae: 10.745017, mean_q: -15.423260, mean_eps: 0.278228\n",
            " 114122/200000: episode: 536, duration: 0.848s, episode steps: 111, steps per second: 131, episode reward: -110.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.198 [0.000, 2.000],  loss: 0.613049, mae: 10.898045, mean_q: -15.653684, mean_eps: 0.277582\n",
            " 114226/200000: episode: 537, duration: 0.678s, episode steps: 104, steps per second: 153, episode reward: -103.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.212 [0.000, 2.000],  loss: 0.657553, mae: 10.866005, mean_q: -15.607922, mean_eps: 0.276898\n",
            " 114342/200000: episode: 538, duration: 0.768s, episode steps: 116, steps per second: 151, episode reward: -115.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.302 [0.000, 2.000],  loss: 0.589008, mae: 10.728671, mean_q: -15.398922, mean_eps: 0.276201\n",
            " 114465/200000: episode: 539, duration: 0.769s, episode steps: 123, steps per second: 160, episode reward: -122.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.366 [0.000, 2.000],  loss: 0.585171, mae: 10.926327, mean_q: -15.707576, mean_eps: 0.275441\n",
            " 114582/200000: episode: 540, duration: 0.759s, episode steps: 117, steps per second: 154, episode reward: -116.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.214 [0.000, 2.000],  loss: 0.672969, mae: 10.943175, mean_q: -15.708601, mean_eps: 0.274681\n",
            " 114765/200000: episode: 541, duration: 1.495s, episode steps: 183, steps per second: 122, episode reward: -182.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.202 [0.000, 2.000],  loss: 0.576953, mae: 10.805142, mean_q: -15.527931, mean_eps: 0.273731\n",
            " 114882/200000: episode: 542, duration: 1.099s, episode steps: 117, steps per second: 106, episode reward: -116.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.231 [0.000, 2.000],  loss: 0.598416, mae: 10.801289, mean_q: -15.545280, mean_eps: 0.272781\n",
            " 114981/200000: episode: 543, duration: 0.874s, episode steps:  99, steps per second: 113, episode reward: -98.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.212 [0.000, 2.000],  loss: 0.637828, mae: 10.781442, mean_q: -15.503405, mean_eps: 0.272097\n",
            " 115081/200000: episode: 544, duration: 0.827s, episode steps: 100, steps per second: 121, episode reward: -99.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.960 [0.000, 2.000],  loss: 0.686417, mae: 10.974737, mean_q: -15.775658, mean_eps: 0.271464\n",
            " 115219/200000: episode: 545, duration: 0.882s, episode steps: 138, steps per second: 157, episode reward: -137.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.087 [0.000, 2.000],  loss: 0.648756, mae: 10.952929, mean_q: -15.737426, mean_eps: 0.270717\n",
            " 115357/200000: episode: 546, duration: 0.895s, episode steps: 138, steps per second: 154, episode reward: -137.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.181 [0.000, 2.000],  loss: 0.592386, mae: 10.925818, mean_q: -15.689346, mean_eps: 0.269843\n",
            " 115527/200000: episode: 547, duration: 1.094s, episode steps: 170, steps per second: 155, episode reward: -169.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.259 [0.000, 2.000],  loss: 0.625309, mae: 10.935025, mean_q: -15.710121, mean_eps: 0.268867\n",
            " 115699/200000: episode: 548, duration: 1.072s, episode steps: 172, steps per second: 160, episode reward: -171.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.843 [0.000, 2.000],  loss: 0.560148, mae: 10.933100, mean_q: -15.740391, mean_eps: 0.267791\n",
            " 115845/200000: episode: 549, duration: 0.946s, episode steps: 146, steps per second: 154, episode reward: -145.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.103 [0.000, 2.000],  loss: 0.581095, mae: 10.956841, mean_q: -15.775865, mean_eps: 0.266777\n",
            " 115964/200000: episode: 550, duration: 0.744s, episode steps: 119, steps per second: 160, episode reward: -118.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.244 [0.000, 2.000],  loss: 0.538118, mae: 10.948368, mean_q: -15.768163, mean_eps: 0.265941\n",
            " 116094/200000: episode: 551, duration: 0.836s, episode steps: 130, steps per second: 156, episode reward: -129.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.162 [0.000, 2.000],  loss: 0.739317, mae: 11.047622, mean_q: -15.857461, mean_eps: 0.265156\n",
            " 116264/200000: episode: 552, duration: 1.073s, episode steps: 170, steps per second: 158, episode reward: -169.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.994 [0.000, 2.000],  loss: 0.625978, mae: 10.991364, mean_q: -15.786429, mean_eps: 0.264206\n",
            " 116410/200000: episode: 553, duration: 0.926s, episode steps: 146, steps per second: 158, episode reward: -145.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.110 [0.000, 2.000],  loss: 0.584107, mae: 11.056704, mean_q: -15.914949, mean_eps: 0.263205\n",
            " 116527/200000: episode: 554, duration: 0.764s, episode steps: 117, steps per second: 153, episode reward: -116.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.393 [0.000, 2.000],  loss: 0.627901, mae: 10.981289, mean_q: -15.774676, mean_eps: 0.262369\n",
            " 116627/200000: episode: 555, duration: 0.771s, episode steps: 100, steps per second: 130, episode reward: -99.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.310 [0.000, 2.000],  loss: 0.661318, mae: 11.077511, mean_q: -15.911031, mean_eps: 0.261685\n",
            " 116742/200000: episode: 556, duration: 1.121s, episode steps: 115, steps per second: 103, episode reward: -114.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.200 [0.000, 2.000],  loss: 0.639586, mae: 11.159481, mean_q: -16.020140, mean_eps: 0.261001\n",
            " 116841/200000: episode: 557, duration: 0.892s, episode steps:  99, steps per second: 111, episode reward: -98.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.020 [0.000, 2.000],  loss: 0.653699, mae: 10.933391, mean_q: -15.679272, mean_eps: 0.260317\n",
            " 116954/200000: episode: 558, duration: 1.078s, episode steps: 113, steps per second: 105, episode reward: -112.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.150 [0.000, 2.000],  loss: 0.539008, mae: 10.930693, mean_q: -15.729627, mean_eps: 0.259646\n",
            " 117100/200000: episode: 559, duration: 1.009s, episode steps: 146, steps per second: 145, episode reward: -145.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.418 [0.000, 2.000],  loss: 0.613551, mae: 11.065108, mean_q: -15.918812, mean_eps: 0.258835\n",
            " 117274/200000: episode: 560, duration: 1.151s, episode steps: 174, steps per second: 151, episode reward: -173.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.270 [0.000, 2.000],  loss: 0.678536, mae: 11.108555, mean_q: -15.973369, mean_eps: 0.257822\n",
            " 117388/200000: episode: 561, duration: 0.745s, episode steps: 114, steps per second: 153, episode reward: -113.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.219 [0.000, 2.000],  loss: 0.609957, mae: 11.175564, mean_q: -16.108046, mean_eps: 0.256910\n",
            " 117480/200000: episode: 562, duration: 0.598s, episode steps:  92, steps per second: 154, episode reward: -91.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.935 [0.000, 2.000],  loss: 0.648963, mae: 11.100745, mean_q: -15.978181, mean_eps: 0.256264\n",
            " 117592/200000: episode: 563, duration: 0.763s, episode steps: 112, steps per second: 147, episode reward: -111.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.161 [0.000, 2.000],  loss: 0.618542, mae: 11.048363, mean_q: -15.911176, mean_eps: 0.255618\n",
            " 117721/200000: episode: 564, duration: 0.843s, episode steps: 129, steps per second: 153, episode reward: -128.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.984 [0.000, 2.000],  loss: 0.617807, mae: 11.079764, mean_q: -15.959559, mean_eps: 0.254845\n",
            " 117860/200000: episode: 565, duration: 0.851s, episode steps: 139, steps per second: 163, episode reward: -138.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.086 [0.000, 2.000],  loss: 0.665057, mae: 11.032550, mean_q: -15.865140, mean_eps: 0.253997\n",
            " 117974/200000: episode: 566, duration: 0.785s, episode steps: 114, steps per second: 145, episode reward: -113.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.816 [0.000, 2.000],  loss: 0.595878, mae: 11.057158, mean_q: -15.911081, mean_eps: 0.253199\n",
            " 118058/200000: episode: 567, duration: 0.551s, episode steps:  84, steps per second: 153, episode reward: -83.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.131 [0.000, 2.000],  loss: 0.643443, mae: 11.127428, mean_q: -15.983670, mean_eps: 0.252565\n",
            " 118151/200000: episode: 568, duration: 0.570s, episode steps:  93, steps per second: 163, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.892 [0.000, 2.000],  loss: 0.573324, mae: 11.179572, mean_q: -16.097552, mean_eps: 0.252008\n",
            " 118255/200000: episode: 569, duration: 0.654s, episode steps: 104, steps per second: 159, episode reward: -103.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.942 [0.000, 2.000],  loss: 0.646663, mae: 11.169468, mean_q: -16.048750, mean_eps: 0.251387\n",
            " 118355/200000: episode: 570, duration: 0.615s, episode steps: 100, steps per second: 163, episode reward: -99.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.910 [0.000, 2.000],  loss: 0.631355, mae: 11.159120, mean_q: -16.063871, mean_eps: 0.250741\n",
            " 118449/200000: episode: 571, duration: 0.689s, episode steps:  94, steps per second: 137, episode reward: -93.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.979 [0.000, 2.000],  loss: 0.666468, mae: 11.226120, mean_q: -16.144834, mean_eps: 0.250121\n",
            " 118566/200000: episode: 572, duration: 1.063s, episode steps: 117, steps per second: 110, episode reward: -116.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.932 [0.000, 2.000],  loss: 0.651127, mae: 11.135377, mean_q: -16.000333, mean_eps: 0.249449\n",
            " 118666/200000: episode: 573, duration: 1.080s, episode steps: 100, steps per second:  93, episode reward: -99.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.790 [0.000, 2.000],  loss: 0.794824, mae: 11.221300, mean_q: -16.115995, mean_eps: 0.248765\n",
            " 118755/200000: episode: 574, duration: 0.912s, episode steps:  89, steps per second:  98, episode reward: -88.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.618100, mae: 11.128409, mean_q: -16.014080, mean_eps: 0.248170\n",
            " 118860/200000: episode: 575, duration: 1.070s, episode steps: 105, steps per second:  98, episode reward: -104.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.591660, mae: 11.162437, mean_q: -16.081950, mean_eps: 0.247562\n",
            " 118985/200000: episode: 576, duration: 0.825s, episode steps: 125, steps per second: 152, episode reward: -124.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.192 [0.000, 2.000],  loss: 0.640888, mae: 11.160601, mean_q: -16.032506, mean_eps: 0.246827\n",
            " 119093/200000: episode: 577, duration: 0.742s, episode steps: 108, steps per second: 145, episode reward: -107.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.102 [0.000, 2.000],  loss: 0.743328, mae: 11.184662, mean_q: -16.050116, mean_eps: 0.246080\n",
            " 119196/200000: episode: 578, duration: 0.723s, episode steps: 103, steps per second: 143, episode reward: -102.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.796 [0.000, 2.000],  loss: 0.630015, mae: 11.184988, mean_q: -16.094076, mean_eps: 0.245421\n",
            " 119319/200000: episode: 579, duration: 0.847s, episode steps: 123, steps per second: 145, episode reward: -122.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.041 [0.000, 2.000],  loss: 0.695832, mae: 11.198286, mean_q: -16.094952, mean_eps: 0.244712\n",
            " 119408/200000: episode: 580, duration: 0.615s, episode steps:  89, steps per second: 145, episode reward: -88.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.966 [0.000, 2.000],  loss: 0.547803, mae: 11.145627, mean_q: -16.068677, mean_eps: 0.244041\n",
            " 119498/200000: episode: 581, duration: 0.578s, episode steps:  90, steps per second: 156, episode reward: -89.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.889 [0.000, 2.000],  loss: 0.634293, mae: 11.142681, mean_q: -16.057844, mean_eps: 0.243471\n",
            " 119614/200000: episode: 582, duration: 0.732s, episode steps: 116, steps per second: 159, episode reward: -115.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.940 [0.000, 2.000],  loss: 0.661341, mae: 11.159776, mean_q: -16.058178, mean_eps: 0.242812\n",
            " 119716/200000: episode: 583, duration: 0.671s, episode steps: 102, steps per second: 152, episode reward: -101.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.049 [0.000, 2.000],  loss: 0.606319, mae: 11.173208, mean_q: -16.096104, mean_eps: 0.242128\n",
            " 119855/200000: episode: 584, duration: 0.833s, episode steps: 139, steps per second: 167, episode reward: -138.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.820 [0.000, 2.000],  loss: 0.646007, mae: 11.279985, mean_q: -16.237433, mean_eps: 0.241368\n",
            " 119946/200000: episode: 585, duration: 0.572s, episode steps:  91, steps per second: 159, episode reward: -90.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.813 [0.000, 2.000],  loss: 0.631196, mae: 11.173420, mean_q: -16.083615, mean_eps: 0.240633\n",
            " 120043/200000: episode: 586, duration: 0.614s, episode steps:  97, steps per second: 158, episode reward: -96.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.979 [0.000, 2.000],  loss: 0.741885, mae: 11.174839, mean_q: -16.063810, mean_eps: 0.240038\n",
            " 120157/200000: episode: 587, duration: 0.740s, episode steps: 114, steps per second: 154, episode reward: -113.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.833 [0.000, 2.000],  loss: 0.703833, mae: 11.291365, mean_q: -16.209965, mean_eps: 0.239367\n",
            " 120241/200000: episode: 588, duration: 0.533s, episode steps:  84, steps per second: 158, episode reward: -83.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.869 [0.000, 2.000],  loss: 0.603212, mae: 11.158039, mean_q: -16.035290, mean_eps: 0.238733\n",
            " 120351/200000: episode: 589, duration: 0.699s, episode steps: 110, steps per second: 157, episode reward: -109.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.764 [0.000, 2.000],  loss: 0.593348, mae: 11.075536, mean_q: -15.940264, mean_eps: 0.238125\n",
            " 120479/200000: episode: 590, duration: 1.182s, episode steps: 128, steps per second: 108, episode reward: -127.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.055 [0.000, 2.000],  loss: 0.608673, mae: 11.137232, mean_q: -16.035421, mean_eps: 0.237378\n",
            " 120600/200000: episode: 591, duration: 1.131s, episode steps: 121, steps per second: 107, episode reward: -120.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.901 [0.000, 2.000],  loss: 0.629195, mae: 11.149681, mean_q: -16.049809, mean_eps: 0.236593\n",
            " 120703/200000: episode: 592, duration: 1.020s, episode steps: 103, steps per second: 101, episode reward: -102.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.029 [0.000, 2.000],  loss: 0.583948, mae: 11.167524, mean_q: -16.083663, mean_eps: 0.235883\n",
            " 120804/200000: episode: 593, duration: 0.697s, episode steps: 101, steps per second: 145, episode reward: -100.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.871 [0.000, 2.000],  loss: 0.564851, mae: 11.045578, mean_q: -15.910814, mean_eps: 0.235237\n",
            " 120889/200000: episode: 594, duration: 0.562s, episode steps:  85, steps per second: 151, episode reward: -84.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.718 [0.000, 2.000],  loss: 0.538252, mae: 11.047771, mean_q: -15.915848, mean_eps: 0.234642\n",
            " 121005/200000: episode: 595, duration: 0.743s, episode steps: 116, steps per second: 156, episode reward: -115.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.862 [0.000, 2.000],  loss: 0.559165, mae: 11.156251, mean_q: -16.062024, mean_eps: 0.233996\n",
            " 121112/200000: episode: 596, duration: 0.712s, episode steps: 107, steps per second: 150, episode reward: -106.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.028 [0.000, 2.000],  loss: 0.670752, mae: 11.197225, mean_q: -16.085936, mean_eps: 0.233299\n",
            " 121237/200000: episode: 597, duration: 0.798s, episode steps: 125, steps per second: 157, episode reward: -124.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.904 [0.000, 2.000],  loss: 0.615231, mae: 11.282729, mean_q: -16.225725, mean_eps: 0.232565\n",
            " 121371/200000: episode: 598, duration: 0.860s, episode steps: 134, steps per second: 156, episode reward: -133.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.836 [0.000, 2.000],  loss: 0.649552, mae: 11.323939, mean_q: -16.275495, mean_eps: 0.231741\n",
            " 121502/200000: episode: 599, duration: 0.833s, episode steps: 131, steps per second: 157, episode reward: -130.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.671687, mae: 11.288667, mean_q: -16.217578, mean_eps: 0.230905\n",
            " 121627/200000: episode: 600, duration: 0.834s, episode steps: 125, steps per second: 150, episode reward: -124.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.904 [0.000, 2.000],  loss: 0.620066, mae: 11.130744, mean_q: -16.011787, mean_eps: 0.230095\n",
            " 121736/200000: episode: 601, duration: 0.749s, episode steps: 109, steps per second: 146, episode reward: -108.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.899 [0.000, 2.000],  loss: 0.665822, mae: 11.268759, mean_q: -16.207307, mean_eps: 0.229360\n",
            " 121865/200000: episode: 602, duration: 0.845s, episode steps: 129, steps per second: 153, episode reward: -128.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.953 [0.000, 2.000],  loss: 0.653120, mae: 11.224903, mean_q: -16.153746, mean_eps: 0.228600\n",
            " 122191/200000: episode: 603, duration: 2.081s, episode steps: 326, steps per second: 157, episode reward: -325.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.767 [0.000, 2.000],  loss: 0.754522, mae: 11.311671, mean_q: -16.238043, mean_eps: 0.227156\n",
            " 122295/200000: episode: 604, duration: 0.836s, episode steps: 104, steps per second: 124, episode reward: -103.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.731 [0.000, 2.000],  loss: 0.604141, mae: 11.247869, mean_q: -16.181326, mean_eps: 0.225801\n",
            " 122389/200000: episode: 605, duration: 0.842s, episode steps:  94, steps per second: 112, episode reward: -93.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.713 [0.000, 2.000],  loss: 0.673253, mae: 11.377532, mean_q: -16.358174, mean_eps: 0.225167\n",
            " 122594/200000: episode: 606, duration: 1.884s, episode steps: 205, steps per second: 109, episode reward: -204.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.239 [0.000, 2.000],  loss: 0.637372, mae: 11.332928, mean_q: -16.297742, mean_eps: 0.224217\n",
            " 122699/200000: episode: 607, duration: 0.771s, episode steps: 105, steps per second: 136, episode reward: -104.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.886 [0.000, 2.000],  loss: 0.688525, mae: 11.345854, mean_q: -16.315953, mean_eps: 0.223242\n",
            " 122840/200000: episode: 608, duration: 0.931s, episode steps: 141, steps per second: 151, episode reward: -140.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.766 [0.000, 2.000],  loss: 0.548507, mae: 11.309830, mean_q: -16.283909, mean_eps: 0.222469\n",
            " 122965/200000: episode: 609, duration: 0.873s, episode steps: 125, steps per second: 143, episode reward: -124.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.120 [0.000, 2.000],  loss: 0.672776, mae: 11.341180, mean_q: -16.306116, mean_eps: 0.221621\n",
            " 123053/200000: episode: 610, duration: 0.600s, episode steps:  88, steps per second: 147, episode reward: -87.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.886 [0.000, 2.000],  loss: 0.665727, mae: 11.422554, mean_q: -16.432454, mean_eps: 0.220937\n",
            " 123141/200000: episode: 611, duration: 0.604s, episode steps:  88, steps per second: 146, episode reward: -87.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.920 [0.000, 2.000],  loss: 0.578733, mae: 11.431570, mean_q: -16.454023, mean_eps: 0.220379\n",
            " 123231/200000: episode: 612, duration: 0.574s, episode steps:  90, steps per second: 157, episode reward: -89.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.933 [0.000, 2.000],  loss: 0.632661, mae: 11.378949, mean_q: -16.342364, mean_eps: 0.219822\n",
            " 123352/200000: episode: 613, duration: 0.786s, episode steps: 121, steps per second: 154, episode reward: -120.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.818 [0.000, 2.000],  loss: 0.744275, mae: 11.553679, mean_q: -16.587985, mean_eps: 0.219163\n",
            " 123510/200000: episode: 614, duration: 1.010s, episode steps: 158, steps per second: 156, episode reward: -157.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.006 [0.000, 2.000],  loss: 0.686729, mae: 11.410348, mean_q: -16.407487, mean_eps: 0.218277\n",
            " 123610/200000: episode: 615, duration: 0.634s, episode steps: 100, steps per second: 158, episode reward: -99.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.010 [0.000, 2.000],  loss: 0.675318, mae: 11.436786, mean_q: -16.440708, mean_eps: 0.217453\n",
            " 123716/200000: episode: 616, duration: 0.701s, episode steps: 106, steps per second: 151, episode reward: -105.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.877 [0.000, 2.000],  loss: 0.676479, mae: 11.403647, mean_q: -16.401270, mean_eps: 0.216807\n",
            " 123798/200000: episode: 617, duration: 0.584s, episode steps:  82, steps per second: 140, episode reward: -81.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.963 [0.000, 2.000],  loss: 0.563746, mae: 11.392642, mean_q: -16.413511, mean_eps: 0.216212\n",
            " 123909/200000: episode: 618, duration: 0.744s, episode steps: 111, steps per second: 149, episode reward: -110.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.829 [0.000, 2.000],  loss: 0.654490, mae: 11.495793, mean_q: -16.530185, mean_eps: 0.215591\n",
            " 124018/200000: episode: 619, duration: 0.746s, episode steps: 109, steps per second: 146, episode reward: -108.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.982 [0.000, 2.000],  loss: 0.706373, mae: 11.443464, mean_q: -16.421477, mean_eps: 0.214895\n",
            " 124101/200000: episode: 620, duration: 0.566s, episode steps:  83, steps per second: 147, episode reward: -82.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.940 [0.000, 2.000],  loss: 0.640426, mae: 11.323678, mean_q: -16.278240, mean_eps: 0.214287\n",
            " 124199/200000: episode: 621, duration: 0.863s, episode steps:  98, steps per second: 114, episode reward: -97.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.806 [0.000, 2.000],  loss: 0.742244, mae: 11.411764, mean_q: -16.384241, mean_eps: 0.213717\n",
            " 124282/200000: episode: 622, duration: 0.802s, episode steps:  83, steps per second: 103, episode reward: -82.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.615535, mae: 11.350274, mean_q: -16.318468, mean_eps: 0.213147\n",
            " 124388/200000: episode: 623, duration: 0.950s, episode steps: 106, steps per second: 112, episode reward: -105.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.849 [0.000, 2.000],  loss: 0.797849, mae: 11.493045, mean_q: -16.486208, mean_eps: 0.212551\n",
            " 124495/200000: episode: 624, duration: 1.054s, episode steps: 107, steps per second: 102, episode reward: -106.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.159 [0.000, 2.000],  loss: 0.614084, mae: 11.474524, mean_q: -16.511843, mean_eps: 0.211880\n",
            " 124590/200000: episode: 625, duration: 0.603s, episode steps:  95, steps per second: 158, episode reward: -94.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.221 [0.000, 2.000],  loss: 0.665372, mae: 11.378854, mean_q: -16.352557, mean_eps: 0.211234\n",
            " 124697/200000: episode: 626, duration: 0.659s, episode steps: 107, steps per second: 162, episode reward: -106.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.196 [0.000, 2.000],  loss: 0.565736, mae: 11.407878, mean_q: -16.439171, mean_eps: 0.210588\n",
            " 124804/200000: episode: 627, duration: 0.655s, episode steps: 107, steps per second: 163, episode reward: -106.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.972 [0.000, 2.000],  loss: 0.695046, mae: 11.422747, mean_q: -16.424227, mean_eps: 0.209917\n",
            " 124956/200000: episode: 628, duration: 0.961s, episode steps: 152, steps per second: 158, episode reward: -151.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.987 [0.000, 2.000],  loss: 0.655877, mae: 11.447469, mean_q: -16.452346, mean_eps: 0.209106\n",
            " 125044/200000: episode: 629, duration: 0.573s, episode steps:  88, steps per second: 154, episode reward: -87.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.114 [0.000, 2.000],  loss: 0.650208, mae: 11.475212, mean_q: -16.504391, mean_eps: 0.208346\n",
            " 125128/200000: episode: 630, duration: 0.558s, episode steps:  84, steps per second: 151, episode reward: -83.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.262 [0.000, 2.000],  loss: 0.796187, mae: 11.388835, mean_q: -16.333445, mean_eps: 0.207801\n",
            " 125217/200000: episode: 631, duration: 0.577s, episode steps:  89, steps per second: 154, episode reward: -88.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.011 [0.000, 2.000],  loss: 0.632849, mae: 11.363890, mean_q: -16.368788, mean_eps: 0.207244\n",
            " 125309/200000: episode: 632, duration: 0.562s, episode steps:  92, steps per second: 164, episode reward: -91.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.120 [0.000, 2.000],  loss: 0.630673, mae: 11.331122, mean_q: -16.298216, mean_eps: 0.206661\n",
            " 125406/200000: episode: 633, duration: 0.623s, episode steps:  97, steps per second: 156, episode reward: -96.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.175 [0.000, 2.000],  loss: 0.787091, mae: 11.411945, mean_q: -16.406927, mean_eps: 0.206066\n",
            " 125502/200000: episode: 634, duration: 0.592s, episode steps:  96, steps per second: 162, episode reward: -95.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.969 [0.000, 2.000],  loss: 0.707520, mae: 11.384828, mean_q: -16.361223, mean_eps: 0.205458\n",
            " 125608/200000: episode: 635, duration: 0.674s, episode steps: 106, steps per second: 157, episode reward: -105.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.943 [0.000, 2.000],  loss: 0.567071, mae: 11.370680, mean_q: -16.365418, mean_eps: 0.204825\n",
            " 125703/200000: episode: 636, duration: 0.615s, episode steps:  95, steps per second: 154, episode reward: -94.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.221 [0.000, 2.000],  loss: 0.702541, mae: 11.341324, mean_q: -16.293264, mean_eps: 0.204191\n",
            " 125871/200000: episode: 637, duration: 1.045s, episode steps: 168, steps per second: 161, episode reward: -167.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.881 [0.000, 2.000],  loss: 0.634234, mae: 11.378012, mean_q: -16.378318, mean_eps: 0.203355\n",
            " 125983/200000: episode: 638, duration: 0.711s, episode steps: 112, steps per second: 157, episode reward: -111.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.776327, mae: 11.406648, mean_q: -16.367765, mean_eps: 0.202469\n",
            " 126102/200000: episode: 639, duration: 0.920s, episode steps: 119, steps per second: 129, episode reward: -118.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.050 [0.000, 2.000],  loss: 0.657243, mae: 11.306919, mean_q: -16.265797, mean_eps: 0.201734\n",
            " 126194/200000: episode: 640, duration: 0.834s, episode steps:  92, steps per second: 110, episode reward: -91.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.891 [0.000, 2.000],  loss: 0.705157, mae: 11.385234, mean_q: -16.363171, mean_eps: 0.201063\n",
            " 126301/200000: episode: 641, duration: 0.938s, episode steps: 107, steps per second: 114, episode reward: -106.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.916 [0.000, 2.000],  loss: 0.675200, mae: 11.377891, mean_q: -16.342584, mean_eps: 0.200429\n",
            " 126394/200000: episode: 642, duration: 0.914s, episode steps:  93, steps per second: 102, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.065 [0.000, 2.000],  loss: 0.594899, mae: 11.370233, mean_q: -16.357760, mean_eps: 0.199796\n",
            " 126517/200000: episode: 643, duration: 0.891s, episode steps: 123, steps per second: 138, episode reward: -122.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.821 [0.000, 2.000],  loss: 0.723733, mae: 11.416123, mean_q: -16.386810, mean_eps: 0.199112\n",
            " 126630/200000: episode: 644, duration: 0.695s, episode steps: 113, steps per second: 163, episode reward: -112.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.133 [0.000, 2.000],  loss: 0.647640, mae: 11.419847, mean_q: -16.412878, mean_eps: 0.198365\n",
            " 126708/200000: episode: 645, duration: 0.529s, episode steps:  78, steps per second: 148, episode reward: -77.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.987 [0.000, 2.000],  loss: 0.652373, mae: 11.356976, mean_q: -16.311231, mean_eps: 0.197769\n",
            " 126803/200000: episode: 646, duration: 0.607s, episode steps:  95, steps per second: 156, episode reward: -94.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.947 [0.000, 2.000],  loss: 0.718508, mae: 11.385487, mean_q: -16.336754, mean_eps: 0.197225\n",
            " 126867/200000: episode: 647, duration: 0.397s, episode steps:  64, steps per second: 161, episode reward: -63.000, mean reward: -0.984 [-1.000,  0.000], mean action: 1.078 [0.000, 2.000],  loss: 0.599535, mae: 11.290335, mean_q: -16.234865, mean_eps: 0.196718\n",
            " 126969/200000: episode: 648, duration: 0.676s, episode steps: 102, steps per second: 151, episode reward: -101.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.020 [0.000, 2.000],  loss: 0.603464, mae: 11.396276, mean_q: -16.404474, mean_eps: 0.196186\n",
            " 127061/200000: episode: 649, duration: 0.610s, episode steps:  92, steps per second: 151, episode reward: -91.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.087 [0.000, 2.000],  loss: 0.674778, mae: 11.404372, mean_q: -16.394375, mean_eps: 0.195565\n",
            " 127186/200000: episode: 650, duration: 0.791s, episode steps: 125, steps per second: 158, episode reward: -124.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.192 [0.000, 2.000],  loss: 0.615708, mae: 11.431582, mean_q: -16.464765, mean_eps: 0.194881\n",
            " 127324/200000: episode: 651, duration: 0.901s, episode steps: 138, steps per second: 153, episode reward: -137.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.319 [0.000, 2.000],  loss: 0.686683, mae: 11.456767, mean_q: -16.483216, mean_eps: 0.194058\n",
            " 127441/200000: episode: 652, duration: 0.771s, episode steps: 117, steps per second: 152, episode reward: -116.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.239 [0.000, 2.000],  loss: 0.600337, mae: 11.389777, mean_q: -16.393351, mean_eps: 0.193247\n",
            " 127557/200000: episode: 653, duration: 0.760s, episode steps: 116, steps per second: 153, episode reward: -115.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.224 [0.000, 2.000],  loss: 0.699070, mae: 11.473563, mean_q: -16.490800, mean_eps: 0.192500\n",
            " 127690/200000: episode: 654, duration: 0.870s, episode steps: 133, steps per second: 153, episode reward: -132.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.940 [0.000, 2.000],  loss: 0.712839, mae: 11.449390, mean_q: -16.438737, mean_eps: 0.191715\n",
            " 127787/200000: episode: 655, duration: 0.586s, episode steps:  97, steps per second: 166, episode reward: -96.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.062 [0.000, 2.000],  loss: 0.693731, mae: 11.432277, mean_q: -16.420343, mean_eps: 0.190993\n",
            " 127913/200000: episode: 656, duration: 0.788s, episode steps: 126, steps per second: 160, episode reward: -125.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.278 [0.000, 2.000],  loss: 0.710706, mae: 11.370331, mean_q: -16.328770, mean_eps: 0.190283\n",
            " 128041/200000: episode: 657, duration: 1.011s, episode steps: 128, steps per second: 127, episode reward: -127.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.141 [0.000, 2.000],  loss: 0.709892, mae: 11.572805, mean_q: -16.625081, mean_eps: 0.189473\n",
            " 128153/200000: episode: 658, duration: 1.013s, episode steps: 112, steps per second: 111, episode reward: -111.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.866 [0.000, 2.000],  loss: 0.746901, mae: 11.556094, mean_q: -16.600912, mean_eps: 0.188713\n",
            " 128266/200000: episode: 659, duration: 0.962s, episode steps: 113, steps per second: 117, episode reward: -112.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.062 [0.000, 2.000],  loss: 0.789482, mae: 11.625630, mean_q: -16.681186, mean_eps: 0.188003\n",
            " 128372/200000: episode: 660, duration: 0.975s, episode steps: 106, steps per second: 109, episode reward: -105.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.132 [0.000, 2.000],  loss: 0.641280, mae: 11.555757, mean_q: -16.610276, mean_eps: 0.187319\n",
            " 128463/200000: episode: 661, duration: 0.597s, episode steps:  91, steps per second: 152, episode reward: -90.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.143 [0.000, 2.000],  loss: 0.714079, mae: 11.613167, mean_q: -16.690772, mean_eps: 0.186699\n",
            " 128586/200000: episode: 662, duration: 0.786s, episode steps: 123, steps per second: 156, episode reward: -122.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.902 [0.000, 2.000],  loss: 0.699116, mae: 11.577478, mean_q: -16.642553, mean_eps: 0.186015\n",
            " 128685/200000: episode: 663, duration: 0.635s, episode steps:  99, steps per second: 156, episode reward: -98.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.182 [0.000, 2.000],  loss: 0.553551, mae: 11.559867, mean_q: -16.679572, mean_eps: 0.185305\n",
            " 128793/200000: episode: 664, duration: 0.712s, episode steps: 108, steps per second: 152, episode reward: -107.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.167 [0.000, 2.000],  loss: 0.614721, mae: 11.482228, mean_q: -16.535573, mean_eps: 0.184647\n",
            " 129293/200000: episode: 665, duration: 3.215s, episode steps: 500, steps per second: 156, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.246 [0.000, 2.000],  loss: 0.717428, mae: 11.621832, mean_q: -16.710234, mean_eps: 0.182721\n",
            " 129392/200000: episode: 666, duration: 0.607s, episode steps:  99, steps per second: 163, episode reward: -98.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.768 [0.000, 2.000],  loss: 0.588997, mae: 11.578633, mean_q: -16.689291, mean_eps: 0.180834\n",
            " 129513/200000: episode: 667, duration: 0.824s, episode steps: 121, steps per second: 147, episode reward: -120.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.240 [0.000, 2.000],  loss: 0.815917, mae: 11.608270, mean_q: -16.689096, mean_eps: 0.180137\n",
            " 129634/200000: episode: 668, duration: 0.815s, episode steps: 121, steps per second: 148, episode reward: -120.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.041 [0.000, 2.000],  loss: 0.703049, mae: 11.531957, mean_q: -16.589503, mean_eps: 0.179365\n",
            " 129732/200000: episode: 669, duration: 0.633s, episode steps:  98, steps per second: 155, episode reward: -97.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.163 [0.000, 2.000],  loss: 0.643797, mae: 11.673627, mean_q: -16.803775, mean_eps: 0.178681\n",
            " 129832/200000: episode: 670, duration: 0.686s, episode steps: 100, steps per second: 146, episode reward: -99.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.020 [0.000, 2.000],  loss: 0.647174, mae: 11.604870, mean_q: -16.690576, mean_eps: 0.178060\n",
            " 129937/200000: episode: 671, duration: 0.834s, episode steps: 105, steps per second: 126, episode reward: -104.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.124 [0.000, 2.000],  loss: 0.682403, mae: 11.538765, mean_q: -16.615952, mean_eps: 0.177401\n",
            " 130015/200000: episode: 672, duration: 0.727s, episode steps:  78, steps per second: 107, episode reward: -77.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.077 [0.000, 2.000],  loss: 0.656474, mae: 11.635153, mean_q: -16.759932, mean_eps: 0.176819\n",
            " 130117/200000: episode: 673, duration: 0.902s, episode steps: 102, steps per second: 113, episode reward: -101.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.118 [0.000, 2.000],  loss: 0.694658, mae: 11.768891, mean_q: -16.937243, mean_eps: 0.176249\n",
            " 130217/200000: episode: 674, duration: 0.940s, episode steps: 100, steps per second: 106, episode reward: -99.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.880 [0.000, 2.000],  loss: 0.617759, mae: 11.681196, mean_q: -16.817634, mean_eps: 0.175603\n",
            " 130301/200000: episode: 675, duration: 0.661s, episode steps:  84, steps per second: 127, episode reward: -83.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.988 [0.000, 2.000],  loss: 0.600169, mae: 11.623369, mean_q: -16.727791, mean_eps: 0.175020\n",
            " 130396/200000: episode: 676, duration: 0.629s, episode steps:  95, steps per second: 151, episode reward: -94.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.800 [0.000, 2.000],  loss: 0.625713, mae: 11.638948, mean_q: -16.748846, mean_eps: 0.174463\n",
            " 130510/200000: episode: 677, duration: 0.764s, episode steps: 114, steps per second: 149, episode reward: -113.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.061 [0.000, 2.000],  loss: 0.709423, mae: 11.634173, mean_q: -16.731822, mean_eps: 0.173804\n",
            " 130612/200000: episode: 678, duration: 0.741s, episode steps: 102, steps per second: 138, episode reward: -101.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.980 [0.000, 2.000],  loss: 0.714330, mae: 11.761227, mean_q: -16.919293, mean_eps: 0.173120\n",
            " 130725/200000: episode: 679, duration: 0.822s, episode steps: 113, steps per second: 138, episode reward: -112.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.018 [0.000, 2.000],  loss: 0.742190, mae: 11.768560, mean_q: -16.926831, mean_eps: 0.172436\n",
            " 130823/200000: episode: 680, duration: 0.586s, episode steps:  98, steps per second: 167, episode reward: -97.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.122 [0.000, 2.000],  loss: 0.818679, mae: 11.614317, mean_q: -16.660695, mean_eps: 0.171765\n",
            " 130922/200000: episode: 681, duration: 0.643s, episode steps:  99, steps per second: 154, episode reward: -98.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.010 [0.000, 2.000],  loss: 0.771331, mae: 11.752265, mean_q: -16.877571, mean_eps: 0.171144\n",
            " 131008/200000: episode: 682, duration: 0.599s, episode steps:  86, steps per second: 144, episode reward: -85.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.105 [0.000, 2.000],  loss: 0.621479, mae: 11.699342, mean_q: -16.825138, mean_eps: 0.170561\n",
            " 131125/200000: episode: 683, duration: 0.776s, episode steps: 117, steps per second: 151, episode reward: -116.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.068 [0.000, 2.000],  loss: 0.670508, mae: 11.685135, mean_q: -16.796084, mean_eps: 0.169915\n",
            " 131267/200000: episode: 684, duration: 0.913s, episode steps: 142, steps per second: 156, episode reward: -141.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.746 [0.000, 2.000],  loss: 0.794019, mae: 11.719105, mean_q: -16.805640, mean_eps: 0.169092\n",
            " 131378/200000: episode: 685, duration: 0.719s, episode steps: 111, steps per second: 154, episode reward: -110.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.721 [0.000, 2.000],  loss: 0.740302, mae: 11.769893, mean_q: -16.902191, mean_eps: 0.168294\n",
            " 131491/200000: episode: 686, duration: 0.722s, episode steps: 113, steps per second: 157, episode reward: -112.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.947 [0.000, 2.000],  loss: 0.795919, mae: 11.700092, mean_q: -16.777274, mean_eps: 0.167585\n",
            " 131606/200000: episode: 687, duration: 0.811s, episode steps: 115, steps per second: 142, episode reward: -114.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.948 [0.000, 2.000],  loss: 0.659865, mae: 11.801168, mean_q: -16.962329, mean_eps: 0.166863\n",
            " 131697/200000: episode: 688, duration: 0.640s, episode steps:  91, steps per second: 142, episode reward: -90.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.934 [0.000, 2.000],  loss: 0.863905, mae: 11.748395, mean_q: -16.831024, mean_eps: 0.166204\n",
            " 131793/200000: episode: 689, duration: 0.817s, episode steps:  96, steps per second: 118, episode reward: -95.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.948 [0.000, 2.000],  loss: 0.742100, mae: 11.714093, mean_q: -16.803175, mean_eps: 0.165609\n",
            " 131882/200000: episode: 690, duration: 0.869s, episode steps:  89, steps per second: 102, episode reward: -88.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.921 [0.000, 2.000],  loss: 0.749897, mae: 11.792366, mean_q: -16.916865, mean_eps: 0.165026\n",
            " 131979/200000: episode: 691, duration: 0.873s, episode steps:  97, steps per second: 111, episode reward: -96.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.979 [0.000, 2.000],  loss: 0.673070, mae: 11.731005, mean_q: -16.859436, mean_eps: 0.164443\n",
            " 132087/200000: episode: 692, duration: 1.025s, episode steps: 108, steps per second: 105, episode reward: -107.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.130 [0.000, 2.000],  loss: 0.698167, mae: 11.875652, mean_q: -17.050000, mean_eps: 0.163797\n",
            " 132184/200000: episode: 693, duration: 0.731s, episode steps:  97, steps per second: 133, episode reward: -96.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.918 [0.000, 2.000],  loss: 0.698482, mae: 11.853781, mean_q: -17.024473, mean_eps: 0.163151\n",
            " 132277/200000: episode: 694, duration: 0.658s, episode steps:  93, steps per second: 141, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.718851, mae: 11.785186, mean_q: -16.930699, mean_eps: 0.162543\n",
            " 132364/200000: episode: 695, duration: 0.662s, episode steps:  87, steps per second: 131, episode reward: -86.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.885 [0.000, 2.000],  loss: 0.725119, mae: 11.820027, mean_q: -16.983121, mean_eps: 0.161973\n",
            " 132864/200000: episode: 696, duration: 3.322s, episode steps: 500, steps per second: 151, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.330 [0.000, 2.000],  loss: 0.709696, mae: 11.906542, mean_q: -17.117186, mean_eps: 0.160124\n",
            " 132965/200000: episode: 697, duration: 0.666s, episode steps: 101, steps per second: 152, episode reward: -100.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.950 [0.000, 2.000],  loss: 0.697668, mae: 11.837599, mean_q: -17.007310, mean_eps: 0.158211\n",
            " 133064/200000: episode: 698, duration: 0.640s, episode steps:  99, steps per second: 155, episode reward: -98.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.899 [0.000, 2.000],  loss: 0.769035, mae: 11.731648, mean_q: -16.830792, mean_eps: 0.157578\n",
            " 133185/200000: episode: 699, duration: 0.785s, episode steps: 121, steps per second: 154, episode reward: -120.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.818 [0.000, 2.000],  loss: 0.651935, mae: 11.687602, mean_q: -16.791917, mean_eps: 0.156881\n",
            " 133282/200000: episode: 700, duration: 0.633s, episode steps:  97, steps per second: 153, episode reward: -96.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.928 [0.000, 2.000],  loss: 0.605573, mae: 11.745095, mean_q: -16.896182, mean_eps: 0.156185\n",
            " 133388/200000: episode: 701, duration: 0.699s, episode steps: 106, steps per second: 152, episode reward: -105.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.858 [0.000, 2.000],  loss: 0.750228, mae: 11.681715, mean_q: -16.751660, mean_eps: 0.155551\n",
            " 133497/200000: episode: 702, duration: 0.759s, episode steps: 109, steps per second: 144, episode reward: -108.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.761 [0.000, 2.000],  loss: 0.749218, mae: 11.659995, mean_q: -16.720151, mean_eps: 0.154867\n",
            " 133580/200000: episode: 703, duration: 0.561s, episode steps:  83, steps per second: 148, episode reward: -82.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.783 [0.000, 2.000],  loss: 0.726814, mae: 11.746974, mean_q: -16.890443, mean_eps: 0.154259\n",
            " 133663/200000: episode: 704, duration: 0.821s, episode steps:  83, steps per second: 101, episode reward: -82.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.645541, mae: 11.718892, mean_q: -16.850645, mean_eps: 0.153740\n",
            " 133777/200000: episode: 705, duration: 1.149s, episode steps: 114, steps per second:  99, episode reward: -113.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.772 [0.000, 2.000],  loss: 0.627980, mae: 11.591859, mean_q: -16.685329, mean_eps: 0.153107\n",
            " 133884/200000: episode: 706, duration: 1.005s, episode steps: 107, steps per second: 107, episode reward: -106.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.879 [0.000, 2.000],  loss: 0.712838, mae: 11.663118, mean_q: -16.755207, mean_eps: 0.152410\n",
            " 133994/200000: episode: 707, duration: 0.941s, episode steps: 110, steps per second: 117, episode reward: -109.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.800 [0.000, 2.000],  loss: 0.777597, mae: 11.685352, mean_q: -16.761411, mean_eps: 0.151726\n",
            " 134121/200000: episode: 708, duration: 0.819s, episode steps: 127, steps per second: 155, episode reward: -126.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.795 [0.000, 2.000],  loss: 0.679348, mae: 11.558067, mean_q: -16.578278, mean_eps: 0.150966\n",
            " 134219/200000: episode: 709, duration: 0.629s, episode steps:  98, steps per second: 156, episode reward: -97.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.102 [0.000, 2.000],  loss: 0.735490, mae: 11.605229, mean_q: -16.638446, mean_eps: 0.150257\n",
            " 134306/200000: episode: 710, duration: 0.575s, episode steps:  87, steps per second: 151, episode reward: -86.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.897 [0.000, 2.000],  loss: 0.636984, mae: 11.626673, mean_q: -16.707953, mean_eps: 0.149674\n",
            " 134377/200000: episode: 711, duration: 0.479s, episode steps:  71, steps per second: 148, episode reward: -70.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.775 [0.000, 2.000],  loss: 0.623315, mae: 11.630131, mean_q: -16.720832, mean_eps: 0.149167\n",
            " 134467/200000: episode: 712, duration: 0.585s, episode steps:  90, steps per second: 154, episode reward: -89.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.889 [0.000, 2.000],  loss: 0.728353, mae: 11.532423, mean_q: -16.537771, mean_eps: 0.148661\n",
            " 134559/200000: episode: 713, duration: 0.612s, episode steps:  92, steps per second: 150, episode reward: -91.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.793 [0.000, 2.000],  loss: 0.748306, mae: 11.521983, mean_q: -16.498135, mean_eps: 0.148091\n",
            " 134641/200000: episode: 714, duration: 0.556s, episode steps:  82, steps per second: 148, episode reward: -81.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.841 [0.000, 2.000],  loss: 0.705818, mae: 11.539876, mean_q: -16.564098, mean_eps: 0.147533\n",
            " 134728/200000: episode: 715, duration: 0.593s, episode steps:  87, steps per second: 147, episode reward: -86.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.954 [0.000, 2.000],  loss: 0.757852, mae: 11.569473, mean_q: -16.573257, mean_eps: 0.147001\n",
            " 134847/200000: episode: 716, duration: 0.840s, episode steps: 119, steps per second: 142, episode reward: -118.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.941 [0.000, 2.000],  loss: 0.682725, mae: 11.556494, mean_q: -16.603197, mean_eps: 0.146355\n",
            " 134966/200000: episode: 717, duration: 0.860s, episode steps: 119, steps per second: 138, episode reward: -118.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.731 [0.000, 2.000],  loss: 0.660710, mae: 11.648622, mean_q: -16.749213, mean_eps: 0.145595\n",
            " 135058/200000: episode: 718, duration: 0.602s, episode steps:  92, steps per second: 153, episode reward: -91.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.674 [0.000, 2.000],  loss: 0.881051, mae: 11.557423, mean_q: -16.550201, mean_eps: 0.144924\n",
            " 135173/200000: episode: 719, duration: 0.886s, episode steps: 115, steps per second: 130, episode reward: -114.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.843 [0.000, 2.000],  loss: 0.783083, mae: 11.502598, mean_q: -16.514541, mean_eps: 0.144265\n",
            " 135250/200000: episode: 720, duration: 0.532s, episode steps:  77, steps per second: 145, episode reward: -76.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.182 [0.000, 2.000],  loss: 0.682566, mae: 11.499961, mean_q: -16.529321, mean_eps: 0.143657\n",
            " 135346/200000: episode: 721, duration: 0.631s, episode steps:  96, steps per second: 152, episode reward: -95.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.969 [0.000, 2.000],  loss: 0.789444, mae: 11.472187, mean_q: -16.454927, mean_eps: 0.143113\n",
            " 135421/200000: episode: 722, duration: 0.613s, episode steps:  75, steps per second: 122, episode reward: -74.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.827 [0.000, 2.000],  loss: 0.724633, mae: 11.533553, mean_q: -16.564118, mean_eps: 0.142568\n",
            " 135531/200000: episode: 723, duration: 1.032s, episode steps: 110, steps per second: 107, episode reward: -109.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.018 [0.000, 2.000],  loss: 0.650209, mae: 11.584460, mean_q: -16.649269, mean_eps: 0.141985\n",
            " 135608/200000: episode: 724, duration: 0.778s, episode steps:  77, steps per second:  99, episode reward: -76.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.935 [0.000, 2.000],  loss: 0.779722, mae: 11.590346, mean_q: -16.588441, mean_eps: 0.141403\n",
            " 135723/200000: episode: 725, duration: 1.131s, episode steps: 115, steps per second: 102, episode reward: -114.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.130 [0.000, 2.000],  loss: 0.689014, mae: 11.540211, mean_q: -16.572551, mean_eps: 0.140795\n",
            " 135824/200000: episode: 726, duration: 0.779s, episode steps: 101, steps per second: 130, episode reward: -100.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.020 [0.000, 2.000],  loss: 0.713711, mae: 11.515730, mean_q: -16.556348, mean_eps: 0.140111\n",
            " 135919/200000: episode: 727, duration: 0.617s, episode steps:  95, steps per second: 154, episode reward: -94.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.168 [0.000, 2.000],  loss: 0.663995, mae: 11.545965, mean_q: -16.620788, mean_eps: 0.139490\n",
            " 136021/200000: episode: 728, duration: 0.694s, episode steps: 102, steps per second: 147, episode reward: -101.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.137 [0.000, 2.000],  loss: 0.872934, mae: 11.631199, mean_q: -16.685831, mean_eps: 0.138857\n",
            " 136145/200000: episode: 729, duration: 0.860s, episode steps: 124, steps per second: 144, episode reward: -123.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.298 [0.000, 2.000],  loss: 0.728339, mae: 11.676590, mean_q: -16.781637, mean_eps: 0.138135\n",
            " 136246/200000: episode: 730, duration: 0.703s, episode steps: 101, steps per second: 144, episode reward: -100.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.178 [0.000, 2.000],  loss: 0.623338, mae: 11.575535, mean_q: -16.671410, mean_eps: 0.137425\n",
            " 136355/200000: episode: 731, duration: 0.745s, episode steps: 109, steps per second: 146, episode reward: -108.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.927 [0.000, 2.000],  loss: 0.735222, mae: 11.578504, mean_q: -16.633312, mean_eps: 0.136767\n",
            " 136461/200000: episode: 732, duration: 0.736s, episode steps: 106, steps per second: 144, episode reward: -105.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.981 [0.000, 2.000],  loss: 0.717253, mae: 11.560543, mean_q: -16.579313, mean_eps: 0.136083\n",
            " 136584/200000: episode: 733, duration: 0.756s, episode steps: 123, steps per second: 163, episode reward: -122.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.317 [0.000, 2.000],  loss: 0.736901, mae: 11.600538, mean_q: -16.654732, mean_eps: 0.135361\n",
            " 136683/200000: episode: 734, duration: 0.640s, episode steps:  99, steps per second: 155, episode reward: -98.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.212 [0.000, 2.000],  loss: 0.770590, mae: 11.593815, mean_q: -16.651201, mean_eps: 0.134664\n",
            " 136783/200000: episode: 735, duration: 0.710s, episode steps: 100, steps per second: 141, episode reward: -99.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.680 [0.000, 2.000],  loss: 0.806965, mae: 11.521364, mean_q: -16.555616, mean_eps: 0.134031\n",
            " 136892/200000: episode: 736, duration: 0.758s, episode steps: 109, steps per second: 144, episode reward: -108.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.092 [0.000, 2.000],  loss: 0.712923, mae: 11.564120, mean_q: -16.634465, mean_eps: 0.133372\n",
            " 136979/200000: episode: 737, duration: 0.593s, episode steps:  87, steps per second: 147, episode reward: -86.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.885 [0.000, 2.000],  loss: 0.657438, mae: 11.599160, mean_q: -16.694292, mean_eps: 0.132751\n",
            " 137081/200000: episode: 738, duration: 0.697s, episode steps: 102, steps per second: 146, episode reward: -101.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.931 [0.000, 2.000],  loss: 0.834857, mae: 11.613155, mean_q: -16.654657, mean_eps: 0.132143\n",
            " 137165/200000: episode: 739, duration: 0.562s, episode steps:  84, steps per second: 149, episode reward: -83.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.131 [0.000, 2.000],  loss: 0.628148, mae: 11.541487, mean_q: -16.616740, mean_eps: 0.131548\n",
            " 137278/200000: episode: 740, duration: 0.961s, episode steps: 113, steps per second: 118, episode reward: -112.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.354 [0.000, 2.000],  loss: 0.738694, mae: 11.660698, mean_q: -16.753616, mean_eps: 0.130927\n",
            " 137446/200000: episode: 741, duration: 1.655s, episode steps: 168, steps per second: 102, episode reward: -167.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.060 [0.000, 2.000],  loss: 0.637242, mae: 11.630008, mean_q: -16.726903, mean_eps: 0.130041\n",
            " 137569/200000: episode: 742, duration: 1.240s, episode steps: 123, steps per second:  99, episode reward: -122.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.976 [0.000, 2.000],  loss: 0.762283, mae: 11.723067, mean_q: -16.840036, mean_eps: 0.129116\n",
            " 137670/200000: episode: 743, duration: 0.672s, episode steps: 101, steps per second: 150, episode reward: -100.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.790443, mae: 11.689106, mean_q: -16.769648, mean_eps: 0.128407\n",
            " 137763/200000: episode: 744, duration: 0.607s, episode steps:  93, steps per second: 153, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.140 [0.000, 2.000],  loss: 0.793751, mae: 11.669080, mean_q: -16.735691, mean_eps: 0.127799\n",
            " 137899/200000: episode: 745, duration: 0.864s, episode steps: 136, steps per second: 157, episode reward: -135.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.360 [0.000, 2.000],  loss: 0.716906, mae: 11.635343, mean_q: -16.727778, mean_eps: 0.127077\n",
            " 137997/200000: episode: 746, duration: 0.623s, episode steps:  98, steps per second: 157, episode reward: -97.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.959 [0.000, 2.000],  loss: 0.694245, mae: 11.537064, mean_q: -16.576773, mean_eps: 0.126329\n",
            " 138109/200000: episode: 747, duration: 0.693s, episode steps: 112, steps per second: 162, episode reward: -111.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.223 [0.000, 2.000],  loss: 0.854240, mae: 11.757413, mean_q: -16.865528, mean_eps: 0.125658\n",
            " 138217/200000: episode: 748, duration: 0.685s, episode steps: 108, steps per second: 158, episode reward: -107.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.148 [0.000, 2.000],  loss: 0.999312, mae: 11.651514, mean_q: -16.667963, mean_eps: 0.124961\n",
            " 138303/200000: episode: 749, duration: 0.526s, episode steps:  86, steps per second: 163, episode reward: -85.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.081 [0.000, 2.000],  loss: 0.715829, mae: 11.551847, mean_q: -16.563052, mean_eps: 0.124353\n",
            " 138406/200000: episode: 750, duration: 0.656s, episode steps: 103, steps per second: 157, episode reward: -102.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.291 [0.000, 2.000],  loss: 0.646505, mae: 11.669624, mean_q: -16.784231, mean_eps: 0.123758\n",
            " 138521/200000: episode: 751, duration: 0.755s, episode steps: 115, steps per second: 152, episode reward: -114.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.913 [0.000, 2.000],  loss: 0.738085, mae: 11.678405, mean_q: -16.769198, mean_eps: 0.123061\n",
            " 138619/200000: episode: 752, duration: 0.614s, episode steps:  98, steps per second: 160, episode reward: -97.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.143 [0.000, 2.000],  loss: 0.742268, mae: 11.703267, mean_q: -16.812879, mean_eps: 0.122390\n",
            " 138729/200000: episode: 753, duration: 0.769s, episode steps: 110, steps per second: 143, episode reward: -109.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.200 [0.000, 2.000],  loss: 0.680653, mae: 11.633222, mean_q: -16.739661, mean_eps: 0.121731\n",
            " 138848/200000: episode: 754, duration: 0.786s, episode steps: 119, steps per second: 151, episode reward: -118.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.328 [0.000, 2.000],  loss: 0.638533, mae: 11.597949, mean_q: -16.678487, mean_eps: 0.121009\n",
            " 138938/200000: episode: 755, duration: 0.589s, episode steps:  90, steps per second: 153, episode reward: -89.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.289 [0.000, 2.000],  loss: 0.802545, mae: 11.699695, mean_q: -16.778641, mean_eps: 0.120351\n",
            " 139052/200000: episode: 756, duration: 0.778s, episode steps: 114, steps per second: 147, episode reward: -113.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.088 [0.000, 2.000],  loss: 0.703674, mae: 11.682363, mean_q: -16.792685, mean_eps: 0.119705\n",
            " 139138/200000: episode: 757, duration: 0.613s, episode steps:  86, steps per second: 140, episode reward: -85.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.116 [0.000, 2.000],  loss: 0.680731, mae: 11.747656, mean_q: -16.895421, mean_eps: 0.119071\n",
            " 139243/200000: episode: 758, duration: 1.000s, episode steps: 105, steps per second: 105, episode reward: -104.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.962 [0.000, 2.000],  loss: 0.746972, mae: 11.748281, mean_q: -16.864562, mean_eps: 0.118463\n",
            " 139336/200000: episode: 759, duration: 0.912s, episode steps:  93, steps per second: 102, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.258 [0.000, 2.000],  loss: 0.725826, mae: 11.657440, mean_q: -16.720354, mean_eps: 0.117843\n",
            " 139439/200000: episode: 760, duration: 1.010s, episode steps: 103, steps per second: 102, episode reward: -102.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.019 [0.000, 2.000],  loss: 0.622448, mae: 11.727673, mean_q: -16.884562, mean_eps: 0.117222\n",
            " 139577/200000: episode: 761, duration: 1.083s, episode steps: 138, steps per second: 127, episode reward: -137.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.899 [0.000, 2.000],  loss: 0.631733, mae: 11.782311, mean_q: -16.959365, mean_eps: 0.116449\n",
            " 139695/200000: episode: 762, duration: 0.784s, episode steps: 118, steps per second: 151, episode reward: -117.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.847 [0.000, 2.000],  loss: 0.668344, mae: 11.744110, mean_q: -16.914206, mean_eps: 0.115639\n",
            " 139779/200000: episode: 763, duration: 0.594s, episode steps:  84, steps per second: 142, episode reward: -83.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.048 [0.000, 2.000],  loss: 0.724636, mae: 11.737353, mean_q: -16.857904, mean_eps: 0.115005\n",
            " 139858/200000: episode: 764, duration: 0.505s, episode steps:  79, steps per second: 157, episode reward: -78.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.949 [0.000, 2.000],  loss: 0.671427, mae: 11.758677, mean_q: -16.913677, mean_eps: 0.114486\n",
            " 139963/200000: episode: 765, duration: 0.690s, episode steps: 105, steps per second: 152, episode reward: -104.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.076 [0.000, 2.000],  loss: 0.697160, mae: 11.832918, mean_q: -17.010690, mean_eps: 0.113903\n",
            " 140050/200000: episode: 766, duration: 0.603s, episode steps:  87, steps per second: 144, episode reward: -86.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.874 [0.000, 2.000],  loss: 0.763699, mae: 11.830178, mean_q: -17.005465, mean_eps: 0.113295\n",
            " 140138/200000: episode: 767, duration: 0.596s, episode steps:  88, steps per second: 148, episode reward: -87.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.114 [0.000, 2.000],  loss: 0.810616, mae: 11.806205, mean_q: -16.958606, mean_eps: 0.112738\n",
            " 140224/200000: episode: 768, duration: 0.573s, episode steps:  86, steps per second: 150, episode reward: -85.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.012 [0.000, 2.000],  loss: 0.692374, mae: 11.709961, mean_q: -16.839417, mean_eps: 0.112193\n",
            " 140317/200000: episode: 769, duration: 0.647s, episode steps:  93, steps per second: 144, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.108 [0.000, 2.000],  loss: 0.641802, mae: 11.788608, mean_q: -16.967374, mean_eps: 0.111623\n",
            " 140408/200000: episode: 770, duration: 0.610s, episode steps:  91, steps per second: 149, episode reward: -90.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.176 [0.000, 2.000],  loss: 0.750273, mae: 11.722786, mean_q: -16.817402, mean_eps: 0.111041\n",
            " 140483/200000: episode: 771, duration: 0.492s, episode steps:  75, steps per second: 152, episode reward: -74.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.040 [0.000, 2.000],  loss: 0.692868, mae: 11.672148, mean_q: -16.777055, mean_eps: 0.110521\n",
            " 140576/200000: episode: 772, duration: 0.606s, episode steps:  93, steps per second: 153, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.237 [0.000, 2.000],  loss: 0.596482, mae: 11.762041, mean_q: -16.931625, mean_eps: 0.109989\n",
            " 140677/200000: episode: 773, duration: 0.705s, episode steps: 101, steps per second: 143, episode reward: -100.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.950 [0.000, 2.000],  loss: 0.745469, mae: 11.952822, mean_q: -17.159270, mean_eps: 0.109369\n",
            " 140768/200000: episode: 774, duration: 0.592s, episode steps:  91, steps per second: 154, episode reward: -90.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.286 [0.000, 2.000],  loss: 0.757555, mae: 11.929843, mean_q: -17.114446, mean_eps: 0.108761\n",
            " 140867/200000: episode: 775, duration: 0.674s, episode steps:  99, steps per second: 147, episode reward: -98.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.121 [0.000, 2.000],  loss: 0.875032, mae: 11.865665, mean_q: -17.005482, mean_eps: 0.108165\n",
            " 140977/200000: episode: 776, duration: 0.823s, episode steps: 110, steps per second: 134, episode reward: -109.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.836 [0.000, 2.000],  loss: 0.860495, mae: 11.876220, mean_q: -17.037812, mean_eps: 0.107494\n",
            " 141048/200000: episode: 777, duration: 0.646s, episode steps:  71, steps per second: 110, episode reward: -70.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.042 [0.000, 2.000],  loss: 0.771512, mae: 11.916687, mean_q: -17.104200, mean_eps: 0.106924\n",
            " 141141/200000: episode: 778, duration: 0.888s, episode steps:  93, steps per second: 105, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.925 [0.000, 2.000],  loss: 0.725213, mae: 11.980105, mean_q: -17.234735, mean_eps: 0.106405\n",
            " 141259/200000: episode: 779, duration: 1.073s, episode steps: 118, steps per second: 110, episode reward: -117.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.864 [0.000, 2.000],  loss: 0.713692, mae: 11.894509, mean_q: -17.105463, mean_eps: 0.105733\n",
            " 141361/200000: episode: 780, duration: 0.916s, episode steps: 102, steps per second: 111, episode reward: -101.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.029 [0.000, 2.000],  loss: 0.758038, mae: 11.888785, mean_q: -17.083579, mean_eps: 0.105037\n",
            " 141436/200000: episode: 781, duration: 0.466s, episode steps:  75, steps per second: 161, episode reward: -74.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.080 [0.000, 2.000],  loss: 0.649158, mae: 11.818261, mean_q: -16.978375, mean_eps: 0.104479\n",
            " 141515/200000: episode: 782, duration: 0.498s, episode steps:  79, steps per second: 159, episode reward: -78.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.873 [0.000, 2.000],  loss: 0.613698, mae: 11.910516, mean_q: -17.124428, mean_eps: 0.103998\n",
            " 141599/200000: episode: 783, duration: 0.542s, episode steps:  84, steps per second: 155, episode reward: -83.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.964 [0.000, 2.000],  loss: 0.689412, mae: 11.982732, mean_q: -17.204309, mean_eps: 0.103479\n",
            " 141675/200000: episode: 784, duration: 0.477s, episode steps:  76, steps per second: 159, episode reward: -75.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.092 [0.000, 2.000],  loss: 0.763942, mae: 11.980919, mean_q: -17.175379, mean_eps: 0.102972\n",
            " 141746/200000: episode: 785, duration: 0.457s, episode steps:  71, steps per second: 155, episode reward: -70.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.662 [0.000, 2.000],  loss: 0.667207, mae: 11.807548, mean_q: -16.940636, mean_eps: 0.102503\n",
            " 141867/200000: episode: 786, duration: 0.769s, episode steps: 121, steps per second: 157, episode reward: -120.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.917 [0.000, 2.000],  loss: 0.671781, mae: 11.841329, mean_q: -17.010344, mean_eps: 0.101895\n",
            " 141959/200000: episode: 787, duration: 0.548s, episode steps:  92, steps per second: 168, episode reward: -91.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.848 [0.000, 2.000],  loss: 0.752282, mae: 11.745393, mean_q: -16.855501, mean_eps: 0.101224\n",
            " 142061/200000: episode: 788, duration: 0.664s, episode steps: 102, steps per second: 154, episode reward: -101.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.137 [0.000, 2.000],  loss: 0.780264, mae: 11.892888, mean_q: -17.084893, mean_eps: 0.100603\n",
            " 142154/200000: episode: 789, duration: 0.571s, episode steps:  93, steps per second: 163, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.946 [0.000, 2.000],  loss: 0.868071, mae: 11.976434, mean_q: -17.157307, mean_eps: 0.099983\n",
            " 142255/200000: episode: 790, duration: 0.636s, episode steps: 101, steps per second: 159, episode reward: -100.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.663 [0.000, 2.000],  loss: 0.766231, mae: 11.944488, mean_q: -17.134682, mean_eps: 0.099375\n",
            " 142755/200000: episode: 791, duration: 3.108s, episode steps: 500, steps per second: 161, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.376 [0.000, 2.000],  loss: 0.743865, mae: 11.871531, mean_q: -17.042360, mean_eps: 0.097475\n",
            " 142856/200000: episode: 792, duration: 0.666s, episode steps: 101, steps per second: 152, episode reward: -100.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.723 [0.000, 2.000],  loss: 0.703156, mae: 11.873556, mean_q: -17.062829, mean_eps: 0.095575\n",
            " 142944/200000: episode: 793, duration: 0.712s, episode steps:  88, steps per second: 124, episode reward: -87.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.966 [0.000, 2.000],  loss: 0.743272, mae: 11.826577, mean_q: -16.950203, mean_eps: 0.094979\n",
            " 143034/200000: episode: 794, duration: 0.793s, episode steps:  90, steps per second: 114, episode reward: -89.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.867 [0.000, 2.000],  loss: 0.806848, mae: 11.787367, mean_q: -16.904312, mean_eps: 0.094409\n",
            " 143147/200000: episode: 795, duration: 0.951s, episode steps: 113, steps per second: 119, episode reward: -112.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.841 [0.000, 2.000],  loss: 0.836183, mae: 11.894275, mean_q: -17.029093, mean_eps: 0.093763\n",
            " 143261/200000: episode: 796, duration: 0.991s, episode steps: 114, steps per second: 115, episode reward: -113.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.123 [0.000, 2.000],  loss: 0.656499, mae: 11.804837, mean_q: -16.958843, mean_eps: 0.093041\n",
            " 143363/200000: episode: 797, duration: 0.726s, episode steps: 102, steps per second: 140, episode reward: -101.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.971 [0.000, 2.000],  loss: 0.681300, mae: 11.843289, mean_q: -17.014797, mean_eps: 0.092357\n",
            " 143446/200000: episode: 798, duration: 0.529s, episode steps:  83, steps per second: 157, episode reward: -82.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.964 [0.000, 2.000],  loss: 0.788019, mae: 11.867004, mean_q: -16.989449, mean_eps: 0.091775\n",
            " 143533/200000: episode: 799, duration: 0.552s, episode steps:  87, steps per second: 157, episode reward: -86.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.805 [0.000, 2.000],  loss: 0.751026, mae: 11.966858, mean_q: -17.167658, mean_eps: 0.091230\n",
            " 143625/200000: episode: 800, duration: 0.570s, episode steps:  92, steps per second: 162, episode reward: -91.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.217 [0.000, 2.000],  loss: 0.739780, mae: 11.936518, mean_q: -17.162111, mean_eps: 0.090660\n",
            " 143737/200000: episode: 801, duration: 0.682s, episode steps: 112, steps per second: 164, episode reward: -111.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.795 [0.000, 2.000],  loss: 0.729959, mae: 11.828166, mean_q: -16.986483, mean_eps: 0.090014\n",
            " 143830/200000: episode: 802, duration: 0.552s, episode steps:  93, steps per second: 169, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.828 [0.000, 2.000],  loss: 0.728492, mae: 11.903504, mean_q: -17.109661, mean_eps: 0.089368\n",
            " 143932/200000: episode: 803, duration: 0.638s, episode steps: 102, steps per second: 160, episode reward: -101.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.088 [0.000, 2.000],  loss: 0.739065, mae: 11.877261, mean_q: -17.059037, mean_eps: 0.088760\n",
            " 144028/200000: episode: 804, duration: 0.647s, episode steps:  96, steps per second: 148, episode reward: -95.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.833 [0.000, 2.000],  loss: 0.728298, mae: 11.801989, mean_q: -16.944218, mean_eps: 0.088139\n",
            " 144126/200000: episode: 805, duration: 0.679s, episode steps:  98, steps per second: 144, episode reward: -97.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.051 [0.000, 2.000],  loss: 0.747413, mae: 11.854404, mean_q: -17.018033, mean_eps: 0.087519\n",
            " 144210/200000: episode: 806, duration: 0.547s, episode steps:  84, steps per second: 154, episode reward: -83.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.083 [0.000, 2.000],  loss: 0.885965, mae: 11.798385, mean_q: -16.881693, mean_eps: 0.086936\n",
            " 144307/200000: episode: 807, duration: 0.624s, episode steps:  97, steps per second: 156, episode reward: -96.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.784 [0.000, 2.000],  loss: 0.739035, mae: 11.787880, mean_q: -16.911019, mean_eps: 0.086366\n",
            " 144377/200000: episode: 808, duration: 0.429s, episode steps:  70, steps per second: 163, episode reward: -69.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.671 [0.000, 2.000],  loss: 0.995692, mae: 11.804715, mean_q: -16.858826, mean_eps: 0.085834\n",
            " 144489/200000: episode: 809, duration: 0.725s, episode steps: 112, steps per second: 154, episode reward: -111.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.089 [0.000, 2.000],  loss: 0.685341, mae: 11.761260, mean_q: -16.888753, mean_eps: 0.085251\n",
            " 144596/200000: episode: 810, duration: 0.692s, episode steps: 107, steps per second: 155, episode reward: -106.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.673 [0.000, 2.000],  loss: 0.752602, mae: 11.677865, mean_q: -16.743458, mean_eps: 0.084567\n",
            " 144668/200000: episode: 811, duration: 0.497s, episode steps:  72, steps per second: 145, episode reward: -71.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.931 [0.000, 2.000],  loss: 0.714476, mae: 11.606222, mean_q: -16.637876, mean_eps: 0.084010\n",
            " 144740/200000: episode: 812, duration: 0.500s, episode steps:  72, steps per second: 144, episode reward: -71.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.681 [0.000, 2.000],  loss: 0.640619, mae: 11.751787, mean_q: -16.864649, mean_eps: 0.083554\n",
            " 144841/200000: episode: 813, duration: 0.732s, episode steps: 101, steps per second: 138, episode reward: -100.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.921 [0.000, 2.000],  loss: 0.735338, mae: 11.909277, mean_q: -17.100663, mean_eps: 0.082997\n",
            " 144934/200000: episode: 814, duration: 0.862s, episode steps:  93, steps per second: 108, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.548 [0.000, 2.000],  loss: 0.810122, mae: 11.916001, mean_q: -17.055511, mean_eps: 0.082376\n",
            " 145020/200000: episode: 815, duration: 0.883s, episode steps:  86, steps per second:  97, episode reward: -85.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.884 [0.000, 2.000],  loss: 0.773478, mae: 11.869923, mean_q: -17.018933, mean_eps: 0.081819\n",
            " 145104/200000: episode: 816, duration: 0.735s, episode steps:  84, steps per second: 114, episode reward: -83.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.607 [0.000, 2.000],  loss: 0.776267, mae: 11.904919, mean_q: -17.051785, mean_eps: 0.081287\n",
            " 145174/200000: episode: 817, duration: 0.762s, episode steps:  70, steps per second:  92, episode reward: -69.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.900 [0.000, 2.000],  loss: 0.664547, mae: 11.810906, mean_q: -16.963573, mean_eps: 0.080793\n",
            " 145244/200000: episode: 818, duration: 0.538s, episode steps:  70, steps per second: 130, episode reward: -69.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.700 [0.000, 2.000],  loss: 0.805430, mae: 11.774159, mean_q: -16.848827, mean_eps: 0.080349\n",
            " 145353/200000: episode: 819, duration: 0.773s, episode steps: 109, steps per second: 141, episode reward: -108.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.028 [0.000, 2.000],  loss: 0.643843, mae: 11.929340, mean_q: -17.137449, mean_eps: 0.079779\n",
            " 145459/200000: episode: 820, duration: 0.737s, episode steps: 106, steps per second: 144, episode reward: -105.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.708 [0.000, 2.000],  loss: 0.671763, mae: 11.914733, mean_q: -17.125821, mean_eps: 0.079095\n",
            " 145573/200000: episode: 821, duration: 0.776s, episode steps: 114, steps per second: 147, episode reward: -113.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.149 [0.000, 2.000],  loss: 0.689607, mae: 11.847136, mean_q: -16.989669, mean_eps: 0.078399\n",
            " 145656/200000: episode: 822, duration: 0.582s, episode steps:  83, steps per second: 143, episode reward: -82.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.663 [0.000, 2.000],  loss: 0.824143, mae: 11.832467, mean_q: -16.949711, mean_eps: 0.077778\n",
            " 145871/200000: episode: 823, duration: 1.458s, episode steps: 215, steps per second: 147, episode reward: -214.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.758 [0.000, 2.000],  loss: 0.752548, mae: 11.884582, mean_q: -17.048841, mean_eps: 0.076841\n",
            " 145968/200000: episode: 824, duration: 0.630s, episode steps:  97, steps per second: 154, episode reward: -96.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.876 [0.000, 2.000],  loss: 0.634647, mae: 11.868691, mean_q: -17.073213, mean_eps: 0.075853\n",
            " 146051/200000: episode: 825, duration: 0.561s, episode steps:  83, steps per second: 148, episode reward: -82.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.988 [0.000, 2.000],  loss: 0.830387, mae: 11.841752, mean_q: -16.983174, mean_eps: 0.075283\n",
            " 146164/200000: episode: 826, duration: 0.777s, episode steps: 113, steps per second: 145, episode reward: -112.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.354 [0.000, 2.000],  loss: 0.779540, mae: 11.834341, mean_q: -16.979033, mean_eps: 0.074662\n",
            " 146267/200000: episode: 827, duration: 0.635s, episode steps: 103, steps per second: 162, episode reward: -102.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.107 [0.000, 2.000],  loss: 0.744844, mae: 11.959482, mean_q: -17.167606, mean_eps: 0.073978\n",
            " 146349/200000: episode: 828, duration: 0.556s, episode steps:  82, steps per second: 148, episode reward: -81.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.085 [0.000, 2.000],  loss: 0.650165, mae: 11.823136, mean_q: -16.977390, mean_eps: 0.073383\n",
            " 146435/200000: episode: 829, duration: 0.515s, episode steps:  86, steps per second: 167, episode reward: -85.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.058 [0.000, 2.000],  loss: 0.661091, mae: 11.842488, mean_q: -17.015294, mean_eps: 0.072851\n",
            " 146597/200000: episode: 830, duration: 1.025s, episode steps: 162, steps per second: 158, episode reward: -161.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.883 [0.000, 2.000],  loss: 0.696385, mae: 11.882535, mean_q: -17.065952, mean_eps: 0.072065\n",
            " 146700/200000: episode: 831, duration: 0.659s, episode steps: 103, steps per second: 156, episode reward: -102.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.058 [0.000, 2.000],  loss: 0.802907, mae: 11.903330, mean_q: -17.031233, mean_eps: 0.071229\n",
            " 146804/200000: episode: 832, duration: 0.972s, episode steps: 104, steps per second: 107, episode reward: -103.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.779 [0.000, 2.000],  loss: 0.684487, mae: 11.783511, mean_q: -16.895337, mean_eps: 0.070583\n",
            " 147242/200000: episode: 833, duration: 3.578s, episode steps: 438, steps per second: 122, episode reward: -437.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.247 [0.000, 2.000],  loss: 0.769108, mae: 11.845384, mean_q: -16.980599, mean_eps: 0.068861\n",
            " 147383/200000: episode: 834, duration: 0.931s, episode steps: 141, steps per second: 151, episode reward: -140.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.738 [0.000, 2.000],  loss: 0.849939, mae: 11.982963, mean_q: -17.184902, mean_eps: 0.067024\n",
            " 147470/200000: episode: 835, duration: 0.641s, episode steps:  87, steps per second: 136, episode reward: -86.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.920 [0.000, 2.000],  loss: 0.842393, mae: 11.958942, mean_q: -17.150017, mean_eps: 0.066302\n",
            " 147579/200000: episode: 836, duration: 0.685s, episode steps: 109, steps per second: 159, episode reward: -108.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.982 [0.000, 2.000],  loss: 0.712408, mae: 11.956017, mean_q: -17.188159, mean_eps: 0.065681\n",
            " 147688/200000: episode: 837, duration: 0.721s, episode steps: 109, steps per second: 151, episode reward: -108.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.018 [0.000, 2.000],  loss: 0.762170, mae: 11.958168, mean_q: -17.163305, mean_eps: 0.064997\n",
            " 147769/200000: episode: 838, duration: 0.507s, episode steps:  81, steps per second: 160, episode reward: -80.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.259 [0.000, 2.000],  loss: 0.779299, mae: 12.021404, mean_q: -17.249508, mean_eps: 0.064389\n",
            " 147877/200000: episode: 839, duration: 0.697s, episode steps: 108, steps per second: 155, episode reward: -107.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.852 [0.000, 2.000],  loss: 0.697751, mae: 11.950915, mean_q: -17.138134, mean_eps: 0.063781\n",
            " 147962/200000: episode: 840, duration: 0.584s, episode steps:  85, steps per second: 146, episode reward: -84.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.106 [0.000, 2.000],  loss: 0.761982, mae: 11.994532, mean_q: -17.189221, mean_eps: 0.063173\n",
            " 148039/200000: episode: 841, duration: 0.474s, episode steps:  77, steps per second: 162, episode reward: -76.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.221 [0.000, 2.000],  loss: 0.827162, mae: 12.046219, mean_q: -17.263338, mean_eps: 0.062667\n",
            " 148154/200000: episode: 842, duration: 0.754s, episode steps: 115, steps per second: 153, episode reward: -114.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.800 [0.000, 2.000],  loss: 0.723108, mae: 11.978234, mean_q: -17.198323, mean_eps: 0.062059\n",
            " 148269/200000: episode: 843, duration: 0.758s, episode steps: 115, steps per second: 152, episode reward: -114.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.104 [0.000, 2.000],  loss: 0.676032, mae: 11.985818, mean_q: -17.215982, mean_eps: 0.061324\n",
            " 148379/200000: episode: 844, duration: 0.747s, episode steps: 110, steps per second: 147, episode reward: -109.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.145 [0.000, 2.000],  loss: 0.746456, mae: 11.916240, mean_q: -17.103603, mean_eps: 0.060615\n",
            " 148464/200000: episode: 845, duration: 0.571s, episode steps:  85, steps per second: 149, episode reward: -84.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.047 [0.000, 2.000],  loss: 0.741178, mae: 11.912798, mean_q: -17.077756, mean_eps: 0.060007\n",
            " 148547/200000: episode: 846, duration: 0.549s, episode steps:  83, steps per second: 151, episode reward: -82.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.084 [0.000, 2.000],  loss: 0.884351, mae: 11.964529, mean_q: -17.107693, mean_eps: 0.059475\n",
            " 148634/200000: episode: 847, duration: 0.764s, episode steps:  87, steps per second: 114, episode reward: -86.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.080 [0.000, 2.000],  loss: 0.716784, mae: 12.040844, mean_q: -17.301134, mean_eps: 0.058930\n",
            " 148747/200000: episode: 848, duration: 1.030s, episode steps: 113, steps per second: 110, episode reward: -112.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.186 [0.000, 2.000],  loss: 0.780966, mae: 11.973511, mean_q: -17.184625, mean_eps: 0.058297\n",
            " 148854/200000: episode: 849, duration: 0.966s, episode steps: 107, steps per second: 111, episode reward: -106.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.121 [0.000, 2.000],  loss: 0.716048, mae: 11.904660, mean_q: -17.068540, mean_eps: 0.057600\n",
            " 148930/200000: episode: 850, duration: 0.831s, episode steps:  76, steps per second:  91, episode reward: -75.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.263 [0.000, 2.000],  loss: 0.873857, mae: 11.974212, mean_q: -17.133013, mean_eps: 0.057017\n",
            " 149016/200000: episode: 851, duration: 0.715s, episode steps:  86, steps per second: 120, episode reward: -85.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.070 [0.000, 2.000],  loss: 0.828423, mae: 12.009558, mean_q: -17.198263, mean_eps: 0.056511\n",
            " 149094/200000: episode: 852, duration: 0.532s, episode steps:  78, steps per second: 147, episode reward: -77.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.013 [0.000, 2.000],  loss: 0.819505, mae: 12.016671, mean_q: -17.220541, mean_eps: 0.055991\n",
            " 149198/200000: episode: 853, duration: 0.686s, episode steps: 104, steps per second: 152, episode reward: -103.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.192 [0.000, 2.000],  loss: 0.766180, mae: 11.986874, mean_q: -17.191544, mean_eps: 0.055409\n",
            " 149281/200000: episode: 854, duration: 0.529s, episode steps:  83, steps per second: 157, episode reward: -82.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.024 [0.000, 2.000],  loss: 0.814963, mae: 11.896786, mean_q: -17.044521, mean_eps: 0.054813\n",
            " 149366/200000: episode: 855, duration: 0.575s, episode steps:  85, steps per second: 148, episode reward: -84.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.082 [0.000, 2.000],  loss: 0.783332, mae: 11.922065, mean_q: -17.085803, mean_eps: 0.054281\n",
            " 149459/200000: episode: 856, duration: 0.558s, episode steps:  93, steps per second: 167, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.151 [0.000, 2.000],  loss: 0.702595, mae: 12.023699, mean_q: -17.267110, mean_eps: 0.053724\n",
            " 149551/200000: episode: 857, duration: 0.618s, episode steps:  92, steps per second: 149, episode reward: -91.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.978 [0.000, 2.000],  loss: 0.905079, mae: 11.988630, mean_q: -17.144753, mean_eps: 0.053141\n",
            " 149663/200000: episode: 858, duration: 0.705s, episode steps: 112, steps per second: 159, episode reward: -111.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.839 [0.000, 2.000],  loss: 0.746709, mae: 12.050807, mean_q: -17.299454, mean_eps: 0.052495\n",
            " 149757/200000: episode: 859, duration: 0.627s, episode steps:  94, steps per second: 150, episode reward: -93.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.149 [0.000, 2.000],  loss: 0.832550, mae: 12.007440, mean_q: -17.210322, mean_eps: 0.051837\n",
            " 149842/200000: episode: 860, duration: 0.530s, episode steps:  85, steps per second: 160, episode reward: -84.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.035 [0.000, 2.000],  loss: 0.788939, mae: 11.931773, mean_q: -17.101958, mean_eps: 0.051267\n",
            " 149931/200000: episode: 861, duration: 0.554s, episode steps:  89, steps per second: 161, episode reward: -88.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.236 [0.000, 2.000],  loss: 0.881408, mae: 11.990728, mean_q: -17.170461, mean_eps: 0.050722\n",
            " 150016/200000: episode: 862, duration: 0.582s, episode steps:  85, steps per second: 146, episode reward: -84.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.176 [0.000, 2.000],  loss: 0.800234, mae: 11.973853, mean_q: -17.163327, mean_eps: 0.050185\n",
            " 150107/200000: episode: 863, duration: 0.569s, episode steps:  91, steps per second: 160, episode reward: -90.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.220 [0.000, 2.000],  loss: 0.758005, mae: 12.055993, mean_q: -17.312662, mean_eps: 0.050000\n",
            " 150198/200000: episode: 864, duration: 0.569s, episode steps:  91, steps per second: 160, episode reward: -90.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.473 [0.000, 2.000],  loss: 0.899934, mae: 12.153254, mean_q: -17.417734, mean_eps: 0.050000\n",
            " 150305/200000: episode: 865, duration: 0.674s, episode steps: 107, steps per second: 159, episode reward: -106.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.850 [0.000, 2.000],  loss: 0.771057, mae: 11.967263, mean_q: -17.187696, mean_eps: 0.050000\n",
            " 150414/200000: episode: 866, duration: 0.724s, episode steps: 109, steps per second: 150, episode reward: -108.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.917 [0.000, 2.000],  loss: 0.729175, mae: 11.934356, mean_q: -17.128144, mean_eps: 0.050000\n",
            " 150510/200000: episode: 867, duration: 0.712s, episode steps:  96, steps per second: 135, episode reward: -95.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.156 [0.000, 2.000],  loss: 0.791876, mae: 11.994752, mean_q: -17.187630, mean_eps: 0.050000\n",
            " 150588/200000: episode: 868, duration: 0.715s, episode steps:  78, steps per second: 109, episode reward: -77.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.077 [0.000, 2.000],  loss: 0.827747, mae: 12.015393, mean_q: -17.215882, mean_eps: 0.050000\n",
            " 150715/200000: episode: 869, duration: 1.176s, episode steps: 127, steps per second: 108, episode reward: -126.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.039 [0.000, 2.000],  loss: 0.741090, mae: 11.946975, mean_q: -17.148445, mean_eps: 0.050000\n",
            " 150806/200000: episode: 870, duration: 0.803s, episode steps:  91, steps per second: 113, episode reward: -90.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.110 [0.000, 2.000],  loss: 0.749707, mae: 11.994302, mean_q: -17.222414, mean_eps: 0.050000\n",
            " 150890/200000: episode: 871, duration: 0.692s, episode steps:  84, steps per second: 121, episode reward: -83.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.107 [0.000, 2.000],  loss: 0.754046, mae: 11.864191, mean_q: -17.024006, mean_eps: 0.050000\n",
            " 150999/200000: episode: 872, duration: 0.697s, episode steps: 109, steps per second: 156, episode reward: -108.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.339 [0.000, 2.000],  loss: 0.813703, mae: 12.031599, mean_q: -17.259764, mean_eps: 0.050000\n",
            " 151084/200000: episode: 873, duration: 0.580s, episode steps:  85, steps per second: 147, episode reward: -84.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.847 [0.000, 2.000],  loss: 0.881589, mae: 12.062757, mean_q: -17.284953, mean_eps: 0.050000\n",
            " 151168/200000: episode: 874, duration: 0.545s, episode steps:  84, steps per second: 154, episode reward: -83.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.905 [0.000, 2.000],  loss: 0.782815, mae: 11.982688, mean_q: -17.180854, mean_eps: 0.050000\n",
            " 151265/200000: episode: 875, duration: 0.614s, episode steps:  97, steps per second: 158, episode reward: -96.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.021 [0.000, 2.000],  loss: 0.783505, mae: 11.973023, mean_q: -17.178448, mean_eps: 0.050000\n",
            " 151345/200000: episode: 876, duration: 0.555s, episode steps:  80, steps per second: 144, episode reward: -79.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.938 [0.000, 2.000],  loss: 0.747631, mae: 11.991203, mean_q: -17.191770, mean_eps: 0.050000\n",
            " 151471/200000: episode: 877, duration: 0.795s, episode steps: 126, steps per second: 158, episode reward: -125.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.286 [0.000, 2.000],  loss: 0.810116, mae: 12.134059, mean_q: -17.393920, mean_eps: 0.050000\n",
            " 151561/200000: episode: 878, duration: 0.586s, episode steps:  90, steps per second: 154, episode reward: -89.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.789 [0.000, 2.000],  loss: 0.844825, mae: 12.023423, mean_q: -17.203682, mean_eps: 0.050000\n",
            " 151657/200000: episode: 879, duration: 0.592s, episode steps:  96, steps per second: 162, episode reward: -95.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.865 [0.000, 2.000],  loss: 0.833810, mae: 12.059965, mean_q: -17.269379, mean_eps: 0.050000\n",
            " 151749/200000: episode: 880, duration: 0.555s, episode steps:  92, steps per second: 166, episode reward: -91.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.217 [0.000, 2.000],  loss: 0.913308, mae: 12.092295, mean_q: -17.311797, mean_eps: 0.050000\n",
            " 151860/200000: episode: 881, duration: 0.723s, episode steps: 111, steps per second: 153, episode reward: -110.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.829 [0.000, 2.000],  loss: 0.607742, mae: 12.012949, mean_q: -17.297039, mean_eps: 0.050000\n",
            " 151981/200000: episode: 882, duration: 0.835s, episode steps: 121, steps per second: 145, episode reward: -120.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.868 [0.000, 2.000],  loss: 0.744231, mae: 12.086784, mean_q: -17.370291, mean_eps: 0.050000\n",
            " 152087/200000: episode: 883, duration: 0.639s, episode steps: 106, steps per second: 166, episode reward: -105.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.934 [0.000, 2.000],  loss: 0.851282, mae: 12.124153, mean_q: -17.400060, mean_eps: 0.050000\n",
            " 152172/200000: episode: 884, duration: 0.561s, episode steps:  85, steps per second: 152, episode reward: -84.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.988 [0.000, 2.000],  loss: 0.870984, mae: 12.194460, mean_q: -17.496740, mean_eps: 0.050000\n",
            " 152265/200000: episode: 885, duration: 0.615s, episode steps:  93, steps per second: 151, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.699 [0.000, 2.000],  loss: 0.835113, mae: 12.119794, mean_q: -17.399458, mean_eps: 0.050000\n",
            " 152373/200000: episode: 886, duration: 0.671s, episode steps: 108, steps per second: 161, episode reward: -107.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.815 [0.000, 2.000],  loss: 0.873927, mae: 12.110686, mean_q: -17.359534, mean_eps: 0.050000\n",
            " 152470/200000: episode: 887, duration: 0.795s, episode steps:  97, steps per second: 122, episode reward: -96.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.711 [0.000, 2.000],  loss: 0.788851, mae: 12.100880, mean_q: -17.369024, mean_eps: 0.050000\n",
            " 152591/200000: episode: 888, duration: 1.100s, episode steps: 121, steps per second: 110, episode reward: -120.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.769 [0.000, 2.000],  loss: 0.757756, mae: 12.097675, mean_q: -17.379772, mean_eps: 0.050000\n",
            " 152676/200000: episode: 889, duration: 0.770s, episode steps:  85, steps per second: 110, episode reward: -84.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.906 [0.000, 2.000],  loss: 0.865510, mae: 12.102961, mean_q: -17.336312, mean_eps: 0.050000\n",
            " 152793/200000: episode: 890, duration: 1.064s, episode steps: 117, steps per second: 110, episode reward: -116.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.154 [0.000, 2.000],  loss: 0.854324, mae: 12.073692, mean_q: -17.298132, mean_eps: 0.050000\n",
            " 152879/200000: episode: 891, duration: 0.559s, episode steps:  86, steps per second: 154, episode reward: -85.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.651 [0.000, 2.000],  loss: 0.818611, mae: 12.151419, mean_q: -17.456219, mean_eps: 0.050000\n",
            " 152957/200000: episode: 892, duration: 0.515s, episode steps:  78, steps per second: 151, episode reward: -77.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.051 [0.000, 2.000],  loss: 0.823340, mae: 12.063706, mean_q: -17.318936, mean_eps: 0.050000\n",
            " 153043/200000: episode: 893, duration: 0.544s, episode steps:  86, steps per second: 158, episode reward: -85.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.221 [0.000, 2.000],  loss: 0.838275, mae: 12.004745, mean_q: -17.219414, mean_eps: 0.050000\n",
            " 153122/200000: episode: 894, duration: 0.522s, episode steps:  79, steps per second: 151, episode reward: -78.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.962 [0.000, 2.000],  loss: 0.845127, mae: 11.995741, mean_q: -17.185314, mean_eps: 0.050000\n",
            " 153199/200000: episode: 895, duration: 0.462s, episode steps:  77, steps per second: 167, episode reward: -76.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.636 [0.000, 2.000],  loss: 0.780504, mae: 11.931294, mean_q: -17.114294, mean_eps: 0.050000\n",
            " 153308/200000: episode: 896, duration: 0.719s, episode steps: 109, steps per second: 152, episode reward: -108.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.716 [0.000, 2.000],  loss: 0.695353, mae: 11.889723, mean_q: -17.108124, mean_eps: 0.050000\n",
            " 153387/200000: episode: 897, duration: 0.518s, episode steps:  79, steps per second: 152, episode reward: -78.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.038 [0.000, 2.000],  loss: 0.637471, mae: 12.005958, mean_q: -17.283040, mean_eps: 0.050000\n",
            " 153458/200000: episode: 898, duration: 0.457s, episode steps:  71, steps per second: 155, episode reward: -70.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.606 [0.000, 2.000],  loss: 0.796328, mae: 12.089022, mean_q: -17.362496, mean_eps: 0.050000\n",
            " 153572/200000: episode: 899, duration: 0.725s, episode steps: 114, steps per second: 157, episode reward: -113.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.842 [0.000, 2.000],  loss: 0.889905, mae: 11.982806, mean_q: -17.170289, mean_eps: 0.050000\n",
            " 153690/200000: episode: 900, duration: 0.765s, episode steps: 118, steps per second: 154, episode reward: -117.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.737 [0.000, 2.000],  loss: 0.844841, mae: 12.047244, mean_q: -17.282840, mean_eps: 0.050000\n",
            " 153821/200000: episode: 901, duration: 0.810s, episode steps: 131, steps per second: 162, episode reward: -130.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.809 [0.000, 2.000],  loss: 0.666998, mae: 12.008499, mean_q: -17.285376, mean_eps: 0.050000\n",
            " 153975/200000: episode: 902, duration: 0.931s, episode steps: 154, steps per second: 165, episode reward: -153.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.753 [0.000, 2.000],  loss: 0.717961, mae: 12.050126, mean_q: -17.333378, mean_eps: 0.050000\n",
            " 154081/200000: episode: 903, duration: 0.671s, episode steps: 106, steps per second: 158, episode reward: -105.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.057 [0.000, 2.000],  loss: 0.725937, mae: 11.976035, mean_q: -17.183606, mean_eps: 0.050000\n",
            " 154169/200000: episode: 904, duration: 0.567s, episode steps:  88, steps per second: 155, episode reward: -87.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.830 [0.000, 2.000],  loss: 0.831182, mae: 12.098689, mean_q: -17.347908, mean_eps: 0.050000\n",
            " 154252/200000: episode: 905, duration: 0.519s, episode steps:  83, steps per second: 160, episode reward: -82.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.518 [0.000, 2.000],  loss: 0.601849, mae: 12.086240, mean_q: -17.380237, mean_eps: 0.050000\n",
            " 154409/200000: episode: 906, duration: 1.211s, episode steps: 157, steps per second: 130, episode reward: -156.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.274 [0.000, 2.000],  loss: 0.836556, mae: 12.092697, mean_q: -17.332571, mean_eps: 0.050000\n",
            " 154514/200000: episode: 907, duration: 0.936s, episode steps: 105, steps per second: 112, episode reward: -104.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.562 [0.000, 2.000],  loss: 0.756273, mae: 12.119003, mean_q: -17.390753, mean_eps: 0.050000\n",
            " 154591/200000: episode: 908, duration: 0.656s, episode steps:  77, steps per second: 117, episode reward: -76.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.857 [0.000, 2.000],  loss: 0.759313, mae: 12.021420, mean_q: -17.219731, mean_eps: 0.050000\n",
            " 154696/200000: episode: 909, duration: 0.953s, episode steps: 105, steps per second: 110, episode reward: -104.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.010 [0.000, 2.000],  loss: 0.793210, mae: 11.922889, mean_q: -17.093656, mean_eps: 0.050000\n",
            " 154801/200000: episode: 910, duration: 0.750s, episode steps: 105, steps per second: 140, episode reward: -104.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.790 [0.000, 2.000],  loss: 0.739762, mae: 11.930107, mean_q: -17.120081, mean_eps: 0.050000\n",
            " 154891/200000: episode: 911, duration: 0.541s, episode steps:  90, steps per second: 166, episode reward: -89.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.689 [0.000, 2.000],  loss: 0.721116, mae: 11.979398, mean_q: -17.197965, mean_eps: 0.050000\n",
            " 154972/200000: episode: 912, duration: 0.535s, episode steps:  81, steps per second: 151, episode reward: -80.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.531 [0.000, 2.000],  loss: 0.646247, mae: 12.085277, mean_q: -17.383774, mean_eps: 0.050000\n",
            " 155050/200000: episode: 913, duration: 0.485s, episode steps:  78, steps per second: 161, episode reward: -77.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.603 [0.000, 2.000],  loss: 0.775720, mae: 12.180163, mean_q: -17.478543, mean_eps: 0.050000\n",
            " 155134/200000: episode: 914, duration: 0.531s, episode steps:  84, steps per second: 158, episode reward: -83.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.893 [0.000, 2.000],  loss: 0.699317, mae: 12.090948, mean_q: -17.333080, mean_eps: 0.050000\n",
            " 155259/200000: episode: 915, duration: 0.761s, episode steps: 125, steps per second: 164, episode reward: -124.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.784 [0.000, 2.000],  loss: 0.809274, mae: 12.034572, mean_q: -17.235945, mean_eps: 0.050000\n",
            " 155356/200000: episode: 916, duration: 0.602s, episode steps:  97, steps per second: 161, episode reward: -96.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.876 [0.000, 2.000],  loss: 0.878566, mae: 12.200344, mean_q: -17.473897, mean_eps: 0.050000\n",
            " 155445/200000: episode: 917, duration: 0.577s, episode steps:  89, steps per second: 154, episode reward: -88.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.742 [0.000, 2.000],  loss: 0.720678, mae: 12.060809, mean_q: -17.322683, mean_eps: 0.050000\n",
            " 155563/200000: episode: 918, duration: 0.716s, episode steps: 118, steps per second: 165, episode reward: -117.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.203 [0.000, 2.000],  loss: 0.854908, mae: 12.062505, mean_q: -17.304554, mean_eps: 0.050000\n",
            " 155668/200000: episode: 919, duration: 0.700s, episode steps: 105, steps per second: 150, episode reward: -104.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.724 [0.000, 2.000],  loss: 0.816931, mae: 12.082313, mean_q: -17.333843, mean_eps: 0.050000\n",
            " 155744/200000: episode: 920, duration: 0.519s, episode steps:  76, steps per second: 146, episode reward: -75.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.789 [0.000, 2.000],  loss: 0.738437, mae: 12.078280, mean_q: -17.317444, mean_eps: 0.050000\n",
            " 156244/200000: episode: 921, duration: 3.106s, episode steps: 500, steps per second: 161, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.602 [0.000, 2.000],  loss: 0.764522, mae: 12.154961, mean_q: -17.449960, mean_eps: 0.050000\n",
            " 156468/200000: episode: 922, duration: 1.904s, episode steps: 224, steps per second: 118, episode reward: -223.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.214 [0.000, 2.000],  loss: 0.833468, mae: 12.160814, mean_q: -17.424202, mean_eps: 0.050000\n",
            " 156555/200000: episode: 923, duration: 0.755s, episode steps:  87, steps per second: 115, episode reward: -86.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.057 [0.000, 2.000],  loss: 0.873110, mae: 12.100882, mean_q: -17.341116, mean_eps: 0.050000\n",
            " 156676/200000: episode: 924, duration: 1.138s, episode steps: 121, steps per second: 106, episode reward: -120.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.132 [0.000, 2.000],  loss: 0.899147, mae: 12.077569, mean_q: -17.249461, mean_eps: 0.050000\n",
            " 156771/200000: episode: 925, duration: 0.616s, episode steps:  95, steps per second: 154, episode reward: -94.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.768 [0.000, 2.000],  loss: 0.924557, mae: 12.149104, mean_q: -17.401810, mean_eps: 0.050000\n",
            " 156857/200000: episode: 926, duration: 0.558s, episode steps:  86, steps per second: 154, episode reward: -85.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.035 [0.000, 2.000],  loss: 0.845626, mae: 12.095221, mean_q: -17.375979, mean_eps: 0.050000\n",
            " 156969/200000: episode: 927, duration: 0.711s, episode steps: 112, steps per second: 158, episode reward: -111.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.232 [0.000, 2.000],  loss: 0.757163, mae: 12.109096, mean_q: -17.428216, mean_eps: 0.050000\n",
            " 157060/200000: episode: 928, duration: 0.576s, episode steps:  91, steps per second: 158, episode reward: -90.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.670 [0.000, 2.000],  loss: 0.839275, mae: 12.112779, mean_q: -17.362344, mean_eps: 0.050000\n",
            " 157160/200000: episode: 929, duration: 0.613s, episode steps: 100, steps per second: 163, episode reward: -99.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.710 [0.000, 2.000],  loss: 0.638465, mae: 12.028264, mean_q: -17.284096, mean_eps: 0.050000\n",
            " 157273/200000: episode: 930, duration: 0.741s, episode steps: 113, steps per second: 153, episode reward: -112.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.699 [0.000, 2.000],  loss: 0.792573, mae: 12.183716, mean_q: -17.489030, mean_eps: 0.050000\n",
            " 157382/200000: episode: 931, duration: 0.680s, episode steps: 109, steps per second: 160, episode reward: -108.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.138 [0.000, 2.000],  loss: 0.789793, mae: 12.090469, mean_q: -17.357874, mean_eps: 0.050000\n",
            " 157468/200000: episode: 932, duration: 0.538s, episode steps:  86, steps per second: 160, episode reward: -85.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.953 [0.000, 2.000],  loss: 0.812629, mae: 12.018800, mean_q: -17.216928, mean_eps: 0.050000\n",
            " 157593/200000: episode: 933, duration: 0.815s, episode steps: 125, steps per second: 153, episode reward: -124.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.816 [0.000, 2.000],  loss: 0.787481, mae: 12.196600, mean_q: -17.494668, mean_eps: 0.050000\n",
            " 157854/200000: episode: 934, duration: 1.620s, episode steps: 261, steps per second: 161, episode reward: -260.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.307 [0.000, 2.000],  loss: 0.781153, mae: 12.079504, mean_q: -17.315364, mean_eps: 0.050000\n",
            " 157937/200000: episode: 935, duration: 0.520s, episode steps:  83, steps per second: 160, episode reward: -82.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.060 [0.000, 2.000],  loss: 0.724288, mae: 12.126786, mean_q: -17.430214, mean_eps: 0.050000\n",
            " 158006/200000: episode: 936, duration: 0.418s, episode steps:  69, steps per second: 165, episode reward: -68.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.783 [0.000, 2.000],  loss: 0.763197, mae: 12.144207, mean_q: -17.465053, mean_eps: 0.050000\n",
            " 158097/200000: episode: 937, duration: 0.603s, episode steps:  91, steps per second: 151, episode reward: -90.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.143 [0.000, 2.000],  loss: 0.882849, mae: 12.148768, mean_q: -17.435104, mean_eps: 0.050000\n",
            " 158597/200000: episode: 938, duration: 4.016s, episode steps: 500, steps per second: 124, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.870 [0.000, 2.000],  loss: 0.787722, mae: 12.095968, mean_q: -17.367423, mean_eps: 0.050000\n",
            " 158693/200000: episode: 939, duration: 0.728s, episode steps:  96, steps per second: 132, episode reward: -95.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.042 [0.000, 2.000],  loss: 0.887275, mae: 11.998086, mean_q: -17.186459, mean_eps: 0.050000\n",
            " 158772/200000: episode: 940, duration: 0.480s, episode steps:  79, steps per second: 164, episode reward: -78.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.684 [0.000, 2.000],  loss: 0.753594, mae: 12.046110, mean_q: -17.325511, mean_eps: 0.050000\n",
            " 159272/200000: episode: 941, duration: 3.085s, episode steps: 500, steps per second: 162, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.290 [0.000, 2.000],  loss: 0.803450, mae: 12.084727, mean_q: -17.357285, mean_eps: 0.050000\n",
            " 159362/200000: episode: 942, duration: 0.573s, episode steps:  90, steps per second: 157, episode reward: -89.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.211 [0.000, 2.000],  loss: 0.779210, mae: 12.124778, mean_q: -17.416688, mean_eps: 0.050000\n",
            " 159473/200000: episode: 943, duration: 0.723s, episode steps: 111, steps per second: 153, episode reward: -110.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.784 [0.000, 2.000],  loss: 0.771417, mae: 12.073968, mean_q: -17.334228, mean_eps: 0.050000\n",
            " 159554/200000: episode: 944, duration: 0.529s, episode steps:  81, steps per second: 153, episode reward: -80.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.148 [0.000, 2.000],  loss: 0.791086, mae: 12.054615, mean_q: -17.320336, mean_eps: 0.050000\n",
            " 159626/200000: episode: 945, duration: 0.466s, episode steps:  72, steps per second: 155, episode reward: -71.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.208 [0.000, 2.000],  loss: 0.929836, mae: 12.137892, mean_q: -17.405547, mean_eps: 0.050000\n",
            " 159735/200000: episode: 946, duration: 0.711s, episode steps: 109, steps per second: 153, episode reward: -108.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.890 [0.000, 2.000],  loss: 0.736849, mae: 12.182050, mean_q: -17.517921, mean_eps: 0.050000\n",
            " 159836/200000: episode: 947, duration: 0.641s, episode steps: 101, steps per second: 158, episode reward: -100.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.267 [0.000, 2.000],  loss: 0.829588, mae: 12.264655, mean_q: -17.612857, mean_eps: 0.050000\n",
            " 159924/200000: episode: 948, duration: 0.572s, episode steps:  88, steps per second: 154, episode reward: -87.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.205 [0.000, 2.000],  loss: 0.777324, mae: 12.068209, mean_q: -17.323933, mean_eps: 0.050000\n",
            " 160424/200000: episode: 949, duration: 3.740s, episode steps: 500, steps per second: 134, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.232 [0.000, 2.000],  loss: 0.883649, mae: 12.225281, mean_q: -17.512857, mean_eps: 0.050000\n",
            " 160537/200000: episode: 950, duration: 0.972s, episode steps: 113, steps per second: 116, episode reward: -112.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.044 [0.000, 2.000],  loss: 0.877610, mae: 12.200225, mean_q: -17.499396, mean_eps: 0.050000\n",
            " 160622/200000: episode: 951, duration: 0.755s, episode steps:  85, steps per second: 113, episode reward: -84.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.212 [0.000, 2.000],  loss: 0.703662, mae: 12.254402, mean_q: -17.615844, mean_eps: 0.050000\n",
            " 160739/200000: episode: 952, duration: 0.703s, episode steps: 117, steps per second: 167, episode reward: -116.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.470 [0.000, 2.000],  loss: 0.854244, mae: 12.299714, mean_q: -17.618822, mean_eps: 0.050000\n",
            " 160838/200000: episode: 953, duration: 0.602s, episode steps:  99, steps per second: 165, episode reward: -98.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.869 [0.000, 2.000],  loss: 0.916643, mae: 12.129905, mean_q: -17.351203, mean_eps: 0.050000\n",
            " 160939/200000: episode: 954, duration: 0.612s, episode steps: 101, steps per second: 165, episode reward: -100.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.218 [0.000, 2.000],  loss: 0.723209, mae: 12.148043, mean_q: -17.423809, mean_eps: 0.050000\n",
            " 161016/200000: episode: 955, duration: 0.489s, episode steps:  77, steps per second: 157, episode reward: -76.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.078 [0.000, 2.000],  loss: 0.755905, mae: 12.195904, mean_q: -17.502172, mean_eps: 0.050000\n",
            " 161102/200000: episode: 956, duration: 0.588s, episode steps:  86, steps per second: 146, episode reward: -85.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.233 [0.000, 2.000],  loss: 0.788934, mae: 12.187282, mean_q: -17.491304, mean_eps: 0.050000\n",
            " 161224/200000: episode: 957, duration: 0.773s, episode steps: 122, steps per second: 158, episode reward: -121.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.213 [0.000, 2.000],  loss: 0.771781, mae: 12.280669, mean_q: -17.652251, mean_eps: 0.050000\n",
            " 161334/200000: episode: 958, duration: 0.705s, episode steps: 110, steps per second: 156, episode reward: -109.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.055 [0.000, 2.000],  loss: 0.763057, mae: 12.262884, mean_q: -17.609104, mean_eps: 0.050000\n",
            " 161422/200000: episode: 959, duration: 0.554s, episode steps:  88, steps per second: 159, episode reward: -87.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.193 [0.000, 2.000],  loss: 0.978157, mae: 12.277994, mean_q: -17.587416, mean_eps: 0.050000\n",
            " 161517/200000: episode: 960, duration: 0.597s, episode steps:  95, steps per second: 159, episode reward: -94.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.105 [0.000, 2.000],  loss: 0.777639, mae: 12.265428, mean_q: -17.615249, mean_eps: 0.050000\n",
            " 161629/200000: episode: 961, duration: 0.695s, episode steps: 112, steps per second: 161, episode reward: -111.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.929 [0.000, 2.000],  loss: 0.707975, mae: 12.303968, mean_q: -17.716350, mean_eps: 0.050000\n",
            " 161723/200000: episode: 962, duration: 0.575s, episode steps:  94, steps per second: 164, episode reward: -93.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.287 [0.000, 2.000],  loss: 0.744287, mae: 12.248608, mean_q: -17.598101, mean_eps: 0.050000\n",
            " 161823/200000: episode: 963, duration: 0.632s, episode steps: 100, steps per second: 158, episode reward: -99.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.090 [0.000, 2.000],  loss: 0.863631, mae: 12.187050, mean_q: -17.452912, mean_eps: 0.050000\n",
            " 161924/200000: episode: 964, duration: 0.698s, episode steps: 101, steps per second: 145, episode reward: -100.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.446 [0.000, 2.000],  loss: 0.875987, mae: 12.233210, mean_q: -17.504858, mean_eps: 0.050000\n",
            " 162019/200000: episode: 965, duration: 0.631s, episode steps:  95, steps per second: 150, episode reward: -94.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.242 [0.000, 2.000],  loss: 0.789438, mae: 12.187986, mean_q: -17.490926, mean_eps: 0.050000\n",
            " 162117/200000: episode: 966, duration: 0.643s, episode steps:  98, steps per second: 152, episode reward: -97.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.306 [0.000, 2.000],  loss: 0.721496, mae: 12.096381, mean_q: -17.374561, mean_eps: 0.050000\n",
            " 162204/200000: episode: 967, duration: 0.722s, episode steps:  87, steps per second: 121, episode reward: -86.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.023 [0.000, 2.000],  loss: 1.002961, mae: 12.175900, mean_q: -17.421023, mean_eps: 0.050000\n",
            " 162314/200000: episode: 968, duration: 0.992s, episode steps: 110, steps per second: 111, episode reward: -109.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.145 [0.000, 2.000],  loss: 0.972208, mae: 12.230365, mean_q: -17.526542, mean_eps: 0.050000\n",
            " 162814/200000: episode: 969, duration: 3.810s, episode steps: 500, steps per second: 131, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.817196, mae: 12.243534, mean_q: -17.558116, mean_eps: 0.050000\n",
            " 162912/200000: episode: 970, duration: 0.623s, episode steps:  98, steps per second: 157, episode reward: -97.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.857 [0.000, 2.000],  loss: 0.773836, mae: 12.235430, mean_q: -17.574031, mean_eps: 0.050000\n",
            " 163041/200000: episode: 971, duration: 0.823s, episode steps: 129, steps per second: 157, episode reward: -128.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.132 [0.000, 2.000],  loss: 0.843176, mae: 12.263172, mean_q: -17.611940, mean_eps: 0.050000\n",
            " 163152/200000: episode: 972, duration: 0.695s, episode steps: 111, steps per second: 160, episode reward: -110.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.414 [0.000, 2.000],  loss: 0.846054, mae: 12.208541, mean_q: -17.527967, mean_eps: 0.050000\n",
            " 163223/200000: episode: 973, duration: 0.429s, episode steps:  71, steps per second: 166, episode reward: -70.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.310 [0.000, 2.000],  loss: 0.857363, mae: 12.284117, mean_q: -17.618067, mean_eps: 0.050000\n",
            " 163314/200000: episode: 974, duration: 0.555s, episode steps:  91, steps per second: 164, episode reward: -90.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.264 [0.000, 2.000],  loss: 0.802692, mae: 12.254519, mean_q: -17.599238, mean_eps: 0.050000\n",
            " 163396/200000: episode: 975, duration: 0.534s, episode steps:  82, steps per second: 153, episode reward: -81.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.024 [0.000, 2.000],  loss: 0.729886, mae: 12.146438, mean_q: -17.440777, mean_eps: 0.050000\n",
            " 163479/200000: episode: 976, duration: 0.563s, episode steps:  83, steps per second: 147, episode reward: -82.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.976 [0.000, 2.000],  loss: 0.909084, mae: 12.240854, mean_q: -17.551782, mean_eps: 0.050000\n",
            " 163583/200000: episode: 977, duration: 0.690s, episode steps: 104, steps per second: 151, episode reward: -103.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.087 [0.000, 2.000],  loss: 0.890084, mae: 12.164083, mean_q: -17.452933, mean_eps: 0.050000\n",
            " 163717/200000: episode: 978, duration: 0.851s, episode steps: 134, steps per second: 157, episode reward: -133.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.179 [0.000, 2.000],  loss: 0.942850, mae: 12.194920, mean_q: -17.443928, mean_eps: 0.050000\n",
            " 163802/200000: episode: 979, duration: 0.538s, episode steps:  85, steps per second: 158, episode reward: -84.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.012 [0.000, 2.000],  loss: 0.734828, mae: 12.242350, mean_q: -17.568370, mean_eps: 0.050000\n",
            " 163888/200000: episode: 980, duration: 0.542s, episode steps:  86, steps per second: 159, episode reward: -85.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.965 [0.000, 2.000],  loss: 0.848600, mae: 12.146451, mean_q: -17.421044, mean_eps: 0.050000\n",
            " 164018/200000: episode: 981, duration: 0.844s, episode steps: 130, steps per second: 154, episode reward: -129.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.862 [0.000, 2.000],  loss: 0.862668, mae: 12.188091, mean_q: -17.490470, mean_eps: 0.050000\n",
            " 164097/200000: episode: 982, duration: 0.481s, episode steps:  79, steps per second: 164, episode reward: -78.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.975 [0.000, 2.000],  loss: 0.777027, mae: 12.194086, mean_q: -17.501926, mean_eps: 0.050000\n",
            " 164206/200000: episode: 983, duration: 0.919s, episode steps: 109, steps per second: 119, episode reward: -108.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.853 [0.000, 2.000],  loss: 0.764511, mae: 12.106987, mean_q: -17.354674, mean_eps: 0.050000\n",
            " 164706/200000: episode: 984, duration: 3.961s, episode steps: 500, steps per second: 126, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.234 [0.000, 2.000],  loss: 0.834955, mae: 12.274605, mean_q: -17.604291, mean_eps: 0.050000\n",
            " 164818/200000: episode: 985, duration: 0.683s, episode steps: 112, steps per second: 164, episode reward: -111.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.946 [0.000, 2.000],  loss: 0.891232, mae: 12.194198, mean_q: -17.489965, mean_eps: 0.050000\n",
            " 164921/200000: episode: 986, duration: 0.640s, episode steps: 103, steps per second: 161, episode reward: -102.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.204 [0.000, 2.000],  loss: 0.752178, mae: 12.191627, mean_q: -17.510813, mean_eps: 0.050000\n",
            " 165023/200000: episode: 987, duration: 0.665s, episode steps: 102, steps per second: 153, episode reward: -101.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.922 [0.000, 2.000],  loss: 0.733025, mae: 12.202582, mean_q: -17.530277, mean_eps: 0.050000\n",
            " 165110/200000: episode: 988, duration: 0.528s, episode steps:  87, steps per second: 165, episode reward: -86.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.897 [0.000, 2.000],  loss: 0.855155, mae: 12.313174, mean_q: -17.645575, mean_eps: 0.050000\n",
            " 165198/200000: episode: 989, duration: 0.549s, episode steps:  88, steps per second: 160, episode reward: -87.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.580 [0.000, 2.000],  loss: 0.899389, mae: 12.320261, mean_q: -17.646491, mean_eps: 0.050000\n",
            " 165288/200000: episode: 990, duration: 0.552s, episode steps:  90, steps per second: 163, episode reward: -89.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.989 [0.000, 2.000],  loss: 0.776112, mae: 12.249986, mean_q: -17.564004, mean_eps: 0.050000\n",
            " 165372/200000: episode: 991, duration: 0.551s, episode steps:  84, steps per second: 152, episode reward: -83.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.881 [0.000, 2.000],  loss: 0.814763, mae: 12.179761, mean_q: -17.423807, mean_eps: 0.050000\n",
            " 165482/200000: episode: 992, duration: 0.719s, episode steps: 110, steps per second: 153, episode reward: -109.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.882 [0.000, 2.000],  loss: 0.834974, mae: 12.301919, mean_q: -17.660881, mean_eps: 0.050000\n",
            " 165588/200000: episode: 993, duration: 0.673s, episode steps: 106, steps per second: 158, episode reward: -105.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.745 [0.000, 2.000],  loss: 0.735332, mae: 12.231500, mean_q: -17.578636, mean_eps: 0.050000\n",
            " 165677/200000: episode: 994, duration: 0.594s, episode steps:  89, steps per second: 150, episode reward: -88.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.112 [0.000, 2.000],  loss: 0.811373, mae: 12.290539, mean_q: -17.655853, mean_eps: 0.050000\n",
            " 165759/200000: episode: 995, duration: 0.480s, episode steps:  82, steps per second: 171, episode reward: -81.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.866 [0.000, 2.000],  loss: 0.818230, mae: 12.199323, mean_q: -17.516181, mean_eps: 0.050000\n",
            " 165915/200000: episode: 996, duration: 0.946s, episode steps: 156, steps per second: 165, episode reward: -155.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.853 [0.000, 2.000],  loss: 0.861710, mae: 12.296725, mean_q: -17.659903, mean_eps: 0.050000\n",
            " 165998/200000: episode: 997, duration: 0.513s, episode steps:  83, steps per second: 162, episode reward: -82.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.988 [0.000, 2.000],  loss: 0.817772, mae: 12.236034, mean_q: -17.574196, mean_eps: 0.050000\n",
            " 166078/200000: episode: 998, duration: 0.508s, episode steps:  80, steps per second: 158, episode reward: -79.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.688 [0.000, 2.000],  loss: 0.827322, mae: 12.277112, mean_q: -17.626333, mean_eps: 0.050000\n",
            " 166158/200000: episode: 999, duration: 0.696s, episode steps:  80, steps per second: 115, episode reward: -79.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.575 [0.000, 2.000],  loss: 1.020901, mae: 12.256938, mean_q: -17.536163, mean_eps: 0.050000\n",
            " 166251/200000: episode: 1000, duration: 0.864s, episode steps:  93, steps per second: 108, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.710 [0.000, 2.000],  loss: 0.777943, mae: 12.239333, mean_q: -17.573609, mean_eps: 0.050000\n",
            " 166345/200000: episode: 1001, duration: 0.810s, episode steps:  94, steps per second: 116, episode reward: -93.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.862 [0.000, 2.000],  loss: 0.840838, mae: 12.369968, mean_q: -17.771551, mean_eps: 0.050000\n",
            " 166441/200000: episode: 1002, duration: 0.903s, episode steps:  96, steps per second: 106, episode reward: -95.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.021 [0.000, 2.000],  loss: 0.794760, mae: 12.288953, mean_q: -17.619529, mean_eps: 0.050000\n",
            " 166535/200000: episode: 1003, duration: 0.621s, episode steps:  94, steps per second: 151, episode reward: -93.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.851 [0.000, 2.000],  loss: 0.793607, mae: 12.214257, mean_q: -17.529336, mean_eps: 0.050000\n",
            " 166656/200000: episode: 1004, duration: 0.733s, episode steps: 121, steps per second: 165, episode reward: -120.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.727 [0.000, 2.000],  loss: 0.750621, mae: 12.293625, mean_q: -17.653929, mean_eps: 0.050000\n",
            " 166782/200000: episode: 1005, duration: 0.783s, episode steps: 126, steps per second: 161, episode reward: -125.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.929 [0.000, 2.000],  loss: 0.828130, mae: 12.280068, mean_q: -17.616738, mean_eps: 0.050000\n",
            " 167282/200000: episode: 1006, duration: 2.985s, episode steps: 500, steps per second: 168, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.432 [0.000, 2.000],  loss: 0.801788, mae: 12.162067, mean_q: -17.443369, mean_eps: 0.050000\n",
            " 167380/200000: episode: 1007, duration: 0.606s, episode steps:  98, steps per second: 162, episode reward: -97.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.990 [0.000, 2.000],  loss: 0.942655, mae: 12.132844, mean_q: -17.349336, mean_eps: 0.050000\n",
            " 167495/200000: episode: 1008, duration: 0.701s, episode steps: 115, steps per second: 164, episode reward: -114.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.991 [0.000, 2.000],  loss: 0.945562, mae: 12.095845, mean_q: -17.295961, mean_eps: 0.050000\n",
            " 167578/200000: episode: 1009, duration: 0.504s, episode steps:  83, steps per second: 165, episode reward: -82.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.807 [0.000, 2.000],  loss: 0.959767, mae: 12.071331, mean_q: -17.237823, mean_eps: 0.050000\n",
            " 167679/200000: episode: 1010, duration: 0.622s, episode steps: 101, steps per second: 162, episode reward: -100.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.733 [0.000, 2.000],  loss: 0.904551, mae: 12.211022, mean_q: -17.484140, mean_eps: 0.050000\n",
            " 167762/200000: episode: 1011, duration: 0.532s, episode steps:  83, steps per second: 156, episode reward: -82.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.940 [0.000, 2.000],  loss: 0.741551, mae: 12.212984, mean_q: -17.544395, mean_eps: 0.050000\n",
            " 167864/200000: episode: 1012, duration: 0.661s, episode steps: 102, steps per second: 154, episode reward: -101.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.922 [0.000, 2.000],  loss: 0.767004, mae: 12.201995, mean_q: -17.538271, mean_eps: 0.050000\n",
            " 167953/200000: episode: 1013, duration: 0.557s, episode steps:  89, steps per second: 160, episode reward: -88.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.854 [0.000, 2.000],  loss: 0.883804, mae: 12.110546, mean_q: -17.358990, mean_eps: 0.050000\n",
            " 168033/200000: episode: 1014, duration: 0.498s, episode steps:  80, steps per second: 161, episode reward: -79.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.125 [0.000, 2.000],  loss: 0.930527, mae: 12.157500, mean_q: -17.427773, mean_eps: 0.050000\n",
            " 168143/200000: episode: 1015, duration: 0.780s, episode steps: 110, steps per second: 141, episode reward: -109.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.600 [0.000, 2.000],  loss: 0.734153, mae: 12.171776, mean_q: -17.492118, mean_eps: 0.050000\n",
            " 168264/200000: episode: 1016, duration: 1.089s, episode steps: 121, steps per second: 111, episode reward: -120.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.661 [0.000, 2.000],  loss: 0.941015, mae: 12.184835, mean_q: -17.450802, mean_eps: 0.050000\n",
            " 168387/200000: episode: 1017, duration: 1.006s, episode steps: 123, steps per second: 122, episode reward: -122.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.756 [0.000, 2.000],  loss: 0.794693, mae: 12.159598, mean_q: -17.450683, mean_eps: 0.050000\n",
            " 168479/200000: episode: 1018, duration: 0.880s, episode steps:  92, steps per second: 105, episode reward: -91.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.793 [0.000, 2.000],  loss: 0.757000, mae: 12.107778, mean_q: -17.374002, mean_eps: 0.050000\n",
            " 168564/200000: episode: 1019, duration: 0.572s, episode steps:  85, steps per second: 149, episode reward: -84.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.812 [0.000, 2.000],  loss: 0.861749, mae: 12.177182, mean_q: -17.464375, mean_eps: 0.050000\n",
            " 168637/200000: episode: 1020, duration: 0.443s, episode steps:  73, steps per second: 165, episode reward: -72.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.699 [0.000, 2.000],  loss: 0.775729, mae: 12.238861, mean_q: -17.569842, mean_eps: 0.050000\n",
            " 168709/200000: episode: 1021, duration: 0.449s, episode steps:  72, steps per second: 160, episode reward: -71.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.708 [0.000, 2.000],  loss: 0.874576, mae: 12.227562, mean_q: -17.522598, mean_eps: 0.050000\n",
            " 168790/200000: episode: 1022, duration: 0.486s, episode steps:  81, steps per second: 167, episode reward: -80.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.568 [0.000, 2.000],  loss: 0.745862, mae: 12.168208, mean_q: -17.475316, mean_eps: 0.050000\n",
            " 168866/200000: episode: 1023, duration: 0.498s, episode steps:  76, steps per second: 153, episode reward: -75.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.895 [0.000, 2.000],  loss: 0.876722, mae: 12.226483, mean_q: -17.503100, mean_eps: 0.050000\n",
            " 169366/200000: episode: 1024, duration: 3.070s, episode steps: 500, steps per second: 163, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.414 [0.000, 2.000],  loss: 0.804130, mae: 12.169840, mean_q: -17.442044, mean_eps: 0.050000\n",
            " 169452/200000: episode: 1025, duration: 0.559s, episode steps:  86, steps per second: 154, episode reward: -85.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.023 [0.000, 2.000],  loss: 0.820612, mae: 12.313448, mean_q: -17.654461, mean_eps: 0.050000\n",
            " 169540/200000: episode: 1026, duration: 0.594s, episode steps:  88, steps per second: 148, episode reward: -87.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.855176, mae: 12.169722, mean_q: -17.455250, mean_eps: 0.050000\n",
            " 169649/200000: episode: 1027, duration: 0.685s, episode steps: 109, steps per second: 159, episode reward: -108.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.239 [0.000, 2.000],  loss: 0.854520, mae: 12.124583, mean_q: -17.385472, mean_eps: 0.050000\n",
            " 169760/200000: episode: 1028, duration: 0.698s, episode steps: 111, steps per second: 159, episode reward: -110.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.937 [0.000, 2.000],  loss: 0.947229, mae: 12.212713, mean_q: -17.442508, mean_eps: 0.050000\n",
            " 169860/200000: episode: 1029, duration: 0.640s, episode steps: 100, steps per second: 156, episode reward: -99.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.080 [0.000, 2.000],  loss: 0.848182, mae: 12.139501, mean_q: -17.370242, mean_eps: 0.050000\n",
            " 169947/200000: episode: 1030, duration: 0.546s, episode steps:  87, steps per second: 159, episode reward: -86.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.851 [0.000, 2.000],  loss: 0.733783, mae: 12.118244, mean_q: -17.386194, mean_eps: 0.050000\n",
            " 170042/200000: episode: 1031, duration: 0.606s, episode steps:  95, steps per second: 157, episode reward: -94.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.821 [0.000, 2.000],  loss: 0.862831, mae: 12.121389, mean_q: -17.369833, mean_eps: 0.050000\n",
            " 170166/200000: episode: 1032, duration: 1.045s, episode steps: 124, steps per second: 119, episode reward: -123.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.202 [0.000, 2.000],  loss: 0.803024, mae: 12.035643, mean_q: -17.263286, mean_eps: 0.050000\n",
            " 170246/200000: episode: 1033, duration: 0.743s, episode steps:  80, steps per second: 108, episode reward: -79.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.175 [0.000, 2.000],  loss: 0.837147, mae: 12.117282, mean_q: -17.358380, mean_eps: 0.050000\n",
            " 170345/200000: episode: 1034, duration: 0.879s, episode steps:  99, steps per second: 113, episode reward: -98.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.232 [0.000, 2.000],  loss: 0.832308, mae: 12.192942, mean_q: -17.470203, mean_eps: 0.050000\n",
            " 170845/200000: episode: 1035, duration: 3.486s, episode steps: 500, steps per second: 143, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.290 [0.000, 2.000],  loss: 0.863079, mae: 12.180045, mean_q: -17.451023, mean_eps: 0.050000\n",
            " 170927/200000: episode: 1036, duration: 0.507s, episode steps:  82, steps per second: 162, episode reward: -81.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.890 [0.000, 2.000],  loss: 0.817660, mae: 12.037749, mean_q: -17.258667, mean_eps: 0.050000\n",
            " 171059/200000: episode: 1037, duration: 0.793s, episode steps: 132, steps per second: 167, episode reward: -131.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.227 [0.000, 2.000],  loss: 0.851693, mae: 12.059588, mean_q: -17.248865, mean_eps: 0.050000\n",
            " 171145/200000: episode: 1038, duration: 0.524s, episode steps:  86, steps per second: 164, episode reward: -85.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.140 [0.000, 2.000],  loss: 0.833093, mae: 11.976868, mean_q: -17.136788, mean_eps: 0.050000\n",
            " 171231/200000: episode: 1039, duration: 0.542s, episode steps:  86, steps per second: 159, episode reward: -85.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.047 [0.000, 2.000],  loss: 0.859361, mae: 11.999015, mean_q: -17.197910, mean_eps: 0.050000\n",
            " 171322/200000: episode: 1040, duration: 0.563s, episode steps:  91, steps per second: 162, episode reward: -90.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.055 [0.000, 2.000],  loss: 0.773004, mae: 11.960002, mean_q: -17.141775, mean_eps: 0.050000\n",
            " 171415/200000: episode: 1041, duration: 0.598s, episode steps:  93, steps per second: 155, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.914 [0.000, 2.000],  loss: 0.827600, mae: 12.033853, mean_q: -17.237584, mean_eps: 0.050000\n",
            " 171500/200000: episode: 1042, duration: 0.537s, episode steps:  85, steps per second: 158, episode reward: -84.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.176 [0.000, 2.000],  loss: 0.749276, mae: 12.036182, mean_q: -17.284246, mean_eps: 0.050000\n",
            " 171584/200000: episode: 1043, duration: 0.575s, episode steps:  84, steps per second: 146, episode reward: -83.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.024 [0.000, 2.000],  loss: 0.744628, mae: 12.037523, mean_q: -17.256705, mean_eps: 0.050000\n",
            " 171662/200000: episode: 1044, duration: 0.499s, episode steps:  78, steps per second: 156, episode reward: -77.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.974 [0.000, 2.000],  loss: 0.836258, mae: 12.038780, mean_q: -17.235852, mean_eps: 0.050000\n",
            " 171763/200000: episode: 1045, duration: 0.593s, episode steps: 101, steps per second: 170, episode reward: -100.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.980 [0.000, 2.000],  loss: 0.946322, mae: 12.042242, mean_q: -17.212536, mean_eps: 0.050000\n",
            " 171848/200000: episode: 1046, duration: 0.562s, episode steps:  85, steps per second: 151, episode reward: -84.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.012 [0.000, 2.000],  loss: 0.838348, mae: 11.983810, mean_q: -17.155652, mean_eps: 0.050000\n",
            " 171925/200000: episode: 1047, duration: 0.483s, episode steps:  77, steps per second: 159, episode reward: -76.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.169 [0.000, 2.000],  loss: 0.786574, mae: 12.100678, mean_q: -17.359281, mean_eps: 0.050000\n",
            " 172019/200000: episode: 1048, duration: 0.591s, episode steps:  94, steps per second: 159, episode reward: -93.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.085 [0.000, 2.000],  loss: 0.803237, mae: 12.115936, mean_q: -17.385802, mean_eps: 0.050000\n",
            " 172137/200000: episode: 1049, duration: 1.038s, episode steps: 118, steps per second: 114, episode reward: -117.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.958 [0.000, 2.000],  loss: 0.775936, mae: 12.100494, mean_q: -17.347979, mean_eps: 0.050000\n",
            " 172222/200000: episode: 1050, duration: 0.733s, episode steps:  85, steps per second: 116, episode reward: -84.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.882 [0.000, 2.000],  loss: 0.823862, mae: 12.131459, mean_q: -17.380352, mean_eps: 0.050000\n",
            " 172373/200000: episode: 1051, duration: 1.354s, episode steps: 151, steps per second: 111, episode reward: -150.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.318 [0.000, 2.000],  loss: 0.817561, mae: 12.152016, mean_q: -17.412229, mean_eps: 0.050000\n",
            " 172481/200000: episode: 1052, duration: 0.757s, episode steps: 108, steps per second: 143, episode reward: -107.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.861 [0.000, 2.000],  loss: 0.879068, mae: 12.206033, mean_q: -17.486559, mean_eps: 0.050000\n",
            " 172618/200000: episode: 1053, duration: 0.806s, episode steps: 137, steps per second: 170, episode reward: -136.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.949 [0.000, 2.000],  loss: 0.871220, mae: 12.203026, mean_q: -17.490809, mean_eps: 0.050000\n",
            " 172706/200000: episode: 1054, duration: 0.537s, episode steps:  88, steps per second: 164, episode reward: -87.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.136 [0.000, 2.000],  loss: 0.833437, mae: 12.208376, mean_q: -17.517134, mean_eps: 0.050000\n",
            " 173206/200000: episode: 1055, duration: 3.050s, episode steps: 500, steps per second: 164, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.460 [0.000, 2.000],  loss: 0.839262, mae: 12.142795, mean_q: -17.408439, mean_eps: 0.050000\n",
            " 173358/200000: episode: 1056, duration: 0.948s, episode steps: 152, steps per second: 160, episode reward: -151.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.796 [0.000, 2.000],  loss: 0.753927, mae: 12.255318, mean_q: -17.603250, mean_eps: 0.050000\n",
            " 173435/200000: episode: 1057, duration: 0.510s, episode steps:  77, steps per second: 151, episode reward: -76.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.839478, mae: 12.164978, mean_q: -17.418672, mean_eps: 0.050000\n",
            " 173529/200000: episode: 1058, duration: 0.629s, episode steps:  94, steps per second: 149, episode reward: -93.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.777 [0.000, 2.000],  loss: 0.765664, mae: 12.151264, mean_q: -17.414907, mean_eps: 0.050000\n",
            " 173610/200000: episode: 1059, duration: 0.509s, episode steps:  81, steps per second: 159, episode reward: -80.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.062 [0.000, 2.000],  loss: 1.122714, mae: 12.267690, mean_q: -17.513365, mean_eps: 0.050000\n",
            " 173711/200000: episode: 1060, duration: 0.637s, episode steps: 101, steps per second: 159, episode reward: -100.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.941 [0.000, 2.000],  loss: 0.758266, mae: 12.235756, mean_q: -17.568501, mean_eps: 0.050000\n",
            " 173832/200000: episode: 1061, duration: 0.744s, episode steps: 121, steps per second: 163, episode reward: -120.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.281 [0.000, 2.000],  loss: 0.891222, mae: 12.316781, mean_q: -17.654192, mean_eps: 0.050000\n",
            " 173942/200000: episode: 1062, duration: 0.700s, episode steps: 110, steps per second: 157, episode reward: -109.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.800 [0.000, 2.000],  loss: 0.877051, mae: 12.153273, mean_q: -17.398127, mean_eps: 0.050000\n",
            " 174028/200000: episode: 1063, duration: 0.559s, episode steps:  86, steps per second: 154, episode reward: -85.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.140 [0.000, 2.000],  loss: 0.814223, mae: 12.054199, mean_q: -17.272625, mean_eps: 0.050000\n",
            " 174115/200000: episode: 1064, duration: 0.752s, episode steps:  87, steps per second: 116, episode reward: -86.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.126 [0.000, 2.000],  loss: 0.942118, mae: 12.133134, mean_q: -17.357541, mean_eps: 0.050000\n",
            " 174210/200000: episode: 1065, duration: 0.863s, episode steps:  95, steps per second: 110, episode reward: -94.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.853 [0.000, 2.000],  loss: 0.773156, mae: 12.167292, mean_q: -17.455904, mean_eps: 0.050000\n",
            " 174325/200000: episode: 1066, duration: 0.987s, episode steps: 115, steps per second: 116, episode reward: -114.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.835 [0.000, 2.000],  loss: 0.802513, mae: 12.058422, mean_q: -17.286534, mean_eps: 0.050000\n",
            " 174446/200000: episode: 1067, duration: 1.012s, episode steps: 121, steps per second: 120, episode reward: -120.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.950 [0.000, 2.000],  loss: 0.823969, mae: 12.040850, mean_q: -17.263883, mean_eps: 0.050000\n",
            " 174545/200000: episode: 1068, duration: 0.597s, episode steps:  99, steps per second: 166, episode reward: -98.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.909 [0.000, 2.000],  loss: 0.848324, mae: 12.122128, mean_q: -17.380222, mean_eps: 0.050000\n",
            " 174639/200000: episode: 1069, duration: 0.553s, episode steps:  94, steps per second: 170, episode reward: -93.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.074 [0.000, 2.000],  loss: 0.820059, mae: 12.108430, mean_q: -17.377348, mean_eps: 0.050000\n",
            " 174726/200000: episode: 1070, duration: 0.531s, episode steps:  87, steps per second: 164, episode reward: -86.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.218 [0.000, 2.000],  loss: 0.763740, mae: 12.084472, mean_q: -17.350409, mean_eps: 0.050000\n",
            " 174812/200000: episode: 1071, duration: 0.531s, episode steps:  86, steps per second: 162, episode reward: -85.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.965 [0.000, 2.000],  loss: 0.902401, mae: 12.118520, mean_q: -17.363543, mean_eps: 0.050000\n",
            " 174925/200000: episode: 1072, duration: 0.693s, episode steps: 113, steps per second: 163, episode reward: -112.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.920 [0.000, 2.000],  loss: 0.842344, mae: 12.136017, mean_q: -17.405976, mean_eps: 0.050000\n",
            " 175043/200000: episode: 1073, duration: 0.702s, episode steps: 118, steps per second: 168, episode reward: -117.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.093 [0.000, 2.000],  loss: 0.776699, mae: 12.226001, mean_q: -17.543221, mean_eps: 0.050000\n",
            " 175154/200000: episode: 1074, duration: 0.697s, episode steps: 111, steps per second: 159, episode reward: -110.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.063 [0.000, 2.000],  loss: 0.783300, mae: 12.175457, mean_q: -17.489213, mean_eps: 0.050000\n",
            " 175234/200000: episode: 1075, duration: 0.479s, episode steps:  80, steps per second: 167, episode reward: -79.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.850 [0.000, 2.000],  loss: 0.800825, mae: 12.121836, mean_q: -17.416282, mean_eps: 0.050000\n",
            " 175312/200000: episode: 1076, duration: 0.509s, episode steps:  78, steps per second: 153, episode reward: -77.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.833 [0.000, 2.000],  loss: 0.829400, mae: 12.118133, mean_q: -17.358571, mean_eps: 0.050000\n",
            " 175382/200000: episode: 1077, duration: 0.490s, episode steps:  70, steps per second: 143, episode reward: -69.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.229 [0.000, 2.000],  loss: 0.800793, mae: 12.186636, mean_q: -17.474034, mean_eps: 0.050000\n",
            " 175494/200000: episode: 1078, duration: 0.695s, episode steps: 112, steps per second: 161, episode reward: -111.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.946 [0.000, 2.000],  loss: 0.915653, mae: 12.173915, mean_q: -17.437306, mean_eps: 0.050000\n",
            " 175994/200000: episode: 1079, duration: 3.122s, episode steps: 500, steps per second: 160, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.338 [0.000, 2.000],  loss: 0.833232, mae: 12.231124, mean_q: -17.546175, mean_eps: 0.050000\n",
            " 176107/200000: episode: 1080, duration: 0.980s, episode steps: 113, steps per second: 115, episode reward: -112.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.805 [0.000, 2.000],  loss: 0.862304, mae: 12.121323, mean_q: -17.376787, mean_eps: 0.050000\n",
            " 176190/200000: episode: 1081, duration: 0.806s, episode steps:  83, steps per second: 103, episode reward: -82.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.072 [0.000, 2.000],  loss: 0.870760, mae: 12.093290, mean_q: -17.345589, mean_eps: 0.050000\n",
            " 176292/200000: episode: 1082, duration: 0.862s, episode steps: 102, steps per second: 118, episode reward: -101.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.980 [0.000, 2.000],  loss: 0.767958, mae: 12.042158, mean_q: -17.281723, mean_eps: 0.050000\n",
            " 176428/200000: episode: 1083, duration: 1.187s, episode steps: 136, steps per second: 115, episode reward: -135.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.632 [0.000, 2.000],  loss: 0.729465, mae: 12.077489, mean_q: -17.345309, mean_eps: 0.050000\n",
            " 176547/200000: episode: 1084, duration: 0.760s, episode steps: 119, steps per second: 157, episode reward: -118.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.899 [0.000, 2.000],  loss: 0.792728, mae: 12.102111, mean_q: -17.365841, mean_eps: 0.050000\n",
            " 176644/200000: episode: 1085, duration: 0.626s, episode steps:  97, steps per second: 155, episode reward: -96.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.876 [0.000, 2.000],  loss: 0.690115, mae: 12.101071, mean_q: -17.427516, mean_eps: 0.050000\n",
            " 176792/200000: episode: 1086, duration: 0.978s, episode steps: 148, steps per second: 151, episode reward: -147.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.196 [0.000, 2.000],  loss: 0.785300, mae: 12.084269, mean_q: -17.352024, mean_eps: 0.050000\n",
            " 176874/200000: episode: 1087, duration: 0.526s, episode steps:  82, steps per second: 156, episode reward: -81.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.866 [0.000, 2.000],  loss: 0.813623, mae: 12.034164, mean_q: -17.281189, mean_eps: 0.050000\n",
            " 176963/200000: episode: 1088, duration: 0.574s, episode steps:  89, steps per second: 155, episode reward: -88.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.798 [0.000, 2.000],  loss: 0.834633, mae: 12.104501, mean_q: -17.362243, mean_eps: 0.050000\n",
            " 177077/200000: episode: 1089, duration: 0.769s, episode steps: 114, steps per second: 148, episode reward: -113.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.982 [0.000, 2.000],  loss: 0.790625, mae: 12.157797, mean_q: -17.459400, mean_eps: 0.050000\n",
            " 177147/200000: episode: 1090, duration: 0.411s, episode steps:  70, steps per second: 170, episode reward: -69.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.971 [0.000, 2.000],  loss: 0.715085, mae: 12.285432, mean_q: -17.651963, mean_eps: 0.050000\n",
            " 177223/200000: episode: 1091, duration: 0.473s, episode steps:  76, steps per second: 161, episode reward: -75.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.079 [0.000, 2.000],  loss: 0.772965, mae: 12.174146, mean_q: -17.466250, mean_eps: 0.050000\n",
            " 177334/200000: episode: 1092, duration: 0.668s, episode steps: 111, steps per second: 166, episode reward: -110.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.748 [0.000, 2.000],  loss: 0.822587, mae: 12.180860, mean_q: -17.471042, mean_eps: 0.050000\n",
            " 177466/200000: episode: 1093, duration: 0.829s, episode steps: 132, steps per second: 159, episode reward: -131.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.826 [0.000, 2.000],  loss: 0.856557, mae: 12.133409, mean_q: -17.399792, mean_eps: 0.050000\n",
            " 177564/200000: episode: 1094, duration: 0.607s, episode steps:  98, steps per second: 161, episode reward: -97.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.102 [0.000, 2.000],  loss: 0.740312, mae: 12.170191, mean_q: -17.492270, mean_eps: 0.050000\n",
            " 177666/200000: episode: 1095, duration: 0.645s, episode steps: 102, steps per second: 158, episode reward: -101.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.696 [0.000, 2.000],  loss: 0.765656, mae: 12.203413, mean_q: -17.540738, mean_eps: 0.050000\n",
            " 177762/200000: episode: 1096, duration: 0.609s, episode steps:  96, steps per second: 158, episode reward: -95.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.677 [0.000, 2.000],  loss: 0.786748, mae: 12.149538, mean_q: -17.445443, mean_eps: 0.050000\n",
            " 177843/200000: episode: 1097, duration: 0.491s, episode steps:  81, steps per second: 165, episode reward: -80.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.778 [0.000, 2.000],  loss: 0.678157, mae: 12.176801, mean_q: -17.535862, mean_eps: 0.050000\n",
            " 177964/200000: episode: 1098, duration: 0.838s, episode steps: 121, steps per second: 144, episode reward: -120.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.140 [0.000, 2.000],  loss: 0.804954, mae: 12.188096, mean_q: -17.493247, mean_eps: 0.050000\n",
            " 178068/200000: episode: 1099, duration: 0.942s, episode steps: 104, steps per second: 110, episode reward: -103.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.923 [0.000, 2.000],  loss: 0.848009, mae: 12.259444, mean_q: -17.595495, mean_eps: 0.050000\n",
            " 178179/200000: episode: 1100, duration: 1.008s, episode steps: 111, steps per second: 110, episode reward: -110.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.180 [0.000, 2.000],  loss: 0.802894, mae: 12.308572, mean_q: -17.650020, mean_eps: 0.050000\n",
            " 178286/200000: episode: 1101, duration: 0.991s, episode steps: 107, steps per second: 108, episode reward: -106.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.121 [0.000, 2.000],  loss: 0.834031, mae: 12.309253, mean_q: -17.621846, mean_eps: 0.050000\n",
            " 178399/200000: episode: 1102, duration: 0.800s, episode steps: 113, steps per second: 141, episode reward: -112.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.575 [0.000, 2.000],  loss: 0.844896, mae: 12.186270, mean_q: -17.448491, mean_eps: 0.050000\n",
            " 178503/200000: episode: 1103, duration: 0.602s, episode steps: 104, steps per second: 173, episode reward: -103.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.510 [0.000, 2.000],  loss: 0.941267, mae: 12.287902, mean_q: -17.588610, mean_eps: 0.050000\n",
            " 178610/200000: episode: 1104, duration: 0.648s, episode steps: 107, steps per second: 165, episode reward: -106.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.907 [0.000, 2.000],  loss: 0.889816, mae: 12.193628, mean_q: -17.462421, mean_eps: 0.050000\n",
            " 178705/200000: episode: 1105, duration: 0.580s, episode steps:  95, steps per second: 164, episode reward: -94.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.800 [0.000, 2.000],  loss: 0.836403, mae: 12.144195, mean_q: -17.415019, mean_eps: 0.050000\n",
            " 178793/200000: episode: 1106, duration: 0.555s, episode steps:  88, steps per second: 159, episode reward: -87.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.830 [0.000, 2.000],  loss: 0.714983, mae: 12.263012, mean_q: -17.638082, mean_eps: 0.050000\n",
            " 178863/200000: episode: 1107, duration: 0.419s, episode steps:  70, steps per second: 167, episode reward: -69.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.857 [0.000, 2.000],  loss: 0.772274, mae: 12.266559, mean_q: -17.635892, mean_eps: 0.050000\n",
            " 178963/200000: episode: 1108, duration: 0.633s, episode steps: 100, steps per second: 158, episode reward: -99.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.870 [0.000, 2.000],  loss: 0.859434, mae: 12.195838, mean_q: -17.519531, mean_eps: 0.050000\n",
            " 179083/200000: episode: 1109, duration: 0.769s, episode steps: 120, steps per second: 156, episode reward: -119.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.733 [0.000, 2.000],  loss: 0.788083, mae: 12.139100, mean_q: -17.438323, mean_eps: 0.050000\n",
            " 179160/200000: episode: 1110, duration: 0.477s, episode steps:  77, steps per second: 162, episode reward: -76.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.870 [0.000, 2.000],  loss: 0.864689, mae: 12.192632, mean_q: -17.463131, mean_eps: 0.050000\n",
            " 179660/200000: episode: 1111, duration: 3.023s, episode steps: 500, steps per second: 165, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.346 [0.000, 2.000],  loss: 0.793170, mae: 12.200780, mean_q: -17.513008, mean_eps: 0.050000\n",
            " 179788/200000: episode: 1112, duration: 0.804s, episode steps: 128, steps per second: 159, episode reward: -127.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.602 [0.000, 2.000],  loss: 0.865770, mae: 12.214591, mean_q: -17.535910, mean_eps: 0.050000\n",
            " 179894/200000: episode: 1113, duration: 0.652s, episode steps: 106, steps per second: 163, episode reward: -105.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.142 [0.000, 2.000],  loss: 0.757720, mae: 12.063768, mean_q: -17.336495, mean_eps: 0.050000\n",
            " 179993/200000: episode: 1114, duration: 0.741s, episode steps:  99, steps per second: 134, episode reward: -98.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.162 [0.000, 2.000],  loss: 0.965870, mae: 12.172280, mean_q: -17.435651, mean_eps: 0.050000\n",
            " 180125/200000: episode: 1115, duration: 1.219s, episode steps: 132, steps per second: 108, episode reward: -131.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.106 [0.000, 2.000],  loss: 0.781152, mae: 11.999831, mean_q: -17.216459, mean_eps: 0.050000\n",
            " 180246/200000: episode: 1116, duration: 0.979s, episode steps: 121, steps per second: 124, episode reward: -120.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.797225, mae: 12.084046, mean_q: -17.338052, mean_eps: 0.050000\n",
            " 180356/200000: episode: 1117, duration: 0.995s, episode steps: 110, steps per second: 111, episode reward: -109.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.809 [0.000, 2.000],  loss: 0.871516, mae: 12.265146, mean_q: -17.598370, mean_eps: 0.050000\n",
            " 180437/200000: episode: 1118, duration: 0.559s, episode steps:  81, steps per second: 145, episode reward: -80.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.938 [0.000, 2.000],  loss: 0.927251, mae: 12.002637, mean_q: -17.194543, mean_eps: 0.050000\n",
            " 180512/200000: episode: 1119, duration: 0.480s, episode steps:  75, steps per second: 156, episode reward: -74.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.667 [0.000, 2.000],  loss: 0.765406, mae: 12.095857, mean_q: -17.367877, mean_eps: 0.050000\n",
            " 180695/200000: episode: 1120, duration: 1.153s, episode steps: 183, steps per second: 159, episode reward: -182.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.831 [0.000, 2.000],  loss: 0.735368, mae: 12.130969, mean_q: -17.446663, mean_eps: 0.050000\n",
            " 180789/200000: episode: 1121, duration: 0.590s, episode steps:  94, steps per second: 159, episode reward: -93.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.979 [0.000, 2.000],  loss: 0.840948, mae: 12.192563, mean_q: -17.502915, mean_eps: 0.050000\n",
            " 180860/200000: episode: 1122, duration: 0.440s, episode steps:  71, steps per second: 161, episode reward: -70.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.817 [0.000, 2.000],  loss: 0.712720, mae: 12.144272, mean_q: -17.437932, mean_eps: 0.050000\n",
            " 180962/200000: episode: 1123, duration: 0.667s, episode steps: 102, steps per second: 153, episode reward: -101.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.627 [0.000, 2.000],  loss: 0.823183, mae: 12.128340, mean_q: -17.384686, mean_eps: 0.050000\n",
            " 181089/200000: episode: 1124, duration: 0.795s, episode steps: 127, steps per second: 160, episode reward: -126.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.173 [0.000, 2.000],  loss: 0.781704, mae: 12.123382, mean_q: -17.376965, mean_eps: 0.050000\n",
            " 181173/200000: episode: 1125, duration: 0.514s, episode steps:  84, steps per second: 164, episode reward: -83.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.905 [0.000, 2.000],  loss: 0.778532, mae: 12.144518, mean_q: -17.439935, mean_eps: 0.050000\n",
            " 181281/200000: episode: 1126, duration: 0.720s, episode steps: 108, steps per second: 150, episode reward: -107.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.157 [0.000, 2.000],  loss: 0.724030, mae: 12.156982, mean_q: -17.489816, mean_eps: 0.050000\n",
            " 181381/200000: episode: 1127, duration: 0.620s, episode steps: 100, steps per second: 161, episode reward: -99.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.100 [0.000, 2.000],  loss: 0.842649, mae: 12.131342, mean_q: -17.385895, mean_eps: 0.050000\n",
            " 181453/200000: episode: 1128, duration: 0.487s, episode steps:  72, steps per second: 148, episode reward: -71.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.722 [0.000, 2.000],  loss: 0.833309, mae: 12.171802, mean_q: -17.444392, mean_eps: 0.050000\n",
            " 181537/200000: episode: 1129, duration: 0.512s, episode steps:  84, steps per second: 164, episode reward: -83.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.893 [0.000, 2.000],  loss: 0.847650, mae: 12.103856, mean_q: -17.341482, mean_eps: 0.050000\n",
            " 181624/200000: episode: 1130, duration: 0.561s, episode steps:  87, steps per second: 155, episode reward: -86.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.690 [0.000, 2.000],  loss: 0.867398, mae: 12.014283, mean_q: -17.194904, mean_eps: 0.050000\n",
            " 181704/200000: episode: 1131, duration: 0.517s, episode steps:  80, steps per second: 155, episode reward: -79.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.800 [0.000, 2.000],  loss: 0.888315, mae: 12.057747, mean_q: -17.275622, mean_eps: 0.050000\n",
            " 181799/200000: episode: 1132, duration: 0.634s, episode steps:  95, steps per second: 150, episode reward: -94.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.642 [0.000, 2.000],  loss: 0.825893, mae: 12.022818, mean_q: -17.221998, mean_eps: 0.050000\n",
            " 181878/200000: episode: 1133, duration: 0.541s, episode steps:  79, steps per second: 146, episode reward: -78.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.848 [0.000, 2.000],  loss: 0.756101, mae: 12.011429, mean_q: -17.219600, mean_eps: 0.050000\n",
            " 181965/200000: episode: 1134, duration: 0.760s, episode steps:  87, steps per second: 114, episode reward: -86.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.552 [0.000, 2.000],  loss: 0.880708, mae: 12.165907, mean_q: -17.434301, mean_eps: 0.050000\n",
            " 182091/200000: episode: 1135, duration: 1.203s, episode steps: 126, steps per second: 105, episode reward: -125.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.071 [0.000, 2.000],  loss: 0.857611, mae: 12.250507, mean_q: -17.576931, mean_eps: 0.050000\n",
            " 182192/200000: episode: 1136, duration: 0.933s, episode steps: 101, steps per second: 108, episode reward: -100.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.980 [0.000, 2.000],  loss: 0.815906, mae: 12.257407, mean_q: -17.586357, mean_eps: 0.050000\n",
            " 182286/200000: episode: 1137, duration: 0.867s, episode steps:  94, steps per second: 108, episode reward: -93.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.138 [0.000, 2.000],  loss: 0.871451, mae: 12.403783, mean_q: -17.789403, mean_eps: 0.050000\n",
            " 182384/200000: episode: 1138, duration: 0.642s, episode steps:  98, steps per second: 153, episode reward: -97.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.071 [0.000, 2.000],  loss: 0.871889, mae: 12.264574, mean_q: -17.611616, mean_eps: 0.050000\n",
            " 182471/200000: episode: 1139, duration: 0.571s, episode steps:  87, steps per second: 152, episode reward: -86.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.092 [0.000, 2.000],  loss: 0.762617, mae: 12.128296, mean_q: -17.410353, mean_eps: 0.050000\n",
            " 182569/200000: episode: 1140, duration: 0.700s, episode steps:  98, steps per second: 140, episode reward: -97.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.561 [0.000, 2.000],  loss: 0.859577, mae: 12.306374, mean_q: -17.653722, mean_eps: 0.050000\n",
            " 182662/200000: episode: 1141, duration: 0.627s, episode steps:  93, steps per second: 148, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.882 [0.000, 2.000],  loss: 0.901205, mae: 12.349120, mean_q: -17.689014, mean_eps: 0.050000\n",
            " 182745/200000: episode: 1142, duration: 0.530s, episode steps:  83, steps per second: 157, episode reward: -82.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.867 [0.000, 2.000],  loss: 0.886103, mae: 12.134749, mean_q: -17.388151, mean_eps: 0.050000\n",
            " 182865/200000: episode: 1143, duration: 0.740s, episode steps: 120, steps per second: 162, episode reward: -119.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.725 [0.000, 2.000],  loss: 0.861871, mae: 12.180603, mean_q: -17.478714, mean_eps: 0.050000\n",
            " 182944/200000: episode: 1144, duration: 0.505s, episode steps:  79, steps per second: 156, episode reward: -78.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.696 [0.000, 2.000],  loss: 0.950334, mae: 12.288924, mean_q: -17.585694, mean_eps: 0.050000\n",
            " 183015/200000: episode: 1145, duration: 0.529s, episode steps:  71, steps per second: 134, episode reward: -70.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.690 [0.000, 2.000],  loss: 0.807599, mae: 12.107225, mean_q: -17.352596, mean_eps: 0.050000\n",
            " 183136/200000: episode: 1146, duration: 0.880s, episode steps: 121, steps per second: 137, episode reward: -120.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.884 [0.000, 2.000],  loss: 0.862037, mae: 12.225437, mean_q: -17.518634, mean_eps: 0.050000\n",
            " 183247/200000: episode: 1147, duration: 0.739s, episode steps: 111, steps per second: 150, episode reward: -110.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.126 [0.000, 2.000],  loss: 0.883594, mae: 12.203693, mean_q: -17.484333, mean_eps: 0.050000\n",
            " 183348/200000: episode: 1148, duration: 0.685s, episode steps: 101, steps per second: 147, episode reward: -100.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.960 [0.000, 2.000],  loss: 0.788004, mae: 12.076955, mean_q: -17.341627, mean_eps: 0.050000\n",
            " 183477/200000: episode: 1149, duration: 0.818s, episode steps: 129, steps per second: 158, episode reward: -128.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.860 [0.000, 2.000],  loss: 0.811309, mae: 12.158959, mean_q: -17.474085, mean_eps: 0.050000\n",
            " 183663/200000: episode: 1150, duration: 1.131s, episode steps: 186, steps per second: 164, episode reward: -185.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.774 [0.000, 2.000],  loss: 0.854553, mae: 12.226522, mean_q: -17.549101, mean_eps: 0.050000\n",
            " 184163/200000: episode: 1151, duration: 4.135s, episode steps: 500, steps per second: 121, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.186 [0.000, 2.000],  loss: 0.858030, mae: 12.205808, mean_q: -17.518734, mean_eps: 0.050000\n",
            " 184255/200000: episode: 1152, duration: 0.541s, episode steps:  92, steps per second: 170, episode reward: -91.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.967 [0.000, 2.000],  loss: 0.849767, mae: 12.231561, mean_q: -17.544168, mean_eps: 0.050000\n",
            " 184349/200000: episode: 1153, duration: 0.605s, episode steps:  94, steps per second: 155, episode reward: -93.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.926 [0.000, 2.000],  loss: 0.915672, mae: 12.275526, mean_q: -17.580320, mean_eps: 0.050000\n",
            " 184450/200000: episode: 1154, duration: 0.585s, episode steps: 101, steps per second: 173, episode reward: -100.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.822 [0.000, 2.000],  loss: 0.912253, mae: 12.164281, mean_q: -17.406853, mean_eps: 0.050000\n",
            " 184575/200000: episode: 1155, duration: 0.767s, episode steps: 125, steps per second: 163, episode reward: -124.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.976 [0.000, 2.000],  loss: 0.908650, mae: 12.223464, mean_q: -17.504178, mean_eps: 0.050000\n",
            " 184692/200000: episode: 1156, duration: 0.721s, episode steps: 117, steps per second: 162, episode reward: -116.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.145 [0.000, 2.000],  loss: 0.950600, mae: 12.217808, mean_q: -17.508251, mean_eps: 0.050000\n",
            " 184784/200000: episode: 1157, duration: 0.556s, episode steps:  92, steps per second: 165, episode reward: -91.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.033 [0.000, 2.000],  loss: 0.926401, mae: 12.191505, mean_q: -17.451938, mean_eps: 0.050000\n",
            " 184868/200000: episode: 1158, duration: 0.523s, episode steps:  84, steps per second: 161, episode reward: -83.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.071 [0.000, 2.000],  loss: 0.906269, mae: 12.237652, mean_q: -17.521883, mean_eps: 0.050000\n",
            " 184956/200000: episode: 1159, duration: 0.545s, episode steps:  88, steps per second: 162, episode reward: -87.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.818 [0.000, 2.000],  loss: 0.909704, mae: 12.215102, mean_q: -17.482707, mean_eps: 0.050000\n",
            " 185056/200000: episode: 1160, duration: 0.632s, episode steps: 100, steps per second: 158, episode reward: -99.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.800 [0.000, 2.000],  loss: 0.946319, mae: 12.225979, mean_q: -17.508236, mean_eps: 0.050000\n",
            " 185139/200000: episode: 1161, duration: 0.544s, episode steps:  83, steps per second: 153, episode reward: -82.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.976 [0.000, 2.000],  loss: 0.788249, mae: 12.265725, mean_q: -17.612445, mean_eps: 0.050000\n",
            " 185639/200000: episode: 1162, duration: 3.083s, episode steps: 500, steps per second: 162, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.276 [0.000, 2.000],  loss: 0.876506, mae: 12.229436, mean_q: -17.501663, mean_eps: 0.050000\n",
            " 185736/200000: episode: 1163, duration: 0.596s, episode steps:  97, steps per second: 163, episode reward: -96.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.175 [0.000, 2.000],  loss: 0.984552, mae: 12.272729, mean_q: -17.566653, mean_eps: 0.050000\n",
            " 185866/200000: episode: 1164, duration: 1.029s, episode steps: 130, steps per second: 126, episode reward: -129.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.123 [0.000, 2.000],  loss: 0.843267, mae: 12.195235, mean_q: -17.488498, mean_eps: 0.050000\n",
            " 185962/200000: episode: 1165, duration: 0.866s, episode steps:  96, steps per second: 111, episode reward: -95.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.260 [0.000, 2.000],  loss: 0.871619, mae: 12.131177, mean_q: -17.379919, mean_eps: 0.050000\n",
            " 186083/200000: episode: 1166, duration: 1.046s, episode steps: 121, steps per second: 116, episode reward: -120.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.033 [0.000, 2.000],  loss: 1.007311, mae: 12.117321, mean_q: -17.298797, mean_eps: 0.050000\n",
            " 186172/200000: episode: 1167, duration: 0.788s, episode steps:  89, steps per second: 113, episode reward: -88.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.899 [0.000, 2.000],  loss: 0.886763, mae: 12.170278, mean_q: -17.414208, mean_eps: 0.050000\n",
            " 186672/200000: episode: 1168, duration: 3.006s, episode steps: 500, steps per second: 166, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.262 [0.000, 2.000],  loss: 0.853860, mae: 12.103445, mean_q: -17.320806, mean_eps: 0.050000\n",
            " 186766/200000: episode: 1169, duration: 0.611s, episode steps:  94, steps per second: 154, episode reward: -93.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.979 [0.000, 2.000],  loss: 0.861140, mae: 12.106008, mean_q: -17.359752, mean_eps: 0.050000\n",
            " 186864/200000: episode: 1170, duration: 0.648s, episode steps:  98, steps per second: 151, episode reward: -97.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.816 [0.000, 2.000],  loss: 0.822960, mae: 12.146280, mean_q: -17.424382, mean_eps: 0.050000\n",
            " 186995/200000: episode: 1171, duration: 0.836s, episode steps: 131, steps per second: 157, episode reward: -130.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.939 [0.000, 2.000],  loss: 0.917128, mae: 12.102606, mean_q: -17.353761, mean_eps: 0.050000\n",
            " 187152/200000: episode: 1172, duration: 0.947s, episode steps: 157, steps per second: 166, episode reward: -156.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.968 [0.000, 2.000],  loss: 0.920516, mae: 12.167161, mean_q: -17.450458, mean_eps: 0.050000\n",
            " 187652/200000: episode: 1173, duration: 3.123s, episode steps: 500, steps per second: 160, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.312 [0.000, 2.000],  loss: 0.860571, mae: 12.191791, mean_q: -17.498271, mean_eps: 0.050000\n",
            " 187766/200000: episode: 1174, duration: 0.778s, episode steps: 114, steps per second: 146, episode reward: -113.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.807 [0.000, 2.000],  loss: 0.879649, mae: 12.142918, mean_q: -17.388461, mean_eps: 0.050000\n",
            " 187855/200000: episode: 1175, duration: 0.759s, episode steps:  89, steps per second: 117, episode reward: -88.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.944 [0.000, 2.000],  loss: 0.926724, mae: 12.239370, mean_q: -17.538251, mean_eps: 0.050000\n",
            " 187954/200000: episode: 1176, duration: 0.931s, episode steps:  99, steps per second: 106, episode reward: -98.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.929 [0.000, 2.000],  loss: 0.820905, mae: 12.239952, mean_q: -17.552491, mean_eps: 0.050000\n",
            " 188075/200000: episode: 1177, duration: 1.067s, episode steps: 121, steps per second: 113, episode reward: -120.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.314 [0.000, 2.000],  loss: 0.883905, mae: 12.148202, mean_q: -17.402209, mean_eps: 0.050000\n",
            " 188225/200000: episode: 1178, duration: 1.189s, episode steps: 150, steps per second: 126, episode reward: -149.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.227 [0.000, 2.000],  loss: 0.862132, mae: 12.180404, mean_q: -17.456984, mean_eps: 0.050000\n",
            " 188332/200000: episode: 1179, duration: 0.705s, episode steps: 107, steps per second: 152, episode reward: -106.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.075 [0.000, 2.000],  loss: 0.989074, mae: 12.267890, mean_q: -17.560881, mean_eps: 0.050000\n",
            " 188412/200000: episode: 1180, duration: 0.545s, episode steps:  80, steps per second: 147, episode reward: -79.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.012 [0.000, 2.000],  loss: 0.853534, mae: 12.243695, mean_q: -17.543713, mean_eps: 0.050000\n",
            " 188534/200000: episode: 1181, duration: 0.830s, episode steps: 122, steps per second: 147, episode reward: -121.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.795 [0.000, 2.000],  loss: 0.865909, mae: 12.245492, mean_q: -17.541090, mean_eps: 0.050000\n",
            " 188657/200000: episode: 1182, duration: 0.796s, episode steps: 123, steps per second: 155, episode reward: -122.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.106 [0.000, 2.000],  loss: 0.846399, mae: 12.177997, mean_q: -17.455230, mean_eps: 0.050000\n",
            " 188740/200000: episode: 1183, duration: 0.527s, episode steps:  83, steps per second: 157, episode reward: -82.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.916 [0.000, 2.000],  loss: 0.847558, mae: 12.115092, mean_q: -17.330308, mean_eps: 0.050000\n",
            " 188857/200000: episode: 1184, duration: 0.775s, episode steps: 117, steps per second: 151, episode reward: -116.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.154 [0.000, 2.000],  loss: 0.856689, mae: 12.167838, mean_q: -17.433591, mean_eps: 0.050000\n",
            " 188963/200000: episode: 1185, duration: 0.703s, episode steps: 106, steps per second: 151, episode reward: -105.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.283 [0.000, 2.000],  loss: 0.841617, mae: 12.077477, mean_q: -17.281261, mean_eps: 0.050000\n",
            " 189463/200000: episode: 1186, duration: 3.261s, episode steps: 500, steps per second: 153, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.394 [0.000, 2.000],  loss: 0.843018, mae: 12.191692, mean_q: -17.486426, mean_eps: 0.050000\n",
            " 189553/200000: episode: 1187, duration: 0.574s, episode steps:  90, steps per second: 157, episode reward: -89.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.867 [0.000, 2.000],  loss: 0.940288, mae: 12.228825, mean_q: -17.511139, mean_eps: 0.050000\n",
            " 189637/200000: episode: 1188, duration: 0.507s, episode steps:  84, steps per second: 166, episode reward: -83.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.679 [0.000, 2.000],  loss: 0.739621, mae: 12.307879, mean_q: -17.714144, mean_eps: 0.050000\n",
            " 189712/200000: episode: 1189, duration: 0.605s, episode steps:  75, steps per second: 124, episode reward: -74.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.827 [0.000, 2.000],  loss: 0.872469, mae: 12.299877, mean_q: -17.657847, mean_eps: 0.050000\n",
            " 189804/200000: episode: 1190, duration: 0.882s, episode steps:  92, steps per second: 104, episode reward: -91.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.967 [0.000, 2.000],  loss: 0.830446, mae: 12.202769, mean_q: -17.506849, mean_eps: 0.050000\n",
            " 189882/200000: episode: 1191, duration: 0.722s, episode steps:  78, steps per second: 108, episode reward: -77.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.859 [0.000, 2.000],  loss: 1.036514, mae: 12.171896, mean_q: -17.418759, mean_eps: 0.050000\n",
            " 190005/200000: episode: 1192, duration: 1.106s, episode steps: 123, steps per second: 111, episode reward: -122.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.748 [0.000, 2.000],  loss: 0.890312, mae: 12.194642, mean_q: -17.484290, mean_eps: 0.050000\n",
            " 190121/200000: episode: 1193, duration: 0.865s, episode steps: 116, steps per second: 134, episode reward: -115.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.147 [0.000, 2.000],  loss: 0.899342, mae: 12.236040, mean_q: -17.548297, mean_eps: 0.050000\n",
            " 190218/200000: episode: 1194, duration: 0.588s, episode steps:  97, steps per second: 165, episode reward: -96.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.979 [0.000, 2.000],  loss: 0.929007, mae: 12.183316, mean_q: -17.462602, mean_eps: 0.050000\n",
            " 190314/200000: episode: 1195, duration: 0.600s, episode steps:  96, steps per second: 160, episode reward: -95.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.812 [0.000, 2.000],  loss: 0.790283, mae: 12.067737, mean_q: -17.279240, mean_eps: 0.050000\n",
            " 190403/200000: episode: 1196, duration: 0.535s, episode steps:  89, steps per second: 166, episode reward: -88.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.876 [0.000, 2.000],  loss: 0.879190, mae: 12.104550, mean_q: -17.347017, mean_eps: 0.050000\n",
            " 190489/200000: episode: 1197, duration: 0.562s, episode steps:  86, steps per second: 153, episode reward: -85.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.105 [0.000, 2.000],  loss: 0.928015, mae: 12.122775, mean_q: -17.372689, mean_eps: 0.050000\n",
            " 190573/200000: episode: 1198, duration: 0.523s, episode steps:  84, steps per second: 161, episode reward: -83.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.607 [0.000, 2.000],  loss: 0.956637, mae: 12.257017, mean_q: -17.552011, mean_eps: 0.050000\n",
            " 190655/200000: episode: 1199, duration: 0.546s, episode steps:  82, steps per second: 150, episode reward: -81.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.780 [0.000, 2.000],  loss: 0.948810, mae: 12.163376, mean_q: -17.390573, mean_eps: 0.050000\n",
            " 190779/200000: episode: 1200, duration: 0.828s, episode steps: 124, steps per second: 150, episode reward: -123.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.032 [0.000, 2.000],  loss: 0.877503, mae: 12.207343, mean_q: -17.514918, mean_eps: 0.050000\n",
            " 190855/200000: episode: 1201, duration: 0.503s, episode steps:  76, steps per second: 151, episode reward: -75.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.947 [0.000, 2.000],  loss: 0.803818, mae: 12.274887, mean_q: -17.607798, mean_eps: 0.050000\n",
            " 190951/200000: episode: 1202, duration: 0.651s, episode steps:  96, steps per second: 148, episode reward: -95.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.073 [0.000, 2.000],  loss: 0.820065, mae: 12.216283, mean_q: -17.539187, mean_eps: 0.050000\n",
            " 191084/200000: episode: 1203, duration: 0.887s, episode steps: 133, steps per second: 150, episode reward: -132.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.053 [0.000, 2.000],  loss: 0.897310, mae: 12.097168, mean_q: -17.338235, mean_eps: 0.050000\n",
            " 191210/200000: episode: 1204, duration: 0.807s, episode steps: 126, steps per second: 156, episode reward: -125.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.690 [0.000, 2.000],  loss: 0.898086, mae: 12.236583, mean_q: -17.551088, mean_eps: 0.050000\n",
            " 191710/200000: episode: 1205, duration: 3.603s, episode steps: 500, steps per second: 139, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.512 [0.000, 2.000],  loss: 0.844276, mae: 12.157561, mean_q: -17.439003, mean_eps: 0.050000\n",
            " 191795/200000: episode: 1206, duration: 0.737s, episode steps:  85, steps per second: 115, episode reward: -84.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.918 [0.000, 2.000],  loss: 0.965082, mae: 12.120041, mean_q: -17.332990, mean_eps: 0.050000\n",
            " 191904/200000: episode: 1207, duration: 1.051s, episode steps: 109, steps per second: 104, episode reward: -108.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.881 [0.000, 2.000],  loss: 0.978337, mae: 12.325105, mean_q: -17.654087, mean_eps: 0.050000\n",
            " 192042/200000: episode: 1208, duration: 1.000s, episode steps: 138, steps per second: 138, episode reward: -137.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.906 [0.000, 2.000],  loss: 0.898915, mae: 12.086295, mean_q: -17.311806, mean_eps: 0.050000\n",
            " 192113/200000: episode: 1209, duration: 0.426s, episode steps:  71, steps per second: 167, episode reward: -70.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.704 [0.000, 2.000],  loss: 0.883171, mae: 12.003034, mean_q: -17.186229, mean_eps: 0.050000\n",
            " 192613/200000: episode: 1210, duration: 3.198s, episode steps: 500, steps per second: 156, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.466 [0.000, 2.000],  loss: 0.848973, mae: 12.095490, mean_q: -17.330030, mean_eps: 0.050000\n",
            " 193113/200000: episode: 1211, duration: 3.258s, episode steps: 500, steps per second: 153, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.418 [0.000, 2.000],  loss: 0.857824, mae: 12.132322, mean_q: -17.390762, mean_eps: 0.050000\n",
            " 193229/200000: episode: 1212, duration: 0.755s, episode steps: 116, steps per second: 154, episode reward: -115.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.750 [0.000, 2.000],  loss: 0.801346, mae: 12.105867, mean_q: -17.379988, mean_eps: 0.050000\n",
            " 193345/200000: episode: 1213, duration: 0.723s, episode steps: 116, steps per second: 160, episode reward: -115.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.862 [0.000, 2.000],  loss: 0.851363, mae: 12.151017, mean_q: -17.443177, mean_eps: 0.050000\n",
            " 193845/200000: episode: 1214, duration: 3.787s, episode steps: 500, steps per second: 132, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.446 [0.000, 2.000],  loss: 0.878298, mae: 12.025619, mean_q: -17.231390, mean_eps: 0.050000\n",
            " 193945/200000: episode: 1215, duration: 0.862s, episode steps: 100, steps per second: 116, episode reward: -99.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.550 [0.000, 2.000],  loss: 0.888188, mae: 12.127251, mean_q: -17.379773, mean_eps: 0.050000\n",
            " 194076/200000: episode: 1216, duration: 0.858s, episode steps: 131, steps per second: 153, episode reward: -130.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.817 [0.000, 2.000],  loss: 0.868808, mae: 12.073931, mean_q: -17.291605, mean_eps: 0.050000\n",
            " 194193/200000: episode: 1217, duration: 0.759s, episode steps: 117, steps per second: 154, episode reward: -116.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.017 [0.000, 2.000],  loss: 0.927573, mae: 12.177953, mean_q: -17.463180, mean_eps: 0.050000\n",
            " 194280/200000: episode: 1218, duration: 0.526s, episode steps:  87, steps per second: 165, episode reward: -86.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.770 [0.000, 2.000],  loss: 0.851733, mae: 12.141978, mean_q: -17.377761, mean_eps: 0.050000\n",
            " 194358/200000: episode: 1219, duration: 0.493s, episode steps:  78, steps per second: 158, episode reward: -77.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.705 [0.000, 2.000],  loss: 0.881090, mae: 12.067500, mean_q: -17.297375, mean_eps: 0.050000\n",
            " 194438/200000: episode: 1220, duration: 0.480s, episode steps:  80, steps per second: 167, episode reward: -79.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.825 [0.000, 2.000],  loss: 0.865601, mae: 12.057520, mean_q: -17.285335, mean_eps: 0.050000\n",
            " 194536/200000: episode: 1221, duration: 0.611s, episode steps:  98, steps per second: 160, episode reward: -97.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.867 [0.000, 2.000],  loss: 0.849463, mae: 11.974264, mean_q: -17.145897, mean_eps: 0.050000\n",
            " 194607/200000: episode: 1222, duration: 0.428s, episode steps:  71, steps per second: 166, episode reward: -70.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.239 [0.000, 2.000],  loss: 0.845199, mae: 12.065789, mean_q: -17.287227, mean_eps: 0.050000\n",
            " 194687/200000: episode: 1223, duration: 0.479s, episode steps:  80, steps per second: 167, episode reward: -79.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.938 [0.000, 2.000],  loss: 0.928239, mae: 12.267208, mean_q: -17.551733, mean_eps: 0.050000\n",
            " 194788/200000: episode: 1224, duration: 0.632s, episode steps: 101, steps per second: 160, episode reward: -100.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.693 [0.000, 2.000],  loss: 0.930926, mae: 12.168623, mean_q: -17.419031, mean_eps: 0.050000\n",
            " 194899/200000: episode: 1225, duration: 0.713s, episode steps: 111, steps per second: 156, episode reward: -110.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.027 [0.000, 2.000],  loss: 0.908766, mae: 12.040384, mean_q: -17.245744, mean_eps: 0.050000\n",
            " 195011/200000: episode: 1226, duration: 0.681s, episode steps: 112, steps per second: 164, episode reward: -111.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.875 [0.000, 2.000],  loss: 0.871505, mae: 11.968561, mean_q: -17.157294, mean_eps: 0.050000\n",
            " 195101/200000: episode: 1227, duration: 0.560s, episode steps:  90, steps per second: 161, episode reward: -89.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.922 [0.000, 2.000],  loss: 0.778900, mae: 12.052187, mean_q: -17.281817, mean_eps: 0.050000\n",
            " 195222/200000: episode: 1228, duration: 0.746s, episode steps: 121, steps per second: 162, episode reward: -120.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.050 [0.000, 2.000],  loss: 0.767418, mae: 12.013282, mean_q: -17.237650, mean_eps: 0.050000\n",
            " 195359/200000: episode: 1229, duration: 0.857s, episode steps: 137, steps per second: 160, episode reward: -136.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.861 [0.000, 2.000],  loss: 0.891536, mae: 12.048664, mean_q: -17.278360, mean_eps: 0.050000\n",
            " 195471/200000: episode: 1230, duration: 0.670s, episode steps: 112, steps per second: 167, episode reward: -111.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.286 [0.000, 2.000],  loss: 0.834099, mae: 12.028382, mean_q: -17.260834, mean_eps: 0.050000\n",
            " 195593/200000: episode: 1231, duration: 0.962s, episode steps: 122, steps per second: 127, episode reward: -121.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.213 [0.000, 2.000],  loss: 0.916207, mae: 12.061245, mean_q: -17.279662, mean_eps: 0.050000\n",
            " 195700/200000: episode: 1232, duration: 0.966s, episode steps: 107, steps per second: 111, episode reward: -106.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.953 [0.000, 2.000],  loss: 0.868455, mae: 12.017090, mean_q: -17.196967, mean_eps: 0.050000\n",
            " 195815/200000: episode: 1233, duration: 0.972s, episode steps: 115, steps per second: 118, episode reward: -114.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.026 [0.000, 2.000],  loss: 0.794534, mae: 12.132408, mean_q: -17.415850, mean_eps: 0.050000\n",
            " 195917/200000: episode: 1234, duration: 0.891s, episode steps: 102, steps per second: 115, episode reward: -101.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.010 [0.000, 2.000],  loss: 0.711585, mae: 12.267821, mean_q: -17.633638, mean_eps: 0.050000\n",
            " 196004/200000: episode: 1235, duration: 0.529s, episode steps:  87, steps per second: 165, episode reward: -86.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.874 [0.000, 2.000],  loss: 0.778328, mae: 12.121535, mean_q: -17.409956, mean_eps: 0.050000\n",
            " 196102/200000: episode: 1236, duration: 0.616s, episode steps:  98, steps per second: 159, episode reward: -97.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.735 [0.000, 2.000],  loss: 0.745268, mae: 12.043242, mean_q: -17.285394, mean_eps: 0.050000\n",
            " 196217/200000: episode: 1237, duration: 0.714s, episode steps: 115, steps per second: 161, episode reward: -114.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.174 [0.000, 2.000],  loss: 0.932209, mae: 12.010911, mean_q: -17.202063, mean_eps: 0.050000\n",
            " 196306/200000: episode: 1238, duration: 0.547s, episode steps:  89, steps per second: 163, episode reward: -88.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.876 [0.000, 2.000],  loss: 0.831764, mae: 12.037149, mean_q: -17.280757, mean_eps: 0.050000\n",
            " 196414/200000: episode: 1239, duration: 0.673s, episode steps: 108, steps per second: 160, episode reward: -107.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.037 [0.000, 2.000],  loss: 0.827942, mae: 12.031526, mean_q: -17.268262, mean_eps: 0.050000\n",
            " 196518/200000: episode: 1240, duration: 0.636s, episode steps: 104, steps per second: 164, episode reward: -103.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.990 [0.000, 2.000],  loss: 0.694765, mae: 12.019227, mean_q: -17.282242, mean_eps: 0.050000\n",
            " 196622/200000: episode: 1241, duration: 0.650s, episode steps: 104, steps per second: 160, episode reward: -103.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.942 [0.000, 2.000],  loss: 0.787363, mae: 12.217418, mean_q: -17.552223, mean_eps: 0.050000\n",
            " 196755/200000: episode: 1242, duration: 0.793s, episode steps: 133, steps per second: 168, episode reward: -132.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.992 [0.000, 2.000],  loss: 0.831556, mae: 12.128586, mean_q: -17.419957, mean_eps: 0.050000\n",
            " 196842/200000: episode: 1243, duration: 0.521s, episode steps:  87, steps per second: 167, episode reward: -86.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.057 [0.000, 2.000],  loss: 0.741867, mae: 12.084818, mean_q: -17.375378, mean_eps: 0.050000\n",
            " 196977/200000: episode: 1244, duration: 0.845s, episode steps: 135, steps per second: 160, episode reward: -134.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.978 [0.000, 2.000],  loss: 0.837120, mae: 12.054917, mean_q: -17.307564, mean_eps: 0.050000\n",
            " 197101/200000: episode: 1245, duration: 0.743s, episode steps: 124, steps per second: 167, episode reward: -123.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.911 [0.000, 2.000],  loss: 0.859062, mae: 11.895378, mean_q: -17.049363, mean_eps: 0.050000\n",
            " 197184/200000: episode: 1246, duration: 0.521s, episode steps:  83, steps per second: 159, episode reward: -82.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.867 [0.000, 2.000],  loss: 0.806967, mae: 11.911762, mean_q: -17.080466, mean_eps: 0.050000\n",
            " 197268/200000: episode: 1247, duration: 0.568s, episode steps:  84, steps per second: 148, episode reward: -83.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.881 [0.000, 2.000],  loss: 0.869543, mae: 12.049916, mean_q: -17.273539, mean_eps: 0.050000\n",
            " 197368/200000: episode: 1248, duration: 0.611s, episode steps: 100, steps per second: 164, episode reward: -99.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.890 [0.000, 2.000],  loss: 0.736679, mae: 11.966342, mean_q: -17.168220, mean_eps: 0.050000\n",
            " 197483/200000: episode: 1249, duration: 0.703s, episode steps: 115, steps per second: 163, episode reward: -114.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.591 [0.000, 2.000],  loss: 0.852150, mae: 12.042803, mean_q: -17.253464, mean_eps: 0.050000\n",
            " 197552/200000: episode: 1250, duration: 0.561s, episode steps:  69, steps per second: 123, episode reward: -68.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.203 [0.000, 2.000],  loss: 0.937904, mae: 12.043650, mean_q: -17.244400, mean_eps: 0.050000\n",
            " 197637/200000: episode: 1251, duration: 0.765s, episode steps:  85, steps per second: 111, episode reward: -84.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.835 [0.000, 2.000],  loss: 0.876769, mae: 11.948539, mean_q: -17.115725, mean_eps: 0.050000\n",
            " 197737/200000: episode: 1252, duration: 0.874s, episode steps: 100, steps per second: 114, episode reward: -99.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.940 [0.000, 2.000],  loss: 0.816403, mae: 11.977109, mean_q: -17.188664, mean_eps: 0.050000\n",
            " 197820/200000: episode: 1253, duration: 0.718s, episode steps:  83, steps per second: 116, episode reward: -82.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.747 [0.000, 2.000],  loss: 0.878193, mae: 12.018218, mean_q: -17.231756, mean_eps: 0.050000\n",
            " 197918/200000: episode: 1254, duration: 0.880s, episode steps:  98, steps per second: 111, episode reward: -97.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.724 [0.000, 2.000],  loss: 0.931707, mae: 12.137059, mean_q: -17.384375, mean_eps: 0.050000\n",
            " 198017/200000: episode: 1255, duration: 0.629s, episode steps:  99, steps per second: 157, episode reward: -98.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.051 [0.000, 2.000],  loss: 0.787921, mae: 12.049003, mean_q: -17.280905, mean_eps: 0.050000\n",
            " 198112/200000: episode: 1256, duration: 0.637s, episode steps:  95, steps per second: 149, episode reward: -94.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.979 [0.000, 2.000],  loss: 0.865035, mae: 12.010924, mean_q: -17.201092, mean_eps: 0.050000\n",
            " 198612/200000: episode: 1257, duration: 3.171s, episode steps: 500, steps per second: 158, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.852 [0.000, 2.000],  loss: 0.850504, mae: 12.041340, mean_q: -17.272538, mean_eps: 0.050000\n",
            " 198707/200000: episode: 1258, duration: 0.587s, episode steps:  95, steps per second: 162, episode reward: -94.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.726 [0.000, 2.000],  loss: 0.813929, mae: 11.974548, mean_q: -17.172722, mean_eps: 0.050000\n",
            " 198808/200000: episode: 1259, duration: 0.639s, episode steps: 101, steps per second: 158, episode reward: -100.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.743 [0.000, 2.000],  loss: 0.819556, mae: 11.975904, mean_q: -17.182524, mean_eps: 0.050000\n",
            " 198911/200000: episode: 1260, duration: 0.630s, episode steps: 103, steps per second: 163, episode reward: -102.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.990 [0.000, 2.000],  loss: 0.801001, mae: 11.897588, mean_q: -17.064898, mean_eps: 0.050000\n",
            " 199022/200000: episode: 1261, duration: 0.678s, episode steps: 111, steps per second: 164, episode reward: -110.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.802 [0.000, 2.000],  loss: 0.880225, mae: 11.941043, mean_q: -17.114958, mean_eps: 0.050000\n",
            " 199152/200000: episode: 1262, duration: 0.816s, episode steps: 130, steps per second: 159, episode reward: -129.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.754 [0.000, 2.000],  loss: 0.793787, mae: 12.017556, mean_q: -17.262396, mean_eps: 0.050000\n",
            " 199260/200000: episode: 1263, duration: 0.689s, episode steps: 108, steps per second: 157, episode reward: -107.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.917 [0.000, 2.000],  loss: 0.802545, mae: 12.052597, mean_q: -17.320665, mean_eps: 0.050000\n",
            " 199375/200000: episode: 1264, duration: 0.727s, episode steps: 115, steps per second: 158, episode reward: -114.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.670 [0.000, 2.000],  loss: 0.840590, mae: 12.049533, mean_q: -17.302135, mean_eps: 0.050000\n",
            " 199509/200000: episode: 1265, duration: 1.003s, episode steps: 134, steps per second: 134, episode reward: -133.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.813 [0.000, 2.000],  loss: 0.812963, mae: 12.159550, mean_q: -17.465097, mean_eps: 0.050000\n",
            " 199610/200000: episode: 1266, duration: 0.922s, episode steps: 101, steps per second: 110, episode reward: -100.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.752 [0.000, 2.000],  loss: 0.844939, mae: 12.047618, mean_q: -17.299605, mean_eps: 0.050000\n",
            " 199696/200000: episode: 1267, duration: 0.830s, episode steps:  86, steps per second: 104, episode reward: -85.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.988 [0.000, 2.000],  loss: 0.773394, mae: 12.001518, mean_q: -17.237587, mean_eps: 0.050000\n",
            " 199810/200000: episode: 1268, duration: 1.079s, episode steps: 114, steps per second: 106, episode reward: -113.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.114 [0.000, 2.000],  loss: 0.724842, mae: 11.982435, mean_q: -17.216815, mean_eps: 0.050000\n",
            " 199937/200000: episode: 1269, duration: 0.888s, episode steps: 127, steps per second: 143, episode reward: -126.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.827 [0.000, 2.000],  loss: 0.800499, mae: 12.054980, mean_q: -17.319416, mean_eps: 0.050000\n",
            "done, took 1240.874 seconds\n",
            "\n",
            "Evaluando DQN...\n",
            "DQN → Recompensa media: -104.55 ± 17.40\n",
            "\n",
            "Entrenando DDQN...\n",
            "Training for 200000 steps ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    500/200000: episode: 1, duration: 1.725s, episode steps: 500, steps per second: 290, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.026 [0.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   1000/200000: episode: 2, duration: 0.767s, episode steps: 500, steps per second: 652, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.992 [0.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   1500/200000: episode: 3, duration: 0.979s, episode steps: 500, steps per second: 511, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.996 [0.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   2000/200000: episode: 4, duration: 1.075s, episode steps: 500, steps per second: 465, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.956 [0.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   2500/200000: episode: 5, duration: 0.970s, episode steps: 500, steps per second: 515, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.934 [0.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   3000/200000: episode: 6, duration: 0.782s, episode steps: 500, steps per second: 639, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.992 [0.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   3500/200000: episode: 7, duration: 0.575s, episode steps: 500, steps per second: 870, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   4000/200000: episode: 8, duration: 0.612s, episode steps: 500, steps per second: 817, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.004 [0.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   4500/200000: episode: 9, duration: 0.614s, episode steps: 500, steps per second: 815, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.938 [0.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   5000/200000: episode: 10, duration: 0.616s, episode steps: 500, steps per second: 811, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.954 [0.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   5500/200000: episode: 11, duration: 6.853s, episode steps: 500, steps per second:  73, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.944 [0.000, 2.000],  loss: 0.306798, mae: 0.682714, mean_q: -0.065169, mean_eps: 0.966750\n",
            "   6000/200000: episode: 12, duration: 3.333s, episode steps: 500, steps per second: 150, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.976 [0.000, 2.000],  loss: 0.123964, mae: 0.751451, mean_q: -0.601369, mean_eps: 0.963596\n",
            "   6500/200000: episode: 13, duration: 2.499s, episode steps: 500, steps per second: 200, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.126821, mae: 1.085349, mean_q: -1.141590, mean_eps: 0.960429\n",
            "   7000/200000: episode: 14, duration: 2.345s, episode steps: 500, steps per second: 213, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.012 [0.000, 2.000],  loss: 0.084815, mae: 1.142408, mean_q: -1.293130, mean_eps: 0.957263\n",
            "   7500/200000: episode: 15, duration: 2.426s, episode steps: 500, steps per second: 206, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.992 [0.000, 2.000],  loss: 0.112597, mae: 1.532308, mean_q: -1.866548, mean_eps: 0.954096\n",
            "   8000/200000: episode: 16, duration: 2.487s, episode steps: 500, steps per second: 201, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.988 [0.000, 2.000],  loss: 0.076143, mae: 1.559278, mean_q: -1.961468, mean_eps: 0.950929\n",
            "   8500/200000: episode: 17, duration: 3.179s, episode steps: 500, steps per second: 157, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.966 [0.000, 2.000],  loss: 0.112359, mae: 1.952836, mean_q: -2.523300, mean_eps: 0.947763\n",
            "   9000/200000: episode: 18, duration: 2.939s, episode steps: 500, steps per second: 170, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.966 [0.000, 2.000],  loss: 0.081389, mae: 1.982947, mean_q: -2.606897, mean_eps: 0.944596\n",
            "   9500/200000: episode: 19, duration: 2.384s, episode steps: 500, steps per second: 210, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.068 [0.000, 2.000],  loss: 0.119606, mae: 2.358346, mean_q: -3.143123, mean_eps: 0.941429\n",
            "  10000/200000: episode: 20, duration: 2.298s, episode steps: 500, steps per second: 218, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.006 [0.000, 2.000],  loss: 0.089075, mae: 2.373407, mean_q: -3.202174, mean_eps: 0.938263\n",
            "  10500/200000: episode: 21, duration: 2.272s, episode steps: 500, steps per second: 220, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.123893, mae: 2.735247, mean_q: -3.717383, mean_eps: 0.935096\n",
            "  11000/200000: episode: 22, duration: 2.920s, episode steps: 500, steps per second: 171, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.028 [0.000, 2.000],  loss: 0.097916, mae: 2.743900, mean_q: -3.767222, mean_eps: 0.931929\n",
            "  11500/200000: episode: 23, duration: 3.015s, episode steps: 500, steps per second: 166, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.066 [0.000, 2.000],  loss: 0.123787, mae: 3.059731, mean_q: -4.218897, mean_eps: 0.928763\n",
            "  11982/200000: episode: 24, duration: 2.217s, episode steps: 482, steps per second: 217, episode reward: -481.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.988 [0.000, 2.000],  loss: 0.105765, mae: 3.066282, mean_q: -4.249693, mean_eps: 0.925647\n",
            "  12482/200000: episode: 25, duration: 2.335s, episode steps: 500, steps per second: 214, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.131533, mae: 3.388754, mean_q: -4.716204, mean_eps: 0.922531\n",
            "  12982/200000: episode: 26, duration: 2.296s, episode steps: 500, steps per second: 218, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.956 [0.000, 2.000],  loss: 0.117346, mae: 3.413431, mean_q: -4.770310, mean_eps: 0.919364\n",
            "  13482/200000: episode: 27, duration: 2.440s, episode steps: 500, steps per second: 205, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.006 [0.000, 2.000],  loss: 0.143033, mae: 3.683490, mean_q: -5.154471, mean_eps: 0.916197\n",
            "  13982/200000: episode: 28, duration: 3.441s, episode steps: 500, steps per second: 145, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.068 [0.000, 2.000],  loss: 0.122298, mae: 3.689644, mean_q: -5.185786, mean_eps: 0.913031\n",
            "  14482/200000: episode: 29, duration: 2.293s, episode steps: 500, steps per second: 218, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.024 [0.000, 2.000],  loss: 0.153871, mae: 3.985242, mean_q: -5.608498, mean_eps: 0.909864\n",
            "  14982/200000: episode: 30, duration: 2.395s, episode steps: 500, steps per second: 209, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.134969, mae: 4.001119, mean_q: -5.642208, mean_eps: 0.906697\n",
            "  15321/200000: episode: 31, duration: 1.700s, episode steps: 339, steps per second: 199, episode reward: -338.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.029 [0.000, 2.000],  loss: 0.159221, mae: 4.256643, mean_q: -6.005224, mean_eps: 0.904037\n",
            "  15821/200000: episode: 32, duration: 2.366s, episode steps: 500, steps per second: 211, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.976 [0.000, 2.000],  loss: 0.145004, mae: 4.284058, mean_q: -6.061029, mean_eps: 0.901377\n",
            "  16321/200000: episode: 33, duration: 3.212s, episode steps: 500, steps per second: 156, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.166876, mae: 4.455839, mean_q: -6.312347, mean_eps: 0.898211\n",
            "  16821/200000: episode: 34, duration: 3.067s, episode steps: 500, steps per second: 163, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.152617, mae: 4.588291, mean_q: -6.513957, mean_eps: 0.895044\n",
            "  17321/200000: episode: 35, duration: 2.410s, episode steps: 500, steps per second: 207, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.988 [0.000, 2.000],  loss: 0.177674, mae: 4.778934, mean_q: -6.789319, mean_eps: 0.891877\n",
            "  17821/200000: episode: 36, duration: 2.445s, episode steps: 500, steps per second: 205, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.175282, mae: 4.909604, mean_q: -6.982298, mean_eps: 0.888711\n",
            "  18321/200000: episode: 37, duration: 2.635s, episode steps: 500, steps per second: 190, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.187154, mae: 5.060402, mean_q: -7.204910, mean_eps: 0.885544\n",
            "  18821/200000: episode: 38, duration: 3.261s, episode steps: 500, steps per second: 153, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.178402, mae: 5.165972, mean_q: -7.362495, mean_eps: 0.882377\n",
            "  19258/200000: episode: 39, duration: 2.539s, episode steps: 437, steps per second: 172, episode reward: -436.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.947 [0.000, 2.000],  loss: 0.186307, mae: 5.318059, mean_q: -7.584538, mean_eps: 0.879413\n",
            "  19758/200000: episode: 40, duration: 2.387s, episode steps: 500, steps per second: 210, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.032 [0.000, 2.000],  loss: 0.203934, mae: 5.454386, mean_q: -7.780060, mean_eps: 0.876449\n",
            "  20196/200000: episode: 41, duration: 2.090s, episode steps: 438, steps per second: 210, episode reward: -437.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.998 [0.000, 2.000],  loss: 0.214073, mae: 5.562697, mean_q: -7.929095, mean_eps: 0.873485\n",
            "  20696/200000: episode: 42, duration: 2.529s, episode steps: 500, steps per second: 198, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.994 [0.000, 2.000],  loss: 0.204778, mae: 5.664466, mean_q: -8.079918, mean_eps: 0.870521\n",
            "  21163/200000: episode: 43, duration: 2.777s, episode steps: 467, steps per second: 168, episode reward: -466.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.034 [0.000, 2.000],  loss: 0.222467, mae: 5.707626, mean_q: -8.149766, mean_eps: 0.867456\n",
            "  21663/200000: episode: 44, duration: 3.194s, episode steps: 500, steps per second: 157, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.012 [0.000, 2.000],  loss: 0.234510, mae: 5.846855, mean_q: -8.348874, mean_eps: 0.864391\n",
            "  22117/200000: episode: 45, duration: 2.258s, episode steps: 454, steps per second: 201, episode reward: -453.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.022 [0.000, 2.000],  loss: 0.221291, mae: 5.868779, mean_q: -8.387180, mean_eps: 0.861363\n",
            "  22465/200000: episode: 46, duration: 1.637s, episode steps: 348, steps per second: 213, episode reward: -347.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.014 [0.000, 2.000],  loss: 0.222299, mae: 5.986463, mean_q: -8.556409, mean_eps: 0.858817\n",
            "  22965/200000: episode: 47, duration: 2.377s, episode steps: 500, steps per second: 210, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.028 [0.000, 2.000],  loss: 0.210290, mae: 5.996490, mean_q: -8.585329, mean_eps: 0.856132\n",
            "  23465/200000: episode: 48, duration: 2.476s, episode steps: 500, steps per second: 202, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.926 [0.000, 2.000],  loss: 0.262656, mae: 6.191493, mean_q: -8.858452, mean_eps: 0.852965\n",
            "  23881/200000: episode: 49, duration: 2.937s, episode steps: 416, steps per second: 142, episode reward: -415.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.041 [0.000, 2.000],  loss: 0.223785, mae: 6.202051, mean_q: -8.889000, mean_eps: 0.850065\n",
            "  24208/200000: episode: 50, duration: 2.132s, episode steps: 327, steps per second: 153, episode reward: -326.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.021 [0.000, 2.000],  loss: 0.282856, mae: 6.272265, mean_q: -8.953690, mean_eps: 0.847721\n",
            "  24555/200000: episode: 51, duration: 1.745s, episode steps: 347, steps per second: 199, episode reward: -346.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.968 [0.000, 2.000],  loss: 0.245182, mae: 6.351174, mean_q: -9.089579, mean_eps: 0.845593\n",
            "  24804/200000: episode: 52, duration: 1.270s, episode steps: 249, steps per second: 196, episode reward: -248.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.137 [0.000, 2.000],  loss: 0.221774, mae: 6.324337, mean_q: -9.049080, mean_eps: 0.843706\n",
            "  25055/200000: episode: 53, duration: 1.230s, episode steps: 251, steps per second: 204, episode reward: -250.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.968 [0.000, 2.000],  loss: 0.274413, mae: 6.330317, mean_q: -9.039216, mean_eps: 0.842123\n",
            "  25555/200000: episode: 54, duration: 2.433s, episode steps: 500, steps per second: 205, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.255709, mae: 6.517924, mean_q: -9.327629, mean_eps: 0.839741\n",
            "  25966/200000: episode: 55, duration: 1.984s, episode steps: 411, steps per second: 207, episode reward: -410.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.027 [0.000, 2.000],  loss: 0.250505, mae: 6.486000, mean_q: -9.276712, mean_eps: 0.836853\n",
            "  26253/200000: episode: 56, duration: 1.957s, episode steps: 287, steps per second: 147, episode reward: -286.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.122 [0.000, 2.000],  loss: 0.281179, mae: 6.687513, mean_q: -9.559048, mean_eps: 0.834637\n",
            "  26703/200000: episode: 57, duration: 2.758s, episode steps: 450, steps per second: 163, episode reward: -449.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.958 [0.000, 2.000],  loss: 0.270043, mae: 6.700472, mean_q: -9.582914, mean_eps: 0.832306\n",
            "  27203/200000: episode: 58, duration: 2.384s, episode steps: 500, steps per second: 210, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.978 [0.000, 2.000],  loss: 0.271430, mae: 6.766112, mean_q: -9.672412, mean_eps: 0.829304\n",
            "  27703/200000: episode: 59, duration: 2.450s, episode steps: 500, steps per second: 204, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.282376, mae: 6.874562, mean_q: -9.829172, mean_eps: 0.826137\n",
            "  28203/200000: episode: 60, duration: 2.416s, episode steps: 500, steps per second: 207, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.044 [0.000, 2.000],  loss: 0.275374, mae: 6.899871, mean_q: -9.879164, mean_eps: 0.822971\n",
            "  28582/200000: episode: 61, duration: 1.803s, episode steps: 379, steps per second: 210, episode reward: -378.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.071 [0.000, 2.000],  loss: 0.286133, mae: 6.985042, mean_q: -10.010123, mean_eps: 0.820184\n",
            "  28875/200000: episode: 62, duration: 2.123s, episode steps: 293, steps per second: 138, episode reward: -292.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.061 [0.000, 2.000],  loss: 0.270187, mae: 6.987453, mean_q: -10.008081, mean_eps: 0.818056\n",
            "  29352/200000: episode: 63, duration: 2.860s, episode steps: 477, steps per second: 167, episode reward: -476.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.052 [0.000, 2.000],  loss: 0.305986, mae: 7.032632, mean_q: -10.065180, mean_eps: 0.815624\n",
            "  29659/200000: episode: 64, duration: 1.504s, episode steps: 307, steps per second: 204, episode reward: -306.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.954 [0.000, 2.000],  loss: 0.345766, mae: 7.071763, mean_q: -10.117010, mean_eps: 0.813141\n",
            "  30017/200000: episode: 65, duration: 1.711s, episode steps: 358, steps per second: 209, episode reward: -357.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.017 [0.000, 2.000],  loss: 0.282922, mae: 7.070146, mean_q: -10.122621, mean_eps: 0.811026\n",
            "  30302/200000: episode: 66, duration: 1.353s, episode steps: 285, steps per second: 211, episode reward: -284.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.035 [0.000, 2.000],  loss: 0.289605, mae: 7.234633, mean_q: -10.364070, mean_eps: 0.808987\n",
            "  30702/200000: episode: 67, duration: 1.987s, episode steps: 400, steps per second: 201, episode reward: -399.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.090 [0.000, 2.000],  loss: 0.330398, mae: 7.250248, mean_q: -10.361213, mean_eps: 0.806821\n",
            "  30949/200000: episode: 68, duration: 1.214s, episode steps: 247, steps per second: 203, episode reward: -246.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.036 [0.000, 2.000],  loss: 0.265090, mae: 7.189110, mean_q: -10.301776, mean_eps: 0.804769\n",
            "  31240/200000: episode: 69, duration: 1.809s, episode steps: 291, steps per second: 161, episode reward: -290.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.918 [0.000, 2.000],  loss: 0.334515, mae: 7.321031, mean_q: -10.465124, mean_eps: 0.803071\n",
            "  31473/200000: episode: 70, duration: 1.786s, episode steps: 233, steps per second: 130, episode reward: -232.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.970 [0.000, 2.000],  loss: 0.323638, mae: 7.352663, mean_q: -10.515570, mean_eps: 0.801412\n",
            "  31872/200000: episode: 71, duration: 2.423s, episode steps: 399, steps per second: 165, episode reward: -398.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.133 [0.000, 2.000],  loss: 0.345580, mae: 7.368634, mean_q: -10.540012, mean_eps: 0.799411\n",
            "  32282/200000: episode: 72, duration: 2.092s, episode steps: 410, steps per second: 196, episode reward: -409.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.054 [0.000, 2.000],  loss: 0.330283, mae: 7.402981, mean_q: -10.583752, mean_eps: 0.796852\n",
            "  32676/200000: episode: 73, duration: 1.947s, episode steps: 394, steps per second: 202, episode reward: -393.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.033 [0.000, 2.000],  loss: 0.314855, mae: 7.428561, mean_q: -10.625727, mean_eps: 0.794306\n",
            "  32863/200000: episode: 74, duration: 0.956s, episode steps: 187, steps per second: 196, episode reward: -186.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.048 [0.000, 2.000],  loss: 0.327918, mae: 7.456382, mean_q: -10.675905, mean_eps: 0.792469\n",
            "  33045/200000: episode: 75, duration: 0.946s, episode steps: 182, steps per second: 192, episode reward: -181.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.154 [0.000, 2.000],  loss: 0.351046, mae: 7.458603, mean_q: -10.666237, mean_eps: 0.791291\n",
            "  33476/200000: episode: 76, duration: 2.053s, episode steps: 431, steps per second: 210, episode reward: -430.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.002 [0.000, 2.000],  loss: 0.365985, mae: 7.587041, mean_q: -10.848220, mean_eps: 0.789353\n",
            "  33871/200000: episode: 77, duration: 2.722s, episode steps: 395, steps per second: 145, episode reward: -394.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.357469, mae: 7.610288, mean_q: -10.899542, mean_eps: 0.786744\n",
            "  34101/200000: episode: 78, duration: 1.562s, episode steps: 230, steps per second: 147, episode reward: -229.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.030 [0.000, 2.000],  loss: 0.349834, mae: 7.534110, mean_q: -10.784454, mean_eps: 0.784755\n",
            "  34441/200000: episode: 79, duration: 1.722s, episode steps: 340, steps per second: 197, episode reward: -339.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.968 [0.000, 2.000],  loss: 0.361549, mae: 7.654103, mean_q: -10.959705, mean_eps: 0.782944\n",
            "  34924/200000: episode: 80, duration: 2.425s, episode steps: 483, steps per second: 199, episode reward: -482.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.990 [0.000, 2.000],  loss: 0.336922, mae: 7.661716, mean_q: -10.978549, mean_eps: 0.780347\n",
            "  35274/200000: episode: 81, duration: 1.798s, episode steps: 350, steps per second: 195, episode reward: -349.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.051 [0.000, 2.000],  loss: 0.376652, mae: 7.737660, mean_q: -11.079698, mean_eps: 0.777713\n",
            "  35734/200000: episode: 82, duration: 2.415s, episode steps: 460, steps per second: 190, episode reward: -459.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.076 [0.000, 2.000],  loss: 0.336829, mae: 7.749821, mean_q: -11.106416, mean_eps: 0.775141\n",
            "  36003/200000: episode: 83, duration: 1.480s, episode steps: 269, steps per second: 182, episode reward: -268.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.045 [0.000, 2.000],  loss: 0.353420, mae: 7.762068, mean_q: -11.115001, mean_eps: 0.772836\n",
            "  36414/200000: episode: 84, duration: 3.146s, episode steps: 411, steps per second: 131, episode reward: -410.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.058 [0.000, 2.000],  loss: 0.372464, mae: 7.811179, mean_q: -11.175745, mean_eps: 0.770683\n",
            "  36696/200000: episode: 85, duration: 1.600s, episode steps: 282, steps per second: 176, episode reward: -281.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.989 [0.000, 2.000],  loss: 0.384526, mae: 7.817349, mean_q: -11.165621, mean_eps: 0.768491\n",
            "  36976/200000: episode: 86, duration: 1.440s, episode steps: 280, steps per second: 194, episode reward: -279.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.029 [0.000, 2.000],  loss: 0.374438, mae: 7.800851, mean_q: -11.152563, mean_eps: 0.766718\n",
            "  37309/200000: episode: 87, duration: 1.807s, episode steps: 333, steps per second: 184, episode reward: -332.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.129 [0.000, 2.000],  loss: 0.403012, mae: 7.946723, mean_q: -11.347440, mean_eps: 0.764767\n",
            "  37569/200000: episode: 88, duration: 1.323s, episode steps: 260, steps per second: 196, episode reward: -259.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.965 [0.000, 2.000],  loss: 0.417889, mae: 7.987466, mean_q: -11.406085, mean_eps: 0.762880\n",
            "  37899/200000: episode: 89, duration: 1.681s, episode steps: 330, steps per second: 196, episode reward: -329.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.094 [0.000, 2.000],  loss: 0.357222, mae: 7.997879, mean_q: -11.423261, mean_eps: 0.761018\n",
            "  38220/200000: episode: 90, duration: 1.648s, episode steps: 321, steps per second: 195, episode reward: -320.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.168 [0.000, 2.000],  loss: 0.380134, mae: 7.962066, mean_q: -11.354200, mean_eps: 0.758966\n",
            "  38472/200000: episode: 91, duration: 1.532s, episode steps: 252, steps per second: 164, episode reward: -251.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.083 [0.000, 2.000],  loss: 0.398303, mae: 7.913367, mean_q: -11.292054, mean_eps: 0.757155\n",
            "  38678/200000: episode: 92, duration: 1.599s, episode steps: 206, steps per second: 129, episode reward: -205.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.927 [0.000, 2.000],  loss: 0.383066, mae: 7.978386, mean_q: -11.389791, mean_eps: 0.755698\n",
            "  38964/200000: episode: 93, duration: 1.819s, episode steps: 286, steps per second: 157, episode reward: -285.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.080 [0.000, 2.000],  loss: 0.362354, mae: 7.923889, mean_q: -11.316271, mean_eps: 0.754140\n",
            "  39236/200000: episode: 94, duration: 1.373s, episode steps: 272, steps per second: 198, episode reward: -271.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.996 [0.000, 2.000],  loss: 0.361664, mae: 7.945610, mean_q: -11.343397, mean_eps: 0.752379\n",
            "  39622/200000: episode: 95, duration: 1.893s, episode steps: 386, steps per second: 204, episode reward: -385.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.091 [0.000, 2.000],  loss: 0.355555, mae: 7.911233, mean_q: -11.313103, mean_eps: 0.750289\n",
            "  39871/200000: episode: 96, duration: 1.296s, episode steps: 249, steps per second: 192, episode reward: -248.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.028 [0.000, 2.000],  loss: 0.381898, mae: 7.987540, mean_q: -11.408928, mean_eps: 0.748275\n",
            "  40081/200000: episode: 97, duration: 1.068s, episode steps: 210, steps per second: 197, episode reward: -209.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.038 [0.000, 2.000],  loss: 0.413022, mae: 7.942456, mean_q: -11.341096, mean_eps: 0.746819\n",
            "  40393/200000: episode: 98, duration: 1.602s, episode steps: 312, steps per second: 195, episode reward: -311.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.112 [0.000, 2.000],  loss: 0.381264, mae: 8.037707, mean_q: -11.487527, mean_eps: 0.745159\n",
            "  40754/200000: episode: 99, duration: 1.793s, episode steps: 361, steps per second: 201, episode reward: -360.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.994 [0.000, 2.000],  loss: 0.371989, mae: 8.012255, mean_q: -11.444046, mean_eps: 0.743031\n",
            "  41033/200000: episode: 100, duration: 2.025s, episode steps: 279, steps per second: 138, episode reward: -278.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.004 [0.000, 2.000],  loss: 0.358845, mae: 8.016480, mean_q: -11.484379, mean_eps: 0.741005\n",
            "  41518/200000: episode: 101, duration: 2.967s, episode steps: 485, steps per second: 163, episode reward: -484.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.130 [0.000, 2.000],  loss: 0.400978, mae: 8.124037, mean_q: -11.627748, mean_eps: 0.738585\n",
            "  41755/200000: episode: 102, duration: 1.196s, episode steps: 237, steps per second: 198, episode reward: -236.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.444458, mae: 8.022669, mean_q: -11.473398, mean_eps: 0.736305\n",
            "  42084/200000: episode: 103, duration: 1.682s, episode steps: 329, steps per second: 196, episode reward: -328.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.058 [0.000, 2.000],  loss: 0.421710, mae: 8.086067, mean_q: -11.549271, mean_eps: 0.734519\n",
            "  42257/200000: episode: 104, duration: 0.899s, episode steps: 173, steps per second: 192, episode reward: -172.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.029 [0.000, 2.000],  loss: 0.445293, mae: 8.276666, mean_q: -11.837481, mean_eps: 0.732923\n",
            "  42456/200000: episode: 105, duration: 0.982s, episode steps: 199, steps per second: 203, episode reward: -198.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.382364, mae: 8.225719, mean_q: -11.779874, mean_eps: 0.731745\n",
            "  42663/200000: episode: 106, duration: 1.122s, episode steps: 207, steps per second: 185, episode reward: -206.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.034 [0.000, 2.000],  loss: 0.383676, mae: 8.208458, mean_q: -11.758086, mean_eps: 0.730466\n",
            "  42862/200000: episode: 107, duration: 1.013s, episode steps: 199, steps per second: 196, episode reward: -198.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.980 [0.000, 2.000],  loss: 0.353344, mae: 8.186521, mean_q: -11.731322, mean_eps: 0.729174\n",
            "  43130/200000: episode: 108, duration: 1.408s, episode steps: 268, steps per second: 190, episode reward: -267.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.123 [0.000, 2.000],  loss: 0.421797, mae: 8.223452, mean_q: -11.768639, mean_eps: 0.727692\n",
            "  43407/200000: episode: 109, duration: 1.832s, episode steps: 277, steps per second: 151, episode reward: -276.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.101 [0.000, 2.000],  loss: 0.451383, mae: 8.279291, mean_q: -11.824352, mean_eps: 0.725969\n",
            "  43664/200000: episode: 110, duration: 1.951s, episode steps: 257, steps per second: 132, episode reward: -256.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.953 [0.000, 2.000],  loss: 0.370391, mae: 8.274860, mean_q: -11.853704, mean_eps: 0.724285\n",
            "  43904/200000: episode: 111, duration: 1.310s, episode steps: 240, steps per second: 183, episode reward: -239.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.979 [0.000, 2.000],  loss: 0.387694, mae: 8.265785, mean_q: -11.837024, mean_eps: 0.722714\n",
            "  44149/200000: episode: 112, duration: 1.287s, episode steps: 245, steps per second: 190, episode reward: -244.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.943 [0.000, 2.000],  loss: 0.432535, mae: 8.276829, mean_q: -11.830322, mean_eps: 0.721169\n",
            "  44411/200000: episode: 113, duration: 1.369s, episode steps: 262, steps per second: 191, episode reward: -261.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.011 [0.000, 2.000],  loss: 0.424010, mae: 8.322650, mean_q: -11.907128, mean_eps: 0.719560\n",
            "  44603/200000: episode: 114, duration: 1.053s, episode steps: 192, steps per second: 182, episode reward: -191.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.348552, mae: 8.309990, mean_q: -11.902696, mean_eps: 0.718129\n",
            "  44853/200000: episode: 115, duration: 1.333s, episode steps: 250, steps per second: 188, episode reward: -249.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.984 [0.000, 2.000],  loss: 0.371686, mae: 8.316517, mean_q: -11.933940, mean_eps: 0.716723\n",
            "  45110/200000: episode: 116, duration: 1.387s, episode steps: 257, steps per second: 185, episode reward: -256.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.969 [0.000, 2.000],  loss: 0.433978, mae: 8.298350, mean_q: -11.894119, mean_eps: 0.715114\n",
            "  45291/200000: episode: 117, duration: 1.014s, episode steps: 181, steps per second: 179, episode reward: -180.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.994 [0.000, 2.000],  loss: 0.412824, mae: 8.377227, mean_q: -11.997973, mean_eps: 0.713733\n",
            "  45485/200000: episode: 118, duration: 1.067s, episode steps: 194, steps per second: 182, episode reward: -193.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.046 [0.000, 2.000],  loss: 0.367027, mae: 8.413345, mean_q: -12.064528, mean_eps: 0.712543\n",
            "  45788/200000: episode: 119, duration: 2.185s, episode steps: 303, steps per second: 139, episode reward: -302.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.040 [0.000, 2.000],  loss: 0.425206, mae: 8.402294, mean_q: -12.032697, mean_eps: 0.710972\n",
            "  46257/200000: episode: 120, duration: 3.103s, episode steps: 469, steps per second: 151, episode reward: -468.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.030 [0.000, 2.000],  loss: 0.436509, mae: 8.510070, mean_q: -12.186500, mean_eps: 0.708527\n",
            "  46515/200000: episode: 121, duration: 1.360s, episode steps: 258, steps per second: 190, episode reward: -257.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.397000, mae: 8.564259, mean_q: -12.283319, mean_eps: 0.706222\n",
            "  46776/200000: episode: 122, duration: 1.358s, episode steps: 261, steps per second: 192, episode reward: -260.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.958 [0.000, 2.000],  loss: 0.401507, mae: 8.578434, mean_q: -12.302072, mean_eps: 0.704588\n",
            "  47029/200000: episode: 123, duration: 1.301s, episode steps: 253, steps per second: 194, episode reward: -252.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.988 [0.000, 2.000],  loss: 0.446308, mae: 8.540551, mean_q: -12.240527, mean_eps: 0.702954\n",
            "  47294/200000: episode: 124, duration: 1.406s, episode steps: 265, steps per second: 188, episode reward: -264.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.977 [0.000, 2.000],  loss: 0.441130, mae: 8.685502, mean_q: -12.450587, mean_eps: 0.701307\n",
            "  47541/200000: episode: 125, duration: 1.319s, episode steps: 247, steps per second: 187, episode reward: -246.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.028 [0.000, 2.000],  loss: 0.399217, mae: 8.657752, mean_q: -12.420614, mean_eps: 0.699686\n",
            "  47802/200000: episode: 126, duration: 1.488s, episode steps: 261, steps per second: 175, episode reward: -260.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.904 [0.000, 2.000],  loss: 0.416080, mae: 8.678368, mean_q: -12.438006, mean_eps: 0.698077\n",
            "  48010/200000: episode: 127, duration: 1.470s, episode steps: 208, steps per second: 142, episode reward: -207.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.034 [0.000, 2.000],  loss: 0.489456, mae: 8.719354, mean_q: -12.483291, mean_eps: 0.696595\n",
            "  48309/200000: episode: 128, duration: 2.315s, episode steps: 299, steps per second: 129, episode reward: -298.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.067 [0.000, 2.000],  loss: 0.426122, mae: 8.753862, mean_q: -12.554994, mean_eps: 0.694987\n",
            "  48534/200000: episode: 129, duration: 1.227s, episode steps: 225, steps per second: 183, episode reward: -224.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.196 [0.000, 2.000],  loss: 0.418280, mae: 8.740517, mean_q: -12.534547, mean_eps: 0.693327\n",
            "  48875/200000: episode: 130, duration: 1.864s, episode steps: 341, steps per second: 183, episode reward: -340.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.015 [0.000, 2.000],  loss: 0.437252, mae: 8.746607, mean_q: -12.535103, mean_eps: 0.691541\n",
            "  49083/200000: episode: 131, duration: 1.195s, episode steps: 208, steps per second: 174, episode reward: -207.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.072 [0.000, 2.000],  loss: 0.443767, mae: 8.860463, mean_q: -12.701969, mean_eps: 0.689806\n",
            "  49225/200000: episode: 132, duration: 0.808s, episode steps: 142, steps per second: 176, episode reward: -141.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.148 [0.000, 2.000],  loss: 0.499557, mae: 8.916988, mean_q: -12.775319, mean_eps: 0.688691\n",
            "  49457/200000: episode: 133, duration: 1.326s, episode steps: 232, steps per second: 175, episode reward: -231.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.974 [0.000, 2.000],  loss: 0.441821, mae: 8.991547, mean_q: -12.895449, mean_eps: 0.687501\n",
            "  49684/200000: episode: 134, duration: 1.227s, episode steps: 227, steps per second: 185, episode reward: -226.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.022 [0.000, 2.000],  loss: 0.430053, mae: 8.935905, mean_q: -12.832041, mean_eps: 0.686057\n",
            "  49860/200000: episode: 135, duration: 0.943s, episode steps: 176, steps per second: 187, episode reward: -175.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.051 [0.000, 2.000],  loss: 0.449772, mae: 8.944803, mean_q: -12.822438, mean_eps: 0.684790\n",
            "  50140/200000: episode: 136, duration: 1.491s, episode steps: 280, steps per second: 188, episode reward: -279.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.107 [0.000, 2.000],  loss: 0.477632, mae: 8.997146, mean_q: -12.888967, mean_eps: 0.683346\n",
            "  50426/200000: episode: 137, duration: 2.221s, episode steps: 286, steps per second: 129, episode reward: -285.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.157 [0.000, 2.000],  loss: 0.467894, mae: 8.955938, mean_q: -12.834358, mean_eps: 0.681547\n",
            "  50633/200000: episode: 138, duration: 1.641s, episode steps: 207, steps per second: 126, episode reward: -206.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.918 [0.000, 2.000],  loss: 0.467599, mae: 9.012264, mean_q: -12.914777, mean_eps: 0.679977\n",
            "  50947/200000: episode: 139, duration: 1.742s, episode steps: 314, steps per second: 180, episode reward: -313.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.022 [0.000, 2.000],  loss: 0.429364, mae: 8.998705, mean_q: -12.896250, mean_eps: 0.678330\n",
            "  51445/200000: episode: 140, duration: 2.813s, episode steps: 498, steps per second: 177, episode reward: -497.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.108 [0.000, 2.000],  loss: 0.438610, mae: 9.101021, mean_q: -13.054856, mean_eps: 0.675759\n",
            "  51619/200000: episode: 141, duration: 0.949s, episode steps: 174, steps per second: 183, episode reward: -173.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.960 [0.000, 2.000],  loss: 0.495582, mae: 9.029633, mean_q: -12.946869, mean_eps: 0.673631\n",
            "  51914/200000: episode: 142, duration: 1.601s, episode steps: 295, steps per second: 184, episode reward: -294.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.044 [0.000, 2.000],  loss: 0.401696, mae: 9.066607, mean_q: -13.016182, mean_eps: 0.672149\n",
            "  52119/200000: episode: 143, duration: 1.145s, episode steps: 205, steps per second: 179, episode reward: -204.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.937 [0.000, 2.000],  loss: 0.473756, mae: 9.071846, mean_q: -13.003893, mean_eps: 0.670565\n",
            "  52480/200000: episode: 144, duration: 2.258s, episode steps: 361, steps per second: 160, episode reward: -360.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.011 [0.000, 2.000],  loss: 0.462751, mae: 9.103169, mean_q: -13.057522, mean_eps: 0.668779\n",
            "  52673/200000: episode: 145, duration: 1.571s, episode steps: 193, steps per second: 123, episode reward: -192.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.052 [0.000, 2.000],  loss: 0.559480, mae: 9.105230, mean_q: -13.030642, mean_eps: 0.667019\n",
            "  52966/200000: episode: 146, duration: 1.971s, episode steps: 293, steps per second: 149, episode reward: -292.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.068 [0.000, 2.000],  loss: 0.529580, mae: 9.156613, mean_q: -13.114146, mean_eps: 0.665473\n",
            "  53151/200000: episode: 147, duration: 1.015s, episode steps: 185, steps per second: 182, episode reward: -184.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.968 [0.000, 2.000],  loss: 0.477069, mae: 9.095101, mean_q: -13.033011, mean_eps: 0.663966\n",
            "  53308/200000: episode: 148, duration: 0.859s, episode steps: 157, steps per second: 183, episode reward: -156.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.006 [0.000, 2.000],  loss: 0.520317, mae: 9.248509, mean_q: -13.250559, mean_eps: 0.662889\n",
            "  53618/200000: episode: 149, duration: 1.640s, episode steps: 310, steps per second: 189, episode reward: -309.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.048 [0.000, 2.000],  loss: 0.465231, mae: 9.191990, mean_q: -13.181952, mean_eps: 0.661407\n",
            "  53785/200000: episode: 150, duration: 0.891s, episode steps: 167, steps per second: 187, episode reward: -166.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.090 [0.000, 2.000],  loss: 0.409650, mae: 9.204533, mean_q: -13.225345, mean_eps: 0.659887\n",
            "  54100/200000: episode: 151, duration: 1.643s, episode steps: 315, steps per second: 192, episode reward: -314.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.070 [0.000, 2.000],  loss: 0.516869, mae: 9.200484, mean_q: -13.196129, mean_eps: 0.658367\n",
            "  54323/200000: episode: 152, duration: 1.153s, episode steps: 223, steps per second: 193, episode reward: -222.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.433769, mae: 9.227192, mean_q: -13.248131, mean_eps: 0.656670\n",
            "  54529/200000: episode: 153, duration: 1.097s, episode steps: 206, steps per second: 188, episode reward: -205.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.092 [0.000, 2.000],  loss: 0.497241, mae: 9.265529, mean_q: -13.279442, mean_eps: 0.655302\n",
            "  54813/200000: episode: 154, duration: 1.832s, episode steps: 284, steps per second: 155, episode reward: -283.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.067 [0.000, 2.000],  loss: 0.446550, mae: 9.227539, mean_q: -13.243840, mean_eps: 0.653744\n",
            "  55313/200000: episode: 155, duration: 3.610s, episode steps: 500, steps per second: 139, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.954 [0.000, 2.000],  loss: 0.493937, mae: 9.217041, mean_q: -13.211926, mean_eps: 0.651261\n",
            "  55497/200000: episode: 156, duration: 1.009s, episode steps: 184, steps per second: 182, episode reward: -183.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.152 [0.000, 2.000],  loss: 0.456991, mae: 9.152777, mean_q: -13.124295, mean_eps: 0.649095\n",
            "  55822/200000: episode: 157, duration: 1.869s, episode steps: 325, steps per second: 174, episode reward: -324.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.055 [0.000, 2.000],  loss: 0.455092, mae: 9.218338, mean_q: -13.239195, mean_eps: 0.647487\n",
            "  56322/200000: episode: 158, duration: 2.731s, episode steps: 500, steps per second: 183, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.244 [0.000, 2.000],  loss: 0.480608, mae: 9.242088, mean_q: -13.259670, mean_eps: 0.644877\n",
            "  56447/200000: episode: 159, duration: 0.727s, episode steps: 125, steps per second: 172, episode reward: -124.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.144 [0.000, 2.000],  loss: 0.494894, mae: 9.253273, mean_q: -13.278553, mean_eps: 0.642901\n",
            "  56704/200000: episode: 160, duration: 1.390s, episode steps: 257, steps per second: 185, episode reward: -256.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.140 [0.000, 2.000],  loss: 0.499709, mae: 9.335824, mean_q: -13.380975, mean_eps: 0.641698\n",
            "  56862/200000: episode: 161, duration: 0.889s, episode steps: 158, steps per second: 178, episode reward: -157.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.108 [0.000, 2.000],  loss: 0.479395, mae: 9.361773, mean_q: -13.448936, mean_eps: 0.640381\n",
            "  57023/200000: episode: 162, duration: 1.183s, episode steps: 161, steps per second: 136, episode reward: -160.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.106 [0.000, 2.000],  loss: 0.501942, mae: 9.307631, mean_q: -13.344067, mean_eps: 0.639367\n",
            "  57203/200000: episode: 163, duration: 1.516s, episode steps: 180, steps per second: 119, episode reward: -179.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.039 [0.000, 2.000],  loss: 0.539213, mae: 9.402403, mean_q: -13.476630, mean_eps: 0.638291\n",
            "  57481/200000: episode: 164, duration: 1.905s, episode steps: 278, steps per second: 146, episode reward: -277.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.050 [0.000, 2.000],  loss: 0.454988, mae: 9.351980, mean_q: -13.436452, mean_eps: 0.636834\n",
            "  57671/200000: episode: 165, duration: 1.099s, episode steps: 190, steps per second: 173, episode reward: -189.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.032 [0.000, 2.000],  loss: 0.465515, mae: 9.356405, mean_q: -13.412474, mean_eps: 0.635352\n",
            "  57892/200000: episode: 166, duration: 1.243s, episode steps: 221, steps per second: 178, episode reward: -220.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.109 [0.000, 2.000],  loss: 0.468973, mae: 9.361730, mean_q: -13.434617, mean_eps: 0.634060\n",
            "  58006/200000: episode: 167, duration: 0.649s, episode steps: 114, steps per second: 176, episode reward: -113.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.088 [0.000, 2.000],  loss: 0.524706, mae: 9.390277, mean_q: -13.466329, mean_eps: 0.632996\n",
            "  58279/200000: episode: 168, duration: 1.496s, episode steps: 273, steps per second: 182, episode reward: -272.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.996 [0.000, 2.000],  loss: 0.483182, mae: 9.297804, mean_q: -13.334398, mean_eps: 0.631767\n",
            "  58482/200000: episode: 169, duration: 1.139s, episode steps: 203, steps per second: 178, episode reward: -202.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.039 [0.000, 2.000],  loss: 0.485179, mae: 9.296882, mean_q: -13.332646, mean_eps: 0.630260\n",
            "  58635/200000: episode: 170, duration: 0.834s, episode steps: 153, steps per second: 184, episode reward: -152.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.124 [0.000, 2.000],  loss: 0.513502, mae: 9.317733, mean_q: -13.360811, mean_eps: 0.629133\n",
            "  58935/200000: episode: 171, duration: 1.604s, episode steps: 300, steps per second: 187, episode reward: -299.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.120 [0.000, 2.000],  loss: 0.458345, mae: 9.316562, mean_q: -13.380818, mean_eps: 0.627701\n",
            "  59157/200000: episode: 172, duration: 1.208s, episode steps: 222, steps per second: 184, episode reward: -221.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.027 [0.000, 2.000],  loss: 0.522475, mae: 9.374586, mean_q: -13.431460, mean_eps: 0.626042\n",
            "  59266/200000: episode: 173, duration: 0.835s, episode steps: 109, steps per second: 130, episode reward: -108.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.073 [0.000, 2.000],  loss: 0.460705, mae: 9.509874, mean_q: -13.654692, mean_eps: 0.624991\n",
            "  59476/200000: episode: 174, duration: 1.678s, episode steps: 210, steps per second: 125, episode reward: -209.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.119 [0.000, 2.000],  loss: 0.569016, mae: 9.386403, mean_q: -13.446086, mean_eps: 0.623990\n",
            "  59701/200000: episode: 175, duration: 1.574s, episode steps: 225, steps per second: 143, episode reward: -224.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.111 [0.000, 2.000],  loss: 0.502962, mae: 9.405524, mean_q: -13.491906, mean_eps: 0.622609\n",
            "  59975/200000: episode: 176, duration: 1.468s, episode steps: 274, steps per second: 187, episode reward: -273.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.084 [0.000, 2.000],  loss: 0.541529, mae: 9.412069, mean_q: -13.498806, mean_eps: 0.621026\n",
            "  60204/200000: episode: 177, duration: 1.309s, episode steps: 229, steps per second: 175, episode reward: -228.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.052 [0.000, 2.000],  loss: 0.444417, mae: 9.535923, mean_q: -13.703439, mean_eps: 0.619443\n",
            "  60355/200000: episode: 178, duration: 0.831s, episode steps: 151, steps per second: 182, episode reward: -150.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.040 [0.000, 2.000],  loss: 0.596028, mae: 9.601837, mean_q: -13.760501, mean_eps: 0.618239\n",
            "  60540/200000: episode: 179, duration: 1.023s, episode steps: 185, steps per second: 181, episode reward: -184.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.016 [0.000, 2.000],  loss: 0.532158, mae: 9.543880, mean_q: -13.680718, mean_eps: 0.617175\n",
            "  60704/200000: episode: 180, duration: 0.969s, episode steps: 164, steps per second: 169, episode reward: -163.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.079 [0.000, 2.000],  loss: 0.454836, mae: 9.502781, mean_q: -13.643387, mean_eps: 0.616073\n",
            "  60933/200000: episode: 181, duration: 1.328s, episode steps: 229, steps per second: 172, episode reward: -228.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.956 [0.000, 2.000],  loss: 0.525974, mae: 9.590710, mean_q: -13.752677, mean_eps: 0.614819\n",
            "  61100/200000: episode: 182, duration: 0.960s, episode steps: 167, steps per second: 174, episode reward: -166.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.150 [0.000, 2.000],  loss: 0.534377, mae: 9.458766, mean_q: -13.568411, mean_eps: 0.613565\n",
            "  61272/200000: episode: 183, duration: 1.031s, episode steps: 172, steps per second: 167, episode reward: -171.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.017 [0.000, 2.000],  loss: 0.556925, mae: 9.551942, mean_q: -13.701283, mean_eps: 0.612501\n",
            "  61497/200000: episode: 184, duration: 1.781s, episode steps: 225, steps per second: 126, episode reward: -224.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.093 [0.000, 2.000],  loss: 0.512636, mae: 9.561460, mean_q: -13.742118, mean_eps: 0.611235\n",
            "  61699/200000: episode: 185, duration: 1.646s, episode steps: 202, steps per second: 123, episode reward: -201.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.134 [0.000, 2.000],  loss: 0.469881, mae: 9.456162, mean_q: -13.569462, mean_eps: 0.609879\n",
            "  61974/200000: episode: 186, duration: 1.805s, episode steps: 275, steps per second: 152, episode reward: -274.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.004 [0.000, 2.000],  loss: 0.441229, mae: 9.494425, mean_q: -13.638084, mean_eps: 0.608372\n",
            "  62177/200000: episode: 187, duration: 1.169s, episode steps: 203, steps per second: 174, episode reward: -202.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.064 [0.000, 2.000],  loss: 0.485487, mae: 9.539443, mean_q: -13.682267, mean_eps: 0.606852\n",
            "  62336/200000: episode: 188, duration: 0.893s, episode steps: 159, steps per second: 178, episode reward: -158.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.138 [0.000, 2.000],  loss: 0.471114, mae: 9.646935, mean_q: -13.850094, mean_eps: 0.605712\n",
            "  62510/200000: episode: 189, duration: 0.952s, episode steps: 174, steps per second: 183, episode reward: -173.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.080 [0.000, 2.000],  loss: 0.579359, mae: 9.603349, mean_q: -13.746158, mean_eps: 0.604661\n",
            "  62665/200000: episode: 190, duration: 0.862s, episode steps: 155, steps per second: 180, episode reward: -154.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.135 [0.000, 2.000],  loss: 0.468715, mae: 9.533982, mean_q: -13.679693, mean_eps: 0.603609\n",
            "  62917/200000: episode: 191, duration: 1.410s, episode steps: 252, steps per second: 179, episode reward: -251.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.087 [0.000, 2.000],  loss: 0.493981, mae: 9.652462, mean_q: -13.845600, mean_eps: 0.602317\n",
            "  63114/200000: episode: 192, duration: 1.150s, episode steps: 197, steps per second: 171, episode reward: -196.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.949 [0.000, 2.000],  loss: 0.513149, mae: 9.553778, mean_q: -13.694866, mean_eps: 0.600899\n",
            "  63349/200000: episode: 193, duration: 1.317s, episode steps: 235, steps per second: 178, episode reward: -234.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.979 [0.000, 2.000],  loss: 0.486745, mae: 9.627894, mean_q: -13.815433, mean_eps: 0.599531\n",
            "  63515/200000: episode: 194, duration: 0.954s, episode steps: 166, steps per second: 174, episode reward: -165.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.066 [0.000, 2.000],  loss: 0.579287, mae: 9.590636, mean_q: -13.737359, mean_eps: 0.598264\n",
            "  63708/200000: episode: 195, duration: 1.634s, episode steps: 193, steps per second: 118, episode reward: -192.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.933 [0.000, 2.000],  loss: 0.433519, mae: 9.585374, mean_q: -13.775507, mean_eps: 0.597137\n",
            "  63907/200000: episode: 196, duration: 1.665s, episode steps: 199, steps per second: 120, episode reward: -198.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.065 [0.000, 2.000],  loss: 0.551743, mae: 9.642508, mean_q: -13.806842, mean_eps: 0.595895\n",
            "  64098/200000: episode: 197, duration: 1.130s, episode steps: 191, steps per second: 169, episode reward: -190.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.194 [0.000, 2.000],  loss: 0.564864, mae: 9.539745, mean_q: -13.666412, mean_eps: 0.594654\n",
            "  64289/200000: episode: 198, duration: 1.064s, episode steps: 191, steps per second: 179, episode reward: -190.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.052 [0.000, 2.000],  loss: 0.552670, mae: 9.677289, mean_q: -13.872864, mean_eps: 0.593438\n",
            "  64461/200000: episode: 199, duration: 0.961s, episode steps: 172, steps per second: 179, episode reward: -171.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.041 [0.000, 2.000],  loss: 0.521867, mae: 9.663207, mean_q: -13.847388, mean_eps: 0.592285\n",
            "  64596/200000: episode: 200, duration: 0.800s, episode steps: 135, steps per second: 169, episode reward: -134.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.230 [0.000, 2.000],  loss: 0.573862, mae: 9.645255, mean_q: -13.803584, mean_eps: 0.591323\n",
            "  64728/200000: episode: 201, duration: 0.784s, episode steps: 132, steps per second: 168, episode reward: -131.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.091 [0.000, 2.000],  loss: 0.487999, mae: 9.595830, mean_q: -13.745126, mean_eps: 0.590487\n",
            "  65003/200000: episode: 202, duration: 1.553s, episode steps: 275, steps per second: 177, episode reward: -274.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.978 [0.000, 2.000],  loss: 0.504861, mae: 9.635366, mean_q: -13.806654, mean_eps: 0.589195\n",
            "  65179/200000: episode: 203, duration: 1.018s, episode steps: 176, steps per second: 173, episode reward: -175.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.949 [0.000, 2.000],  loss: 0.523996, mae: 9.675252, mean_q: -13.846128, mean_eps: 0.587763\n",
            "  65342/200000: episode: 204, duration: 0.907s, episode steps: 163, steps per second: 180, episode reward: -162.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.123 [0.000, 2.000],  loss: 0.549165, mae: 9.609508, mean_q: -13.745826, mean_eps: 0.586687\n",
            "  65512/200000: episode: 205, duration: 0.996s, episode steps: 170, steps per second: 171, episode reward: -169.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.018 [0.000, 2.000],  loss: 0.513230, mae: 9.660631, mean_q: -13.848131, mean_eps: 0.585635\n",
            "  65750/200000: episode: 206, duration: 1.642s, episode steps: 238, steps per second: 145, episode reward: -237.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.945 [0.000, 2.000],  loss: 0.591308, mae: 9.643150, mean_q: -13.804544, mean_eps: 0.584343\n",
            "  65999/200000: episode: 207, duration: 2.103s, episode steps: 249, steps per second: 118, episode reward: -248.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.173 [0.000, 2.000],  loss: 0.536638, mae: 9.661338, mean_q: -13.839710, mean_eps: 0.582798\n",
            "  66198/200000: episode: 208, duration: 1.491s, episode steps: 199, steps per second: 133, episode reward: -198.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.045 [0.000, 2.000],  loss: 0.510981, mae: 9.658641, mean_q: -13.851444, mean_eps: 0.581379\n",
            "  66309/200000: episode: 209, duration: 0.656s, episode steps: 111, steps per second: 169, episode reward: -110.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.874 [0.000, 2.000],  loss: 0.510111, mae: 9.715831, mean_q: -13.924869, mean_eps: 0.580391\n",
            "  66427/200000: episode: 210, duration: 0.669s, episode steps: 118, steps per second: 176, episode reward: -117.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.992 [0.000, 2.000],  loss: 0.583546, mae: 9.659848, mean_q: -13.845837, mean_eps: 0.579669\n",
            "  66597/200000: episode: 211, duration: 0.993s, episode steps: 170, steps per second: 171, episode reward: -169.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.029 [0.000, 2.000],  loss: 0.512015, mae: 9.611376, mean_q: -13.789826, mean_eps: 0.578757\n",
            "  66801/200000: episode: 212, duration: 1.256s, episode steps: 204, steps per second: 162, episode reward: -203.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.034 [0.000, 2.000],  loss: 0.514830, mae: 9.630092, mean_q: -13.794311, mean_eps: 0.577567\n",
            "  66979/200000: episode: 213, duration: 1.121s, episode steps: 178, steps per second: 159, episode reward: -177.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.972 [0.000, 2.000],  loss: 0.501602, mae: 9.729431, mean_q: -13.974109, mean_eps: 0.576363\n",
            "  67135/200000: episode: 214, duration: 0.946s, episode steps: 156, steps per second: 165, episode reward: -155.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.987 [0.000, 2.000],  loss: 0.500808, mae: 9.616899, mean_q: -13.796664, mean_eps: 0.575312\n",
            "  67323/200000: episode: 215, duration: 1.116s, episode steps: 188, steps per second: 168, episode reward: -187.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.090 [0.000, 2.000],  loss: 0.558783, mae: 9.683711, mean_q: -13.896835, mean_eps: 0.574223\n",
            "  67485/200000: episode: 216, duration: 0.959s, episode steps: 162, steps per second: 169, episode reward: -161.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.068 [0.000, 2.000],  loss: 0.499407, mae: 9.589906, mean_q: -13.769956, mean_eps: 0.573108\n",
            "  67709/200000: episode: 217, duration: 1.311s, episode steps: 224, steps per second: 171, episode reward: -223.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.036 [0.000, 2.000],  loss: 0.621786, mae: 9.657258, mean_q: -13.840044, mean_eps: 0.571879\n",
            "  67852/200000: episode: 218, duration: 1.086s, episode steps: 143, steps per second: 132, episode reward: -142.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.014 [0.000, 2.000],  loss: 0.535043, mae: 9.658026, mean_q: -13.864420, mean_eps: 0.570727\n",
            "  68127/200000: episode: 219, duration: 2.268s, episode steps: 275, steps per second: 121, episode reward: -274.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.004 [0.000, 2.000],  loss: 0.506118, mae: 9.698455, mean_q: -13.919440, mean_eps: 0.569409\n",
            "  68331/200000: episode: 220, duration: 1.290s, episode steps: 204, steps per second: 158, episode reward: -203.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.005 [0.000, 2.000],  loss: 0.540434, mae: 9.737768, mean_q: -13.971452, mean_eps: 0.567889\n",
            "  68556/200000: episode: 221, duration: 1.271s, episode steps: 225, steps per second: 177, episode reward: -224.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.027 [0.000, 2.000],  loss: 0.533646, mae: 9.816810, mean_q: -14.118664, mean_eps: 0.566534\n",
            "  68750/200000: episode: 222, duration: 1.132s, episode steps: 194, steps per second: 171, episode reward: -193.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.959 [0.000, 2.000],  loss: 0.483326, mae: 9.687915, mean_q: -13.912952, mean_eps: 0.565204\n",
            "  68890/200000: episode: 223, duration: 0.825s, episode steps: 140, steps per second: 170, episode reward: -139.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.107 [0.000, 2.000],  loss: 0.489046, mae: 9.823793, mean_q: -14.127021, mean_eps: 0.564140\n",
            "  69015/200000: episode: 224, duration: 0.711s, episode steps: 125, steps per second: 176, episode reward: -124.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.928 [0.000, 2.000],  loss: 0.573113, mae: 9.737682, mean_q: -13.987691, mean_eps: 0.563304\n",
            "  69219/200000: episode: 225, duration: 1.157s, episode steps: 204, steps per second: 176, episode reward: -203.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.029 [0.000, 2.000],  loss: 0.530242, mae: 9.799097, mean_q: -14.049911, mean_eps: 0.562265\n",
            "  69378/200000: episode: 226, duration: 0.926s, episode steps: 159, steps per second: 172, episode reward: -158.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.151 [0.000, 2.000],  loss: 0.501716, mae: 9.877168, mean_q: -14.191511, mean_eps: 0.561113\n",
            "  69549/200000: episode: 227, duration: 0.971s, episode steps: 171, steps per second: 176, episode reward: -170.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.889 [0.000, 2.000],  loss: 0.528363, mae: 9.733356, mean_q: -13.986728, mean_eps: 0.560061\n",
            "  69716/200000: episode: 228, duration: 0.937s, episode steps: 167, steps per second: 178, episode reward: -166.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.096 [0.000, 2.000],  loss: 0.454741, mae: 9.797743, mean_q: -14.098181, mean_eps: 0.558997\n",
            "  69895/200000: episode: 229, duration: 1.039s, episode steps: 179, steps per second: 172, episode reward: -178.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.095 [0.000, 2.000],  loss: 0.510696, mae: 9.820140, mean_q: -14.105156, mean_eps: 0.557908\n",
            "  70072/200000: episode: 230, duration: 1.433s, episode steps: 177, steps per second: 124, episode reward: -176.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.994 [0.000, 2.000],  loss: 0.598303, mae: 9.721403, mean_q: -13.939897, mean_eps: 0.556781\n",
            "  70247/200000: episode: 231, duration: 1.461s, episode steps: 175, steps per second: 120, episode reward: -174.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.086 [0.000, 2.000],  loss: 0.568263, mae: 9.855851, mean_q: -14.145822, mean_eps: 0.555666\n",
            "  70445/200000: episode: 232, duration: 1.376s, episode steps: 198, steps per second: 144, episode reward: -197.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.157 [0.000, 2.000],  loss: 0.607220, mae: 9.868887, mean_q: -14.140021, mean_eps: 0.554475\n",
            "  70622/200000: episode: 233, duration: 1.003s, episode steps: 177, steps per second: 177, episode reward: -176.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.096 [0.000, 2.000],  loss: 0.533846, mae: 9.830425, mean_q: -14.108617, mean_eps: 0.553285\n",
            "  70757/200000: episode: 234, duration: 0.819s, episode steps: 135, steps per second: 165, episode reward: -134.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.119 [0.000, 2.000],  loss: 0.595744, mae: 9.800755, mean_q: -14.052783, mean_eps: 0.552297\n",
            "  70899/200000: episode: 235, duration: 0.851s, episode steps: 142, steps per second: 167, episode reward: -141.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.092 [0.000, 2.000],  loss: 0.475924, mae: 9.770227, mean_q: -14.038346, mean_eps: 0.551423\n",
            "  71082/200000: episode: 236, duration: 1.090s, episode steps: 183, steps per second: 168, episode reward: -182.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.175 [0.000, 2.000],  loss: 0.486385, mae: 9.831416, mean_q: -14.133247, mean_eps: 0.550397\n",
            "  71245/200000: episode: 237, duration: 0.979s, episode steps: 163, steps per second: 167, episode reward: -162.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.098 [0.000, 2.000],  loss: 0.515539, mae: 9.923137, mean_q: -14.268719, mean_eps: 0.549295\n",
            "  71430/200000: episode: 238, duration: 1.073s, episode steps: 185, steps per second: 172, episode reward: -184.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.027 [0.000, 2.000],  loss: 0.524808, mae: 9.856494, mean_q: -14.149162, mean_eps: 0.548193\n",
            "  71619/200000: episode: 239, duration: 1.062s, episode steps: 189, steps per second: 178, episode reward: -188.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.164 [0.000, 2.000],  loss: 0.575261, mae: 9.825730, mean_q: -14.111737, mean_eps: 0.547015\n",
            "  71795/200000: episode: 240, duration: 1.037s, episode steps: 176, steps per second: 170, episode reward: -175.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.023 [0.000, 2.000],  loss: 0.570088, mae: 9.926130, mean_q: -14.270156, mean_eps: 0.545862\n",
            "  71952/200000: episode: 241, duration: 0.913s, episode steps: 157, steps per second: 172, episode reward: -156.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.045 [0.000, 2.000],  loss: 0.569072, mae: 9.909741, mean_q: -14.247880, mean_eps: 0.544811\n",
            "  72132/200000: episode: 242, duration: 1.436s, episode steps: 180, steps per second: 125, episode reward: -179.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.939 [0.000, 2.000],  loss: 0.497652, mae: 9.862545, mean_q: -14.201080, mean_eps: 0.543747\n",
            "  72321/200000: episode: 243, duration: 1.609s, episode steps: 189, steps per second: 117, episode reward: -188.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.079 [0.000, 2.000],  loss: 0.545781, mae: 9.885208, mean_q: -14.206366, mean_eps: 0.542569\n",
            "  72488/200000: episode: 244, duration: 1.328s, episode steps: 167, steps per second: 126, episode reward: -166.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.126 [0.000, 2.000],  loss: 0.481434, mae: 9.936256, mean_q: -14.301641, mean_eps: 0.541441\n",
            "  72643/200000: episode: 245, duration: 0.923s, episode steps: 155, steps per second: 168, episode reward: -154.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.084 [0.000, 2.000],  loss: 0.561975, mae: 10.014806, mean_q: -14.388136, mean_eps: 0.540428\n",
            "  72833/200000: episode: 246, duration: 1.152s, episode steps: 190, steps per second: 165, episode reward: -189.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.189 [0.000, 2.000],  loss: 0.551897, mae: 9.916821, mean_q: -14.256474, mean_eps: 0.539326\n",
            "  72975/200000: episode: 247, duration: 0.828s, episode steps: 142, steps per second: 171, episode reward: -141.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.560309, mae: 9.940453, mean_q: -14.272604, mean_eps: 0.538275\n",
            "  73177/200000: episode: 248, duration: 1.250s, episode steps: 202, steps per second: 162, episode reward: -201.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.069 [0.000, 2.000],  loss: 0.485673, mae: 9.916669, mean_q: -14.263037, mean_eps: 0.537185\n",
            "  73341/200000: episode: 249, duration: 0.953s, episode steps: 164, steps per second: 172, episode reward: -163.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.030 [0.000, 2.000],  loss: 0.523502, mae: 10.013140, mean_q: -14.393115, mean_eps: 0.536020\n",
            "  73515/200000: episode: 250, duration: 0.972s, episode steps: 174, steps per second: 179, episode reward: -173.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.948 [0.000, 2.000],  loss: 0.561692, mae: 10.028645, mean_q: -14.414460, mean_eps: 0.534956\n",
            "  73628/200000: episode: 251, duration: 0.665s, episode steps: 113, steps per second: 170, episode reward: -112.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.027 [0.000, 2.000],  loss: 0.548242, mae: 9.960104, mean_q: -14.298333, mean_eps: 0.534057\n",
            "  73806/200000: episode: 252, duration: 1.017s, episode steps: 178, steps per second: 175, episode reward: -177.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.826 [0.000, 2.000],  loss: 0.576940, mae: 9.974775, mean_q: -14.312737, mean_eps: 0.533132\n",
            "  73976/200000: episode: 253, duration: 0.984s, episode steps: 170, steps per second: 173, episode reward: -169.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.176 [0.000, 2.000],  loss: 0.539443, mae: 9.889602, mean_q: -14.199773, mean_eps: 0.532030\n",
            "  74087/200000: episode: 254, duration: 0.623s, episode steps: 111, steps per second: 178, episode reward: -110.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.045 [0.000, 2.000],  loss: 0.569834, mae: 10.033575, mean_q: -14.425397, mean_eps: 0.531143\n",
            "  74187/200000: episode: 255, duration: 0.765s, episode steps: 100, steps per second: 131, episode reward: -99.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.970 [0.000, 2.000],  loss: 0.570452, mae: 10.097775, mean_q: -14.498660, mean_eps: 0.530472\n",
            "  74413/200000: episode: 256, duration: 1.862s, episode steps: 226, steps per second: 121, episode reward: -225.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.062 [0.000, 2.000],  loss: 0.542699, mae: 10.117033, mean_q: -14.537212, mean_eps: 0.529433\n",
            "  74643/200000: episode: 257, duration: 1.649s, episode steps: 230, steps per second: 140, episode reward: -229.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.970 [0.000, 2.000],  loss: 0.564657, mae: 10.125975, mean_q: -14.552737, mean_eps: 0.527989\n",
            "  74847/200000: episode: 258, duration: 1.223s, episode steps: 204, steps per second: 167, episode reward: -203.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.005 [0.000, 2.000],  loss: 0.591631, mae: 10.139699, mean_q: -14.557247, mean_eps: 0.526621\n",
            "  75011/200000: episode: 259, duration: 0.961s, episode steps: 164, steps per second: 171, episode reward: -163.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.006 [0.000, 2.000],  loss: 0.539943, mae: 10.063072, mean_q: -14.458459, mean_eps: 0.525456\n",
            "  75129/200000: episode: 260, duration: 0.714s, episode steps: 118, steps per second: 165, episode reward: -117.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.119 [0.000, 2.000],  loss: 0.546567, mae: 10.126331, mean_q: -14.551258, mean_eps: 0.524557\n",
            "  75629/200000: episode: 261, duration: 3.015s, episode steps: 500, steps per second: 166, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.236 [0.000, 2.000],  loss: 0.543413, mae: 10.150646, mean_q: -14.585857, mean_eps: 0.522593\n",
            "  75805/200000: episode: 262, duration: 1.105s, episode steps: 176, steps per second: 159, episode reward: -175.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.034 [0.000, 2.000],  loss: 0.591704, mae: 10.058430, mean_q: -14.437692, mean_eps: 0.520453\n",
            "  75969/200000: episode: 263, duration: 0.977s, episode steps: 164, steps per second: 168, episode reward: -163.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.037 [0.000, 2.000],  loss: 0.504632, mae: 10.091198, mean_q: -14.502429, mean_eps: 0.519376\n",
            "  76136/200000: episode: 264, duration: 1.007s, episode steps: 167, steps per second: 166, episode reward: -166.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.144 [0.000, 2.000],  loss: 0.634186, mae: 10.081888, mean_q: -14.471042, mean_eps: 0.518337\n",
            "  76325/200000: episode: 265, duration: 1.536s, episode steps: 189, steps per second: 123, episode reward: -188.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.005 [0.000, 2.000],  loss: 0.579967, mae: 10.088113, mean_q: -14.503716, mean_eps: 0.517210\n",
            "  76489/200000: episode: 266, duration: 1.430s, episode steps: 164, steps per second: 115, episode reward: -163.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.982 [0.000, 2.000],  loss: 0.516885, mae: 10.135518, mean_q: -14.568816, mean_eps: 0.516083\n",
            "  76642/200000: episode: 267, duration: 1.293s, episode steps: 153, steps per second: 118, episode reward: -152.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.961 [0.000, 2.000],  loss: 0.568436, mae: 10.110541, mean_q: -14.508126, mean_eps: 0.515082\n",
            "  76857/200000: episode: 268, duration: 1.342s, episode steps: 215, steps per second: 160, episode reward: -214.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.065 [0.000, 2.000],  loss: 0.617779, mae: 10.109307, mean_q: -14.503344, mean_eps: 0.513917\n",
            "  77023/200000: episode: 269, duration: 1.008s, episode steps: 166, steps per second: 165, episode reward: -165.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.114 [0.000, 2.000],  loss: 0.617422, mae: 10.096259, mean_q: -14.488902, mean_eps: 0.512713\n",
            "  77156/200000: episode: 270, duration: 0.845s, episode steps: 133, steps per second: 157, episode reward: -132.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.910 [0.000, 2.000],  loss: 0.518311, mae: 10.059567, mean_q: -14.459980, mean_eps: 0.511776\n",
            "  77286/200000: episode: 271, duration: 0.856s, episode steps: 130, steps per second: 152, episode reward: -129.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.938 [0.000, 2.000],  loss: 0.517366, mae: 10.099589, mean_q: -14.518517, mean_eps: 0.510940\n",
            "  77446/200000: episode: 272, duration: 1.007s, episode steps: 160, steps per second: 159, episode reward: -159.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.988 [0.000, 2.000],  loss: 0.588582, mae: 10.094150, mean_q: -14.489059, mean_eps: 0.510015\n",
            "  77590/200000: episode: 273, duration: 0.876s, episode steps: 144, steps per second: 164, episode reward: -143.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.056 [0.000, 2.000],  loss: 0.579703, mae: 10.072316, mean_q: -14.468238, mean_eps: 0.509053\n",
            "  77712/200000: episode: 274, duration: 0.741s, episode steps: 122, steps per second: 165, episode reward: -121.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.992 [0.000, 2.000],  loss: 0.633812, mae: 10.076739, mean_q: -14.431531, mean_eps: 0.508217\n",
            "  77836/200000: episode: 275, duration: 0.750s, episode steps: 124, steps per second: 165, episode reward: -123.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.097 [0.000, 2.000],  loss: 0.571723, mae: 10.095424, mean_q: -14.481417, mean_eps: 0.507444\n",
            "  78010/200000: episode: 276, duration: 1.038s, episode steps: 174, steps per second: 168, episode reward: -173.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.925 [0.000, 2.000],  loss: 0.538369, mae: 10.081651, mean_q: -14.476296, mean_eps: 0.506494\n",
            "  78132/200000: episode: 277, duration: 0.734s, episode steps: 122, steps per second: 166, episode reward: -121.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.836 [0.000, 2.000],  loss: 0.589100, mae: 10.023170, mean_q: -14.369260, mean_eps: 0.505557\n",
            "  78316/200000: episode: 278, duration: 1.402s, episode steps: 184, steps per second: 131, episode reward: -183.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.924 [0.000, 2.000],  loss: 0.587656, mae: 10.166492, mean_q: -14.591547, mean_eps: 0.504594\n",
            "  78539/200000: episode: 279, duration: 1.944s, episode steps: 223, steps per second: 115, episode reward: -222.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.022 [0.000, 2.000],  loss: 0.546311, mae: 10.187019, mean_q: -14.616600, mean_eps: 0.503302\n",
            "  78739/200000: episode: 280, duration: 1.379s, episode steps: 200, steps per second: 145, episode reward: -199.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.810 [0.000, 2.000],  loss: 0.527501, mae: 10.126265, mean_q: -14.524529, mean_eps: 0.501959\n",
            "  78863/200000: episode: 281, duration: 0.741s, episode steps: 124, steps per second: 167, episode reward: -123.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.032 [0.000, 2.000],  loss: 0.603720, mae: 10.078652, mean_q: -14.456465, mean_eps: 0.500933\n",
            "  79030/200000: episode: 282, duration: 1.041s, episode steps: 167, steps per second: 160, episode reward: -166.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.006 [0.000, 2.000],  loss: 0.521857, mae: 10.073791, mean_q: -14.484707, mean_eps: 0.500009\n",
            "  79167/200000: episode: 283, duration: 0.820s, episode steps: 137, steps per second: 167, episode reward: -136.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.964 [0.000, 2.000],  loss: 0.500687, mae: 10.044094, mean_q: -14.414119, mean_eps: 0.499046\n",
            "  79355/200000: episode: 284, duration: 1.094s, episode steps: 188, steps per second: 172, episode reward: -187.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.064 [0.000, 2.000],  loss: 0.538384, mae: 10.146502, mean_q: -14.569943, mean_eps: 0.498020\n",
            "  79463/200000: episode: 285, duration: 0.625s, episode steps: 108, steps per second: 173, episode reward: -107.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.889 [0.000, 2.000],  loss: 0.433524, mae: 10.103556, mean_q: -14.540745, mean_eps: 0.497083\n",
            "  79678/200000: episode: 286, duration: 1.257s, episode steps: 215, steps per second: 171, episode reward: -214.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.023 [0.000, 2.000],  loss: 0.523063, mae: 10.141051, mean_q: -14.575588, mean_eps: 0.496057\n",
            "  79841/200000: episode: 287, duration: 0.952s, episode steps: 163, steps per second: 171, episode reward: -162.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.031 [0.000, 2.000],  loss: 0.561803, mae: 10.156141, mean_q: -14.579174, mean_eps: 0.494853\n",
            "  79981/200000: episode: 288, duration: 0.803s, episode steps: 140, steps per second: 174, episode reward: -139.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.236 [0.000, 2.000],  loss: 0.600337, mae: 10.162129, mean_q: -14.590519, mean_eps: 0.493891\n",
            "  80108/200000: episode: 289, duration: 0.772s, episode steps: 127, steps per second: 164, episode reward: -126.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.984 [0.000, 2.000],  loss: 0.585971, mae: 10.138322, mean_q: -14.522476, mean_eps: 0.493055\n",
            "  80275/200000: episode: 290, duration: 0.987s, episode steps: 167, steps per second: 169, episode reward: -166.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.982 [0.000, 2.000],  loss: 0.556946, mae: 10.191082, mean_q: -14.610647, mean_eps: 0.492130\n",
            "  80420/200000: episode: 291, duration: 1.199s, episode steps: 145, steps per second: 121, episode reward: -144.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.931 [0.000, 2.000],  loss: 0.526653, mae: 10.172057, mean_q: -14.613959, mean_eps: 0.491142\n",
            "  80581/200000: episode: 292, duration: 1.390s, episode steps: 161, steps per second: 116, episode reward: -160.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.975 [0.000, 2.000],  loss: 0.560367, mae: 10.239574, mean_q: -14.697649, mean_eps: 0.490167\n",
            "  80714/200000: episode: 293, duration: 1.157s, episode steps: 133, steps per second: 115, episode reward: -132.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.045 [0.000, 2.000],  loss: 0.493321, mae: 10.097153, mean_q: -14.507242, mean_eps: 0.489229\n",
            "  80876/200000: episode: 294, duration: 1.035s, episode steps: 162, steps per second: 157, episode reward: -161.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.914 [0.000, 2.000],  loss: 0.516267, mae: 10.235293, mean_q: -14.716032, mean_eps: 0.488305\n",
            "  81040/200000: episode: 295, duration: 1.040s, episode steps: 164, steps per second: 158, episode reward: -163.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.043 [0.000, 2.000],  loss: 0.645181, mae: 10.307144, mean_q: -14.806445, mean_eps: 0.487279\n",
            "  81236/200000: episode: 296, duration: 1.235s, episode steps: 196, steps per second: 159, episode reward: -195.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.918 [0.000, 2.000],  loss: 0.632244, mae: 10.394042, mean_q: -14.938420, mean_eps: 0.486139\n",
            "  81390/200000: episode: 297, duration: 0.970s, episode steps: 154, steps per second: 159, episode reward: -153.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.026 [0.000, 2.000],  loss: 0.577572, mae: 10.463191, mean_q: -15.037809, mean_eps: 0.485024\n",
            "  81531/200000: episode: 298, duration: 0.873s, episode steps: 141, steps per second: 162, episode reward: -140.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.057 [0.000, 2.000],  loss: 0.514383, mae: 10.319690, mean_q: -14.840933, mean_eps: 0.484087\n",
            "  81670/200000: episode: 299, duration: 0.866s, episode steps: 139, steps per second: 161, episode reward: -138.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.935 [0.000, 2.000],  loss: 0.563505, mae: 10.507897, mean_q: -15.102585, mean_eps: 0.483200\n",
            "  81821/200000: episode: 300, duration: 0.925s, episode steps: 151, steps per second: 163, episode reward: -150.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.020 [0.000, 2.000],  loss: 0.548685, mae: 10.468065, mean_q: -15.048487, mean_eps: 0.482275\n",
            "  81985/200000: episode: 301, duration: 1.045s, episode steps: 164, steps per second: 157, episode reward: -163.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.134 [0.000, 2.000],  loss: 0.536940, mae: 10.425259, mean_q: -14.986749, mean_eps: 0.481275\n",
            "  82109/200000: episode: 302, duration: 0.807s, episode steps: 124, steps per second: 154, episode reward: -123.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.065 [0.000, 2.000],  loss: 0.576297, mae: 10.532310, mean_q: -15.141555, mean_eps: 0.480363\n",
            "  82280/200000: episode: 303, duration: 1.196s, episode steps: 171, steps per second: 143, episode reward: -170.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.988 [0.000, 2.000],  loss: 0.696592, mae: 10.517661, mean_q: -15.111274, mean_eps: 0.479438\n",
            "  82447/200000: episode: 304, duration: 1.605s, episode steps: 167, steps per second: 104, episode reward: -166.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.012 [0.000, 2.000],  loss: 0.551203, mae: 10.492696, mean_q: -15.111103, mean_eps: 0.478374\n",
            "  82568/200000: episode: 305, duration: 1.158s, episode steps: 121, steps per second: 104, episode reward: -120.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.950 [0.000, 2.000],  loss: 0.529398, mae: 10.544256, mean_q: -15.202659, mean_eps: 0.477462\n",
            "  82750/200000: episode: 306, duration: 1.499s, episode steps: 182, steps per second: 121, episode reward: -181.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.967 [0.000, 2.000],  loss: 0.595388, mae: 10.497796, mean_q: -15.096648, mean_eps: 0.476499\n",
            "  82898/200000: episode: 307, duration: 1.016s, episode steps: 148, steps per second: 146, episode reward: -147.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.270 [0.000, 2.000],  loss: 0.625453, mae: 10.596942, mean_q: -15.258735, mean_eps: 0.475448\n",
            "  83054/200000: episode: 308, duration: 1.022s, episode steps: 156, steps per second: 153, episode reward: -155.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.006 [0.000, 2.000],  loss: 0.624377, mae: 10.624157, mean_q: -15.275431, mean_eps: 0.474485\n",
            "  83225/200000: episode: 309, duration: 1.117s, episode steps: 171, steps per second: 153, episode reward: -170.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.029 [0.000, 2.000],  loss: 0.624068, mae: 10.696416, mean_q: -15.380751, mean_eps: 0.473447\n",
            "  83376/200000: episode: 310, duration: 1.045s, episode steps: 151, steps per second: 145, episode reward: -150.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.219 [0.000, 2.000],  loss: 0.525167, mae: 10.696261, mean_q: -15.411622, mean_eps: 0.472433\n",
            "  83527/200000: episode: 311, duration: 1.020s, episode steps: 151, steps per second: 148, episode reward: -150.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.119 [0.000, 2.000],  loss: 0.625336, mae: 10.632993, mean_q: -15.300881, mean_eps: 0.471483\n",
            "  83704/200000: episode: 312, duration: 1.144s, episode steps: 177, steps per second: 155, episode reward: -176.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.017 [0.000, 2.000],  loss: 0.665841, mae: 10.714201, mean_q: -15.396382, mean_eps: 0.470445\n",
            "  83811/200000: episode: 313, duration: 0.746s, episode steps: 107, steps per second: 143, episode reward: -106.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.056 [0.000, 2.000],  loss: 0.533500, mae: 10.584567, mean_q: -15.206723, mean_eps: 0.469545\n",
            "  83986/200000: episode: 314, duration: 1.093s, episode steps: 175, steps per second: 160, episode reward: -174.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.097 [0.000, 2.000],  loss: 0.619744, mae: 10.626028, mean_q: -15.268131, mean_eps: 0.468646\n",
            "  84118/200000: episode: 315, duration: 0.840s, episode steps: 132, steps per second: 157, episode reward: -131.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.144 [0.000, 2.000],  loss: 0.628291, mae: 10.670247, mean_q: -15.334081, mean_eps: 0.467671\n",
            "  84283/200000: episode: 316, duration: 1.616s, episode steps: 165, steps per second: 102, episode reward: -164.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.018 [0.000, 2.000],  loss: 0.590100, mae: 10.603389, mean_q: -15.239289, mean_eps: 0.466733\n",
            "  84428/200000: episode: 317, duration: 1.368s, episode steps: 145, steps per second: 106, episode reward: -144.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.138 [0.000, 2.000],  loss: 0.720449, mae: 10.711604, mean_q: -15.394085, mean_eps: 0.465758\n",
            "  84615/200000: episode: 318, duration: 1.373s, episode steps: 187, steps per second: 136, episode reward: -186.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.160 [0.000, 2.000],  loss: 0.707866, mae: 10.677760, mean_q: -15.351127, mean_eps: 0.464707\n",
            "  84752/200000: episode: 319, duration: 0.858s, episode steps: 137, steps per second: 160, episode reward: -136.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.971 [0.000, 2.000],  loss: 0.598042, mae: 10.722468, mean_q: -15.418171, mean_eps: 0.463681\n",
            "  84890/200000: episode: 320, duration: 0.919s, episode steps: 138, steps per second: 150, episode reward: -137.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.043 [0.000, 2.000],  loss: 0.648802, mae: 10.716916, mean_q: -15.397098, mean_eps: 0.462807\n",
            "  85045/200000: episode: 321, duration: 0.987s, episode steps: 155, steps per second: 157, episode reward: -154.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.181 [0.000, 2.000],  loss: 0.618566, mae: 10.694891, mean_q: -15.373315, mean_eps: 0.461869\n",
            "  85214/200000: episode: 322, duration: 1.080s, episode steps: 169, steps per second: 156, episode reward: -168.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.160 [0.000, 2.000],  loss: 0.683392, mae: 10.893229, mean_q: -15.684729, mean_eps: 0.460843\n",
            "  85360/200000: episode: 323, duration: 0.915s, episode steps: 146, steps per second: 160, episode reward: -145.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.185 [0.000, 2.000],  loss: 0.680546, mae: 10.787755, mean_q: -15.505061, mean_eps: 0.459855\n",
            "  85562/200000: episode: 324, duration: 1.270s, episode steps: 202, steps per second: 159, episode reward: -201.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.178 [0.000, 2.000],  loss: 0.514089, mae: 10.690714, mean_q: -15.414788, mean_eps: 0.458753\n",
            "  85756/200000: episode: 325, duration: 1.223s, episode steps: 194, steps per second: 159, episode reward: -193.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.227 [0.000, 2.000],  loss: 0.625260, mae: 10.774926, mean_q: -15.496120, mean_eps: 0.457499\n",
            "  85855/200000: episode: 326, duration: 0.671s, episode steps:  99, steps per second: 148, episode reward: -98.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.071 [0.000, 2.000],  loss: 0.493862, mae: 10.893697, mean_q: -15.711538, mean_eps: 0.456575\n",
            "  86033/200000: episode: 327, duration: 1.124s, episode steps: 178, steps per second: 158, episode reward: -177.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.079 [0.000, 2.000],  loss: 0.649046, mae: 10.828752, mean_q: -15.566702, mean_eps: 0.455688\n",
            "  86202/200000: episode: 328, duration: 1.490s, episode steps: 169, steps per second: 113, episode reward: -168.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.006 [0.000, 2.000],  loss: 0.575264, mae: 10.780034, mean_q: -15.500516, mean_eps: 0.454586\n",
            "  86311/200000: episode: 329, duration: 1.032s, episode steps: 109, steps per second: 106, episode reward: -108.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.018 [0.000, 2.000],  loss: 0.602778, mae: 10.812637, mean_q: -15.549104, mean_eps: 0.453712\n",
            "  86443/200000: episode: 330, duration: 1.254s, episode steps: 132, steps per second: 105, episode reward: -131.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.106 [0.000, 2.000],  loss: 0.527961, mae: 10.735030, mean_q: -15.474858, mean_eps: 0.452952\n",
            "  86632/200000: episode: 331, duration: 1.209s, episode steps: 189, steps per second: 156, episode reward: -188.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.984 [0.000, 2.000],  loss: 0.566537, mae: 10.810843, mean_q: -15.579239, mean_eps: 0.451939\n",
            "  86811/200000: episode: 332, duration: 1.111s, episode steps: 179, steps per second: 161, episode reward: -178.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.905 [0.000, 2.000],  loss: 0.685221, mae: 10.725954, mean_q: -15.433096, mean_eps: 0.450773\n",
            "  86946/200000: episode: 333, duration: 0.854s, episode steps: 135, steps per second: 158, episode reward: -134.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.904 [0.000, 2.000],  loss: 0.561223, mae: 10.876902, mean_q: -15.660543, mean_eps: 0.449773\n",
            "  87089/200000: episode: 334, duration: 0.930s, episode steps: 143, steps per second: 154, episode reward: -142.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.951 [0.000, 2.000],  loss: 0.681099, mae: 10.872394, mean_q: -15.610866, mean_eps: 0.448886\n",
            "  87192/200000: episode: 335, duration: 0.683s, episode steps: 103, steps per second: 151, episode reward: -102.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.864 [0.000, 2.000],  loss: 0.609991, mae: 10.889159, mean_q: -15.658399, mean_eps: 0.448113\n",
            "  87310/200000: episode: 336, duration: 0.791s, episode steps: 118, steps per second: 149, episode reward: -117.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.915 [0.000, 2.000],  loss: 0.572461, mae: 10.884751, mean_q: -15.652891, mean_eps: 0.447417\n",
            "  87427/200000: episode: 337, duration: 0.733s, episode steps: 117, steps per second: 160, episode reward: -116.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.889 [0.000, 2.000],  loss: 0.693299, mae: 10.865947, mean_q: -15.622210, mean_eps: 0.446669\n",
            "  87893/200000: episode: 338, duration: 3.186s, episode steps: 466, steps per second: 146, episode reward: -465.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.736 [0.000, 2.000],  loss: 0.643530, mae: 10.842889, mean_q: -15.593407, mean_eps: 0.444820\n",
            "  88033/200000: episode: 339, duration: 1.176s, episode steps: 140, steps per second: 119, episode reward: -139.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.907 [0.000, 2.000],  loss: 0.657242, mae: 10.867229, mean_q: -15.609341, mean_eps: 0.442895\n",
            "  88147/200000: episode: 340, duration: 1.064s, episode steps: 114, steps per second: 107, episode reward: -113.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.912 [0.000, 2.000],  loss: 0.634655, mae: 10.913972, mean_q: -15.693720, mean_eps: 0.442097\n",
            "  88262/200000: episode: 341, duration: 1.045s, episode steps: 115, steps per second: 110, episode reward: -114.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.991 [0.000, 2.000],  loss: 0.513008, mae: 10.930583, mean_q: -15.761677, mean_eps: 0.441375\n",
            "  88414/200000: episode: 342, duration: 1.261s, episode steps: 152, steps per second: 121, episode reward: -151.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.053 [0.000, 2.000],  loss: 0.618591, mae: 11.004769, mean_q: -15.838297, mean_eps: 0.440526\n",
            "  88577/200000: episode: 343, duration: 1.135s, episode steps: 163, steps per second: 144, episode reward: -162.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.896 [0.000, 2.000],  loss: 0.672337, mae: 10.890293, mean_q: -15.644693, mean_eps: 0.439525\n",
            "  88719/200000: episode: 344, duration: 0.923s, episode steps: 142, steps per second: 154, episode reward: -141.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.746 [0.000, 2.000],  loss: 0.598472, mae: 10.993274, mean_q: -15.837111, mean_eps: 0.438563\n",
            "  88880/200000: episode: 345, duration: 1.111s, episode steps: 161, steps per second: 145, episode reward: -160.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.043 [0.000, 2.000],  loss: 0.640210, mae: 10.966150, mean_q: -15.773869, mean_eps: 0.437613\n",
            "  89053/200000: episode: 346, duration: 1.271s, episode steps: 173, steps per second: 136, episode reward: -172.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.942 [0.000, 2.000],  loss: 0.618879, mae: 10.943806, mean_q: -15.730238, mean_eps: 0.436549\n",
            "  89196/200000: episode: 347, duration: 1.172s, episode steps: 143, steps per second: 122, episode reward: -142.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.119 [0.000, 2.000],  loss: 0.623344, mae: 11.020738, mean_q: -15.843074, mean_eps: 0.435548\n",
            "  89321/200000: episode: 348, duration: 0.994s, episode steps: 125, steps per second: 126, episode reward: -124.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.792 [0.000, 2.000],  loss: 0.640343, mae: 11.002156, mean_q: -15.837830, mean_eps: 0.434699\n",
            "  89524/200000: episode: 349, duration: 1.333s, episode steps: 203, steps per second: 152, episode reward: -202.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.039 [0.000, 2.000],  loss: 0.672688, mae: 11.061963, mean_q: -15.922106, mean_eps: 0.433661\n",
            "  89686/200000: episode: 350, duration: 1.142s, episode steps: 162, steps per second: 142, episode reward: -161.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.852 [0.000, 2.000],  loss: 0.663002, mae: 11.004907, mean_q: -15.840362, mean_eps: 0.432508\n",
            "  89906/200000: episode: 351, duration: 2.238s, episode steps: 220, steps per second:  98, episode reward: -219.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.973 [0.000, 2.000],  loss: 0.627260, mae: 10.953019, mean_q: -15.770349, mean_eps: 0.431292\n",
            "  90084/200000: episode: 352, duration: 1.788s, episode steps: 178, steps per second: 100, episode reward: -177.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.700107, mae: 11.066085, mean_q: -15.944742, mean_eps: 0.430038\n",
            "  90220/200000: episode: 353, duration: 0.961s, episode steps: 136, steps per second: 142, episode reward: -135.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.985 [0.000, 2.000],  loss: 0.698836, mae: 11.135416, mean_q: -16.026794, mean_eps: 0.429050\n",
            "  90379/200000: episode: 354, duration: 1.128s, episode steps: 159, steps per second: 141, episode reward: -158.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.994 [0.000, 2.000],  loss: 0.637072, mae: 11.063124, mean_q: -15.918826, mean_eps: 0.428113\n",
            "  90504/200000: episode: 355, duration: 0.898s, episode steps: 125, steps per second: 139, episode reward: -124.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.976 [0.000, 2.000],  loss: 0.653142, mae: 11.082613, mean_q: -15.957231, mean_eps: 0.427213\n",
            "  90653/200000: episode: 356, duration: 1.091s, episode steps: 149, steps per second: 137, episode reward: -148.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.067 [0.000, 2.000],  loss: 0.644855, mae: 11.147403, mean_q: -16.046718, mean_eps: 0.426339\n",
            "  90774/200000: episode: 357, duration: 0.784s, episode steps: 121, steps per second: 154, episode reward: -120.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.033 [0.000, 2.000],  loss: 0.584148, mae: 11.176419, mean_q: -16.083883, mean_eps: 0.425478\n",
            "  90960/200000: episode: 358, duration: 1.270s, episode steps: 186, steps per second: 146, episode reward: -185.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.016 [0.000, 2.000],  loss: 0.626428, mae: 11.130094, mean_q: -16.022360, mean_eps: 0.424515\n",
            "  91127/200000: episode: 359, duration: 1.110s, episode steps: 167, steps per second: 150, episode reward: -166.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.042 [0.000, 2.000],  loss: 0.670723, mae: 11.125306, mean_q: -16.011267, mean_eps: 0.423401\n",
            "  91286/200000: episode: 360, duration: 1.085s, episode steps: 159, steps per second: 147, episode reward: -158.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.151 [0.000, 2.000],  loss: 0.712978, mae: 11.116182, mean_q: -15.987198, mean_eps: 0.422362\n",
            "  91408/200000: episode: 361, duration: 0.816s, episode steps: 122, steps per second: 149, episode reward: -121.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.992 [0.000, 2.000],  loss: 0.659799, mae: 11.215922, mean_q: -16.142840, mean_eps: 0.421475\n",
            "  91526/200000: episode: 362, duration: 0.821s, episode steps: 118, steps per second: 144, episode reward: -117.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.975 [0.000, 2.000],  loss: 0.702947, mae: 11.216108, mean_q: -16.119198, mean_eps: 0.420715\n",
            "  91668/200000: episode: 363, duration: 1.395s, episode steps: 142, steps per second: 102, episode reward: -141.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.042 [0.000, 2.000],  loss: 0.741172, mae: 11.206325, mean_q: -16.092150, mean_eps: 0.419892\n",
            "  91804/200000: episode: 364, duration: 1.275s, episode steps: 136, steps per second: 107, episode reward: -135.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.816 [0.000, 2.000],  loss: 0.596100, mae: 11.125640, mean_q: -16.022546, mean_eps: 0.419018\n",
            "  91995/200000: episode: 365, duration: 1.576s, episode steps: 191, steps per second: 121, episode reward: -190.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.937 [0.000, 2.000],  loss: 0.667202, mae: 11.126351, mean_q: -16.006825, mean_eps: 0.417979\n",
            "  92121/200000: episode: 366, duration: 0.859s, episode steps: 126, steps per second: 147, episode reward: -125.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.976 [0.000, 2.000],  loss: 0.589912, mae: 11.116670, mean_q: -16.022855, mean_eps: 0.416966\n",
            "  92278/200000: episode: 367, duration: 1.003s, episode steps: 157, steps per second: 156, episode reward: -156.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.631718, mae: 11.052750, mean_q: -15.900094, mean_eps: 0.416067\n",
            "  92381/200000: episode: 368, duration: 0.744s, episode steps: 103, steps per second: 139, episode reward: -102.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.194 [0.000, 2.000],  loss: 0.572382, mae: 11.032439, mean_q: -15.894339, mean_eps: 0.415243\n",
            "  92652/200000: episode: 369, duration: 1.951s, episode steps: 271, steps per second: 139, episode reward: -270.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.852 [0.000, 2.000],  loss: 0.649299, mae: 11.126650, mean_q: -16.014597, mean_eps: 0.414065\n",
            "  92837/200000: episode: 370, duration: 1.220s, episode steps: 185, steps per second: 152, episode reward: -184.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.054 [0.000, 2.000],  loss: 0.594200, mae: 11.085913, mean_q: -15.964398, mean_eps: 0.412621\n",
            "  92948/200000: episode: 371, duration: 0.758s, episode steps: 111, steps per second: 146, episode reward: -110.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.054 [0.000, 2.000],  loss: 0.634684, mae: 11.068347, mean_q: -15.933271, mean_eps: 0.411684\n",
            "  93043/200000: episode: 372, duration: 0.652s, episode steps:  95, steps per second: 146, episode reward: -94.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.926 [0.000, 2.000],  loss: 0.768570, mae: 11.096836, mean_q: -15.949158, mean_eps: 0.411038\n",
            "  93198/200000: episode: 373, duration: 1.086s, episode steps: 155, steps per second: 143, episode reward: -154.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.206 [0.000, 2.000],  loss: 0.593885, mae: 11.106805, mean_q: -15.992539, mean_eps: 0.410240\n",
            "  93347/200000: episode: 374, duration: 1.193s, episode steps: 149, steps per second: 125, episode reward: -148.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.228 [0.000, 2.000],  loss: 0.664219, mae: 11.152389, mean_q: -16.061844, mean_eps: 0.409277\n",
            "  93512/200000: episode: 375, duration: 1.747s, episode steps: 165, steps per second:  94, episode reward: -164.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.952 [0.000, 2.000],  loss: 0.670701, mae: 11.146042, mean_q: -16.022870, mean_eps: 0.408289\n",
            "  93621/200000: episode: 376, duration: 1.208s, episode steps: 109, steps per second:  90, episode reward: -108.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.982 [0.000, 2.000],  loss: 0.750808, mae: 11.267276, mean_q: -16.181839, mean_eps: 0.407415\n",
            "  93771/200000: episode: 377, duration: 1.161s, episode steps: 150, steps per second: 129, episode reward: -149.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.133 [0.000, 2.000],  loss: 0.648767, mae: 11.137694, mean_q: -16.021766, mean_eps: 0.406592\n",
            "  93893/200000: episode: 378, duration: 0.901s, episode steps: 122, steps per second: 135, episode reward: -121.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.049 [0.000, 2.000],  loss: 0.640678, mae: 11.154324, mean_q: -16.072395, mean_eps: 0.405731\n",
            "  93998/200000: episode: 379, duration: 0.720s, episode steps: 105, steps per second: 146, episode reward: -104.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.076 [0.000, 2.000],  loss: 0.663701, mae: 11.145374, mean_q: -16.036471, mean_eps: 0.405009\n",
            "  94183/200000: episode: 380, duration: 1.287s, episode steps: 185, steps per second: 144, episode reward: -184.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.178 [0.000, 2.000],  loss: 0.563294, mae: 11.193072, mean_q: -16.117887, mean_eps: 0.404097\n",
            "  94336/200000: episode: 381, duration: 1.209s, episode steps: 153, steps per second: 127, episode reward: -152.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.281 [0.000, 2.000],  loss: 0.702165, mae: 11.215492, mean_q: -16.120329, mean_eps: 0.403033\n",
            "  94494/200000: episode: 382, duration: 1.203s, episode steps: 158, steps per second: 131, episode reward: -157.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.152 [0.000, 2.000],  loss: 0.706845, mae: 11.216721, mean_q: -16.135084, mean_eps: 0.402045\n",
            "  94655/200000: episode: 383, duration: 1.132s, episode steps: 161, steps per second: 142, episode reward: -160.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.174 [0.000, 2.000],  loss: 0.594486, mae: 11.206125, mean_q: -16.149380, mean_eps: 0.401031\n",
            "  94799/200000: episode: 384, duration: 0.975s, episode steps: 144, steps per second: 148, episode reward: -143.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.194 [0.000, 2.000],  loss: 0.694503, mae: 11.186571, mean_q: -16.080268, mean_eps: 0.400069\n",
            "  94908/200000: episode: 385, duration: 0.728s, episode steps: 109, steps per second: 150, episode reward: -108.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.211 [0.000, 2.000],  loss: 0.719475, mae: 11.175244, mean_q: -16.062994, mean_eps: 0.399271\n",
            "  95034/200000: episode: 386, duration: 0.843s, episode steps: 126, steps per second: 149, episode reward: -125.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.976 [0.000, 2.000],  loss: 0.631030, mae: 11.360218, mean_q: -16.349489, mean_eps: 0.398523\n",
            "  95174/200000: episode: 387, duration: 1.317s, episode steps: 140, steps per second: 106, episode reward: -139.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.064 [0.000, 2.000],  loss: 0.817000, mae: 11.369892, mean_q: -16.311778, mean_eps: 0.397675\n",
            "  95291/200000: episode: 388, duration: 1.023s, episode steps: 117, steps per second: 114, episode reward: -116.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.724508, mae: 11.363253, mean_q: -16.362891, mean_eps: 0.396864\n",
            "  95408/200000: episode: 389, duration: 1.203s, episode steps: 117, steps per second:  97, episode reward: -116.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.145 [0.000, 2.000],  loss: 0.673199, mae: 11.350951, mean_q: -16.336168, mean_eps: 0.396129\n",
            "  95512/200000: episode: 390, duration: 0.736s, episode steps: 104, steps per second: 141, episode reward: -103.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.038 [0.000, 2.000],  loss: 0.513431, mae: 11.450449, mean_q: -16.543553, mean_eps: 0.395433\n",
            "  95639/200000: episode: 391, duration: 0.842s, episode steps: 127, steps per second: 151, episode reward: -126.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.937 [0.000, 2.000],  loss: 0.642223, mae: 11.369496, mean_q: -16.370643, mean_eps: 0.394698\n",
            "  95940/200000: episode: 392, duration: 1.944s, episode steps: 301, steps per second: 155, episode reward: -300.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.748 [0.000, 2.000],  loss: 0.693484, mae: 11.380108, mean_q: -16.362106, mean_eps: 0.393343\n",
            "  96131/200000: episode: 393, duration: 1.249s, episode steps: 191, steps per second: 153, episode reward: -190.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.079 [0.000, 2.000],  loss: 0.672369, mae: 11.345607, mean_q: -16.336055, mean_eps: 0.391785\n",
            "  96261/200000: episode: 394, duration: 0.899s, episode steps: 130, steps per second: 145, episode reward: -129.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.985 [0.000, 2.000],  loss: 0.646618, mae: 11.332838, mean_q: -16.304689, mean_eps: 0.390759\n",
            "  96348/200000: episode: 395, duration: 0.628s, episode steps:  87, steps per second: 139, episode reward: -86.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.138 [0.000, 2.000],  loss: 0.688153, mae: 11.294433, mean_q: -16.270571, mean_eps: 0.390075\n",
            "  96451/200000: episode: 396, duration: 0.723s, episode steps: 103, steps per second: 143, episode reward: -102.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.019 [0.000, 2.000],  loss: 0.768911, mae: 11.409509, mean_q: -16.388013, mean_eps: 0.389479\n",
            "  96557/200000: episode: 397, duration: 0.729s, episode steps: 106, steps per second: 145, episode reward: -105.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.991 [0.000, 2.000],  loss: 0.632744, mae: 11.367357, mean_q: -16.362082, mean_eps: 0.388808\n",
            "  96685/200000: episode: 398, duration: 0.885s, episode steps: 128, steps per second: 145, episode reward: -127.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.250 [0.000, 2.000],  loss: 0.633528, mae: 11.421159, mean_q: -16.433572, mean_eps: 0.388061\n",
            "  96802/200000: episode: 399, duration: 0.809s, episode steps: 117, steps per second: 145, episode reward: -116.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.111 [0.000, 2.000],  loss: 0.777364, mae: 11.470918, mean_q: -16.483218, mean_eps: 0.387288\n",
            "  96897/200000: episode: 400, duration: 0.726s, episode steps:  95, steps per second: 131, episode reward: -94.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.771586, mae: 11.372306, mean_q: -16.320571, mean_eps: 0.386617\n",
            "  97173/200000: episode: 401, duration: 2.636s, episode steps: 276, steps per second: 105, episode reward: -275.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.199 [0.000, 2.000],  loss: 0.675183, mae: 11.387209, mean_q: -16.381849, mean_eps: 0.385439\n",
            "  97303/200000: episode: 402, duration: 1.073s, episode steps: 130, steps per second: 121, episode reward: -129.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.992 [0.000, 2.000],  loss: 0.809662, mae: 11.440944, mean_q: -16.416182, mean_eps: 0.384159\n",
            "  97445/200000: episode: 403, duration: 1.002s, episode steps: 142, steps per second: 142, episode reward: -141.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.915 [0.000, 2.000],  loss: 0.672621, mae: 11.395160, mean_q: -16.398446, mean_eps: 0.383298\n",
            "  97578/200000: episode: 404, duration: 0.930s, episode steps: 133, steps per second: 143, episode reward: -132.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.053 [0.000, 2.000],  loss: 0.637622, mae: 11.369743, mean_q: -16.361022, mean_eps: 0.382424\n",
            "  97733/200000: episode: 405, duration: 1.059s, episode steps: 155, steps per second: 146, episode reward: -154.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.174 [0.000, 2.000],  loss: 0.719267, mae: 11.440805, mean_q: -16.488792, mean_eps: 0.381512\n",
            "  97867/200000: episode: 406, duration: 0.896s, episode steps: 134, steps per second: 150, episode reward: -133.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.112 [0.000, 2.000],  loss: 0.672870, mae: 11.411117, mean_q: -16.444103, mean_eps: 0.380600\n",
            "  97980/200000: episode: 407, duration: 0.775s, episode steps: 113, steps per second: 146, episode reward: -112.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.973 [0.000, 2.000],  loss: 0.731448, mae: 11.413270, mean_q: -16.441149, mean_eps: 0.379827\n",
            "  98118/200000: episode: 408, duration: 0.902s, episode steps: 138, steps per second: 153, episode reward: -137.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.043 [0.000, 2.000],  loss: 0.690953, mae: 11.422991, mean_q: -16.443874, mean_eps: 0.379029\n",
            "  98331/200000: episode: 409, duration: 1.394s, episode steps: 213, steps per second: 153, episode reward: -212.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.803 [0.000, 2.000],  loss: 0.690420, mae: 11.492172, mean_q: -16.549175, mean_eps: 0.377915\n",
            "  98492/200000: episode: 410, duration: 1.157s, episode steps: 161, steps per second: 139, episode reward: -160.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.839 [0.000, 2.000],  loss: 0.654268, mae: 11.421718, mean_q: -16.429886, mean_eps: 0.376737\n",
            "  98641/200000: episode: 411, duration: 1.068s, episode steps: 149, steps per second: 140, episode reward: -148.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.886 [0.000, 2.000],  loss: 0.782753, mae: 11.498098, mean_q: -16.523127, mean_eps: 0.375749\n",
            "  98734/200000: episode: 412, duration: 0.774s, episode steps:  93, steps per second: 120, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.118 [0.000, 2.000],  loss: 0.687305, mae: 11.446351, mean_q: -16.460932, mean_eps: 0.374976\n",
            "  98898/200000: episode: 413, duration: 1.558s, episode steps: 164, steps per second: 105, episode reward: -163.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.890 [0.000, 2.000],  loss: 0.741261, mae: 11.515163, mean_q: -16.544456, mean_eps: 0.374165\n",
            "  99005/200000: episode: 414, duration: 1.101s, episode steps: 107, steps per second:  97, episode reward: -106.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.850 [0.000, 2.000],  loss: 0.606438, mae: 11.375990, mean_q: -16.377694, mean_eps: 0.373304\n",
            "  99109/200000: episode: 415, duration: 0.842s, episode steps: 104, steps per second: 123, episode reward: -103.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.981 [0.000, 2.000],  loss: 0.706570, mae: 11.369584, mean_q: -16.345412, mean_eps: 0.372633\n",
            "  99218/200000: episode: 416, duration: 0.746s, episode steps: 109, steps per second: 146, episode reward: -108.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.982 [0.000, 2.000],  loss: 0.633537, mae: 11.463440, mean_q: -16.523059, mean_eps: 0.371961\n",
            "  99312/200000: episode: 417, duration: 0.652s, episode steps:  94, steps per second: 144, episode reward: -93.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.936 [0.000, 2.000],  loss: 0.870726, mae: 11.466772, mean_q: -16.505023, mean_eps: 0.371328\n",
            "  99444/200000: episode: 418, duration: 0.949s, episode steps: 132, steps per second: 139, episode reward: -131.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.114 [0.000, 2.000],  loss: 0.698676, mae: 11.473526, mean_q: -16.505673, mean_eps: 0.370619\n",
            "  99563/200000: episode: 419, duration: 0.857s, episode steps: 119, steps per second: 139, episode reward: -118.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.025 [0.000, 2.000],  loss: 0.756941, mae: 11.450830, mean_q: -16.500151, mean_eps: 0.369821\n",
            "  99687/200000: episode: 420, duration: 0.901s, episode steps: 124, steps per second: 138, episode reward: -123.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.879 [0.000, 2.000],  loss: 0.716347, mae: 11.335784, mean_q: -16.321199, mean_eps: 0.369048\n",
            "  99818/200000: episode: 421, duration: 0.925s, episode steps: 131, steps per second: 142, episode reward: -130.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.061 [0.000, 2.000],  loss: 0.637874, mae: 11.332688, mean_q: -16.326563, mean_eps: 0.368237\n",
            "  99974/200000: episode: 422, duration: 1.122s, episode steps: 156, steps per second: 139, episode reward: -155.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.981 [0.000, 2.000],  loss: 0.710486, mae: 11.402552, mean_q: -16.435073, mean_eps: 0.367325\n",
            " 100094/200000: episode: 423, duration: 0.812s, episode steps: 120, steps per second: 148, episode reward: -119.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.025 [0.000, 2.000],  loss: 0.802658, mae: 11.405822, mean_q: -16.414211, mean_eps: 0.366451\n",
            " 100558/200000: episode: 424, duration: 3.455s, episode steps: 464, steps per second: 134, episode reward: -463.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.828 [0.000, 2.000],  loss: 0.664777, mae: 11.514989, mean_q: -16.573758, mean_eps: 0.364602\n",
            " 100670/200000: episode: 425, duration: 1.177s, episode steps: 112, steps per second:  95, episode reward: -111.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.723 [0.000, 2.000],  loss: 0.660853, mae: 11.444008, mean_q: -16.468741, mean_eps: 0.362778\n",
            " 100768/200000: episode: 426, duration: 1.016s, episode steps:  98, steps per second:  96, episode reward: -97.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.980 [0.000, 2.000],  loss: 0.750896, mae: 11.364321, mean_q: -16.309744, mean_eps: 0.362119\n",
            " 100883/200000: episode: 427, duration: 0.965s, episode steps: 115, steps per second: 119, episode reward: -114.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.930 [0.000, 2.000],  loss: 0.662586, mae: 11.469191, mean_q: -16.495451, mean_eps: 0.361448\n",
            " 101035/200000: episode: 428, duration: 1.089s, episode steps: 152, steps per second: 140, episode reward: -151.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.941 [0.000, 2.000],  loss: 0.608471, mae: 11.454317, mean_q: -16.494358, mean_eps: 0.360599\n",
            " 101170/200000: episode: 429, duration: 1.025s, episode steps: 135, steps per second: 132, episode reward: -134.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.926 [0.000, 2.000],  loss: 0.839746, mae: 11.676322, mean_q: -16.785413, mean_eps: 0.359687\n",
            " 101371/200000: episode: 430, duration: 1.439s, episode steps: 201, steps per second: 140, episode reward: -200.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.871 [0.000, 2.000],  loss: 0.776450, mae: 11.647536, mean_q: -16.747273, mean_eps: 0.358623\n",
            " 101549/200000: episode: 431, duration: 1.326s, episode steps: 178, steps per second: 134, episode reward: -177.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.955 [0.000, 2.000],  loss: 0.741116, mae: 11.529163, mean_q: -16.579857, mean_eps: 0.357420\n",
            " 101728/200000: episode: 432, duration: 1.210s, episode steps: 179, steps per second: 148, episode reward: -178.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.117 [0.000, 2.000],  loss: 0.660049, mae: 11.570029, mean_q: -16.663934, mean_eps: 0.356293\n",
            " 101884/200000: episode: 433, duration: 1.052s, episode steps: 156, steps per second: 148, episode reward: -155.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.955 [0.000, 2.000],  loss: 0.729443, mae: 11.621930, mean_q: -16.738100, mean_eps: 0.355241\n",
            " 102007/200000: episode: 434, duration: 0.834s, episode steps: 123, steps per second: 147, episode reward: -122.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.943 [0.000, 2.000],  loss: 0.783371, mae: 11.463836, mean_q: -16.447881, mean_eps: 0.354355\n",
            " 102097/200000: episode: 435, duration: 0.613s, episode steps:  90, steps per second: 147, episode reward: -89.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.659841, mae: 11.533634, mean_q: -16.585752, mean_eps: 0.353671\n",
            " 102265/200000: episode: 436, duration: 1.269s, episode steps: 168, steps per second: 132, episode reward: -167.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.833 [0.000, 2.000],  loss: 0.837690, mae: 11.588234, mean_q: -16.653755, mean_eps: 0.352847\n",
            " 102401/200000: episode: 437, duration: 1.309s, episode steps: 136, steps per second: 104, episode reward: -135.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.132 [0.000, 2.000],  loss: 0.783572, mae: 11.474750, mean_q: -16.499647, mean_eps: 0.351885\n",
            " 102549/200000: episode: 438, duration: 1.434s, episode steps: 148, steps per second: 103, episode reward: -147.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.899 [0.000, 2.000],  loss: 0.667742, mae: 11.411899, mean_q: -16.431688, mean_eps: 0.350985\n",
            " 102677/200000: episode: 439, duration: 0.900s, episode steps: 128, steps per second: 142, episode reward: -127.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.859 [0.000, 2.000],  loss: 0.707290, mae: 11.419189, mean_q: -16.436236, mean_eps: 0.350111\n",
            " 102777/200000: episode: 440, duration: 0.693s, episode steps: 100, steps per second: 144, episode reward: -99.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.990 [0.000, 2.000],  loss: 0.717886, mae: 11.558401, mean_q: -16.637813, mean_eps: 0.349389\n",
            " 102871/200000: episode: 441, duration: 0.656s, episode steps:  94, steps per second: 143, episode reward: -93.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.053 [0.000, 2.000],  loss: 0.665073, mae: 11.580006, mean_q: -16.668955, mean_eps: 0.348781\n",
            " 103005/200000: episode: 442, duration: 0.999s, episode steps: 134, steps per second: 134, episode reward: -133.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.097 [0.000, 2.000],  loss: 0.764334, mae: 11.562849, mean_q: -16.617189, mean_eps: 0.348059\n",
            " 103122/200000: episode: 443, duration: 0.826s, episode steps: 117, steps per second: 142, episode reward: -116.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.051 [0.000, 2.000],  loss: 0.863070, mae: 11.324060, mean_q: -16.237347, mean_eps: 0.347261\n",
            " 103235/200000: episode: 444, duration: 0.746s, episode steps: 113, steps per second: 151, episode reward: -112.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.062 [0.000, 2.000],  loss: 0.716755, mae: 11.282860, mean_q: -16.224576, mean_eps: 0.346539\n",
            " 103343/200000: episode: 445, duration: 0.758s, episode steps: 108, steps per second: 142, episode reward: -107.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.065 [0.000, 2.000],  loss: 0.723754, mae: 11.333144, mean_q: -16.286449, mean_eps: 0.345843\n",
            " 103476/200000: episode: 446, duration: 0.946s, episode steps: 133, steps per second: 141, episode reward: -132.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.992 [0.000, 2.000],  loss: 0.645175, mae: 11.425764, mean_q: -16.453963, mean_eps: 0.345083\n",
            " 103588/200000: episode: 447, duration: 0.787s, episode steps: 112, steps per second: 142, episode reward: -111.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.036 [0.000, 2.000],  loss: 0.862200, mae: 11.319628, mean_q: -16.270777, mean_eps: 0.344310\n",
            " 103739/200000: episode: 448, duration: 1.026s, episode steps: 151, steps per second: 147, episode reward: -150.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.934 [0.000, 2.000],  loss: 0.669197, mae: 11.326146, mean_q: -16.328239, mean_eps: 0.343474\n",
            " 103892/200000: episode: 449, duration: 1.075s, episode steps: 153, steps per second: 142, episode reward: -152.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.948 [0.000, 2.000],  loss: 0.588146, mae: 11.353191, mean_q: -16.383100, mean_eps: 0.342511\n",
            " 104032/200000: episode: 450, duration: 1.107s, episode steps: 140, steps per second: 126, episode reward: -139.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.929 [0.000, 2.000],  loss: 0.622789, mae: 11.296749, mean_q: -16.268636, mean_eps: 0.341587\n",
            " 104159/200000: episode: 451, duration: 1.285s, episode steps: 127, steps per second:  99, episode reward: -126.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.937 [0.000, 2.000],  loss: 0.714848, mae: 11.405574, mean_q: -16.437784, mean_eps: 0.340738\n",
            " 104285/200000: episode: 452, duration: 1.229s, episode steps: 126, steps per second: 103, episode reward: -125.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.921 [0.000, 2.000],  loss: 0.614433, mae: 11.394967, mean_q: -16.432181, mean_eps: 0.339927\n",
            " 104370/200000: episode: 453, duration: 0.851s, episode steps:  85, steps per second: 100, episode reward: -84.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.024 [0.000, 2.000],  loss: 0.669466, mae: 11.272677, mean_q: -16.207588, mean_eps: 0.339256\n",
            " 104483/200000: episode: 454, duration: 0.834s, episode steps: 113, steps per second: 136, episode reward: -112.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.965 [0.000, 2.000],  loss: 0.584658, mae: 11.433774, mean_q: -16.482715, mean_eps: 0.338635\n",
            " 104631/200000: episode: 455, duration: 1.065s, episode steps: 148, steps per second: 139, episode reward: -147.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.081 [0.000, 2.000],  loss: 0.625523, mae: 11.356161, mean_q: -16.346861, mean_eps: 0.337812\n",
            " 104766/200000: episode: 456, duration: 1.023s, episode steps: 135, steps per second: 132, episode reward: -134.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.081 [0.000, 2.000],  loss: 0.731479, mae: 11.458922, mean_q: -16.479188, mean_eps: 0.336913\n",
            " 104902/200000: episode: 457, duration: 1.123s, episode steps: 136, steps per second: 121, episode reward: -135.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.824 [0.000, 2.000],  loss: 0.565598, mae: 11.412395, mean_q: -16.440903, mean_eps: 0.336051\n",
            " 104987/200000: episode: 458, duration: 0.701s, episode steps:  85, steps per second: 121, episode reward: -84.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.773463, mae: 11.422400, mean_q: -16.398598, mean_eps: 0.335355\n",
            " 105085/200000: episode: 459, duration: 0.736s, episode steps:  98, steps per second: 133, episode reward: -97.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.133 [0.000, 2.000],  loss: 0.568770, mae: 11.239180, mean_q: -16.198304, mean_eps: 0.334772\n",
            " 105188/200000: episode: 460, duration: 0.759s, episode steps: 103, steps per second: 136, episode reward: -102.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.107 [0.000, 2.000],  loss: 0.644852, mae: 11.265550, mean_q: -16.194100, mean_eps: 0.334139\n",
            " 105319/200000: episode: 461, duration: 0.930s, episode steps: 131, steps per second: 141, episode reward: -130.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.099 [0.000, 2.000],  loss: 0.534792, mae: 11.263835, mean_q: -16.248031, mean_eps: 0.333404\n",
            " 105443/200000: episode: 462, duration: 0.877s, episode steps: 124, steps per second: 141, episode reward: -123.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.355 [0.000, 2.000],  loss: 0.631683, mae: 11.279488, mean_q: -16.252268, mean_eps: 0.332593\n",
            " 105568/200000: episode: 463, duration: 0.912s, episode steps: 125, steps per second: 137, episode reward: -124.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.240 [0.000, 2.000],  loss: 0.609316, mae: 11.190944, mean_q: -16.099877, mean_eps: 0.331808\n",
            " 105682/200000: episode: 464, duration: 0.833s, episode steps: 114, steps per second: 137, episode reward: -113.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.193 [0.000, 2.000],  loss: 0.621710, mae: 11.270875, mean_q: -16.232592, mean_eps: 0.331048\n",
            " 105794/200000: episode: 465, duration: 1.083s, episode steps: 112, steps per second: 103, episode reward: -111.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.982 [0.000, 2.000],  loss: 0.737120, mae: 11.198909, mean_q: -16.107605, mean_eps: 0.330326\n",
            " 105942/200000: episode: 466, duration: 1.397s, episode steps: 148, steps per second: 106, episode reward: -147.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.074 [0.000, 2.000],  loss: 0.590481, mae: 11.331937, mean_q: -16.319625, mean_eps: 0.329503\n",
            " 106080/200000: episode: 467, duration: 1.272s, episode steps: 138, steps per second: 109, episode reward: -137.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.297 [0.000, 2.000],  loss: 0.659438, mae: 11.149095, mean_q: -16.051298, mean_eps: 0.328603\n",
            " 106225/200000: episode: 468, duration: 1.064s, episode steps: 145, steps per second: 136, episode reward: -144.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.028 [0.000, 2.000],  loss: 0.660539, mae: 11.230698, mean_q: -16.162019, mean_eps: 0.327704\n",
            " 106322/200000: episode: 469, duration: 0.653s, episode steps:  97, steps per second: 149, episode reward: -96.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.124 [0.000, 2.000],  loss: 0.692206, mae: 11.203860, mean_q: -16.100113, mean_eps: 0.326931\n",
            " 106430/200000: episode: 470, duration: 0.751s, episode steps: 108, steps per second: 144, episode reward: -107.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.935 [0.000, 2.000],  loss: 0.619685, mae: 11.104243, mean_q: -15.971653, mean_eps: 0.326285\n",
            " 106564/200000: episode: 471, duration: 0.902s, episode steps: 134, steps per second: 149, episode reward: -133.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.104 [0.000, 2.000],  loss: 0.720534, mae: 11.139352, mean_q: -16.018293, mean_eps: 0.325525\n",
            " 106681/200000: episode: 472, duration: 0.808s, episode steps: 117, steps per second: 145, episode reward: -116.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.651021, mae: 11.180889, mean_q: -16.096842, mean_eps: 0.324727\n",
            " 106810/200000: episode: 473, duration: 0.906s, episode steps: 129, steps per second: 142, episode reward: -128.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.054 [0.000, 2.000],  loss: 0.643159, mae: 11.194363, mean_q: -16.122300, mean_eps: 0.323942\n",
            " 106916/200000: episode: 474, duration: 0.751s, episode steps: 106, steps per second: 141, episode reward: -105.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.953 [0.000, 2.000],  loss: 0.602800, mae: 11.181034, mean_q: -16.110101, mean_eps: 0.323207\n",
            " 107033/200000: episode: 475, duration: 0.839s, episode steps: 117, steps per second: 140, episode reward: -116.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.094 [0.000, 2.000],  loss: 0.717509, mae: 11.186424, mean_q: -16.087382, mean_eps: 0.322498\n",
            " 107198/200000: episode: 476, duration: 1.111s, episode steps: 165, steps per second: 149, episode reward: -164.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.879 [0.000, 2.000],  loss: 0.675737, mae: 11.244720, mean_q: -16.180638, mean_eps: 0.321599\n",
            " 107288/200000: episode: 477, duration: 0.626s, episode steps:  90, steps per second: 144, episode reward: -89.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.944 [0.000, 2.000],  loss: 0.578698, mae: 11.236376, mean_q: -16.185664, mean_eps: 0.320801\n",
            " 107392/200000: episode: 478, duration: 0.781s, episode steps: 104, steps per second: 133, episode reward: -103.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.087 [0.000, 2.000],  loss: 0.769891, mae: 11.279211, mean_q: -16.209449, mean_eps: 0.320193\n",
            " 107477/200000: episode: 479, duration: 0.735s, episode steps:  85, steps per second: 116, episode reward: -84.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.129 [0.000, 2.000],  loss: 0.659537, mae: 11.196284, mean_q: -16.109706, mean_eps: 0.319585\n",
            " 107580/200000: episode: 480, duration: 1.037s, episode steps: 103, steps per second:  99, episode reward: -102.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.639887, mae: 11.233442, mean_q: -16.159722, mean_eps: 0.318989\n",
            " 107675/200000: episode: 481, duration: 0.944s, episode steps:  95, steps per second: 101, episode reward: -94.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.053 [0.000, 2.000],  loss: 0.839344, mae: 11.215719, mean_q: -16.093809, mean_eps: 0.318369\n",
            " 107818/200000: episode: 482, duration: 1.417s, episode steps: 143, steps per second: 101, episode reward: -142.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.056 [0.000, 2.000],  loss: 0.576647, mae: 11.184058, mean_q: -16.096748, mean_eps: 0.317609\n",
            " 107976/200000: episode: 483, duration: 1.092s, episode steps: 158, steps per second: 145, episode reward: -157.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.842 [0.000, 2.000],  loss: 0.746681, mae: 11.235767, mean_q: -16.143889, mean_eps: 0.316659\n",
            " 108064/200000: episode: 484, duration: 0.684s, episode steps:  88, steps per second: 129, episode reward: -87.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.148 [0.000, 2.000],  loss: 0.643885, mae: 11.302189, mean_q: -16.245671, mean_eps: 0.315886\n",
            " 108161/200000: episode: 485, duration: 0.725s, episode steps:  97, steps per second: 134, episode reward: -96.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.021 [0.000, 2.000],  loss: 0.660685, mae: 11.325536, mean_q: -16.307907, mean_eps: 0.315291\n",
            " 108286/200000: episode: 486, duration: 0.866s, episode steps: 125, steps per second: 144, episode reward: -124.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.936 [0.000, 2.000],  loss: 0.645979, mae: 11.317115, mean_q: -16.280203, mean_eps: 0.314581\n",
            " 108370/200000: episode: 487, duration: 0.615s, episode steps:  84, steps per second: 137, episode reward: -83.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.952 [0.000, 2.000],  loss: 0.743948, mae: 11.327741, mean_q: -16.288996, mean_eps: 0.313923\n",
            " 108470/200000: episode: 488, duration: 0.671s, episode steps: 100, steps per second: 149, episode reward: -99.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.060 [0.000, 2.000],  loss: 0.712653, mae: 11.195284, mean_q: -16.084521, mean_eps: 0.313340\n",
            " 108592/200000: episode: 489, duration: 0.844s, episode steps: 122, steps per second: 145, episode reward: -121.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.959 [0.000, 2.000],  loss: 0.570939, mae: 11.203795, mean_q: -16.143058, mean_eps: 0.312643\n",
            " 108718/200000: episode: 490, duration: 0.855s, episode steps: 126, steps per second: 147, episode reward: -125.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.063 [0.000, 2.000],  loss: 0.666657, mae: 11.214314, mean_q: -16.142313, mean_eps: 0.311858\n",
            " 108917/200000: episode: 491, duration: 1.359s, episode steps: 199, steps per second: 146, episode reward: -198.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.824 [0.000, 2.000],  loss: 0.614513, mae: 11.265961, mean_q: -16.228127, mean_eps: 0.310819\n",
            " 109009/200000: episode: 492, duration: 0.632s, episode steps:  92, steps per second: 146, episode reward: -91.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.087 [0.000, 2.000],  loss: 0.664664, mae: 11.206383, mean_q: -16.116375, mean_eps: 0.309895\n",
            " 109128/200000: episode: 493, duration: 0.833s, episode steps: 119, steps per second: 143, episode reward: -118.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.025 [0.000, 2.000],  loss: 0.612892, mae: 11.246291, mean_q: -16.193243, mean_eps: 0.309236\n",
            " 109225/200000: episode: 494, duration: 0.736s, episode steps:  97, steps per second: 132, episode reward: -96.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.031 [0.000, 2.000],  loss: 0.592185, mae: 11.247970, mean_q: -16.216941, mean_eps: 0.308552\n",
            " 109349/200000: episode: 495, duration: 1.267s, episode steps: 124, steps per second:  98, episode reward: -123.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.169 [0.000, 2.000],  loss: 0.699271, mae: 11.284198, mean_q: -16.235558, mean_eps: 0.307843\n",
            " 109510/200000: episode: 496, duration: 1.568s, episode steps: 161, steps per second: 103, episode reward: -160.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.963 [0.000, 2.000],  loss: 0.652605, mae: 11.244139, mean_q: -16.185759, mean_eps: 0.306943\n",
            " 109632/200000: episode: 497, duration: 1.073s, episode steps: 122, steps per second: 114, episode reward: -121.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.975 [0.000, 2.000],  loss: 0.663635, mae: 11.308045, mean_q: -16.273295, mean_eps: 0.306057\n",
            " 109902/200000: episode: 498, duration: 1.836s, episode steps: 270, steps per second: 147, episode reward: -269.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.200 [0.000, 2.000],  loss: 0.617676, mae: 11.232857, mean_q: -16.168524, mean_eps: 0.304815\n",
            " 110025/200000: episode: 499, duration: 0.846s, episode steps: 123, steps per second: 145, episode reward: -122.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.976 [0.000, 2.000],  loss: 0.606347, mae: 11.284413, mean_q: -16.240986, mean_eps: 0.303561\n",
            " 110142/200000: episode: 500, duration: 0.808s, episode steps: 117, steps per second: 145, episode reward: -116.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.915 [0.000, 2.000],  loss: 0.724036, mae: 11.378282, mean_q: -16.374069, mean_eps: 0.302801\n",
            " 110267/200000: episode: 501, duration: 0.833s, episode steps: 125, steps per second: 150, episode reward: -124.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.856 [0.000, 2.000],  loss: 0.659569, mae: 11.330953, mean_q: -16.321977, mean_eps: 0.302041\n",
            " 110422/200000: episode: 502, duration: 1.081s, episode steps: 155, steps per second: 143, episode reward: -154.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.161 [0.000, 2.000],  loss: 0.597327, mae: 11.275291, mean_q: -16.247301, mean_eps: 0.301155\n",
            " 110551/200000: episode: 503, duration: 0.918s, episode steps: 129, steps per second: 141, episode reward: -128.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.922 [0.000, 2.000],  loss: 0.667562, mae: 11.357452, mean_q: -16.353523, mean_eps: 0.300255\n",
            " 110650/200000: episode: 504, duration: 0.720s, episode steps:  99, steps per second: 137, episode reward: -98.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.687 [0.000, 2.000],  loss: 0.586686, mae: 11.286231, mean_q: -16.280033, mean_eps: 0.299533\n",
            " 110782/200000: episode: 505, duration: 0.920s, episode steps: 132, steps per second: 144, episode reward: -131.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.841 [0.000, 2.000],  loss: 0.594778, mae: 11.311631, mean_q: -16.286646, mean_eps: 0.298799\n",
            " 110930/200000: episode: 506, duration: 1.014s, episode steps: 148, steps per second: 146, episode reward: -147.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.919 [0.000, 2.000],  loss: 0.688460, mae: 11.322052, mean_q: -16.270478, mean_eps: 0.297912\n",
            " 111030/200000: episode: 507, duration: 0.774s, episode steps: 100, steps per second: 129, episode reward: -99.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.970 [0.000, 2.000],  loss: 0.619583, mae: 11.316604, mean_q: -16.291857, mean_eps: 0.297127\n",
            " 111161/200000: episode: 508, duration: 1.244s, episode steps: 131, steps per second: 105, episode reward: -130.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.638537, mae: 11.382322, mean_q: -16.382179, mean_eps: 0.296392\n",
            " 111275/200000: episode: 509, duration: 1.071s, episode steps: 114, steps per second: 106, episode reward: -113.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.965 [0.000, 2.000],  loss: 0.614382, mae: 11.286581, mean_q: -16.237480, mean_eps: 0.295619\n",
            " 111400/200000: episode: 510, duration: 1.209s, episode steps: 125, steps per second: 103, episode reward: -124.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.768 [0.000, 2.000],  loss: 0.634159, mae: 11.302659, mean_q: -16.273235, mean_eps: 0.294872\n",
            " 111569/200000: episode: 511, duration: 1.127s, episode steps: 169, steps per second: 150, episode reward: -168.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.735195, mae: 11.319142, mean_q: -16.269552, mean_eps: 0.293935\n",
            " 111688/200000: episode: 512, duration: 0.818s, episode steps: 119, steps per second: 145, episode reward: -118.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.966 [0.000, 2.000],  loss: 0.719778, mae: 11.345237, mean_q: -16.316515, mean_eps: 0.293023\n",
            " 111810/200000: episode: 513, duration: 0.804s, episode steps: 122, steps per second: 152, episode reward: -121.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.934 [0.000, 2.000],  loss: 0.588889, mae: 11.386780, mean_q: -16.404208, mean_eps: 0.292263\n",
            " 111997/200000: episode: 514, duration: 1.277s, episode steps: 187, steps per second: 146, episode reward: -186.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.834 [0.000, 2.000],  loss: 0.656734, mae: 11.370595, mean_q: -16.360217, mean_eps: 0.291275\n",
            " 112131/200000: episode: 515, duration: 0.901s, episode steps: 134, steps per second: 149, episode reward: -133.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.836 [0.000, 2.000],  loss: 0.649151, mae: 11.319885, mean_q: -16.297519, mean_eps: 0.290261\n",
            " 112229/200000: episode: 516, duration: 0.693s, episode steps:  98, steps per second: 141, episode reward: -97.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.949 [0.000, 2.000],  loss: 0.661425, mae: 11.409023, mean_q: -16.423116, mean_eps: 0.289527\n",
            " 112388/200000: episode: 517, duration: 1.098s, episode steps: 159, steps per second: 145, episode reward: -158.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.044 [0.000, 2.000],  loss: 0.665507, mae: 11.340963, mean_q: -16.323456, mean_eps: 0.288716\n",
            " 112511/200000: episode: 518, duration: 0.843s, episode steps: 123, steps per second: 146, episode reward: -122.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.740 [0.000, 2.000],  loss: 0.629879, mae: 11.290715, mean_q: -16.256449, mean_eps: 0.287829\n",
            " 112615/200000: episode: 519, duration: 0.714s, episode steps: 104, steps per second: 146, episode reward: -103.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.904 [0.000, 2.000],  loss: 0.615555, mae: 11.375779, mean_q: -16.364770, mean_eps: 0.287107\n",
            " 112749/200000: episode: 520, duration: 0.893s, episode steps: 134, steps per second: 150, episode reward: -133.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.172 [0.000, 2.000],  loss: 0.618811, mae: 11.420432, mean_q: -16.430653, mean_eps: 0.286347\n",
            " 112887/200000: episode: 521, duration: 1.109s, episode steps: 138, steps per second: 124, episode reward: -137.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.043 [0.000, 2.000],  loss: 0.643724, mae: 11.416640, mean_q: -16.429727, mean_eps: 0.285486\n",
            " 112992/200000: episode: 522, duration: 1.094s, episode steps: 105, steps per second:  96, episode reward: -104.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.810 [0.000, 2.000],  loss: 0.626491, mae: 11.380267, mean_q: -16.377150, mean_eps: 0.284726\n",
            " 113119/200000: episode: 523, duration: 1.241s, episode steps: 127, steps per second: 102, episode reward: -126.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.874 [0.000, 2.000],  loss: 0.602352, mae: 11.383740, mean_q: -16.394095, mean_eps: 0.283991\n",
            " 113229/200000: episode: 524, duration: 0.992s, episode steps: 110, steps per second: 111, episode reward: -109.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.073 [0.000, 2.000],  loss: 0.656682, mae: 11.375783, mean_q: -16.350910, mean_eps: 0.283231\n",
            " 113356/200000: episode: 525, duration: 0.901s, episode steps: 127, steps per second: 141, episode reward: -126.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.039 [0.000, 2.000],  loss: 0.674001, mae: 11.486563, mean_q: -16.543553, mean_eps: 0.282484\n",
            " 113440/200000: episode: 526, duration: 0.597s, episode steps:  84, steps per second: 141, episode reward: -83.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.619516, mae: 11.383248, mean_q: -16.396153, mean_eps: 0.281825\n",
            " 113601/200000: episode: 527, duration: 1.138s, episode steps: 161, steps per second: 141, episode reward: -160.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.994 [0.000, 2.000],  loss: 0.655836, mae: 11.451183, mean_q: -16.482008, mean_eps: 0.281040\n",
            " 113698/200000: episode: 528, duration: 0.656s, episode steps:  97, steps per second: 148, episode reward: -96.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.031 [0.000, 2.000],  loss: 0.697533, mae: 11.435750, mean_q: -16.449167, mean_eps: 0.280217\n",
            " 113800/200000: episode: 529, duration: 0.695s, episode steps: 102, steps per second: 147, episode reward: -101.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.912 [0.000, 2.000],  loss: 0.679065, mae: 11.484387, mean_q: -16.530450, mean_eps: 0.279596\n",
            " 113934/200000: episode: 530, duration: 0.935s, episode steps: 134, steps per second: 143, episode reward: -133.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.851 [0.000, 2.000],  loss: 0.678112, mae: 11.509364, mean_q: -16.558103, mean_eps: 0.278849\n",
            " 114043/200000: episode: 531, duration: 0.713s, episode steps: 109, steps per second: 153, episode reward: -108.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.908 [0.000, 2.000],  loss: 0.632247, mae: 11.399906, mean_q: -16.394504, mean_eps: 0.278076\n",
            " 114155/200000: episode: 532, duration: 0.770s, episode steps: 112, steps per second: 145, episode reward: -111.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.742053, mae: 11.477274, mean_q: -16.523798, mean_eps: 0.277379\n",
            " 114308/200000: episode: 533, duration: 1.078s, episode steps: 153, steps per second: 142, episode reward: -152.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.869 [0.000, 2.000],  loss: 0.768893, mae: 11.379549, mean_q: -16.365907, mean_eps: 0.276543\n",
            " 114516/200000: episode: 534, duration: 1.414s, episode steps: 208, steps per second: 147, episode reward: -207.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.894 [0.000, 2.000],  loss: 0.709354, mae: 11.451721, mean_q: -16.480475, mean_eps: 0.275403\n",
            " 114629/200000: episode: 535, duration: 0.809s, episode steps: 113, steps per second: 140, episode reward: -112.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.956 [0.000, 2.000],  loss: 0.784295, mae: 11.531590, mean_q: -16.570459, mean_eps: 0.274377\n",
            " 114800/200000: episode: 536, duration: 1.621s, episode steps: 171, steps per second: 106, episode reward: -170.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.842 [0.000, 2.000],  loss: 0.702499, mae: 11.402004, mean_q: -16.414492, mean_eps: 0.273478\n",
            " 114973/200000: episode: 537, duration: 1.692s, episode steps: 173, steps per second: 102, episode reward: -172.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.116 [0.000, 2.000],  loss: 0.657474, mae: 11.473251, mean_q: -16.540951, mean_eps: 0.272389\n",
            " 115092/200000: episode: 538, duration: 0.825s, episode steps: 119, steps per second: 144, episode reward: -118.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.866 [0.000, 2.000],  loss: 0.727157, mae: 11.387813, mean_q: -16.374639, mean_eps: 0.271464\n",
            " 115220/200000: episode: 539, duration: 0.892s, episode steps: 128, steps per second: 143, episode reward: -127.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.859 [0.000, 2.000],  loss: 0.605595, mae: 11.375772, mean_q: -16.373871, mean_eps: 0.270691\n",
            " 115368/200000: episode: 540, duration: 1.054s, episode steps: 148, steps per second: 140, episode reward: -147.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.122 [0.000, 2.000],  loss: 0.702315, mae: 11.486328, mean_q: -16.527119, mean_eps: 0.269817\n",
            " 115491/200000: episode: 541, duration: 0.848s, episode steps: 123, steps per second: 145, episode reward: -122.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.041 [0.000, 2.000],  loss: 0.651429, mae: 11.479937, mean_q: -16.547767, mean_eps: 0.268956\n",
            " 115586/200000: episode: 542, duration: 0.633s, episode steps:  95, steps per second: 150, episode reward: -94.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.979 [0.000, 2.000],  loss: 0.618850, mae: 11.379392, mean_q: -16.395356, mean_eps: 0.268259\n",
            " 115682/200000: episode: 543, duration: 0.704s, episode steps:  96, steps per second: 136, episode reward: -95.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.010 [0.000, 2.000],  loss: 0.731005, mae: 11.351861, mean_q: -16.325354, mean_eps: 0.267651\n",
            " 115797/200000: episode: 544, duration: 0.789s, episode steps: 115, steps per second: 146, episode reward: -114.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.122 [0.000, 2.000],  loss: 0.673350, mae: 11.432056, mean_q: -16.477268, mean_eps: 0.266980\n",
            " 115933/200000: episode: 545, duration: 0.904s, episode steps: 136, steps per second: 150, episode reward: -135.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.051 [0.000, 2.000],  loss: 0.729319, mae: 11.489778, mean_q: -16.529797, mean_eps: 0.266182\n",
            " 116035/200000: episode: 546, duration: 0.650s, episode steps: 102, steps per second: 157, episode reward: -101.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.843 [0.000, 2.000],  loss: 0.724269, mae: 11.524631, mean_q: -16.578766, mean_eps: 0.265435\n",
            " 116169/200000: episode: 547, duration: 0.927s, episode steps: 134, steps per second: 144, episode reward: -133.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.910 [0.000, 2.000],  loss: 0.755151, mae: 11.424185, mean_q: -16.432125, mean_eps: 0.264687\n",
            " 116275/200000: episode: 548, duration: 0.764s, episode steps: 106, steps per second: 139, episode reward: -105.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.038 [0.000, 2.000],  loss: 0.591068, mae: 11.492616, mean_q: -16.590258, mean_eps: 0.263927\n",
            " 116405/200000: episode: 549, duration: 0.892s, episode steps: 130, steps per second: 146, episode reward: -129.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.992 [0.000, 2.000],  loss: 0.674990, mae: 11.548098, mean_q: -16.639129, mean_eps: 0.263180\n",
            " 116532/200000: episode: 550, duration: 1.233s, episode steps: 127, steps per second: 103, episode reward: -126.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.953 [0.000, 2.000],  loss: 0.816873, mae: 11.511247, mean_q: -16.551776, mean_eps: 0.262369\n",
            " 116619/200000: episode: 551, duration: 0.848s, episode steps:  87, steps per second: 103, episode reward: -86.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.046 [0.000, 2.000],  loss: 0.608488, mae: 11.553496, mean_q: -16.650001, mean_eps: 0.261698\n",
            " 116724/200000: episode: 552, duration: 1.063s, episode steps: 105, steps per second:  99, episode reward: -104.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.048 [0.000, 2.000],  loss: 0.694966, mae: 11.446064, mean_q: -16.464911, mean_eps: 0.261090\n",
            " 116840/200000: episode: 553, duration: 0.976s, episode steps: 116, steps per second: 119, episode reward: -115.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.026 [0.000, 2.000],  loss: 0.891137, mae: 11.600853, mean_q: -16.646061, mean_eps: 0.260393\n",
            " 116976/200000: episode: 554, duration: 1.025s, episode steps: 136, steps per second: 133, episode reward: -135.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.809 [0.000, 2.000],  loss: 0.693432, mae: 11.490845, mean_q: -16.535243, mean_eps: 0.259595\n",
            " 117075/200000: episode: 555, duration: 0.640s, episode steps:  99, steps per second: 155, episode reward: -98.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.040 [0.000, 2.000],  loss: 0.633699, mae: 11.689235, mean_q: -16.860190, mean_eps: 0.258848\n",
            " 117210/200000: episode: 556, duration: 0.930s, episode steps: 135, steps per second: 145, episode reward: -134.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.207 [0.000, 2.000],  loss: 0.641501, mae: 11.720538, mean_q: -16.887690, mean_eps: 0.258101\n",
            " 117323/200000: episode: 557, duration: 0.806s, episode steps: 113, steps per second: 140, episode reward: -112.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.221 [0.000, 2.000],  loss: 0.809681, mae: 11.698174, mean_q: -16.847876, mean_eps: 0.257315\n",
            " 117494/200000: episode: 558, duration: 1.132s, episode steps: 171, steps per second: 151, episode reward: -170.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.035 [0.000, 2.000],  loss: 0.682396, mae: 11.673177, mean_q: -16.826586, mean_eps: 0.256416\n",
            " 117615/200000: episode: 559, duration: 0.779s, episode steps: 121, steps per second: 155, episode reward: -120.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.083 [0.000, 2.000],  loss: 0.753595, mae: 11.698572, mean_q: -16.839550, mean_eps: 0.255491\n",
            " 117710/200000: episode: 560, duration: 0.622s, episode steps:  95, steps per second: 153, episode reward: -94.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.074 [0.000, 2.000],  loss: 0.637062, mae: 11.722351, mean_q: -16.923835, mean_eps: 0.254807\n",
            " 117810/200000: episode: 561, duration: 0.657s, episode steps: 100, steps per second: 152, episode reward: -99.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.190 [0.000, 2.000],  loss: 0.731301, mae: 11.790623, mean_q: -16.976209, mean_eps: 0.254187\n",
            " 117917/200000: episode: 562, duration: 0.733s, episode steps: 107, steps per second: 146, episode reward: -106.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.673207, mae: 11.730806, mean_q: -16.907588, mean_eps: 0.253528\n",
            " 118035/200000: episode: 563, duration: 0.786s, episode steps: 118, steps per second: 150, episode reward: -117.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.949 [0.000, 2.000],  loss: 0.726864, mae: 11.694887, mean_q: -16.830054, mean_eps: 0.252819\n",
            " 118214/200000: episode: 564, duration: 1.217s, episode steps: 179, steps per second: 147, episode reward: -178.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.983 [0.000, 2.000],  loss: 0.685301, mae: 11.674320, mean_q: -16.808029, mean_eps: 0.251881\n",
            " 118314/200000: episode: 565, duration: 0.918s, episode steps: 100, steps per second: 109, episode reward: -99.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.050 [0.000, 2.000],  loss: 0.675378, mae: 11.763037, mean_q: -16.960951, mean_eps: 0.250995\n",
            " 118431/200000: episode: 566, duration: 1.131s, episode steps: 117, steps per second: 103, episode reward: -116.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.085 [0.000, 2.000],  loss: 0.669970, mae: 11.749234, mean_q: -16.929775, mean_eps: 0.250311\n",
            " 118542/200000: episode: 567, duration: 1.091s, episode steps: 111, steps per second: 102, episode reward: -110.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.027 [0.000, 2.000],  loss: 0.709713, mae: 11.700536, mean_q: -16.848029, mean_eps: 0.249589\n",
            " 118686/200000: episode: 568, duration: 1.030s, episode steps: 144, steps per second: 140, episode reward: -143.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.910 [0.000, 2.000],  loss: 0.681754, mae: 11.680365, mean_q: -16.828738, mean_eps: 0.248778\n",
            " 118791/200000: episode: 569, duration: 0.690s, episode steps: 105, steps per second: 152, episode reward: -104.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.029 [0.000, 2.000],  loss: 0.774038, mae: 11.693569, mean_q: -16.814609, mean_eps: 0.247993\n",
            " 118882/200000: episode: 570, duration: 0.586s, episode steps:  91, steps per second: 155, episode reward: -90.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.033 [0.000, 2.000],  loss: 0.807344, mae: 11.877844, mean_q: -17.096778, mean_eps: 0.247372\n",
            " 118991/200000: episode: 571, duration: 0.732s, episode steps: 109, steps per second: 149, episode reward: -108.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.945 [0.000, 2.000],  loss: 0.804839, mae: 11.732989, mean_q: -16.873989, mean_eps: 0.246739\n",
            " 119101/200000: episode: 572, duration: 0.727s, episode steps: 110, steps per second: 151, episode reward: -109.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.073 [0.000, 2.000],  loss: 0.713067, mae: 11.670485, mean_q: -16.782929, mean_eps: 0.246042\n",
            " 119239/200000: episode: 573, duration: 0.899s, episode steps: 138, steps per second: 154, episode reward: -137.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.138 [0.000, 2.000],  loss: 0.795338, mae: 11.668340, mean_q: -16.763118, mean_eps: 0.245257\n",
            " 119358/200000: episode: 574, duration: 0.816s, episode steps: 119, steps per second: 146, episode reward: -118.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.185 [0.000, 2.000],  loss: 0.687325, mae: 11.652415, mean_q: -16.781203, mean_eps: 0.244446\n",
            " 119445/200000: episode: 575, duration: 0.582s, episode steps:  87, steps per second: 150, episode reward: -86.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.092 [0.000, 2.000],  loss: 0.719367, mae: 11.619238, mean_q: -16.723976, mean_eps: 0.243787\n",
            " 119561/200000: episode: 576, duration: 0.807s, episode steps: 116, steps per second: 144, episode reward: -115.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.974 [0.000, 2.000],  loss: 0.711427, mae: 11.604815, mean_q: -16.711052, mean_eps: 0.243141\n",
            " 119666/200000: episode: 577, duration: 0.700s, episode steps: 105, steps per second: 150, episode reward: -104.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.133 [0.000, 2.000],  loss: 0.780097, mae: 11.651932, mean_q: -16.759114, mean_eps: 0.242445\n",
            " 119772/200000: episode: 578, duration: 0.731s, episode steps: 106, steps per second: 145, episode reward: -105.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.085 [0.000, 2.000],  loss: 0.673898, mae: 11.701018, mean_q: -16.859740, mean_eps: 0.241786\n",
            " 119867/200000: episode: 579, duration: 0.653s, episode steps:  95, steps per second: 146, episode reward: -94.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.053 [0.000, 2.000],  loss: 0.761344, mae: 11.660849, mean_q: -16.768009, mean_eps: 0.241153\n",
            " 119973/200000: episode: 580, duration: 0.755s, episode steps: 106, steps per second: 140, episode reward: -105.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.198 [0.000, 2.000],  loss: 0.737822, mae: 11.627777, mean_q: -16.726331, mean_eps: 0.240507\n",
            " 120106/200000: episode: 581, duration: 1.110s, episode steps: 133, steps per second: 120, episode reward: -132.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.256 [0.000, 2.000],  loss: 0.716814, mae: 11.697125, mean_q: -16.844295, mean_eps: 0.239747\n",
            " 120220/200000: episode: 582, duration: 1.174s, episode steps: 114, steps per second:  97, episode reward: -113.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.263 [0.000, 2.000],  loss: 0.719342, mae: 11.682137, mean_q: -16.831304, mean_eps: 0.238974\n",
            " 120325/200000: episode: 583, duration: 1.030s, episode steps: 105, steps per second: 102, episode reward: -104.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.067 [0.000, 2.000],  loss: 0.738576, mae: 11.641586, mean_q: -16.760910, mean_eps: 0.238277\n",
            " 120410/200000: episode: 584, duration: 0.816s, episode steps:  85, steps per second: 104, episode reward: -84.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.071 [0.000, 2.000],  loss: 0.834270, mae: 11.730853, mean_q: -16.876719, mean_eps: 0.237669\n",
            " 120527/200000: episode: 585, duration: 0.772s, episode steps: 117, steps per second: 152, episode reward: -116.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.188 [0.000, 2.000],  loss: 0.670376, mae: 11.607330, mean_q: -16.731307, mean_eps: 0.237036\n",
            " 120626/200000: episode: 586, duration: 0.654s, episode steps:  99, steps per second: 151, episode reward: -98.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.040 [0.000, 2.000],  loss: 0.762853, mae: 11.612295, mean_q: -16.686080, mean_eps: 0.236352\n",
            " 120757/200000: episode: 587, duration: 0.867s, episode steps: 131, steps per second: 151, episode reward: -130.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.099 [0.000, 2.000],  loss: 0.662185, mae: 11.586084, mean_q: -16.681073, mean_eps: 0.235617\n",
            " 120868/200000: episode: 588, duration: 0.729s, episode steps: 111, steps per second: 152, episode reward: -110.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.054 [0.000, 2.000],  loss: 0.611037, mae: 11.728773, mean_q: -16.904111, mean_eps: 0.234857\n",
            " 120985/200000: episode: 589, duration: 0.780s, episode steps: 117, steps per second: 150, episode reward: -116.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.171 [0.000, 2.000],  loss: 0.732483, mae: 11.813184, mean_q: -17.018383, mean_eps: 0.234135\n",
            " 121111/200000: episode: 590, duration: 0.853s, episode steps: 126, steps per second: 148, episode reward: -125.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.190 [0.000, 2.000],  loss: 0.682041, mae: 11.669647, mean_q: -16.809549, mean_eps: 0.233363\n",
            " 121219/200000: episode: 591, duration: 0.727s, episode steps: 108, steps per second: 149, episode reward: -107.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.880 [0.000, 2.000],  loss: 0.698748, mae: 11.753051, mean_q: -16.944081, mean_eps: 0.232628\n",
            " 121319/200000: episode: 592, duration: 0.651s, episode steps: 100, steps per second: 154, episode reward: -99.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.950 [0.000, 2.000],  loss: 0.575097, mae: 11.690199, mean_q: -16.872261, mean_eps: 0.231969\n",
            " 121413/200000: episode: 593, duration: 0.640s, episode steps:  94, steps per second: 147, episode reward: -93.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.021 [0.000, 2.000],  loss: 0.771778, mae: 11.717579, mean_q: -16.861501, mean_eps: 0.231349\n",
            " 121552/200000: episode: 594, duration: 0.916s, episode steps: 139, steps per second: 152, episode reward: -138.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.086 [0.000, 2.000],  loss: 0.714918, mae: 11.781771, mean_q: -16.965382, mean_eps: 0.230614\n",
            " 121654/200000: episode: 595, duration: 0.722s, episode steps: 102, steps per second: 141, episode reward: -101.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.098 [0.000, 2.000],  loss: 0.768265, mae: 11.792512, mean_q: -16.952655, mean_eps: 0.229854\n",
            " 121800/200000: episode: 596, duration: 1.042s, episode steps: 146, steps per second: 140, episode reward: -145.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.219 [0.000, 2.000],  loss: 0.727976, mae: 11.711500, mean_q: -16.850247, mean_eps: 0.229069\n",
            " 121905/200000: episode: 597, duration: 0.869s, episode steps: 105, steps per second: 121, episode reward: -104.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.867 [0.000, 2.000],  loss: 0.702667, mae: 11.779665, mean_q: -16.953919, mean_eps: 0.228271\n",
            " 122032/200000: episode: 598, duration: 1.215s, episode steps: 127, steps per second: 105, episode reward: -126.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.118 [0.000, 2.000],  loss: 0.808485, mae: 11.700831, mean_q: -16.802434, mean_eps: 0.227536\n",
            " 122264/200000: episode: 599, duration: 2.209s, episode steps: 232, steps per second: 105, episode reward: -231.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.263 [0.000, 2.000],  loss: 0.731090, mae: 11.735793, mean_q: -16.897547, mean_eps: 0.226409\n",
            " 122391/200000: episode: 600, duration: 0.894s, episode steps: 127, steps per second: 142, episode reward: -126.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.937 [0.000, 2.000],  loss: 0.786545, mae: 11.649332, mean_q: -16.746798, mean_eps: 0.225269\n",
            " 122498/200000: episode: 601, duration: 0.699s, episode steps: 107, steps per second: 153, episode reward: -106.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.271 [0.000, 2.000],  loss: 0.699479, mae: 11.716880, mean_q: -16.853521, mean_eps: 0.224521\n",
            " 122613/200000: episode: 602, duration: 0.754s, episode steps: 115, steps per second: 152, episode reward: -114.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.896 [0.000, 2.000],  loss: 0.715789, mae: 11.735449, mean_q: -16.894344, mean_eps: 0.223812\n",
            " 122719/200000: episode: 603, duration: 0.689s, episode steps: 106, steps per second: 154, episode reward: -105.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.925 [0.000, 2.000],  loss: 0.688538, mae: 11.761260, mean_q: -16.946584, mean_eps: 0.223115\n",
            " 122849/200000: episode: 604, duration: 0.866s, episode steps: 130, steps per second: 150, episode reward: -129.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.954 [0.000, 2.000],  loss: 0.800112, mae: 11.625920, mean_q: -16.713518, mean_eps: 0.222368\n",
            " 123030/200000: episode: 605, duration: 1.131s, episode steps: 181, steps per second: 160, episode reward: -180.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.160 [0.000, 2.000],  loss: 0.689052, mae: 11.672257, mean_q: -16.816447, mean_eps: 0.221380\n",
            " 123109/200000: episode: 606, duration: 0.519s, episode steps:  79, steps per second: 152, episode reward: -78.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.089 [0.000, 2.000],  loss: 0.704558, mae: 11.649225, mean_q: -16.772101, mean_eps: 0.220557\n",
            " 123212/200000: episode: 607, duration: 0.672s, episode steps: 103, steps per second: 153, episode reward: -102.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.835 [0.000, 2.000],  loss: 0.692391, mae: 11.740160, mean_q: -16.916031, mean_eps: 0.219987\n",
            " 123306/200000: episode: 608, duration: 0.619s, episode steps:  94, steps per second: 152, episode reward: -93.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.021 [0.000, 2.000],  loss: 0.640336, mae: 11.710329, mean_q: -16.867057, mean_eps: 0.219366\n",
            " 123403/200000: episode: 609, duration: 0.624s, episode steps:  97, steps per second: 155, episode reward: -96.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.010 [0.000, 2.000],  loss: 0.778621, mae: 11.736222, mean_q: -16.876193, mean_eps: 0.218758\n",
            " 123552/200000: episode: 610, duration: 0.994s, episode steps: 149, steps per second: 150, episode reward: -148.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.007 [0.000, 2.000],  loss: 0.743520, mae: 11.661503, mean_q: -16.766475, mean_eps: 0.217985\n",
            " 123673/200000: episode: 611, duration: 0.860s, episode steps: 121, steps per second: 141, episode reward: -120.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.950 [0.000, 2.000],  loss: 0.761722, mae: 11.643208, mean_q: -16.747925, mean_eps: 0.217124\n",
            " 123782/200000: episode: 612, duration: 0.920s, episode steps: 109, steps per second: 118, episode reward: -108.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.917 [0.000, 2.000],  loss: 0.657912, mae: 11.668773, mean_q: -16.813227, mean_eps: 0.216389\n",
            " 123887/200000: episode: 613, duration: 1.020s, episode steps: 105, steps per second: 103, episode reward: -104.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.952 [0.000, 2.000],  loss: 0.725817, mae: 11.707372, mean_q: -16.846870, mean_eps: 0.215718\n",
            " 123995/200000: episode: 614, duration: 0.971s, episode steps: 108, steps per second: 111, episode reward: -107.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.843 [0.000, 2.000],  loss: 0.748440, mae: 11.740949, mean_q: -16.882811, mean_eps: 0.215047\n",
            " 124127/200000: episode: 615, duration: 1.143s, episode steps: 132, steps per second: 116, episode reward: -131.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.924 [0.000, 2.000],  loss: 0.733174, mae: 11.740054, mean_q: -16.904707, mean_eps: 0.214287\n",
            " 124246/200000: episode: 616, duration: 0.779s, episode steps: 119, steps per second: 153, episode reward: -118.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.748 [0.000, 2.000],  loss: 0.804619, mae: 11.854279, mean_q: -17.037908, mean_eps: 0.213489\n",
            " 124360/200000: episode: 617, duration: 0.790s, episode steps: 114, steps per second: 144, episode reward: -113.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.816 [0.000, 2.000],  loss: 0.673389, mae: 11.675759, mean_q: -16.804895, mean_eps: 0.212754\n",
            " 124486/200000: episode: 618, duration: 0.859s, episode steps: 126, steps per second: 147, episode reward: -125.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.984 [0.000, 2.000],  loss: 0.713024, mae: 11.786498, mean_q: -16.984246, mean_eps: 0.211994\n",
            " 124581/200000: episode: 619, duration: 0.650s, episode steps:  95, steps per second: 146, episode reward: -94.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.979 [0.000, 2.000],  loss: 0.841802, mae: 11.737794, mean_q: -16.885055, mean_eps: 0.211285\n",
            " 124723/200000: episode: 620, duration: 0.954s, episode steps: 142, steps per second: 149, episode reward: -141.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.958 [0.000, 2.000],  loss: 0.788646, mae: 11.714262, mean_q: -16.829784, mean_eps: 0.210537\n",
            " 124897/200000: episode: 621, duration: 1.163s, episode steps: 174, steps per second: 150, episode reward: -173.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.891 [0.000, 2.000],  loss: 0.664058, mae: 11.786126, mean_q: -16.980075, mean_eps: 0.209537\n",
            " 125024/200000: episode: 622, duration: 0.936s, episode steps: 127, steps per second: 136, episode reward: -126.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.866 [0.000, 2.000],  loss: 0.740667, mae: 11.716095, mean_q: -16.857458, mean_eps: 0.208587\n",
            " 125140/200000: episode: 623, duration: 0.882s, episode steps: 116, steps per second: 131, episode reward: -115.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.828 [0.000, 2.000],  loss: 0.734655, mae: 11.779805, mean_q: -16.942981, mean_eps: 0.207827\n",
            " 125251/200000: episode: 624, duration: 0.802s, episode steps: 111, steps per second: 138, episode reward: -110.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.829 [0.000, 2.000],  loss: 0.689425, mae: 11.754807, mean_q: -16.928159, mean_eps: 0.207105\n",
            " 125404/200000: episode: 625, duration: 1.051s, episode steps: 153, steps per second: 146, episode reward: -152.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.190 [0.000, 2.000],  loss: 0.739042, mae: 11.845911, mean_q: -17.055089, mean_eps: 0.206269\n",
            " 125500/200000: episode: 626, duration: 0.683s, episode steps:  96, steps per second: 140, episode reward: -95.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.823 [0.000, 2.000],  loss: 0.863271, mae: 11.791298, mean_q: -16.935832, mean_eps: 0.205483\n",
            " 125591/200000: episode: 627, duration: 0.847s, episode steps:  91, steps per second: 107, episode reward: -90.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.956 [0.000, 2.000],  loss: 0.683098, mae: 11.816340, mean_q: -17.026201, mean_eps: 0.204888\n",
            " 125690/200000: episode: 628, duration: 0.959s, episode steps:  99, steps per second: 103, episode reward: -98.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.929 [0.000, 2.000],  loss: 0.804058, mae: 11.827400, mean_q: -17.015206, mean_eps: 0.204280\n",
            " 125772/200000: episode: 629, duration: 0.745s, episode steps:  82, steps per second: 110, episode reward: -81.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.927 [0.000, 2.000],  loss: 0.883203, mae: 11.753254, mean_q: -16.872788, mean_eps: 0.203710\n",
            " 125889/200000: episode: 630, duration: 1.126s, episode steps: 117, steps per second: 104, episode reward: -116.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.897 [0.000, 2.000],  loss: 0.641282, mae: 11.812544, mean_q: -17.033615, mean_eps: 0.203077\n",
            " 126030/200000: episode: 631, duration: 0.934s, episode steps: 141, steps per second: 151, episode reward: -140.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.943 [0.000, 2.000],  loss: 0.662481, mae: 11.850571, mean_q: -17.082115, mean_eps: 0.202253\n",
            " 126161/200000: episode: 632, duration: 0.900s, episode steps: 131, steps per second: 146, episode reward: -130.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.076 [0.000, 2.000],  loss: 0.763966, mae: 11.790212, mean_q: -16.955075, mean_eps: 0.201392\n",
            " 126333/200000: episode: 633, duration: 1.155s, episode steps: 172, steps per second: 149, episode reward: -171.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.907 [0.000, 2.000],  loss: 0.724328, mae: 11.855800, mean_q: -17.058008, mean_eps: 0.200429\n",
            " 126451/200000: episode: 634, duration: 0.771s, episode steps: 118, steps per second: 153, episode reward: -117.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.025 [0.000, 2.000],  loss: 0.663322, mae: 11.792874, mean_q: -16.985551, mean_eps: 0.199517\n",
            " 126767/200000: episode: 635, duration: 2.006s, episode steps: 316, steps per second: 158, episode reward: -315.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.832 [0.000, 2.000],  loss: 0.719744, mae: 11.779473, mean_q: -16.936052, mean_eps: 0.198149\n",
            " 126884/200000: episode: 636, duration: 0.764s, episode steps: 117, steps per second: 153, episode reward: -116.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.009 [0.000, 2.000],  loss: 0.688398, mae: 11.802350, mean_q: -16.973012, mean_eps: 0.196781\n",
            " 126984/200000: episode: 637, duration: 0.658s, episode steps: 100, steps per second: 152, episode reward: -99.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.840 [0.000, 2.000],  loss: 0.664939, mae: 11.882173, mean_q: -17.112726, mean_eps: 0.196097\n",
            " 127085/200000: episode: 638, duration: 0.676s, episode steps: 101, steps per second: 149, episode reward: -100.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.119 [0.000, 2.000],  loss: 0.683954, mae: 11.780456, mean_q: -16.952762, mean_eps: 0.195451\n",
            " 127195/200000: episode: 639, duration: 0.713s, episode steps: 110, steps per second: 154, episode reward: -109.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.845 [0.000, 2.000],  loss: 0.806589, mae: 11.887601, mean_q: -17.090450, mean_eps: 0.194780\n",
            " 127335/200000: episode: 640, duration: 0.906s, episode steps: 140, steps per second: 154, episode reward: -139.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.743 [0.000, 2.000],  loss: 0.776806, mae: 11.737440, mean_q: -16.867779, mean_eps: 0.193995\n",
            " 127464/200000: episode: 641, duration: 1.097s, episode steps: 129, steps per second: 118, episode reward: -128.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.829 [0.000, 2.000],  loss: 0.717498, mae: 11.757372, mean_q: -16.903872, mean_eps: 0.193146\n",
            " 127549/200000: episode: 642, duration: 0.883s, episode steps:  85, steps per second:  96, episode reward: -84.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.918 [0.000, 2.000],  loss: 0.616145, mae: 11.897070, mean_q: -17.144665, mean_eps: 0.192462\n",
            " 127637/200000: episode: 643, duration: 0.785s, episode steps:  88, steps per second: 112, episode reward: -87.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.966 [0.000, 2.000],  loss: 0.701457, mae: 11.854313, mean_q: -17.079221, mean_eps: 0.191905\n",
            " 127754/200000: episode: 644, duration: 1.109s, episode steps: 117, steps per second: 105, episode reward: -116.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.043 [0.000, 2.000],  loss: 0.797406, mae: 11.814515, mean_q: -16.992915, mean_eps: 0.191259\n",
            " 127847/200000: episode: 645, duration: 0.664s, episode steps:  93, steps per second: 140, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.903 [0.000, 2.000],  loss: 0.802593, mae: 11.766636, mean_q: -16.899980, mean_eps: 0.190600\n",
            " 127935/200000: episode: 646, duration: 0.602s, episode steps:  88, steps per second: 146, episode reward: -87.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.182 [0.000, 2.000],  loss: 0.670143, mae: 11.940039, mean_q: -17.221350, mean_eps: 0.190030\n",
            " 128053/200000: episode: 647, duration: 0.796s, episode steps: 118, steps per second: 148, episode reward: -117.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.958 [0.000, 2.000],  loss: 0.745780, mae: 11.977188, mean_q: -17.224455, mean_eps: 0.189371\n",
            " 128130/200000: episode: 648, duration: 0.505s, episode steps:  77, steps per second: 152, episode reward: -76.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.169 [0.000, 2.000],  loss: 0.688767, mae: 11.897529, mean_q: -17.141218, mean_eps: 0.188751\n",
            " 128217/200000: episode: 649, duration: 0.590s, episode steps:  87, steps per second: 147, episode reward: -86.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.977 [0.000, 2.000],  loss: 0.688702, mae: 11.953741, mean_q: -17.225558, mean_eps: 0.188231\n",
            " 128306/200000: episode: 650, duration: 0.583s, episode steps:  89, steps per second: 153, episode reward: -88.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.843 [0.000, 2.000],  loss: 0.753627, mae: 12.024665, mean_q: -17.312518, mean_eps: 0.187674\n",
            " 128399/200000: episode: 651, duration: 0.669s, episode steps:  93, steps per second: 139, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.086 [0.000, 2.000],  loss: 0.716543, mae: 12.069827, mean_q: -17.405090, mean_eps: 0.187104\n",
            " 128492/200000: episode: 652, duration: 0.663s, episode steps:  93, steps per second: 140, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.946 [0.000, 2.000],  loss: 0.758741, mae: 11.984173, mean_q: -17.243639, mean_eps: 0.186521\n",
            " 128618/200000: episode: 653, duration: 0.856s, episode steps: 126, steps per second: 147, episode reward: -125.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.992 [0.000, 2.000],  loss: 0.822945, mae: 12.059294, mean_q: -17.346423, mean_eps: 0.185825\n",
            " 128788/200000: episode: 654, duration: 1.173s, episode steps: 170, steps per second: 145, episode reward: -169.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.171 [0.000, 2.000],  loss: 0.735322, mae: 11.989228, mean_q: -17.282615, mean_eps: 0.184887\n",
            " 128892/200000: episode: 655, duration: 0.749s, episode steps: 104, steps per second: 139, episode reward: -103.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.010 [0.000, 2.000],  loss: 0.684204, mae: 11.951634, mean_q: -17.253275, mean_eps: 0.184026\n",
            " 129011/200000: episode: 656, duration: 0.844s, episode steps: 119, steps per second: 141, episode reward: -118.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.950 [0.000, 2.000],  loss: 0.744053, mae: 11.961696, mean_q: -17.234077, mean_eps: 0.183317\n",
            " 129121/200000: episode: 657, duration: 0.838s, episode steps: 110, steps per second: 131, episode reward: -109.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.109 [0.000, 2.000],  loss: 0.777240, mae: 12.075482, mean_q: -17.394043, mean_eps: 0.182582\n",
            " 129229/200000: episode: 658, duration: 0.970s, episode steps: 108, steps per second: 111, episode reward: -107.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.935 [0.000, 2.000],  loss: 0.711322, mae: 11.983479, mean_q: -17.258962, mean_eps: 0.181885\n",
            " 129325/200000: episode: 659, duration: 1.041s, episode steps:  96, steps per second:  92, episode reward: -95.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.969 [0.000, 2.000],  loss: 0.677152, mae: 12.063799, mean_q: -17.393147, mean_eps: 0.181239\n",
            " 129425/200000: episode: 660, duration: 0.981s, episode steps: 100, steps per second: 102, episode reward: -99.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.150 [0.000, 2.000],  loss: 0.816006, mae: 12.041093, mean_q: -17.314465, mean_eps: 0.180619\n",
            " 129517/200000: episode: 661, duration: 0.959s, episode steps:  92, steps per second:  96, episode reward: -91.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.174 [0.000, 2.000],  loss: 0.715572, mae: 12.019079, mean_q: -17.300859, mean_eps: 0.180011\n",
            " 129636/200000: episode: 662, duration: 0.899s, episode steps: 119, steps per second: 132, episode reward: -118.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.134 [0.000, 2.000],  loss: 0.883238, mae: 12.152807, mean_q: -17.468662, mean_eps: 0.179352\n",
            " 129798/200000: episode: 663, duration: 1.193s, episode steps: 162, steps per second: 136, episode reward: -161.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.889 [0.000, 2.000],  loss: 0.782596, mae: 12.012761, mean_q: -17.306461, mean_eps: 0.178465\n",
            " 129902/200000: episode: 664, duration: 0.736s, episode steps: 104, steps per second: 141, episode reward: -103.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.125 [0.000, 2.000],  loss: 0.746361, mae: 12.042567, mean_q: -17.343477, mean_eps: 0.177617\n",
            " 130044/200000: episode: 665, duration: 1.030s, episode steps: 142, steps per second: 138, episode reward: -141.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.007 [0.000, 2.000],  loss: 0.731783, mae: 12.085425, mean_q: -17.423555, mean_eps: 0.176844\n",
            " 130138/200000: episode: 666, duration: 0.678s, episode steps:  94, steps per second: 139, episode reward: -93.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.128 [0.000, 2.000],  loss: 0.711824, mae: 12.105834, mean_q: -17.454722, mean_eps: 0.176097\n",
            " 130230/200000: episode: 667, duration: 0.656s, episode steps:  92, steps per second: 140, episode reward: -91.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.098 [0.000, 2.000],  loss: 0.751649, mae: 12.220076, mean_q: -17.616047, mean_eps: 0.175501\n",
            " 130320/200000: episode: 668, duration: 0.646s, episode steps:  90, steps per second: 139, episode reward: -89.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.978 [0.000, 2.000],  loss: 0.855659, mae: 12.162043, mean_q: -17.511332, mean_eps: 0.174931\n",
            " 130429/200000: episode: 669, duration: 0.785s, episode steps: 109, steps per second: 139, episode reward: -108.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.018 [0.000, 2.000],  loss: 0.895775, mae: 12.065050, mean_q: -17.337016, mean_eps: 0.174298\n",
            " 130571/200000: episode: 670, duration: 0.995s, episode steps: 142, steps per second: 143, episode reward: -141.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.056 [0.000, 2.000],  loss: 0.852031, mae: 12.010331, mean_q: -17.262091, mean_eps: 0.173500\n",
            " 130654/200000: episode: 671, duration: 0.601s, episode steps:  83, steps per second: 138, episode reward: -82.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.133 [0.000, 2.000],  loss: 0.777627, mae: 12.027681, mean_q: -17.310030, mean_eps: 0.172791\n",
            " 130794/200000: episode: 672, duration: 0.958s, episode steps: 140, steps per second: 146, episode reward: -139.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.100 [0.000, 2.000],  loss: 0.808008, mae: 12.046976, mean_q: -17.326229, mean_eps: 0.172081\n",
            " 130885/200000: episode: 673, duration: 0.611s, episode steps:  91, steps per second: 149, episode reward: -90.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.209 [0.000, 2.000],  loss: 0.743284, mae: 12.023137, mean_q: -17.305559, mean_eps: 0.171347\n",
            " 130983/200000: episode: 674, duration: 0.963s, episode steps:  98, steps per second: 102, episode reward: -97.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.143 [0.000, 2.000],  loss: 0.925181, mae: 12.097885, mean_q: -17.375254, mean_eps: 0.170751\n",
            " 131105/200000: episode: 675, duration: 1.236s, episode steps: 122, steps per second:  99, episode reward: -121.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.115 [0.000, 2.000],  loss: 0.723847, mae: 12.189534, mean_q: -17.568764, mean_eps: 0.170055\n",
            " 131209/200000: episode: 676, duration: 1.027s, episode steps: 104, steps per second: 101, episode reward: -103.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.019 [0.000, 2.000],  loss: 0.784382, mae: 12.265273, mean_q: -17.663097, mean_eps: 0.169333\n",
            " 131299/200000: episode: 677, duration: 0.750s, episode steps:  90, steps per second: 120, episode reward: -89.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.156 [0.000, 2.000],  loss: 0.781064, mae: 12.251372, mean_q: -17.666428, mean_eps: 0.168725\n",
            " 131426/200000: episode: 678, duration: 0.897s, episode steps: 127, steps per second: 142, episode reward: -126.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.063 [0.000, 2.000],  loss: 0.911078, mae: 12.322243, mean_q: -17.712147, mean_eps: 0.168041\n",
            " 131519/200000: episode: 679, duration: 0.580s, episode steps:  93, steps per second: 160, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.054 [0.000, 2.000],  loss: 0.845369, mae: 12.254162, mean_q: -17.641505, mean_eps: 0.167344\n",
            " 131645/200000: episode: 680, duration: 0.855s, episode steps: 126, steps per second: 147, episode reward: -125.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.087 [0.000, 2.000],  loss: 0.736242, mae: 12.251203, mean_q: -17.668812, mean_eps: 0.166647\n",
            " 131734/200000: episode: 681, duration: 0.602s, episode steps:  89, steps per second: 148, episode reward: -88.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.112 [0.000, 2.000],  loss: 0.777534, mae: 12.310146, mean_q: -17.736146, mean_eps: 0.165963\n",
            " 131906/200000: episode: 682, duration: 1.198s, episode steps: 172, steps per second: 144, episode reward: -171.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.151 [0.000, 2.000],  loss: 0.844964, mae: 12.275005, mean_q: -17.659257, mean_eps: 0.165140\n",
            " 132044/200000: episode: 683, duration: 0.955s, episode steps: 138, steps per second: 144, episode reward: -137.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.043 [0.000, 2.000],  loss: 0.819585, mae: 12.259345, mean_q: -17.651519, mean_eps: 0.164165\n",
            " 132129/200000: episode: 684, duration: 0.604s, episode steps:  85, steps per second: 141, episode reward: -84.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.176 [0.000, 2.000],  loss: 0.836523, mae: 12.331623, mean_q: -17.748638, mean_eps: 0.163455\n",
            " 132228/200000: episode: 685, duration: 0.645s, episode steps:  99, steps per second: 154, episode reward: -98.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.182 [0.000, 2.000],  loss: 0.833289, mae: 12.284105, mean_q: -17.685985, mean_eps: 0.162873\n",
            " 132337/200000: episode: 686, duration: 0.785s, episode steps: 109, steps per second: 139, episode reward: -108.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.055 [0.000, 2.000],  loss: 0.764263, mae: 12.270872, mean_q: -17.685226, mean_eps: 0.162214\n",
            " 132709/200000: episode: 687, duration: 2.703s, episode steps: 372, steps per second: 138, episode reward: -371.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.852 [0.000, 2.000],  loss: 0.792611, mae: 12.280163, mean_q: -17.677924, mean_eps: 0.160681\n",
            " 132823/200000: episode: 688, duration: 1.100s, episode steps: 114, steps per second: 104, episode reward: -113.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.237 [0.000, 2.000],  loss: 0.910044, mae: 12.305545, mean_q: -17.696426, mean_eps: 0.159149\n",
            " 132936/200000: episode: 689, duration: 1.133s, episode steps: 113, steps per second: 100, episode reward: -112.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.283 [0.000, 2.000],  loss: 0.762214, mae: 12.221865, mean_q: -17.607538, mean_eps: 0.158439\n",
            " 133056/200000: episode: 690, duration: 1.210s, episode steps: 120, steps per second:  99, episode reward: -119.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.025 [0.000, 2.000],  loss: 0.753521, mae: 12.321255, mean_q: -17.761648, mean_eps: 0.157705\n",
            " 133163/200000: episode: 691, duration: 0.796s, episode steps: 107, steps per second: 134, episode reward: -106.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.037 [0.000, 2.000],  loss: 0.790962, mae: 12.344509, mean_q: -17.790738, mean_eps: 0.156983\n",
            " 133249/200000: episode: 692, duration: 0.604s, episode steps:  86, steps per second: 142, episode reward: -85.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.070 [0.000, 2.000],  loss: 0.855009, mae: 12.270613, mean_q: -17.656750, mean_eps: 0.156362\n",
            " 133363/200000: episode: 693, duration: 0.807s, episode steps: 114, steps per second: 141, episode reward: -113.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.035 [0.000, 2.000],  loss: 0.876318, mae: 12.366084, mean_q: -17.784847, mean_eps: 0.155729\n",
            " 133454/200000: episode: 694, duration: 0.699s, episode steps:  91, steps per second: 130, episode reward: -90.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.099 [0.000, 2.000],  loss: 1.032333, mae: 12.442387, mean_q: -17.856507, mean_eps: 0.155083\n",
            " 133571/200000: episode: 695, duration: 0.811s, episode steps: 117, steps per second: 144, episode reward: -116.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.872 [0.000, 2.000],  loss: 0.825863, mae: 12.373796, mean_q: -17.811137, mean_eps: 0.154424\n",
            " 133668/200000: episode: 696, duration: 0.683s, episode steps:  97, steps per second: 142, episode reward: -96.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.113 [0.000, 2.000],  loss: 0.856952, mae: 12.338380, mean_q: -17.735242, mean_eps: 0.153753\n",
            " 133819/200000: episode: 697, duration: 0.992s, episode steps: 151, steps per second: 152, episode reward: -150.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.834 [0.000, 2.000],  loss: 0.869429, mae: 12.377139, mean_q: -17.808617, mean_eps: 0.152967\n",
            " 133949/200000: episode: 698, duration: 0.883s, episode steps: 130, steps per second: 147, episode reward: -129.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.792 [0.000, 2.000],  loss: 0.760523, mae: 12.268271, mean_q: -17.674933, mean_eps: 0.152068\n",
            " 134026/200000: episode: 699, duration: 0.499s, episode steps:  77, steps per second: 154, episode reward: -76.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.143 [0.000, 2.000],  loss: 0.823653, mae: 12.277006, mean_q: -17.670096, mean_eps: 0.151409\n",
            " 134097/200000: episode: 700, duration: 0.494s, episode steps:  71, steps per second: 144, episode reward: -70.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.972 [0.000, 2.000],  loss: 0.849918, mae: 12.214401, mean_q: -17.573621, mean_eps: 0.150941\n",
            " 134243/200000: episode: 701, duration: 1.001s, episode steps: 146, steps per second: 146, episode reward: -145.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.719 [0.000, 2.000],  loss: 0.868059, mae: 12.192665, mean_q: -17.542115, mean_eps: 0.150257\n",
            " 134361/200000: episode: 702, duration: 0.856s, episode steps: 118, steps per second: 138, episode reward: -117.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.881 [0.000, 2.000],  loss: 0.835639, mae: 12.284824, mean_q: -17.674613, mean_eps: 0.149421\n",
            " 134474/200000: episode: 703, duration: 0.891s, episode steps: 113, steps per second: 127, episode reward: -112.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.035 [0.000, 2.000],  loss: 0.739337, mae: 12.248159, mean_q: -17.625793, mean_eps: 0.148686\n",
            " 134616/200000: episode: 704, duration: 1.414s, episode steps: 142, steps per second: 100, episode reward: -141.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.915 [0.000, 2.000],  loss: 0.901940, mae: 12.281524, mean_q: -17.645275, mean_eps: 0.147888\n",
            " 134713/200000: episode: 705, duration: 0.928s, episode steps:  97, steps per second: 105, episode reward: -96.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.918 [0.000, 2.000],  loss: 0.810414, mae: 12.401501, mean_q: -17.867940, mean_eps: 0.147128\n",
            " 134823/200000: episode: 706, duration: 1.027s, episode steps: 110, steps per second: 107, episode reward: -109.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.036 [0.000, 2.000],  loss: 0.786422, mae: 12.236498, mean_q: -17.614835, mean_eps: 0.146469\n",
            " 134924/200000: episode: 707, duration: 0.690s, episode steps: 101, steps per second: 146, episode reward: -100.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.178 [0.000, 2.000],  loss: 0.865557, mae: 12.348978, mean_q: -17.759463, mean_eps: 0.145811\n",
            " 135037/200000: episode: 708, duration: 0.796s, episode steps: 113, steps per second: 142, episode reward: -112.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.973 [0.000, 2.000],  loss: 0.885248, mae: 12.341110, mean_q: -17.735411, mean_eps: 0.145127\n",
            " 135109/200000: episode: 709, duration: 0.480s, episode steps:  72, steps per second: 150, episode reward: -71.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.111 [0.000, 2.000],  loss: 0.718842, mae: 12.381230, mean_q: -17.869761, mean_eps: 0.144531\n",
            " 135173/200000: episode: 710, duration: 0.443s, episode steps:  64, steps per second: 144, episode reward: -63.000, mean reward: -0.984 [-1.000,  0.000], mean action: 0.859 [0.000, 2.000],  loss: 0.932415, mae: 12.295680, mean_q: -17.674240, mean_eps: 0.144101\n",
            " 135318/200000: episode: 711, duration: 0.966s, episode steps: 145, steps per second: 150, episode reward: -144.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.876 [0.000, 2.000],  loss: 0.805602, mae: 12.335964, mean_q: -17.759747, mean_eps: 0.143442\n",
            " 135414/200000: episode: 712, duration: 0.660s, episode steps:  96, steps per second: 145, episode reward: -95.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.042 [0.000, 2.000],  loss: 0.696721, mae: 12.359632, mean_q: -17.823526, mean_eps: 0.142682\n",
            " 135511/200000: episode: 713, duration: 0.666s, episode steps:  97, steps per second: 146, episode reward: -96.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.897 [0.000, 2.000],  loss: 0.881187, mae: 12.467015, mean_q: -17.937324, mean_eps: 0.142074\n",
            " 135589/200000: episode: 714, duration: 0.588s, episode steps:  78, steps per second: 133, episode reward: -77.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.115 [0.000, 2.000],  loss: 0.773511, mae: 12.446490, mean_q: -17.924753, mean_eps: 0.141517\n",
            " 135674/200000: episode: 715, duration: 0.601s, episode steps:  85, steps per second: 141, episode reward: -84.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.059 [0.000, 2.000],  loss: 0.960854, mae: 12.472689, mean_q: -17.899727, mean_eps: 0.140997\n",
            " 135788/200000: episode: 716, duration: 0.853s, episode steps: 114, steps per second: 134, episode reward: -113.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.792618, mae: 12.281370, mean_q: -17.674699, mean_eps: 0.140377\n",
            " 135866/200000: episode: 717, duration: 0.539s, episode steps:  78, steps per second: 145, episode reward: -77.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.910 [0.000, 2.000],  loss: 0.793523, mae: 12.323533, mean_q: -17.730226, mean_eps: 0.139769\n",
            " 135973/200000: episode: 718, duration: 0.726s, episode steps: 107, steps per second: 147, episode reward: -106.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.981 [0.000, 2.000],  loss: 0.905907, mae: 12.441962, mean_q: -17.895709, mean_eps: 0.139173\n",
            " 136088/200000: episode: 719, duration: 0.766s, episode steps: 115, steps per second: 150, episode reward: -114.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.983 [0.000, 2.000],  loss: 0.839547, mae: 12.446329, mean_q: -17.932744, mean_eps: 0.138477\n",
            " 136198/200000: episode: 720, duration: 0.744s, episode steps: 110, steps per second: 148, episode reward: -109.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.109 [0.000, 2.000],  loss: 0.797522, mae: 12.454483, mean_q: -17.928206, mean_eps: 0.137767\n",
            " 136314/200000: episode: 721, duration: 1.002s, episode steps: 116, steps per second: 116, episode reward: -115.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.888 [0.000, 2.000],  loss: 0.790429, mae: 12.469684, mean_q: -17.962032, mean_eps: 0.137045\n",
            " 136408/200000: episode: 722, duration: 0.950s, episode steps:  94, steps per second:  99, episode reward: -93.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.872 [0.000, 2.000],  loss: 0.902649, mae: 12.437319, mean_q: -17.861863, mean_eps: 0.136387\n",
            " 136517/200000: episode: 723, duration: 1.001s, episode steps: 109, steps per second: 109, episode reward: -108.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.009 [0.000, 2.000],  loss: 0.830519, mae: 12.435021, mean_q: -17.871974, mean_eps: 0.135741\n",
            " 136591/200000: episode: 724, duration: 0.768s, episode steps:  74, steps per second:  96, episode reward: -73.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.838 [0.000, 2.000],  loss: 0.927749, mae: 12.468550, mean_q: -17.926894, mean_eps: 0.135158\n",
            " 136704/200000: episode: 725, duration: 0.780s, episode steps: 113, steps per second: 145, episode reward: -112.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.796 [0.000, 2.000],  loss: 0.951298, mae: 12.404467, mean_q: -17.831190, mean_eps: 0.134575\n",
            " 136789/200000: episode: 726, duration: 0.588s, episode steps:  85, steps per second: 144, episode reward: -84.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.965 [0.000, 2.000],  loss: 0.693654, mae: 12.427417, mean_q: -17.917191, mean_eps: 0.133942\n",
            " 136888/200000: episode: 727, duration: 0.657s, episode steps:  99, steps per second: 151, episode reward: -98.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.677 [0.000, 2.000],  loss: 0.745981, mae: 12.508825, mean_q: -18.050580, mean_eps: 0.133359\n",
            " 136971/200000: episode: 728, duration: 0.544s, episode steps:  83, steps per second: 153, episode reward: -82.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.048 [0.000, 2.000],  loss: 0.804116, mae: 12.489144, mean_q: -17.997599, mean_eps: 0.132789\n",
            " 137094/200000: episode: 729, duration: 0.823s, episode steps: 123, steps per second: 149, episode reward: -122.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.984 [0.000, 2.000],  loss: 0.775781, mae: 12.359583, mean_q: -17.782755, mean_eps: 0.132131\n",
            " 137186/200000: episode: 730, duration: 0.660s, episode steps:  92, steps per second: 139, episode reward: -91.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.863642, mae: 12.320584, mean_q: -17.731915, mean_eps: 0.131447\n",
            " 137290/200000: episode: 731, duration: 0.710s, episode steps: 104, steps per second: 147, episode reward: -103.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.933 [0.000, 2.000],  loss: 0.843052, mae: 12.426653, mean_q: -17.868211, mean_eps: 0.130826\n",
            " 137384/200000: episode: 732, duration: 0.631s, episode steps:  94, steps per second: 149, episode reward: -93.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.809 [0.000, 2.000],  loss: 0.815219, mae: 12.379386, mean_q: -17.806407, mean_eps: 0.130205\n",
            " 137460/200000: episode: 733, duration: 0.549s, episode steps:  76, steps per second: 138, episode reward: -75.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.250 [0.000, 2.000],  loss: 0.988896, mae: 12.416353, mean_q: -17.823421, mean_eps: 0.129673\n",
            " 137561/200000: episode: 734, duration: 0.714s, episode steps: 101, steps per second: 141, episode reward: -100.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.881 [0.000, 2.000],  loss: 0.804935, mae: 12.378871, mean_q: -17.809227, mean_eps: 0.129103\n",
            " 137655/200000: episode: 735, duration: 0.616s, episode steps:  94, steps per second: 153, episode reward: -93.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.149 [0.000, 2.000],  loss: 0.951221, mae: 12.510368, mean_q: -17.988790, mean_eps: 0.128483\n",
            " 137781/200000: episode: 736, duration: 0.867s, episode steps: 126, steps per second: 145, episode reward: -125.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.214 [0.000, 2.000],  loss: 0.897952, mae: 12.523451, mean_q: -18.011901, mean_eps: 0.127786\n",
            " 137892/200000: episode: 737, duration: 0.788s, episode steps: 111, steps per second: 141, episode reward: -110.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.063 [0.000, 2.000],  loss: 0.883826, mae: 12.495739, mean_q: -17.988469, mean_eps: 0.127039\n",
            " 137982/200000: episode: 738, duration: 0.612s, episode steps:  90, steps per second: 147, episode reward: -89.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.868443, mae: 12.460963, mean_q: -17.950397, mean_eps: 0.126405\n",
            " 138095/200000: episode: 739, duration: 0.942s, episode steps: 113, steps per second: 120, episode reward: -112.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.080 [0.000, 2.000],  loss: 0.755016, mae: 12.506968, mean_q: -18.029424, mean_eps: 0.125759\n",
            " 138184/200000: episode: 740, duration: 0.891s, episode steps:  89, steps per second: 100, episode reward: -88.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.067 [0.000, 2.000],  loss: 0.989301, mae: 12.482507, mean_q: -17.926175, mean_eps: 0.125126\n",
            " 138283/200000: episode: 741, duration: 0.969s, episode steps:  99, steps per second: 102, episode reward: -98.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.030 [0.000, 2.000],  loss: 0.846353, mae: 12.439119, mean_q: -17.905042, mean_eps: 0.124531\n",
            " 138376/200000: episode: 742, duration: 1.017s, episode steps:  93, steps per second:  91, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.065 [0.000, 2.000],  loss: 0.846741, mae: 12.497007, mean_q: -17.993962, mean_eps: 0.123923\n",
            " 138485/200000: episode: 743, duration: 0.796s, episode steps: 109, steps per second: 137, episode reward: -108.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.073 [0.000, 2.000],  loss: 0.835184, mae: 12.664943, mean_q: -18.257133, mean_eps: 0.123277\n",
            " 138572/200000: episode: 744, duration: 0.565s, episode steps:  87, steps per second: 154, episode reward: -86.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.023 [0.000, 2.000],  loss: 0.725570, mae: 12.419548, mean_q: -17.898333, mean_eps: 0.122656\n",
            " 138694/200000: episode: 745, duration: 0.850s, episode steps: 122, steps per second: 144, episode reward: -121.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.205 [0.000, 2.000],  loss: 0.855469, mae: 12.318890, mean_q: -17.717289, mean_eps: 0.121997\n",
            " 138786/200000: episode: 746, duration: 0.630s, episode steps:  92, steps per second: 146, episode reward: -91.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.174 [0.000, 2.000],  loss: 0.778916, mae: 12.490171, mean_q: -17.997145, mean_eps: 0.121313\n",
            " 138873/200000: episode: 747, duration: 0.569s, episode steps:  87, steps per second: 153, episode reward: -86.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.011 [0.000, 2.000],  loss: 0.895622, mae: 12.375005, mean_q: -17.805206, mean_eps: 0.120743\n",
            " 138957/200000: episode: 748, duration: 0.558s, episode steps:  84, steps per second: 151, episode reward: -83.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.071 [0.000, 2.000],  loss: 0.901685, mae: 12.452485, mean_q: -17.890601, mean_eps: 0.120199\n",
            " 139043/200000: episode: 749, duration: 0.565s, episode steps:  86, steps per second: 152, episode reward: -85.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.023 [0.000, 2.000],  loss: 0.746172, mae: 12.512445, mean_q: -18.042593, mean_eps: 0.119667\n",
            " 139127/200000: episode: 750, duration: 0.581s, episode steps:  84, steps per second: 145, episode reward: -83.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.060 [0.000, 2.000],  loss: 0.816080, mae: 12.548253, mean_q: -18.090628, mean_eps: 0.119135\n",
            " 139216/200000: episode: 751, duration: 0.608s, episode steps:  89, steps per second: 146, episode reward: -88.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.803559, mae: 12.540770, mean_q: -18.058977, mean_eps: 0.118590\n",
            " 139322/200000: episode: 752, duration: 0.773s, episode steps: 106, steps per second: 137, episode reward: -105.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.132 [0.000, 2.000],  loss: 0.923703, mae: 12.614015, mean_q: -18.143519, mean_eps: 0.117969\n",
            " 139467/200000: episode: 753, duration: 1.010s, episode steps: 145, steps per second: 144, episode reward: -144.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.048 [0.000, 2.000],  loss: 0.945342, mae: 12.620018, mean_q: -18.153168, mean_eps: 0.117171\n",
            " 139569/200000: episode: 754, duration: 0.702s, episode steps: 102, steps per second: 145, episode reward: -101.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.078 [0.000, 2.000],  loss: 1.039858, mae: 12.586904, mean_q: -18.079879, mean_eps: 0.116386\n",
            " 139680/200000: episode: 755, duration: 0.819s, episode steps: 111, steps per second: 135, episode reward: -110.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.153 [0.000, 2.000],  loss: 0.936427, mae: 12.378912, mean_q: -17.783100, mean_eps: 0.115715\n",
            " 139770/200000: episode: 756, duration: 0.628s, episode steps:  90, steps per second: 143, episode reward: -89.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.244 [0.000, 2.000],  loss: 0.828438, mae: 12.484735, mean_q: -17.960968, mean_eps: 0.115081\n",
            " 139933/200000: episode: 757, duration: 1.480s, episode steps: 163, steps per second: 110, episode reward: -162.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.092 [0.000, 2.000],  loss: 0.892399, mae: 12.647701, mean_q: -18.191904, mean_eps: 0.114271\n",
            " 140050/200000: episode: 758, duration: 1.118s, episode steps: 117, steps per second: 105, episode reward: -116.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.026 [0.000, 2.000],  loss: 0.806388, mae: 12.488269, mean_q: -17.980864, mean_eps: 0.113384\n",
            " 140165/200000: episode: 759, duration: 1.143s, episode steps: 115, steps per second: 101, episode reward: -114.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.365 [0.000, 2.000],  loss: 0.828846, mae: 12.478346, mean_q: -17.943977, mean_eps: 0.112649\n",
            " 140287/200000: episode: 760, duration: 0.869s, episode steps: 122, steps per second: 140, episode reward: -121.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.139 [0.000, 2.000],  loss: 0.826802, mae: 12.626182, mean_q: -18.176723, mean_eps: 0.111902\n",
            " 140401/200000: episode: 761, duration: 0.803s, episode steps: 114, steps per second: 142, episode reward: -113.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.018 [0.000, 2.000],  loss: 0.811904, mae: 12.612754, mean_q: -18.172961, mean_eps: 0.111155\n",
            " 140536/200000: episode: 762, duration: 0.932s, episode steps: 135, steps per second: 145, episode reward: -134.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.163 [0.000, 2.000],  loss: 0.786563, mae: 12.488236, mean_q: -17.995232, mean_eps: 0.110369\n",
            " 140619/200000: episode: 763, duration: 0.592s, episode steps:  83, steps per second: 140, episode reward: -82.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.145 [0.000, 2.000],  loss: 0.876828, mae: 12.406440, mean_q: -17.833621, mean_eps: 0.109685\n",
            " 140706/200000: episode: 764, duration: 0.636s, episode steps:  87, steps per second: 137, episode reward: -86.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.057 [0.000, 2.000],  loss: 0.729089, mae: 12.410736, mean_q: -17.888551, mean_eps: 0.109141\n",
            " 140782/200000: episode: 765, duration: 0.571s, episode steps:  76, steps per second: 133, episode reward: -75.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.224 [0.000, 2.000],  loss: 0.824544, mae: 12.572345, mean_q: -18.108628, mean_eps: 0.108621\n",
            " 140861/200000: episode: 766, duration: 0.557s, episode steps:  79, steps per second: 142, episode reward: -78.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.190 [0.000, 2.000],  loss: 0.735582, mae: 12.530703, mean_q: -18.066084, mean_eps: 0.108127\n",
            " 140986/200000: episode: 767, duration: 1.002s, episode steps: 125, steps per second: 125, episode reward: -124.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.112 [0.000, 2.000],  loss: 0.868031, mae: 12.492521, mean_q: -17.989081, mean_eps: 0.107481\n",
            " 141079/200000: episode: 768, duration: 0.759s, episode steps:  93, steps per second: 122, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.043 [0.000, 2.000],  loss: 0.860359, mae: 12.478175, mean_q: -17.943205, mean_eps: 0.106797\n",
            " 141207/200000: episode: 769, duration: 0.972s, episode steps: 128, steps per second: 132, episode reward: -127.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.141 [0.000, 2.000],  loss: 0.803755, mae: 12.584856, mean_q: -18.130801, mean_eps: 0.106101\n",
            " 141296/200000: episode: 770, duration: 0.669s, episode steps:  89, steps per second: 133, episode reward: -88.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.225 [0.000, 2.000],  loss: 0.900669, mae: 12.629015, mean_q: -18.173268, mean_eps: 0.105417\n",
            " 141383/200000: episode: 771, duration: 0.672s, episode steps:  87, steps per second: 130, episode reward: -86.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.115 [0.000, 2.000],  loss: 0.908275, mae: 12.551802, mean_q: -18.056060, mean_eps: 0.104859\n",
            " 141478/200000: episode: 772, duration: 0.708s, episode steps:  95, steps per second: 134, episode reward: -94.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.937 [0.000, 2.000],  loss: 0.729581, mae: 12.537513, mean_q: -18.049664, mean_eps: 0.104277\n",
            " 141602/200000: episode: 773, duration: 1.188s, episode steps: 124, steps per second: 104, episode reward: -123.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.121 [0.000, 2.000],  loss: 0.800706, mae: 12.711035, mean_q: -18.303571, mean_eps: 0.103580\n",
            " 141697/200000: episode: 774, duration: 1.050s, episode steps:  95, steps per second:  90, episode reward: -94.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.137 [0.000, 2.000],  loss: 0.878123, mae: 12.665513, mean_q: -18.226494, mean_eps: 0.102883\n",
            " 141789/200000: episode: 775, duration: 0.879s, episode steps:  92, steps per second: 105, episode reward: -91.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.163 [0.000, 2.000],  loss: 1.039088, mae: 12.626986, mean_q: -18.144095, mean_eps: 0.102288\n",
            " 141889/200000: episode: 776, duration: 0.888s, episode steps: 100, steps per second: 113, episode reward: -99.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.030 [0.000, 2.000],  loss: 0.935173, mae: 12.685820, mean_q: -18.266532, mean_eps: 0.101680\n",
            " 142022/200000: episode: 777, duration: 0.878s, episode steps: 133, steps per second: 152, episode reward: -132.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.015 [0.000, 2.000],  loss: 0.835288, mae: 12.574070, mean_q: -18.121062, mean_eps: 0.100945\n",
            " 142135/200000: episode: 778, duration: 0.758s, episode steps: 113, steps per second: 149, episode reward: -112.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.885 [0.000, 2.000],  loss: 0.812571, mae: 12.451877, mean_q: -17.920724, mean_eps: 0.100173\n",
            " 142245/200000: episode: 779, duration: 0.758s, episode steps: 110, steps per second: 145, episode reward: -109.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.073 [0.000, 2.000],  loss: 0.844062, mae: 12.467011, mean_q: -17.924236, mean_eps: 0.099463\n",
            " 142358/200000: episode: 780, duration: 0.792s, episode steps: 113, steps per second: 143, episode reward: -112.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.947 [0.000, 2.000],  loss: 0.811733, mae: 12.454860, mean_q: -17.921770, mean_eps: 0.098754\n",
            " 142629/200000: episode: 781, duration: 1.843s, episode steps: 271, steps per second: 147, episode reward: -270.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.059 [0.000, 2.000],  loss: 0.832010, mae: 12.533224, mean_q: -18.054833, mean_eps: 0.097538\n",
            " 142736/200000: episode: 782, duration: 0.705s, episode steps: 107, steps per second: 152, episode reward: -106.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.037 [0.000, 2.000],  loss: 0.817044, mae: 12.530097, mean_q: -18.032731, mean_eps: 0.096347\n",
            " 142862/200000: episode: 783, duration: 0.861s, episode steps: 126, steps per second: 146, episode reward: -125.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.024 [0.000, 2.000],  loss: 0.756625, mae: 12.470853, mean_q: -17.970141, mean_eps: 0.095613\n",
            " 142985/200000: episode: 784, duration: 0.796s, episode steps: 123, steps per second: 155, episode reward: -122.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.803449, mae: 12.475084, mean_q: -17.953122, mean_eps: 0.094815\n",
            " 143099/200000: episode: 785, duration: 0.754s, episode steps: 114, steps per second: 151, episode reward: -113.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.991 [0.000, 2.000],  loss: 0.928506, mae: 12.435729, mean_q: -17.852982, mean_eps: 0.094067\n",
            " 143233/200000: episode: 786, duration: 0.877s, episode steps: 134, steps per second: 153, episode reward: -133.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.187 [0.000, 2.000],  loss: 0.905929, mae: 12.475184, mean_q: -17.910753, mean_eps: 0.093282\n",
            " 143334/200000: episode: 787, duration: 0.658s, episode steps: 101, steps per second: 154, episode reward: -100.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.931 [0.000, 2.000],  loss: 0.913451, mae: 12.418090, mean_q: -17.828507, mean_eps: 0.092535\n",
            " 143440/200000: episode: 788, duration: 1.084s, episode steps: 106, steps per second:  98, episode reward: -105.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.142 [0.000, 2.000],  loss: 0.865212, mae: 12.495746, mean_q: -17.937440, mean_eps: 0.091889\n",
            " 143535/200000: episode: 789, duration: 0.961s, episode steps:  95, steps per second:  99, episode reward: -94.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.074 [0.000, 2.000],  loss: 0.874652, mae: 12.486341, mean_q: -17.938782, mean_eps: 0.091255\n",
            " 143647/200000: episode: 790, duration: 1.065s, episode steps: 112, steps per second: 105, episode reward: -111.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.116 [0.000, 2.000],  loss: 0.788504, mae: 12.443022, mean_q: -17.897021, mean_eps: 0.090597\n",
            " 143735/200000: episode: 791, duration: 0.751s, episode steps:  88, steps per second: 117, episode reward: -87.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.239 [0.000, 2.000],  loss: 0.960961, mae: 12.431202, mean_q: -17.844888, mean_eps: 0.089963\n",
            " 143842/200000: episode: 792, duration: 0.680s, episode steps: 107, steps per second: 157, episode reward: -106.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.206 [0.000, 2.000],  loss: 0.736952, mae: 12.324028, mean_q: -17.755824, mean_eps: 0.089343\n",
            " 143975/200000: episode: 793, duration: 0.869s, episode steps: 133, steps per second: 153, episode reward: -132.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.241 [0.000, 2.000],  loss: 0.935086, mae: 12.447323, mean_q: -17.847278, mean_eps: 0.088583\n",
            " 144090/200000: episode: 794, duration: 0.776s, episode steps: 115, steps per second: 148, episode reward: -114.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.983 [0.000, 2.000],  loss: 0.851412, mae: 12.506386, mean_q: -17.975237, mean_eps: 0.087797\n",
            " 144208/200000: episode: 795, duration: 0.850s, episode steps: 118, steps per second: 139, episode reward: -117.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.992 [0.000, 2.000],  loss: 0.865290, mae: 12.518115, mean_q: -17.979081, mean_eps: 0.087063\n",
            " 144303/200000: episode: 796, duration: 0.719s, episode steps:  95, steps per second: 132, episode reward: -94.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.137 [0.000, 2.000],  loss: 0.884741, mae: 12.657575, mean_q: -18.201245, mean_eps: 0.086391\n",
            " 144389/200000: episode: 797, duration: 0.613s, episode steps:  86, steps per second: 140, episode reward: -85.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.965 [0.000, 2.000],  loss: 1.064482, mae: 12.659235, mean_q: -18.147240, mean_eps: 0.085809\n",
            " 144494/200000: episode: 798, duration: 0.712s, episode steps: 105, steps per second: 147, episode reward: -104.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.876 [0.000, 2.000],  loss: 0.796908, mae: 12.554022, mean_q: -18.059424, mean_eps: 0.085201\n",
            " 144580/200000: episode: 799, duration: 0.577s, episode steps:  86, steps per second: 149, episode reward: -85.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.070 [0.000, 2.000],  loss: 0.810750, mae: 12.574577, mean_q: -18.101624, mean_eps: 0.084605\n",
            " 144686/200000: episode: 800, duration: 0.744s, episode steps: 106, steps per second: 142, episode reward: -105.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.962 [0.000, 2.000],  loss: 0.949531, mae: 12.610894, mean_q: -18.127720, mean_eps: 0.083997\n",
            " 144819/200000: episode: 801, duration: 0.889s, episode steps: 133, steps per second: 150, episode reward: -132.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.940 [0.000, 2.000],  loss: 0.819556, mae: 12.544324, mean_q: -18.059071, mean_eps: 0.083237\n",
            " 144923/200000: episode: 802, duration: 0.712s, episode steps: 104, steps per second: 146, episode reward: -103.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.125 [0.000, 2.000],  loss: 0.865122, mae: 12.492901, mean_q: -17.961729, mean_eps: 0.082490\n",
            " 145031/200000: episode: 803, duration: 0.699s, episode steps: 108, steps per second: 155, episode reward: -107.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.907 [0.000, 2.000],  loss: 0.833937, mae: 12.444232, mean_q: -17.907292, mean_eps: 0.081819\n",
            " 145135/200000: episode: 804, duration: 0.727s, episode steps: 104, steps per second: 143, episode reward: -103.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.923 [0.000, 2.000],  loss: 0.852002, mae: 12.424679, mean_q: -17.848495, mean_eps: 0.081147\n",
            " 145227/200000: episode: 805, duration: 0.844s, episode steps:  92, steps per second: 109, episode reward: -91.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.022 [0.000, 2.000],  loss: 0.889113, mae: 12.573169, mean_q: -18.061868, mean_eps: 0.080527\n",
            " 145350/200000: episode: 806, duration: 1.180s, episode steps: 123, steps per second: 104, episode reward: -122.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.805 [0.000, 2.000],  loss: 0.848457, mae: 12.483636, mean_q: -17.948361, mean_eps: 0.079843\n",
            " 145473/200000: episode: 807, duration: 1.194s, episode steps: 123, steps per second: 103, episode reward: -122.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.894 [0.000, 2.000],  loss: 0.899371, mae: 12.397174, mean_q: -17.817087, mean_eps: 0.079057\n",
            " 145618/200000: episode: 808, duration: 1.056s, episode steps: 145, steps per second: 137, episode reward: -144.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.159 [0.000, 2.000],  loss: 0.792819, mae: 12.517884, mean_q: -18.036390, mean_eps: 0.078209\n",
            " 145696/200000: episode: 809, duration: 0.572s, episode steps:  78, steps per second: 136, episode reward: -77.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.064 [0.000, 2.000],  loss: 0.880228, mae: 12.544256, mean_q: -18.050121, mean_eps: 0.077512\n",
            " 145803/200000: episode: 810, duration: 0.739s, episode steps: 107, steps per second: 145, episode reward: -106.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.720 [0.000, 2.000],  loss: 0.853394, mae: 12.385546, mean_q: -17.787626, mean_eps: 0.076929\n",
            " 145896/200000: episode: 811, duration: 0.647s, episode steps:  93, steps per second: 144, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.731 [0.000, 2.000],  loss: 0.875981, mae: 12.547357, mean_q: -18.001542, mean_eps: 0.076296\n",
            " 146000/200000: episode: 812, duration: 0.731s, episode steps: 104, steps per second: 142, episode reward: -103.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.827 [0.000, 2.000],  loss: 0.786333, mae: 12.375606, mean_q: -17.767199, mean_eps: 0.075675\n",
            " 146105/200000: episode: 813, duration: 0.695s, episode steps: 105, steps per second: 151, episode reward: -104.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.152 [0.000, 2.000],  loss: 0.874777, mae: 12.514247, mean_q: -17.988776, mean_eps: 0.075004\n",
            " 146207/200000: episode: 814, duration: 0.722s, episode steps: 102, steps per second: 141, episode reward: -101.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.971 [0.000, 2.000],  loss: 0.830289, mae: 12.561876, mean_q: -18.060763, mean_eps: 0.074345\n",
            " 146291/200000: episode: 815, duration: 0.617s, episode steps:  84, steps per second: 136, episode reward: -83.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.893 [0.000, 2.000],  loss: 0.940766, mae: 12.436093, mean_q: -17.853160, mean_eps: 0.073763\n",
            " 146406/200000: episode: 816, duration: 0.808s, episode steps: 115, steps per second: 142, episode reward: -114.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.009 [0.000, 2.000],  loss: 0.891717, mae: 12.603697, mean_q: -18.119705, mean_eps: 0.073129\n",
            " 146493/200000: episode: 817, duration: 0.617s, episode steps:  87, steps per second: 141, episode reward: -86.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.023 [0.000, 2.000],  loss: 0.957735, mae: 12.515346, mean_q: -17.965642, mean_eps: 0.072483\n",
            " 146607/200000: episode: 818, duration: 0.774s, episode steps: 114, steps per second: 147, episode reward: -113.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.035 [0.000, 2.000],  loss: 0.820103, mae: 12.417071, mean_q: -17.855319, mean_eps: 0.071850\n",
            " 146691/200000: episode: 819, duration: 0.557s, episode steps:  84, steps per second: 151, episode reward: -83.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.060 [0.000, 2.000],  loss: 0.844843, mae: 12.430247, mean_q: -17.869447, mean_eps: 0.071229\n",
            " 146774/200000: episode: 820, duration: 0.550s, episode steps:  83, steps per second: 151, episode reward: -82.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.060 [0.000, 2.000],  loss: 0.940644, mae: 12.506017, mean_q: -17.944312, mean_eps: 0.070697\n",
            " 146865/200000: episode: 821, duration: 0.604s, episode steps:  91, steps per second: 151, episode reward: -90.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.868 [0.000, 2.000],  loss: 0.975016, mae: 12.496515, mean_q: -17.945091, mean_eps: 0.070140\n",
            " 146970/200000: episode: 822, duration: 0.826s, episode steps: 105, steps per second: 127, episode reward: -104.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.238 [0.000, 2.000],  loss: 0.860593, mae: 12.446674, mean_q: -17.889728, mean_eps: 0.069519\n",
            " 147075/200000: episode: 823, duration: 0.991s, episode steps: 105, steps per second: 106, episode reward: -104.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.857 [0.000, 2.000],  loss: 0.919927, mae: 12.685673, mean_q: -18.222425, mean_eps: 0.068861\n",
            " 147189/200000: episode: 824, duration: 1.126s, episode steps: 114, steps per second: 101, episode reward: -113.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.096 [0.000, 2.000],  loss: 0.874219, mae: 12.661828, mean_q: -18.215261, mean_eps: 0.068164\n",
            " 147301/200000: episode: 825, duration: 1.146s, episode steps: 112, steps per second:  98, episode reward: -111.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.955 [0.000, 2.000],  loss: 0.920049, mae: 12.655427, mean_q: -18.183237, mean_eps: 0.067442\n",
            " 147391/200000: episode: 826, duration: 0.643s, episode steps:  90, steps per second: 140, episode reward: -89.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.111 [0.000, 2.000],  loss: 0.781841, mae: 12.542790, mean_q: -18.067195, mean_eps: 0.066809\n",
            " 147520/200000: episode: 827, duration: 0.952s, episode steps: 129, steps per second: 135, episode reward: -128.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.915 [0.000, 2.000],  loss: 0.777310, mae: 12.566525, mean_q: -18.104964, mean_eps: 0.066125\n",
            " 147601/200000: episode: 828, duration: 0.562s, episode steps:  81, steps per second: 144, episode reward: -80.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.049 [0.000, 2.000],  loss: 1.032405, mae: 12.703519, mean_q: -18.250291, mean_eps: 0.065453\n",
            " 147687/200000: episode: 829, duration: 0.604s, episode steps:  86, steps per second: 142, episode reward: -85.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.093 [0.000, 2.000],  loss: 0.819411, mae: 12.711797, mean_q: -18.314563, mean_eps: 0.064921\n",
            " 147796/200000: episode: 830, duration: 0.785s, episode steps: 109, steps per second: 139, episode reward: -108.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.193 [0.000, 2.000],  loss: 0.842029, mae: 12.668566, mean_q: -18.241843, mean_eps: 0.064313\n",
            " 147886/200000: episode: 831, duration: 0.633s, episode steps:  90, steps per second: 142, episode reward: -89.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.967 [0.000, 2.000],  loss: 0.976614, mae: 12.681901, mean_q: -18.232891, mean_eps: 0.063680\n",
            " 148014/200000: episode: 832, duration: 0.926s, episode steps: 128, steps per second: 138, episode reward: -127.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.141 [0.000, 2.000],  loss: 0.864204, mae: 12.623669, mean_q: -18.172533, mean_eps: 0.062983\n",
            " 148138/200000: episode: 833, duration: 0.898s, episode steps: 124, steps per second: 138, episode reward: -123.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.113 [0.000, 2.000],  loss: 0.921315, mae: 12.618303, mean_q: -18.115248, mean_eps: 0.062185\n",
            " 148236/200000: episode: 834, duration: 0.689s, episode steps:  98, steps per second: 142, episode reward: -97.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.939 [0.000, 2.000],  loss: 0.949409, mae: 12.707823, mean_q: -18.284680, mean_eps: 0.061489\n",
            " 148342/200000: episode: 835, duration: 0.757s, episode steps: 106, steps per second: 140, episode reward: -105.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.094 [0.000, 2.000],  loss: 0.972870, mae: 12.770795, mean_q: -18.378645, mean_eps: 0.060843\n",
            " 148434/200000: episode: 836, duration: 0.639s, episode steps:  92, steps per second: 144, episode reward: -91.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.065 [0.000, 2.000],  loss: 0.797913, mae: 12.605583, mean_q: -18.185339, mean_eps: 0.060209\n",
            " 148607/200000: episode: 837, duration: 1.218s, episode steps: 173, steps per second: 142, episode reward: -172.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.902 [0.000, 2.000],  loss: 0.952882, mae: 12.686904, mean_q: -18.246577, mean_eps: 0.059373\n",
            " 148723/200000: episode: 838, duration: 0.991s, episode steps: 116, steps per second: 117, episode reward: -115.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.241 [0.000, 2.000],  loss: 0.958339, mae: 12.717789, mean_q: -18.278114, mean_eps: 0.058461\n",
            " 148815/200000: episode: 839, duration: 0.900s, episode steps:  92, steps per second: 102, episode reward: -91.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.130 [0.000, 2.000],  loss: 0.762112, mae: 12.726191, mean_q: -18.344130, mean_eps: 0.057803\n",
            " 148912/200000: episode: 840, duration: 0.981s, episode steps:  97, steps per second:  99, episode reward: -96.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.144 [0.000, 2.000],  loss: 0.919056, mae: 12.692683, mean_q: -18.255429, mean_eps: 0.057207\n",
            " 149052/200000: episode: 841, duration: 1.376s, episode steps: 140, steps per second: 102, episode reward: -139.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.114 [0.000, 2.000],  loss: 0.878931, mae: 12.675422, mean_q: -18.230419, mean_eps: 0.056460\n",
            " 149180/200000: episode: 842, duration: 0.878s, episode steps: 128, steps per second: 146, episode reward: -127.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.047 [0.000, 2.000],  loss: 0.945225, mae: 12.665294, mean_q: -18.190268, mean_eps: 0.055611\n",
            " 149283/200000: episode: 843, duration: 0.720s, episode steps: 103, steps per second: 143, episode reward: -102.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.340 [0.000, 2.000],  loss: 0.880160, mae: 12.727952, mean_q: -18.301353, mean_eps: 0.054877\n",
            " 149387/200000: episode: 844, duration: 0.704s, episode steps: 104, steps per second: 148, episode reward: -103.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.212 [0.000, 2.000],  loss: 1.029431, mae: 12.650851, mean_q: -18.148658, mean_eps: 0.054218\n",
            " 149493/200000: episode: 845, duration: 0.729s, episode steps: 106, steps per second: 145, episode reward: -105.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.170 [0.000, 2.000],  loss: 0.854238, mae: 12.772847, mean_q: -18.389091, mean_eps: 0.053547\n",
            " 149617/200000: episode: 846, duration: 0.873s, episode steps: 124, steps per second: 142, episode reward: -123.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.177 [0.000, 2.000],  loss: 1.031353, mae: 12.750869, mean_q: -18.307011, mean_eps: 0.052812\n",
            " 149693/200000: episode: 847, duration: 0.527s, episode steps:  76, steps per second: 144, episode reward: -75.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.066 [0.000, 2.000],  loss: 0.949059, mae: 12.644631, mean_q: -18.145283, mean_eps: 0.052179\n",
            " 149788/200000: episode: 848, duration: 0.635s, episode steps:  95, steps per second: 150, episode reward: -94.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.032 [0.000, 2.000],  loss: 0.931355, mae: 12.706466, mean_q: -18.270023, mean_eps: 0.051647\n",
            " 149873/200000: episode: 849, duration: 0.578s, episode steps:  85, steps per second: 147, episode reward: -84.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.906 [0.000, 2.000],  loss: 0.858008, mae: 12.706481, mean_q: -18.292204, mean_eps: 0.051077\n",
            " 149978/200000: episode: 850, duration: 0.697s, episode steps: 105, steps per second: 151, episode reward: -104.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.086 [0.000, 2.000],  loss: 0.977938, mae: 12.728519, mean_q: -18.278485, mean_eps: 0.050469\n",
            " 150059/200000: episode: 851, duration: 0.560s, episode steps:  81, steps per second: 145, episode reward: -80.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.086 [0.000, 2.000],  loss: 0.900262, mae: 12.637129, mean_q: -18.122391, mean_eps: 0.050019\n",
            " 150129/200000: episode: 852, duration: 0.448s, episode steps:  70, steps per second: 156, episode reward: -69.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.329 [0.000, 2.000],  loss: 0.854517, mae: 12.707496, mean_q: -18.272555, mean_eps: 0.050000\n",
            " 150220/200000: episode: 853, duration: 0.615s, episode steps:  91, steps per second: 148, episode reward: -90.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.132 [0.000, 2.000],  loss: 0.901670, mae: 12.712764, mean_q: -18.260640, mean_eps: 0.050000\n",
            " 150323/200000: episode: 854, duration: 0.686s, episode steps: 103, steps per second: 150, episode reward: -102.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.922 [0.000, 2.000],  loss: 0.855782, mae: 12.608703, mean_q: -18.118913, mean_eps: 0.050000\n",
            " 150406/200000: episode: 855, duration: 0.590s, episode steps:  83, steps per second: 141, episode reward: -82.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.217 [0.000, 2.000],  loss: 0.999949, mae: 12.703412, mean_q: -18.205055, mean_eps: 0.050000\n",
            " 150501/200000: episode: 856, duration: 0.738s, episode steps:  95, steps per second: 129, episode reward: -94.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.158 [0.000, 2.000],  loss: 0.861978, mae: 12.687973, mean_q: -18.234441, mean_eps: 0.050000\n",
            " 150584/200000: episode: 857, duration: 0.744s, episode steps:  83, steps per second: 112, episode reward: -82.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.193 [0.000, 2.000],  loss: 0.838706, mae: 12.683408, mean_q: -18.240660, mean_eps: 0.050000\n",
            " 150668/200000: episode: 858, duration: 0.881s, episode steps:  84, steps per second:  95, episode reward: -83.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.012 [0.000, 2.000],  loss: 0.859032, mae: 12.672232, mean_q: -18.209980, mean_eps: 0.050000\n",
            " 150799/200000: episode: 859, duration: 1.262s, episode steps: 131, steps per second: 104, episode reward: -130.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.061 [0.000, 2.000],  loss: 0.983542, mae: 12.700664, mean_q: -18.225413, mean_eps: 0.050000\n",
            " 150920/200000: episode: 860, duration: 0.964s, episode steps: 121, steps per second: 125, episode reward: -120.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.901 [0.000, 2.000],  loss: 0.953713, mae: 12.665072, mean_q: -18.195318, mean_eps: 0.050000\n",
            " 151017/200000: episode: 861, duration: 0.678s, episode steps:  97, steps per second: 143, episode reward: -96.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.825 [0.000, 2.000],  loss: 0.855755, mae: 12.560683, mean_q: -18.053595, mean_eps: 0.050000\n",
            " 151107/200000: episode: 862, duration: 0.606s, episode steps:  90, steps per second: 149, episode reward: -89.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.911 [0.000, 2.000],  loss: 0.994621, mae: 12.581913, mean_q: -18.070550, mean_eps: 0.050000\n",
            " 151190/200000: episode: 863, duration: 0.555s, episode steps:  83, steps per second: 149, episode reward: -82.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.108 [0.000, 2.000],  loss: 0.916560, mae: 12.475889, mean_q: -17.928632, mean_eps: 0.050000\n",
            " 151330/200000: episode: 864, duration: 0.979s, episode steps: 140, steps per second: 143, episode reward: -139.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.957 [0.000, 2.000],  loss: 0.946008, mae: 12.434500, mean_q: -17.856874, mean_eps: 0.050000\n",
            " 151423/200000: episode: 865, duration: 0.609s, episode steps:  93, steps per second: 153, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.108 [0.000, 2.000],  loss: 0.898513, mae: 12.585879, mean_q: -18.089674, mean_eps: 0.050000\n",
            " 151514/200000: episode: 866, duration: 0.614s, episode steps:  91, steps per second: 148, episode reward: -90.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.835 [0.000, 2.000],  loss: 0.996564, mae: 12.589833, mean_q: -18.078938, mean_eps: 0.050000\n",
            " 151614/200000: episode: 867, duration: 0.697s, episode steps: 100, steps per second: 144, episode reward: -99.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.730 [0.000, 2.000],  loss: 0.986566, mae: 12.547078, mean_q: -18.009168, mean_eps: 0.050000\n",
            " 151722/200000: episode: 868, duration: 0.819s, episode steps: 108, steps per second: 132, episode reward: -107.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.130 [0.000, 2.000],  loss: 0.899499, mae: 12.667802, mean_q: -18.220857, mean_eps: 0.050000\n",
            " 151855/200000: episode: 869, duration: 0.923s, episode steps: 133, steps per second: 144, episode reward: -132.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.955 [0.000, 2.000],  loss: 0.954811, mae: 12.473224, mean_q: -17.915786, mean_eps: 0.050000\n",
            " 151957/200000: episode: 870, duration: 0.697s, episode steps: 102, steps per second: 146, episode reward: -101.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.147 [0.000, 2.000],  loss: 0.897924, mae: 12.515457, mean_q: -18.016597, mean_eps: 0.050000\n",
            " 152026/200000: episode: 871, duration: 0.469s, episode steps:  69, steps per second: 147, episode reward: -68.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.203 [0.000, 2.000],  loss: 0.935114, mae: 12.628594, mean_q: -18.135568, mean_eps: 0.050000\n",
            " 152124/200000: episode: 872, duration: 0.659s, episode steps:  98, steps per second: 149, episode reward: -97.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.143 [0.000, 2.000],  loss: 0.833360, mae: 12.626002, mean_q: -18.174233, mean_eps: 0.050000\n",
            " 152240/200000: episode: 873, duration: 0.814s, episode steps: 116, steps per second: 142, episode reward: -115.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.862 [0.000, 2.000],  loss: 0.781727, mae: 12.592912, mean_q: -18.136793, mean_eps: 0.050000\n",
            " 152314/200000: episode: 874, duration: 0.729s, episode steps:  74, steps per second: 102, episode reward: -73.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.932 [0.000, 2.000],  loss: 1.027727, mae: 12.770007, mean_q: -18.331483, mean_eps: 0.050000\n",
            " 152395/200000: episode: 875, duration: 0.766s, episode steps:  81, steps per second: 106, episode reward: -80.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.074 [0.000, 2.000],  loss: 1.001936, mae: 12.641175, mean_q: -18.146750, mean_eps: 0.050000\n",
            " 152491/200000: episode: 876, duration: 0.958s, episode steps:  96, steps per second: 100, episode reward: -95.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.115 [0.000, 2.000],  loss: 0.832114, mae: 12.598351, mean_q: -18.116711, mean_eps: 0.050000\n",
            " 152618/200000: episode: 877, duration: 1.337s, episode steps: 127, steps per second:  95, episode reward: -126.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.031 [0.000, 2.000],  loss: 0.927456, mae: 12.595617, mean_q: -18.098007, mean_eps: 0.050000\n",
            " 152709/200000: episode: 878, duration: 0.625s, episode steps:  91, steps per second: 146, episode reward: -90.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.022 [0.000, 2.000],  loss: 0.919258, mae: 12.518532, mean_q: -18.001260, mean_eps: 0.050000\n",
            " 152795/200000: episode: 879, duration: 0.602s, episode steps:  86, steps per second: 143, episode reward: -85.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.174 [0.000, 2.000],  loss: 1.036865, mae: 12.527109, mean_q: -17.976817, mean_eps: 0.050000\n",
            " 152892/200000: episode: 880, duration: 0.682s, episode steps:  97, steps per second: 142, episode reward: -96.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.289 [0.000, 2.000],  loss: 0.888003, mae: 12.497727, mean_q: -17.964333, mean_eps: 0.050000\n",
            " 153005/200000: episode: 881, duration: 0.808s, episode steps: 113, steps per second: 140, episode reward: -112.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.159 [0.000, 2.000],  loss: 0.929626, mae: 12.648523, mean_q: -18.201372, mean_eps: 0.050000\n",
            " 153227/200000: episode: 882, duration: 1.484s, episode steps: 222, steps per second: 150, episode reward: -221.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.171 [0.000, 2.000],  loss: 0.933121, mae: 12.742785, mean_q: -18.337067, mean_eps: 0.050000\n",
            " 153331/200000: episode: 883, duration: 0.723s, episode steps: 104, steps per second: 144, episode reward: -103.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.875 [0.000, 2.000],  loss: 0.924808, mae: 12.749039, mean_q: -18.333925, mean_eps: 0.050000\n",
            " 153439/200000: episode: 884, duration: 0.764s, episode steps: 108, steps per second: 141, episode reward: -107.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.046 [0.000, 2.000],  loss: 0.783523, mae: 12.756270, mean_q: -18.399992, mean_eps: 0.050000\n",
            " 153523/200000: episode: 885, duration: 0.616s, episode steps:  84, steps per second: 136, episode reward: -83.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.893 [0.000, 2.000],  loss: 0.926572, mae: 12.947580, mean_q: -18.626091, mean_eps: 0.050000\n",
            " 153628/200000: episode: 886, duration: 0.727s, episode steps: 105, steps per second: 144, episode reward: -104.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.143 [0.000, 2.000],  loss: 0.934496, mae: 12.848242, mean_q: -18.487253, mean_eps: 0.050000\n",
            " 153763/200000: episode: 887, duration: 0.965s, episode steps: 135, steps per second: 140, episode reward: -134.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.096 [0.000, 2.000],  loss: 1.004818, mae: 12.722114, mean_q: -18.273982, mean_eps: 0.050000\n",
            " 153854/200000: episode: 888, duration: 0.630s, episode steps:  91, steps per second: 145, episode reward: -90.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.022 [0.000, 2.000],  loss: 0.995130, mae: 12.727306, mean_q: -18.292703, mean_eps: 0.050000\n",
            " 153985/200000: episode: 889, duration: 0.925s, episode steps: 131, steps per second: 142, episode reward: -130.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.198 [0.000, 2.000],  loss: 0.919904, mae: 12.723254, mean_q: -18.305794, mean_eps: 0.050000\n",
            " 154088/200000: episode: 890, duration: 0.879s, episode steps: 103, steps per second: 117, episode reward: -102.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.146 [0.000, 2.000],  loss: 0.871834, mae: 12.748134, mean_q: -18.323100, mean_eps: 0.050000\n",
            " 154201/200000: episode: 891, duration: 1.141s, episode steps: 113, steps per second:  99, episode reward: -112.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.956 [0.000, 2.000],  loss: 0.872860, mae: 12.713685, mean_q: -18.278456, mean_eps: 0.050000\n",
            " 154287/200000: episode: 892, duration: 0.795s, episode steps:  86, steps per second: 108, episode reward: -85.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.837 [0.000, 2.000],  loss: 1.078641, mae: 12.721954, mean_q: -18.218600, mean_eps: 0.050000\n",
            " 154399/200000: episode: 893, duration: 1.077s, episode steps: 112, steps per second: 104, episode reward: -111.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.911 [0.000, 2.000],  loss: 1.000474, mae: 12.850582, mean_q: -18.480196, mean_eps: 0.050000\n",
            " 154515/200000: episode: 894, duration: 0.753s, episode steps: 116, steps per second: 154, episode reward: -115.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.147 [0.000, 2.000],  loss: 1.010308, mae: 12.783624, mean_q: -18.363380, mean_eps: 0.050000\n",
            " 154625/200000: episode: 895, duration: 0.721s, episode steps: 110, steps per second: 153, episode reward: -109.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.082 [0.000, 2.000],  loss: 0.861426, mae: 12.765583, mean_q: -18.376279, mean_eps: 0.050000\n",
            " 154714/200000: episode: 896, duration: 0.561s, episode steps:  89, steps per second: 159, episode reward: -88.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.865 [0.000, 2.000],  loss: 1.031064, mae: 12.924881, mean_q: -18.558797, mean_eps: 0.050000\n",
            " 154816/200000: episode: 897, duration: 0.694s, episode steps: 102, steps per second: 147, episode reward: -101.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.010 [0.000, 2.000],  loss: 0.882688, mae: 12.798404, mean_q: -18.419878, mean_eps: 0.050000\n",
            " 154932/200000: episode: 898, duration: 0.774s, episode steps: 116, steps per second: 150, episode reward: -115.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.034 [0.000, 2.000],  loss: 0.822767, mae: 12.871601, mean_q: -18.519672, mean_eps: 0.050000\n",
            " 155005/200000: episode: 899, duration: 0.479s, episode steps:  73, steps per second: 153, episode reward: -72.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.712 [0.000, 2.000],  loss: 1.115918, mae: 12.769307, mean_q: -18.309656, mean_eps: 0.050000\n",
            " 155136/200000: episode: 900, duration: 0.863s, episode steps: 131, steps per second: 152, episode reward: -130.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.687 [0.000, 2.000],  loss: 0.947067, mae: 12.699969, mean_q: -18.211378, mean_eps: 0.050000\n",
            " 155230/200000: episode: 901, duration: 0.645s, episode steps:  94, steps per second: 146, episode reward: -93.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.064 [0.000, 2.000],  loss: 0.978264, mae: 12.715810, mean_q: -18.266742, mean_eps: 0.050000\n",
            " 155354/200000: episode: 902, duration: 0.841s, episode steps: 124, steps per second: 147, episode reward: -123.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.742 [0.000, 2.000],  loss: 0.839247, mae: 12.759619, mean_q: -18.349209, mean_eps: 0.050000\n",
            " 155453/200000: episode: 903, duration: 0.649s, episode steps:  99, steps per second: 153, episode reward: -98.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.081 [0.000, 2.000],  loss: 0.908532, mae: 12.787751, mean_q: -18.365434, mean_eps: 0.050000\n",
            " 155539/200000: episode: 904, duration: 0.564s, episode steps:  86, steps per second: 152, episode reward: -85.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.640 [0.000, 2.000],  loss: 0.860901, mae: 12.697403, mean_q: -18.233487, mean_eps: 0.050000\n",
            " 155637/200000: episode: 905, duration: 0.661s, episode steps:  98, steps per second: 148, episode reward: -97.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.816 [0.000, 2.000],  loss: 0.859952, mae: 12.818478, mean_q: -18.419193, mean_eps: 0.050000\n",
            " 155701/200000: episode: 906, duration: 0.433s, episode steps:  64, steps per second: 148, episode reward: -63.000, mean reward: -0.984 [-1.000,  0.000], mean action: 0.844 [0.000, 2.000],  loss: 0.956312, mae: 12.838944, mean_q: -18.424483, mean_eps: 0.050000\n",
            " 155800/200000: episode: 907, duration: 0.722s, episode steps:  99, steps per second: 137, episode reward: -98.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.667 [0.000, 2.000],  loss: 0.827568, mae: 12.671551, mean_q: -18.203244, mean_eps: 0.050000\n",
            " 155928/200000: episode: 908, duration: 1.045s, episode steps: 128, steps per second: 123, episode reward: -127.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.086 [0.000, 2.000],  loss: 0.921907, mae: 12.840481, mean_q: -18.436654, mean_eps: 0.050000\n",
            " 156034/200000: episode: 909, duration: 1.099s, episode steps: 106, steps per second:  96, episode reward: -105.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.009 [0.000, 2.000],  loss: 0.944437, mae: 12.801930, mean_q: -18.372856, mean_eps: 0.050000\n",
            " 156130/200000: episode: 910, duration: 0.887s, episode steps:  96, steps per second: 108, episode reward: -95.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.990 [0.000, 2.000],  loss: 0.993520, mae: 12.853007, mean_q: -18.460870, mean_eps: 0.050000\n",
            " 156195/200000: episode: 911, duration: 0.681s, episode steps:  65, steps per second:  95, episode reward: -64.000, mean reward: -0.985 [-1.000,  0.000], mean action: 0.831 [0.000, 2.000],  loss: 0.901905, mae: 12.814668, mean_q: -18.399239, mean_eps: 0.050000\n",
            " 156282/200000: episode: 912, duration: 0.696s, episode steps:  87, steps per second: 125, episode reward: -86.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.989 [0.000, 2.000],  loss: 0.822149, mae: 12.814040, mean_q: -18.430150, mean_eps: 0.050000\n",
            " 156376/200000: episode: 913, duration: 0.605s, episode steps:  94, steps per second: 155, episode reward: -93.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.011 [0.000, 2.000],  loss: 0.989691, mae: 12.738837, mean_q: -18.266607, mean_eps: 0.050000\n",
            " 156458/200000: episode: 914, duration: 0.590s, episode steps:  82, steps per second: 139, episode reward: -81.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.927 [0.000, 2.000],  loss: 0.862413, mae: 12.716670, mean_q: -18.269772, mean_eps: 0.050000\n",
            " 156554/200000: episode: 915, duration: 0.628s, episode steps:  96, steps per second: 153, episode reward: -95.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.031 [0.000, 2.000],  loss: 1.062004, mae: 12.834444, mean_q: -18.395080, mean_eps: 0.050000\n",
            " 156671/200000: episode: 916, duration: 0.783s, episode steps: 117, steps per second: 150, episode reward: -116.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.094 [0.000, 2.000],  loss: 1.045222, mae: 12.798579, mean_q: -18.345709, mean_eps: 0.050000\n",
            " 156763/200000: episode: 917, duration: 0.631s, episode steps:  92, steps per second: 146, episode reward: -91.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.989 [0.000, 2.000],  loss: 1.005840, mae: 12.854715, mean_q: -18.448099, mean_eps: 0.050000\n",
            " 156868/200000: episode: 918, duration: 0.703s, episode steps: 105, steps per second: 149, episode reward: -104.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.752 [0.000, 2.000],  loss: 0.984359, mae: 12.750500, mean_q: -18.290703, mean_eps: 0.050000\n",
            " 156970/200000: episode: 919, duration: 0.689s, episode steps: 102, steps per second: 148, episode reward: -101.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.147 [0.000, 2.000],  loss: 0.935275, mae: 12.696759, mean_q: -18.213250, mean_eps: 0.050000\n",
            " 157086/200000: episode: 920, duration: 0.811s, episode steps: 116, steps per second: 143, episode reward: -115.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.181 [0.000, 2.000],  loss: 1.088994, mae: 12.830163, mean_q: -18.384474, mean_eps: 0.050000\n",
            " 157197/200000: episode: 921, duration: 0.758s, episode steps: 111, steps per second: 146, episode reward: -110.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.099 [0.000, 2.000],  loss: 1.004440, mae: 12.976196, mean_q: -18.621218, mean_eps: 0.050000\n",
            " 157283/200000: episode: 922, duration: 0.558s, episode steps:  86, steps per second: 154, episode reward: -85.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.047 [0.000, 2.000],  loss: 0.960091, mae: 12.892215, mean_q: -18.514853, mean_eps: 0.050000\n",
            " 157377/200000: episode: 923, duration: 0.705s, episode steps:  94, steps per second: 133, episode reward: -93.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.085 [0.000, 2.000],  loss: 0.902014, mae: 12.880606, mean_q: -18.497212, mean_eps: 0.050000\n",
            " 157498/200000: episode: 924, duration: 0.832s, episode steps: 121, steps per second: 146, episode reward: -120.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.818 [0.000, 2.000],  loss: 0.931843, mae: 12.751866, mean_q: -18.290831, mean_eps: 0.050000\n",
            " 157595/200000: episode: 925, duration: 0.628s, episode steps:  97, steps per second: 154, episode reward: -96.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.010 [0.000, 2.000],  loss: 0.875469, mae: 12.813847, mean_q: -18.382473, mean_eps: 0.050000\n",
            " 157684/200000: episode: 926, duration: 0.614s, episode steps:  89, steps per second: 145, episode reward: -88.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.944 [0.000, 2.000],  loss: 0.828468, mae: 12.839614, mean_q: -18.449894, mean_eps: 0.050000\n",
            " 157789/200000: episode: 927, duration: 1.016s, episode steps: 105, steps per second: 103, episode reward: -104.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.943 [0.000, 2.000],  loss: 0.951256, mae: 12.837848, mean_q: -18.430124, mean_eps: 0.050000\n",
            " 157874/200000: episode: 928, duration: 0.840s, episode steps:  85, steps per second: 101, episode reward: -84.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.047 [0.000, 2.000],  loss: 0.933336, mae: 12.842395, mean_q: -18.456624, mean_eps: 0.050000\n",
            " 157965/200000: episode: 929, duration: 0.872s, episode steps:  91, steps per second: 104, episode reward: -90.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.978 [0.000, 2.000],  loss: 0.990177, mae: 12.875617, mean_q: -18.491705, mean_eps: 0.050000\n",
            " 158080/200000: episode: 930, duration: 1.023s, episode steps: 115, steps per second: 112, episode reward: -114.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.104 [0.000, 2.000],  loss: 1.042274, mae: 12.806568, mean_q: -18.368061, mean_eps: 0.050000\n",
            " 158184/200000: episode: 931, duration: 0.726s, episode steps: 104, steps per second: 143, episode reward: -103.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.125 [0.000, 2.000],  loss: 0.884948, mae: 12.729874, mean_q: -18.300311, mean_eps: 0.050000\n",
            " 158259/200000: episode: 932, duration: 0.531s, episode steps:  75, steps per second: 141, episode reward: -74.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.107 [0.000, 2.000],  loss: 1.103970, mae: 12.757883, mean_q: -18.248581, mean_eps: 0.050000\n",
            " 158386/200000: episode: 933, duration: 0.868s, episode steps: 127, steps per second: 146, episode reward: -126.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.118 [0.000, 2.000],  loss: 0.962261, mae: 12.864840, mean_q: -18.465857, mean_eps: 0.050000\n",
            " 158512/200000: episode: 934, duration: 0.831s, episode steps: 126, steps per second: 152, episode reward: -125.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.167 [0.000, 2.000],  loss: 0.991882, mae: 12.948459, mean_q: -18.608328, mean_eps: 0.050000\n",
            " 158610/200000: episode: 935, duration: 0.642s, episode steps:  98, steps per second: 153, episode reward: -97.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.082 [0.000, 2.000],  loss: 1.007512, mae: 12.934424, mean_q: -18.566945, mean_eps: 0.050000\n",
            " 158686/200000: episode: 936, duration: 0.527s, episode steps:  76, steps per second: 144, episode reward: -75.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.118 [0.000, 2.000],  loss: 0.833629, mae: 12.850961, mean_q: -18.508423, mean_eps: 0.050000\n",
            " 158807/200000: episode: 937, duration: 0.793s, episode steps: 121, steps per second: 153, episode reward: -120.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.083 [0.000, 2.000],  loss: 0.927570, mae: 12.862602, mean_q: -18.496546, mean_eps: 0.050000\n",
            " 158891/200000: episode: 938, duration: 0.577s, episode steps:  84, steps per second: 146, episode reward: -83.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.071 [0.000, 2.000],  loss: 0.883793, mae: 12.827811, mean_q: -18.468094, mean_eps: 0.050000\n",
            " 159029/200000: episode: 939, duration: 0.994s, episode steps: 138, steps per second: 139, episode reward: -137.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.167 [0.000, 2.000],  loss: 1.001895, mae: 12.838624, mean_q: -18.432912, mean_eps: 0.050000\n",
            " 159122/200000: episode: 940, duration: 0.626s, episode steps:  93, steps per second: 149, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.151 [0.000, 2.000],  loss: 0.852873, mae: 12.586510, mean_q: -18.073921, mean_eps: 0.050000\n",
            " 159219/200000: episode: 941, duration: 0.673s, episode steps:  97, steps per second: 144, episode reward: -96.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.031 [0.000, 2.000],  loss: 0.895751, mae: 12.645417, mean_q: -18.164074, mean_eps: 0.050000\n",
            " 159409/200000: episode: 942, duration: 1.309s, episode steps: 190, steps per second: 145, episode reward: -189.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.837 [0.000, 2.000],  loss: 1.000595, mae: 12.768813, mean_q: -18.317460, mean_eps: 0.050000\n",
            " 159516/200000: episode: 943, duration: 0.871s, episode steps: 107, steps per second: 123, episode reward: -106.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.178 [0.000, 2.000],  loss: 1.020215, mae: 12.643470, mean_q: -18.116381, mean_eps: 0.050000\n",
            " 159633/200000: episode: 944, duration: 1.223s, episode steps: 117, steps per second:  96, episode reward: -116.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.154 [0.000, 2.000],  loss: 0.831052, mae: 12.751009, mean_q: -18.319363, mean_eps: 0.050000\n",
            " 159747/200000: episode: 945, duration: 1.122s, episode steps: 114, steps per second: 102, episode reward: -113.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.930 [0.000, 2.000],  loss: 0.916654, mae: 12.740526, mean_q: -18.291408, mean_eps: 0.050000\n",
            " 159897/200000: episode: 946, duration: 1.361s, episode steps: 150, steps per second: 110, episode reward: -149.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.120 [0.000, 2.000],  loss: 0.959739, mae: 12.822415, mean_q: -18.406656, mean_eps: 0.050000\n",
            " 159973/200000: episode: 947, duration: 0.512s, episode steps:  76, steps per second: 148, episode reward: -75.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.171 [0.000, 2.000],  loss: 0.967106, mae: 12.744068, mean_q: -18.285490, mean_eps: 0.050000\n",
            " 160057/200000: episode: 948, duration: 0.552s, episode steps:  84, steps per second: 152, episode reward: -83.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.071 [0.000, 2.000],  loss: 1.008441, mae: 12.604191, mean_q: -18.070262, mean_eps: 0.050000\n",
            " 160153/200000: episode: 949, duration: 0.647s, episode steps:  96, steps per second: 148, episode reward: -95.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.010 [0.000, 2.000],  loss: 0.943270, mae: 12.603901, mean_q: -18.085031, mean_eps: 0.050000\n",
            " 160259/200000: episode: 950, duration: 0.711s, episode steps: 106, steps per second: 149, episode reward: -105.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.906 [0.000, 2.000],  loss: 1.020719, mae: 12.633922, mean_q: -18.106462, mean_eps: 0.050000\n",
            " 160362/200000: episode: 951, duration: 0.684s, episode steps: 103, steps per second: 151, episode reward: -102.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.078 [0.000, 2.000],  loss: 0.897935, mae: 12.600180, mean_q: -18.122278, mean_eps: 0.050000\n",
            " 160447/200000: episode: 952, duration: 0.562s, episode steps:  85, steps per second: 151, episode reward: -84.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.859 [0.000, 2.000],  loss: 0.861099, mae: 12.684809, mean_q: -18.255264, mean_eps: 0.050000\n",
            " 160532/200000: episode: 953, duration: 0.603s, episode steps:  85, steps per second: 141, episode reward: -84.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.871 [0.000, 2.000],  loss: 0.854383, mae: 12.679058, mean_q: -18.249019, mean_eps: 0.050000\n",
            " 160642/200000: episode: 954, duration: 0.755s, episode steps: 110, steps per second: 146, episode reward: -109.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.864 [0.000, 2.000],  loss: 0.986569, mae: 12.660613, mean_q: -18.162866, mean_eps: 0.050000\n",
            " 160795/200000: episode: 955, duration: 1.036s, episode steps: 153, steps per second: 148, episode reward: -152.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.020 [0.000, 2.000],  loss: 0.957755, mae: 12.664622, mean_q: -18.184609, mean_eps: 0.050000\n",
            " 161032/200000: episode: 956, duration: 1.600s, episode steps: 237, steps per second: 148, episode reward: -236.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.139 [0.000, 2.000],  loss: 0.881804, mae: 12.601368, mean_q: -18.105904, mean_eps: 0.050000\n",
            " 161115/200000: episode: 957, duration: 0.542s, episode steps:  83, steps per second: 153, episode reward: -82.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.012 [0.000, 2.000],  loss: 0.974091, mae: 12.590273, mean_q: -18.071987, mean_eps: 0.050000\n",
            " 161213/200000: episode: 958, duration: 0.715s, episode steps:  98, steps per second: 137, episode reward: -97.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.888 [0.000, 2.000],  loss: 0.909690, mae: 12.692130, mean_q: -18.237895, mean_eps: 0.050000\n",
            " 161333/200000: episode: 959, duration: 0.891s, episode steps: 120, steps per second: 135, episode reward: -119.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.067 [0.000, 2.000],  loss: 0.934710, mae: 12.652347, mean_q: -18.165591, mean_eps: 0.050000\n",
            " 161466/200000: episode: 960, duration: 1.280s, episode steps: 133, steps per second: 104, episode reward: -132.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.872 [0.000, 2.000],  loss: 0.896556, mae: 12.615587, mean_q: -18.123212, mean_eps: 0.050000\n",
            " 161549/200000: episode: 961, duration: 0.770s, episode steps:  83, steps per second: 108, episode reward: -82.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.169 [0.000, 2.000],  loss: 0.996134, mae: 12.665911, mean_q: -18.180289, mean_eps: 0.050000\n",
            " 161657/200000: episode: 962, duration: 1.085s, episode steps: 108, steps per second: 100, episode reward: -107.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.157 [0.000, 2.000],  loss: 0.978755, mae: 12.600681, mean_q: -18.091062, mean_eps: 0.050000\n",
            " 161746/200000: episode: 963, duration: 0.598s, episode steps:  89, steps per second: 149, episode reward: -88.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.921 [0.000, 2.000],  loss: 0.940092, mae: 12.735212, mean_q: -18.288098, mean_eps: 0.050000\n",
            " 161869/200000: episode: 964, duration: 0.838s, episode steps: 123, steps per second: 147, episode reward: -122.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.902 [0.000, 2.000],  loss: 0.901467, mae: 12.647673, mean_q: -18.148914, mean_eps: 0.050000\n",
            " 161975/200000: episode: 965, duration: 0.695s, episode steps: 106, steps per second: 152, episode reward: -105.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.830 [0.000, 2.000],  loss: 0.942714, mae: 12.595111, mean_q: -18.061456, mean_eps: 0.050000\n",
            " 162087/200000: episode: 966, duration: 0.738s, episode steps: 112, steps per second: 152, episode reward: -111.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.027 [0.000, 2.000],  loss: 0.906129, mae: 12.719511, mean_q: -18.266792, mean_eps: 0.050000\n",
            " 162212/200000: episode: 967, duration: 0.859s, episode steps: 125, steps per second: 145, episode reward: -124.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.968 [0.000, 2.000],  loss: 0.931428, mae: 12.720202, mean_q: -18.250225, mean_eps: 0.050000\n",
            " 162329/200000: episode: 968, duration: 0.823s, episode steps: 117, steps per second: 142, episode reward: -116.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.145 [0.000, 2.000],  loss: 0.912778, mae: 12.693657, mean_q: -18.229742, mean_eps: 0.050000\n",
            " 162422/200000: episode: 969, duration: 0.659s, episode steps:  93, steps per second: 141, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.161 [0.000, 2.000],  loss: 0.990083, mae: 12.811671, mean_q: -18.393207, mean_eps: 0.050000\n",
            " 162513/200000: episode: 970, duration: 0.645s, episode steps:  91, steps per second: 141, episode reward: -90.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.923 [0.000, 2.000],  loss: 0.951868, mae: 12.682574, mean_q: -18.181625, mean_eps: 0.050000\n",
            " 162614/200000: episode: 971, duration: 0.685s, episode steps: 101, steps per second: 147, episode reward: -100.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.887117, mae: 12.740521, mean_q: -18.323305, mean_eps: 0.050000\n",
            " 162722/200000: episode: 972, duration: 0.721s, episode steps: 108, steps per second: 150, episode reward: -107.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.981 [0.000, 2.000],  loss: 0.980231, mae: 12.629910, mean_q: -18.110462, mean_eps: 0.050000\n",
            " 162829/200000: episode: 973, duration: 0.723s, episode steps: 107, steps per second: 148, episode reward: -106.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.206 [0.000, 2.000],  loss: 1.070041, mae: 12.755054, mean_q: -18.272470, mean_eps: 0.050000\n",
            " 162921/200000: episode: 974, duration: 0.654s, episode steps:  92, steps per second: 141, episode reward: -91.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.804 [0.000, 2.000],  loss: 0.955970, mae: 12.765771, mean_q: -18.331346, mean_eps: 0.050000\n",
            " 163015/200000: episode: 975, duration: 0.674s, episode steps:  94, steps per second: 140, episode reward: -93.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.915 [0.000, 2.000],  loss: 0.985076, mae: 12.800355, mean_q: -18.383609, mean_eps: 0.050000\n",
            " 163107/200000: episode: 976, duration: 0.642s, episode steps:  92, steps per second: 143, episode reward: -91.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.891 [0.000, 2.000],  loss: 0.937833, mae: 12.825017, mean_q: -18.408161, mean_eps: 0.050000\n",
            " 163228/200000: episode: 977, duration: 1.207s, episode steps: 121, steps per second: 100, episode reward: -120.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.058 [0.000, 2.000],  loss: 0.909575, mae: 12.888718, mean_q: -18.526941, mean_eps: 0.050000\n",
            " 163325/200000: episode: 978, duration: 0.941s, episode steps:  97, steps per second: 103, episode reward: -96.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.887 [0.000, 2.000],  loss: 0.965551, mae: 12.880643, mean_q: -18.509408, mean_eps: 0.050000\n",
            " 163437/200000: episode: 979, duration: 1.186s, episode steps: 112, steps per second:  94, episode reward: -111.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.161 [0.000, 2.000],  loss: 1.001316, mae: 12.989585, mean_q: -18.640330, mean_eps: 0.050000\n",
            " 163581/200000: episode: 980, duration: 1.063s, episode steps: 144, steps per second: 135, episode reward: -143.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.951 [0.000, 2.000],  loss: 1.039241, mae: 12.942744, mean_q: -18.577461, mean_eps: 0.050000\n",
            " 163662/200000: episode: 981, duration: 0.565s, episode steps:  81, steps per second: 143, episode reward: -80.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.901 [0.000, 2.000],  loss: 0.946165, mae: 12.879350, mean_q: -18.503262, mean_eps: 0.050000\n",
            " 163759/200000: episode: 982, duration: 0.695s, episode steps:  97, steps per second: 140, episode reward: -96.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.216 [0.000, 2.000],  loss: 0.931813, mae: 12.838284, mean_q: -18.455140, mean_eps: 0.050000\n",
            " 163854/200000: episode: 983, duration: 0.638s, episode steps:  95, steps per second: 149, episode reward: -94.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.253 [0.000, 2.000],  loss: 0.980008, mae: 12.790641, mean_q: -18.369984, mean_eps: 0.050000\n",
            " 163952/200000: episode: 984, duration: 0.700s, episode steps:  98, steps per second: 140, episode reward: -97.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.980 [0.000, 2.000],  loss: 0.895972, mae: 12.782738, mean_q: -18.399905, mean_eps: 0.050000\n",
            " 164045/200000: episode: 985, duration: 0.695s, episode steps:  93, steps per second: 134, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.161 [0.000, 2.000],  loss: 1.054814, mae: 12.886165, mean_q: -18.480365, mean_eps: 0.050000\n",
            " 164174/200000: episode: 986, duration: 0.854s, episode steps: 129, steps per second: 151, episode reward: -128.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.217 [0.000, 2.000],  loss: 0.928476, mae: 12.858632, mean_q: -18.467768, mean_eps: 0.050000\n",
            " 164259/200000: episode: 987, duration: 0.574s, episode steps:  85, steps per second: 148, episode reward: -84.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.059 [0.000, 2.000],  loss: 0.954307, mae: 12.836448, mean_q: -18.439865, mean_eps: 0.050000\n",
            " 164358/200000: episode: 988, duration: 0.727s, episode steps:  99, steps per second: 136, episode reward: -98.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.889 [0.000, 2.000],  loss: 1.020842, mae: 12.809281, mean_q: -18.394731, mean_eps: 0.050000\n",
            " 164449/200000: episode: 989, duration: 0.660s, episode steps:  91, steps per second: 138, episode reward: -90.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.044 [0.000, 2.000],  loss: 0.952463, mae: 12.905621, mean_q: -18.542453, mean_eps: 0.050000\n",
            " 164585/200000: episode: 990, duration: 0.963s, episode steps: 136, steps per second: 141, episode reward: -135.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.103 [0.000, 2.000],  loss: 1.146011, mae: 12.917569, mean_q: -18.536440, mean_eps: 0.050000\n",
            " 164683/200000: episode: 991, duration: 0.643s, episode steps:  98, steps per second: 153, episode reward: -97.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.133 [0.000, 2.000],  loss: 1.108685, mae: 12.888996, mean_q: -18.463982, mean_eps: 0.050000\n",
            " 164780/200000: episode: 992, duration: 0.719s, episode steps:  97, steps per second: 135, episode reward: -96.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.928 [0.000, 2.000],  loss: 0.913766, mae: 12.900447, mean_q: -18.546171, mean_eps: 0.050000\n",
            " 164906/200000: episode: 993, duration: 1.054s, episode steps: 126, steps per second: 120, episode reward: -125.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.071 [0.000, 2.000],  loss: 0.999547, mae: 12.979599, mean_q: -18.637698, mean_eps: 0.050000\n",
            " 164990/200000: episode: 994, duration: 0.915s, episode steps:  84, steps per second:  92, episode reward: -83.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.095 [0.000, 2.000],  loss: 0.998250, mae: 12.943623, mean_q: -18.588563, mean_eps: 0.050000\n",
            " 165102/200000: episode: 995, duration: 1.242s, episode steps: 112, steps per second:  90, episode reward: -111.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.071 [0.000, 2.000],  loss: 1.050539, mae: 12.938931, mean_q: -18.563535, mean_eps: 0.050000\n",
            " 165261/200000: episode: 996, duration: 1.463s, episode steps: 159, steps per second: 109, episode reward: -158.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.157 [0.000, 2.000],  loss: 0.946664, mae: 13.116340, mean_q: -18.852986, mean_eps: 0.050000\n",
            " 165357/200000: episode: 997, duration: 0.711s, episode steps:  96, steps per second: 135, episode reward: -95.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.062 [0.000, 2.000],  loss: 1.038703, mae: 12.957016, mean_q: -18.585102, mean_eps: 0.050000\n",
            " 165453/200000: episode: 998, duration: 0.705s, episode steps:  96, steps per second: 136, episode reward: -95.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.271 [0.000, 2.000],  loss: 0.978029, mae: 12.900934, mean_q: -18.511299, mean_eps: 0.050000\n",
            " 165544/200000: episode: 999, duration: 0.652s, episode steps:  91, steps per second: 140, episode reward: -90.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.901 [0.000, 2.000],  loss: 1.023699, mae: 12.944344, mean_q: -18.561451, mean_eps: 0.050000\n",
            " 165647/200000: episode: 1000, duration: 0.773s, episode steps: 103, steps per second: 133, episode reward: -102.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.951 [0.000, 2.000],  loss: 1.073693, mae: 12.961778, mean_q: -18.581633, mean_eps: 0.050000\n",
            " 165807/200000: episode: 1001, duration: 1.094s, episode steps: 160, steps per second: 146, episode reward: -159.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.144 [0.000, 2.000],  loss: 0.895938, mae: 12.879807, mean_q: -18.509057, mean_eps: 0.050000\n",
            " 165956/200000: episode: 1002, duration: 1.005s, episode steps: 149, steps per second: 148, episode reward: -148.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.940 [0.000, 2.000],  loss: 0.912626, mae: 12.938393, mean_q: -18.604863, mean_eps: 0.050000\n",
            " 166037/200000: episode: 1003, duration: 0.555s, episode steps:  81, steps per second: 146, episode reward: -80.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.963 [0.000, 2.000],  loss: 1.022209, mae: 12.805126, mean_q: -18.358861, mean_eps: 0.050000\n",
            " 166143/200000: episode: 1004, duration: 0.740s, episode steps: 106, steps per second: 143, episode reward: -105.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.179 [0.000, 2.000],  loss: 1.033695, mae: 12.892693, mean_q: -18.483736, mean_eps: 0.050000\n",
            " 166332/200000: episode: 1005, duration: 1.289s, episode steps: 189, steps per second: 147, episode reward: -188.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.772 [0.000, 2.000],  loss: 0.988735, mae: 12.806854, mean_q: -18.380911, mean_eps: 0.050000\n",
            " 166451/200000: episode: 1006, duration: 0.870s, episode steps: 119, steps per second: 137, episode reward: -118.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.739 [0.000, 2.000],  loss: 0.923645, mae: 12.811203, mean_q: -18.432192, mean_eps: 0.050000\n",
            " 166533/200000: episode: 1007, duration: 0.617s, episode steps:  82, steps per second: 133, episode reward: -81.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.024 [0.000, 2.000],  loss: 0.978767, mae: 12.826409, mean_q: -18.420960, mean_eps: 0.050000\n",
            " 166633/200000: episode: 1008, duration: 0.797s, episode steps: 100, steps per second: 126, episode reward: -99.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.120 [0.000, 2.000],  loss: 1.088837, mae: 12.877376, mean_q: -18.464511, mean_eps: 0.050000\n",
            " 166745/200000: episode: 1009, duration: 1.110s, episode steps: 112, steps per second: 101, episode reward: -111.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.080 [0.000, 2.000],  loss: 0.986386, mae: 12.867856, mean_q: -18.478565, mean_eps: 0.050000\n",
            " 166836/200000: episode: 1010, duration: 0.849s, episode steps:  91, steps per second: 107, episode reward: -90.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.033 [0.000, 2.000],  loss: 0.949877, mae: 12.845449, mean_q: -18.428862, mean_eps: 0.050000\n",
            " 166913/200000: episode: 1011, duration: 0.806s, episode steps:  77, steps per second:  96, episode reward: -76.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.117 [0.000, 2.000],  loss: 0.935003, mae: 12.986730, mean_q: -18.652346, mean_eps: 0.050000\n",
            " 166999/200000: episode: 1012, duration: 0.716s, episode steps:  86, steps per second: 120, episode reward: -85.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.151 [0.000, 2.000],  loss: 1.061397, mae: 12.973810, mean_q: -18.624372, mean_eps: 0.050000\n",
            " 167207/200000: episode: 1013, duration: 1.405s, episode steps: 208, steps per second: 148, episode reward: -207.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.938 [0.000, 2.000],  loss: 0.933746, mae: 12.999793, mean_q: -18.702980, mean_eps: 0.050000\n",
            " 167301/200000: episode: 1014, duration: 0.662s, episode steps:  94, steps per second: 142, episode reward: -93.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.021 [0.000, 2.000],  loss: 0.971769, mae: 12.897464, mean_q: -18.522946, mean_eps: 0.050000\n",
            " 167397/200000: episode: 1015, duration: 0.628s, episode steps:  96, steps per second: 153, episode reward: -95.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.823 [0.000, 2.000],  loss: 0.974132, mae: 12.834352, mean_q: -18.433463, mean_eps: 0.050000\n",
            " 167521/200000: episode: 1016, duration: 0.839s, episode steps: 124, steps per second: 148, episode reward: -123.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.952 [0.000, 2.000],  loss: 0.994524, mae: 12.946687, mean_q: -18.584042, mean_eps: 0.050000\n",
            " 167652/200000: episode: 1017, duration: 0.903s, episode steps: 131, steps per second: 145, episode reward: -130.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.771 [0.000, 2.000],  loss: 0.979608, mae: 12.928012, mean_q: -18.584100, mean_eps: 0.050000\n",
            " 167787/200000: episode: 1018, duration: 0.939s, episode steps: 135, steps per second: 144, episode reward: -134.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.141 [0.000, 2.000],  loss: 0.963593, mae: 12.954109, mean_q: -18.621629, mean_eps: 0.050000\n",
            " 167909/200000: episode: 1019, duration: 0.851s, episode steps: 122, steps per second: 143, episode reward: -121.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.057 [0.000, 2.000],  loss: 1.009770, mae: 12.929885, mean_q: -18.554649, mean_eps: 0.050000\n",
            " 168008/200000: episode: 1020, duration: 0.674s, episode steps:  99, steps per second: 147, episode reward: -98.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.697 [0.000, 2.000],  loss: 1.026038, mae: 12.809971, mean_q: -18.376074, mean_eps: 0.050000\n",
            " 168113/200000: episode: 1021, duration: 0.743s, episode steps: 105, steps per second: 141, episode reward: -104.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.124 [0.000, 2.000],  loss: 0.933646, mae: 12.873469, mean_q: -18.497809, mean_eps: 0.050000\n",
            " 168232/200000: episode: 1022, duration: 0.816s, episode steps: 119, steps per second: 146, episode reward: -118.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.084 [0.000, 2.000],  loss: 0.987720, mae: 12.917960, mean_q: -18.521894, mean_eps: 0.050000\n",
            " 168348/200000: episode: 1023, duration: 0.820s, episode steps: 116, steps per second: 141, episode reward: -115.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.078 [0.000, 2.000],  loss: 0.949906, mae: 12.752049, mean_q: -18.279191, mean_eps: 0.050000\n",
            " 168491/200000: episode: 1024, duration: 1.225s, episode steps: 143, steps per second: 117, episode reward: -142.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.783 [0.000, 2.000],  loss: 0.962513, mae: 12.802604, mean_q: -18.357490, mean_eps: 0.050000\n",
            " 168569/200000: episode: 1025, duration: 0.807s, episode steps:  78, steps per second:  97, episode reward: -77.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.026 [0.000, 2.000],  loss: 1.002934, mae: 12.847031, mean_q: -18.397161, mean_eps: 0.050000\n",
            " 168693/200000: episode: 1026, duration: 1.156s, episode steps: 124, steps per second: 107, episode reward: -123.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.952 [0.000, 2.000],  loss: 0.973718, mae: 12.849328, mean_q: -18.430808, mean_eps: 0.050000\n",
            " 168787/200000: episode: 1027, duration: 0.846s, episode steps:  94, steps per second: 111, episode reward: -93.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.734 [0.000, 2.000],  loss: 0.798109, mae: 12.685218, mean_q: -18.223066, mean_eps: 0.050000\n",
            " 168880/200000: episode: 1028, duration: 0.651s, episode steps:  93, steps per second: 143, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.957 [0.000, 2.000],  loss: 1.013983, mae: 12.862392, mean_q: -18.442689, mean_eps: 0.050000\n",
            " 168973/200000: episode: 1029, duration: 0.662s, episode steps:  93, steps per second: 140, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.817 [0.000, 2.000],  loss: 0.998504, mae: 12.895457, mean_q: -18.506782, mean_eps: 0.050000\n",
            " 169078/200000: episode: 1030, duration: 0.681s, episode steps: 105, steps per second: 154, episode reward: -104.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.067 [0.000, 2.000],  loss: 1.095715, mae: 12.825422, mean_q: -18.390840, mean_eps: 0.050000\n",
            " 169234/200000: episode: 1031, duration: 1.044s, episode steps: 156, steps per second: 149, episode reward: -155.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.974 [0.000, 2.000],  loss: 1.062636, mae: 12.769540, mean_q: -18.314208, mean_eps: 0.050000\n",
            " 169338/200000: episode: 1032, duration: 0.742s, episode steps: 104, steps per second: 140, episode reward: -103.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.067 [0.000, 2.000],  loss: 0.887841, mae: 12.739149, mean_q: -18.293889, mean_eps: 0.050000\n",
            " 169458/200000: episode: 1033, duration: 0.841s, episode steps: 120, steps per second: 143, episode reward: -119.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.208 [0.000, 2.000],  loss: 0.927926, mae: 12.838923, mean_q: -18.438830, mean_eps: 0.050000\n",
            " 169549/200000: episode: 1034, duration: 0.623s, episode steps:  91, steps per second: 146, episode reward: -90.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.110 [0.000, 2.000],  loss: 1.071443, mae: 12.854451, mean_q: -18.428381, mean_eps: 0.050000\n",
            " 169687/200000: episode: 1035, duration: 1.093s, episode steps: 138, steps per second: 126, episode reward: -137.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.014 [0.000, 2.000],  loss: 0.847049, mae: 12.863333, mean_q: -18.496644, mean_eps: 0.050000\n",
            " 169768/200000: episode: 1036, duration: 0.657s, episode steps:  81, steps per second: 123, episode reward: -80.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.049 [0.000, 2.000],  loss: 0.933304, mae: 12.819075, mean_q: -18.399050, mean_eps: 0.050000\n",
            " 169878/200000: episode: 1037, duration: 0.867s, episode steps: 110, steps per second: 127, episode reward: -109.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.155 [0.000, 2.000],  loss: 0.994289, mae: 12.828614, mean_q: -18.401206, mean_eps: 0.050000\n",
            " 169993/200000: episode: 1038, duration: 0.832s, episode steps: 115, steps per second: 138, episode reward: -114.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.096 [0.000, 2.000],  loss: 1.075919, mae: 12.759009, mean_q: -18.280789, mean_eps: 0.050000\n",
            " 170075/200000: episode: 1039, duration: 0.612s, episode steps:  82, steps per second: 134, episode reward: -81.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.037 [0.000, 2.000],  loss: 1.105000, mae: 12.745181, mean_q: -18.241239, mean_eps: 0.050000\n",
            " 170187/200000: episode: 1040, duration: 1.012s, episode steps: 112, steps per second: 111, episode reward: -111.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.134 [0.000, 2.000],  loss: 0.932870, mae: 12.823028, mean_q: -18.409516, mean_eps: 0.050000\n",
            " 170287/200000: episode: 1041, duration: 1.059s, episode steps: 100, steps per second:  94, episode reward: -99.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.200 [0.000, 2.000],  loss: 0.954607, mae: 12.891752, mean_q: -18.523558, mean_eps: 0.050000\n",
            " 170413/200000: episode: 1042, duration: 1.215s, episode steps: 126, steps per second: 104, episode reward: -125.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.968 [0.000, 2.000],  loss: 1.002850, mae: 12.778729, mean_q: -18.318689, mean_eps: 0.050000\n",
            " 170530/200000: episode: 1043, duration: 1.022s, episode steps: 117, steps per second: 114, episode reward: -116.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.222 [0.000, 2.000],  loss: 1.129274, mae: 12.739290, mean_q: -18.228012, mean_eps: 0.050000\n",
            " 170640/200000: episode: 1044, duration: 0.774s, episode steps: 110, steps per second: 142, episode reward: -109.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.255 [0.000, 2.000],  loss: 1.071808, mae: 12.813515, mean_q: -18.342390, mean_eps: 0.050000\n",
            " 170744/200000: episode: 1045, duration: 0.742s, episode steps: 104, steps per second: 140, episode reward: -103.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.990 [0.000, 2.000],  loss: 0.942056, mae: 12.761794, mean_q: -18.318582, mean_eps: 0.050000\n",
            " 170849/200000: episode: 1046, duration: 0.757s, episode steps: 105, steps per second: 139, episode reward: -104.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.067 [0.000, 2.000],  loss: 0.900581, mae: 12.905679, mean_q: -18.550347, mean_eps: 0.050000\n",
            " 170937/200000: episode: 1047, duration: 0.590s, episode steps:  88, steps per second: 149, episode reward: -87.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.193 [0.000, 2.000],  loss: 0.880314, mae: 12.873785, mean_q: -18.519170, mean_eps: 0.050000\n",
            " 171064/200000: episode: 1048, duration: 0.965s, episode steps: 127, steps per second: 132, episode reward: -126.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.276 [0.000, 2.000],  loss: 0.998680, mae: 12.775526, mean_q: -18.337400, mean_eps: 0.050000\n",
            " 171177/200000: episode: 1049, duration: 0.844s, episode steps: 113, steps per second: 134, episode reward: -112.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.257 [0.000, 2.000],  loss: 0.900437, mae: 12.792023, mean_q: -18.353047, mean_eps: 0.050000\n",
            " 171273/200000: episode: 1050, duration: 0.709s, episode steps:  96, steps per second: 135, episode reward: -95.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.281 [0.000, 2.000],  loss: 0.991966, mae: 12.859683, mean_q: -18.434899, mean_eps: 0.050000\n",
            " 171382/200000: episode: 1051, duration: 0.743s, episode steps: 109, steps per second: 147, episode reward: -108.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.119 [0.000, 2.000],  loss: 1.022924, mae: 12.707460, mean_q: -18.234767, mean_eps: 0.050000\n",
            " 171490/200000: episode: 1052, duration: 0.770s, episode steps: 108, steps per second: 140, episode reward: -107.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.176 [0.000, 2.000],  loss: 1.039746, mae: 12.732385, mean_q: -18.260336, mean_eps: 0.050000\n",
            " 171637/200000: episode: 1053, duration: 1.017s, episode steps: 147, steps per second: 145, episode reward: -146.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.136 [0.000, 2.000],  loss: 0.934359, mae: 12.731309, mean_q: -18.285050, mean_eps: 0.050000\n",
            " 171756/200000: episode: 1054, duration: 0.809s, episode steps: 119, steps per second: 147, episode reward: -118.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.328 [0.000, 2.000],  loss: 0.998363, mae: 12.711767, mean_q: -18.236475, mean_eps: 0.050000\n",
            " 171884/200000: episode: 1055, duration: 0.941s, episode steps: 128, steps per second: 136, episode reward: -127.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.227 [0.000, 2.000],  loss: 0.922300, mae: 12.725937, mean_q: -18.269578, mean_eps: 0.050000\n",
            " 172030/200000: episode: 1056, duration: 1.562s, episode steps: 146, steps per second:  93, episode reward: -145.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.295 [0.000, 2.000],  loss: 1.013700, mae: 12.781267, mean_q: -18.333447, mean_eps: 0.050000\n",
            " 172200/200000: episode: 1057, duration: 1.794s, episode steps: 170, steps per second:  95, episode reward: -169.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.212 [0.000, 2.000],  loss: 0.987610, mae: 12.848884, mean_q: -18.423558, mean_eps: 0.050000\n",
            " 172296/200000: episode: 1058, duration: 0.714s, episode steps:  96, steps per second: 134, episode reward: -95.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.250 [0.000, 2.000],  loss: 1.020133, mae: 12.744827, mean_q: -18.260195, mean_eps: 0.050000\n",
            " 172417/200000: episode: 1059, duration: 0.857s, episode steps: 121, steps per second: 141, episode reward: -120.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.033 [0.000, 2.000],  loss: 0.896436, mae: 12.712449, mean_q: -18.254793, mean_eps: 0.050000\n",
            " 172539/200000: episode: 1060, duration: 0.853s, episode steps: 122, steps per second: 143, episode reward: -121.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.213 [0.000, 2.000],  loss: 1.048352, mae: 12.792877, mean_q: -18.342596, mean_eps: 0.050000\n",
            " 172623/200000: episode: 1061, duration: 0.607s, episode steps:  84, steps per second: 138, episode reward: -83.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.095 [0.000, 2.000],  loss: 0.994141, mae: 12.653257, mean_q: -18.134173, mean_eps: 0.050000\n",
            " 172699/200000: episode: 1062, duration: 0.518s, episode steps:  76, steps per second: 147, episode reward: -75.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.105 [0.000, 2.000],  loss: 1.070783, mae: 12.734461, mean_q: -18.244512, mean_eps: 0.050000\n",
            " 172885/200000: episode: 1063, duration: 1.315s, episode steps: 186, steps per second: 141, episode reward: -185.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.005 [0.000, 2.000],  loss: 0.992892, mae: 12.776688, mean_q: -18.325273, mean_eps: 0.050000\n",
            " 173045/200000: episode: 1064, duration: 1.076s, episode steps: 160, steps per second: 149, episode reward: -159.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.131 [0.000, 2.000],  loss: 0.984240, mae: 12.748636, mean_q: -18.276609, mean_eps: 0.050000\n",
            " 173130/200000: episode: 1065, duration: 0.587s, episode steps:  85, steps per second: 145, episode reward: -84.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.141 [0.000, 2.000],  loss: 1.007864, mae: 12.858996, mean_q: -18.459659, mean_eps: 0.050000\n",
            " 173229/200000: episode: 1066, duration: 0.708s, episode steps:  99, steps per second: 140, episode reward: -98.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.263 [0.000, 2.000],  loss: 1.021262, mae: 12.826680, mean_q: -18.395246, mean_eps: 0.050000\n",
            " 173329/200000: episode: 1067, duration: 0.716s, episode steps: 100, steps per second: 140, episode reward: -99.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.100 [0.000, 2.000],  loss: 0.984977, mae: 12.776889, mean_q: -18.329319, mean_eps: 0.050000\n",
            " 173438/200000: episode: 1068, duration: 0.766s, episode steps: 109, steps per second: 142, episode reward: -108.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.982 [0.000, 2.000],  loss: 0.903129, mae: 12.876966, mean_q: -18.507688, mean_eps: 0.050000\n",
            " 173576/200000: episode: 1069, duration: 0.965s, episode steps: 138, steps per second: 143, episode reward: -137.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.130 [0.000, 2.000],  loss: 1.026523, mae: 12.830728, mean_q: -18.392814, mean_eps: 0.050000\n",
            " 173676/200000: episode: 1070, duration: 0.885s, episode steps: 100, steps per second: 113, episode reward: -99.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.030 [0.000, 2.000],  loss: 1.097271, mae: 12.819168, mean_q: -18.371728, mean_eps: 0.050000\n",
            " 173789/200000: episode: 1071, duration: 1.166s, episode steps: 113, steps per second:  97, episode reward: -112.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.973 [0.000, 2.000],  loss: 0.997062, mae: 12.887180, mean_q: -18.504353, mean_eps: 0.050000\n",
            " 173909/200000: episode: 1072, duration: 1.162s, episode steps: 120, steps per second: 103, episode reward: -119.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.125 [0.000, 2.000],  loss: 1.044445, mae: 12.796329, mean_q: -18.344627, mean_eps: 0.050000\n",
            " 174023/200000: episode: 1073, duration: 1.011s, episode steps: 114, steps per second: 113, episode reward: -113.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.175 [0.000, 2.000],  loss: 1.068549, mae: 12.903310, mean_q: -18.501090, mean_eps: 0.050000\n",
            " 174191/200000: episode: 1074, duration: 1.198s, episode steps: 168, steps per second: 140, episode reward: -167.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.774 [0.000, 2.000],  loss: 0.912402, mae: 12.968657, mean_q: -18.633152, mean_eps: 0.050000\n",
            " 174300/200000: episode: 1075, duration: 0.757s, episode steps: 109, steps per second: 144, episode reward: -108.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 1.167062, mae: 12.947158, mean_q: -18.534314, mean_eps: 0.050000\n",
            " 174414/200000: episode: 1076, duration: 0.812s, episode steps: 114, steps per second: 140, episode reward: -113.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.895 [0.000, 2.000],  loss: 0.991148, mae: 12.970473, mean_q: -18.633833, mean_eps: 0.050000\n",
            " 174521/200000: episode: 1077, duration: 0.743s, episode steps: 107, steps per second: 144, episode reward: -106.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.047 [0.000, 2.000],  loss: 1.101890, mae: 13.000855, mean_q: -18.646597, mean_eps: 0.050000\n",
            " 174627/200000: episode: 1078, duration: 0.720s, episode steps: 106, steps per second: 147, episode reward: -105.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.075 [0.000, 2.000],  loss: 1.168969, mae: 13.078770, mean_q: -18.754440, mean_eps: 0.050000\n",
            " 174789/200000: episode: 1079, duration: 1.107s, episode steps: 162, steps per second: 146, episode reward: -161.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.142 [0.000, 2.000],  loss: 0.991961, mae: 12.947416, mean_q: -18.591144, mean_eps: 0.050000\n",
            " 174911/200000: episode: 1080, duration: 0.803s, episode steps: 122, steps per second: 152, episode reward: -121.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.107 [0.000, 2.000],  loss: 1.071481, mae: 13.027608, mean_q: -18.692291, mean_eps: 0.050000\n",
            " 174995/200000: episode: 1081, duration: 0.591s, episode steps:  84, steps per second: 142, episode reward: -83.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.202 [0.000, 2.000],  loss: 0.872522, mae: 12.992028, mean_q: -18.676605, mean_eps: 0.050000\n",
            " 175079/200000: episode: 1082, duration: 0.561s, episode steps:  84, steps per second: 150, episode reward: -83.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.214 [0.000, 2.000],  loss: 1.046846, mae: 12.869322, mean_q: -18.462771, mean_eps: 0.050000\n",
            " 175243/200000: episode: 1083, duration: 1.129s, episode steps: 164, steps per second: 145, episode reward: -163.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.854 [0.000, 2.000],  loss: 1.090661, mae: 12.898217, mean_q: -18.493624, mean_eps: 0.050000\n",
            " 175363/200000: episode: 1084, duration: 0.834s, episode steps: 120, steps per second: 144, episode reward: -119.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.858 [0.000, 2.000],  loss: 0.868048, mae: 12.939539, mean_q: -18.614733, mean_eps: 0.050000\n",
            " 175475/200000: episode: 1085, duration: 1.018s, episode steps: 112, steps per second: 110, episode reward: -111.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.804 [0.000, 2.000],  loss: 0.938690, mae: 13.011105, mean_q: -18.705322, mean_eps: 0.050000\n",
            " 175621/200000: episode: 1086, duration: 1.510s, episode steps: 146, steps per second:  97, episode reward: -145.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.151 [0.000, 2.000],  loss: 1.030454, mae: 12.997384, mean_q: -18.646715, mean_eps: 0.050000\n",
            " 175722/200000: episode: 1087, duration: 1.034s, episode steps: 101, steps per second:  98, episode reward: -100.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.990 [0.000, 2.000],  loss: 0.978025, mae: 12.915513, mean_q: -18.519323, mean_eps: 0.050000\n",
            " 175864/200000: episode: 1088, duration: 1.083s, episode steps: 142, steps per second: 131, episode reward: -141.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.190 [0.000, 2.000],  loss: 1.071650, mae: 13.007834, mean_q: -18.647072, mean_eps: 0.050000\n",
            " 175975/200000: episode: 1089, duration: 0.759s, episode steps: 111, steps per second: 146, episode reward: -110.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.153 [0.000, 2.000],  loss: 0.927651, mae: 12.958967, mean_q: -18.618276, mean_eps: 0.050000\n",
            " 176070/200000: episode: 1090, duration: 0.705s, episode steps:  95, steps per second: 135, episode reward: -94.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.053 [0.000, 2.000],  loss: 0.966435, mae: 13.034333, mean_q: -18.722629, mean_eps: 0.050000\n",
            " 176224/200000: episode: 1091, duration: 1.028s, episode steps: 154, steps per second: 150, episode reward: -153.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.890 [0.000, 2.000],  loss: 1.021441, mae: 13.071527, mean_q: -18.765528, mean_eps: 0.050000\n",
            " 176322/200000: episode: 1092, duration: 0.659s, episode steps:  98, steps per second: 149, episode reward: -97.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.827 [0.000, 2.000],  loss: 0.995516, mae: 12.941713, mean_q: -18.562568, mean_eps: 0.050000\n",
            " 176449/200000: episode: 1093, duration: 0.863s, episode steps: 127, steps per second: 147, episode reward: -126.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.079 [0.000, 2.000],  loss: 1.043873, mae: 13.065383, mean_q: -18.739043, mean_eps: 0.050000\n",
            " 176558/200000: episode: 1094, duration: 0.737s, episode steps: 109, steps per second: 148, episode reward: -108.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.936 [0.000, 2.000],  loss: 1.059571, mae: 13.051066, mean_q: -18.719521, mean_eps: 0.050000\n",
            " 176668/200000: episode: 1095, duration: 0.767s, episode steps: 110, steps per second: 143, episode reward: -109.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.927 [0.000, 2.000],  loss: 0.988078, mae: 12.965944, mean_q: -18.605742, mean_eps: 0.050000\n",
            " 176812/200000: episode: 1096, duration: 1.047s, episode steps: 144, steps per second: 138, episode reward: -143.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.910 [0.000, 2.000],  loss: 0.918347, mae: 12.991052, mean_q: -18.665524, mean_eps: 0.050000\n",
            " 176928/200000: episode: 1097, duration: 0.878s, episode steps: 116, steps per second: 132, episode reward: -115.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.198 [0.000, 2.000],  loss: 0.934769, mae: 12.896002, mean_q: -18.514103, mean_eps: 0.050000\n",
            " 177023/200000: episode: 1098, duration: 0.635s, episode steps:  95, steps per second: 150, episode reward: -94.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.989 [0.000, 2.000],  loss: 0.949926, mae: 12.962836, mean_q: -18.608657, mean_eps: 0.050000\n",
            " 177115/200000: episode: 1099, duration: 0.647s, episode steps:  92, steps per second: 142, episode reward: -91.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.022 [0.000, 2.000],  loss: 1.062233, mae: 13.070860, mean_q: -18.742241, mean_eps: 0.050000\n",
            " 177235/200000: episode: 1100, duration: 1.006s, episode steps: 120, steps per second: 119, episode reward: -119.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.742 [0.000, 2.000],  loss: 1.136065, mae: 12.950657, mean_q: -18.528403, mean_eps: 0.050000\n",
            " 177359/200000: episode: 1101, duration: 1.227s, episode steps: 124, steps per second: 101, episode reward: -123.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.839 [0.000, 2.000],  loss: 0.969440, mae: 13.028336, mean_q: -18.701593, mean_eps: 0.050000\n",
            " 177475/200000: episode: 1102, duration: 1.118s, episode steps: 116, steps per second: 104, episode reward: -115.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.871 [0.000, 2.000],  loss: 0.965495, mae: 12.939600, mean_q: -18.573162, mean_eps: 0.050000\n",
            " 177556/200000: episode: 1103, duration: 0.754s, episode steps:  81, steps per second: 107, episode reward: -80.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.716 [0.000, 2.000],  loss: 0.955849, mae: 13.080456, mean_q: -18.774349, mean_eps: 0.050000\n",
            " 177662/200000: episode: 1104, duration: 0.725s, episode steps: 106, steps per second: 146, episode reward: -105.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.764 [0.000, 2.000],  loss: 1.084781, mae: 13.136979, mean_q: -18.809246, mean_eps: 0.050000\n",
            " 177779/200000: episode: 1105, duration: 0.769s, episode steps: 117, steps per second: 152, episode reward: -116.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.932 [0.000, 2.000],  loss: 1.063601, mae: 13.153395, mean_q: -18.833791, mean_eps: 0.050000\n",
            " 177899/200000: episode: 1106, duration: 0.835s, episode steps: 120, steps per second: 144, episode reward: -119.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.892 [0.000, 2.000],  loss: 0.900552, mae: 13.074074, mean_q: -18.754890, mean_eps: 0.050000\n",
            " 178024/200000: episode: 1107, duration: 0.920s, episode steps: 125, steps per second: 136, episode reward: -124.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.664 [0.000, 2.000],  loss: 0.982252, mae: 13.038444, mean_q: -18.716151, mean_eps: 0.050000\n",
            " 178128/200000: episode: 1108, duration: 0.762s, episode steps: 104, steps per second: 137, episode reward: -103.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.981 [0.000, 2.000],  loss: 0.927571, mae: 13.100446, mean_q: -18.808327, mean_eps: 0.050000\n",
            " 178284/200000: episode: 1109, duration: 1.060s, episode steps: 156, steps per second: 147, episode reward: -155.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.929 [0.000, 2.000],  loss: 0.976893, mae: 13.069721, mean_q: -18.744688, mean_eps: 0.050000\n",
            " 178409/200000: episode: 1110, duration: 0.873s, episode steps: 125, steps per second: 143, episode reward: -124.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.040 [0.000, 2.000],  loss: 1.054637, mae: 13.047997, mean_q: -18.697418, mean_eps: 0.050000\n",
            " 178506/200000: episode: 1111, duration: 0.719s, episode steps:  97, steps per second: 135, episode reward: -96.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.948 [0.000, 2.000],  loss: 1.044085, mae: 12.985505, mean_q: -18.615654, mean_eps: 0.050000\n",
            " 178654/200000: episode: 1112, duration: 1.093s, episode steps: 148, steps per second: 135, episode reward: -147.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.142 [0.000, 2.000],  loss: 1.109507, mae: 12.878535, mean_q: -18.437625, mean_eps: 0.050000\n",
            " 178823/200000: episode: 1113, duration: 1.190s, episode steps: 169, steps per second: 142, episode reward: -168.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.817 [0.000, 2.000],  loss: 1.033755, mae: 12.965142, mean_q: -18.576414, mean_eps: 0.050000\n",
            " 178900/200000: episode: 1114, duration: 0.568s, episode steps:  77, steps per second: 136, episode reward: -76.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.935 [0.000, 2.000],  loss: 1.154962, mae: 12.916107, mean_q: -18.445265, mean_eps: 0.050000\n",
            " 179033/200000: episode: 1115, duration: 1.256s, episode steps: 133, steps per second: 106, episode reward: -132.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.030 [0.000, 2.000],  loss: 0.925919, mae: 12.977660, mean_q: -18.620872, mean_eps: 0.050000\n",
            " 179119/200000: episode: 1116, duration: 0.874s, episode steps:  86, steps per second:  98, episode reward: -85.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 1.036796, mae: 13.078770, mean_q: -18.761855, mean_eps: 0.050000\n",
            " 179233/200000: episode: 1117, duration: 1.170s, episode steps: 114, steps per second:  97, episode reward: -113.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.868 [0.000, 2.000],  loss: 1.110138, mae: 13.004710, mean_q: -18.632078, mean_eps: 0.050000\n",
            " 179395/200000: episode: 1118, duration: 1.334s, episode steps: 162, steps per second: 121, episode reward: -161.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.093 [0.000, 2.000],  loss: 1.054724, mae: 12.982308, mean_q: -18.613938, mean_eps: 0.050000\n",
            " 179508/200000: episode: 1119, duration: 0.818s, episode steps: 113, steps per second: 138, episode reward: -112.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.814 [0.000, 2.000],  loss: 0.988396, mae: 12.993560, mean_q: -18.608007, mean_eps: 0.050000\n",
            " 179620/200000: episode: 1120, duration: 0.826s, episode steps: 112, steps per second: 136, episode reward: -111.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.875 [0.000, 2.000],  loss: 0.975194, mae: 13.084952, mean_q: -18.782047, mean_eps: 0.050000\n",
            " 179736/200000: episode: 1121, duration: 0.877s, episode steps: 116, steps per second: 132, episode reward: -115.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.121 [0.000, 2.000],  loss: 1.025404, mae: 13.194864, mean_q: -18.926963, mean_eps: 0.050000\n",
            " 179822/200000: episode: 1122, duration: 0.607s, episode steps:  86, steps per second: 142, episode reward: -85.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.965 [0.000, 2.000],  loss: 0.971096, mae: 13.157727, mean_q: -18.893097, mean_eps: 0.050000\n",
            " 179893/200000: episode: 1123, duration: 0.487s, episode steps:  71, steps per second: 146, episode reward: -70.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.915 [0.000, 2.000],  loss: 1.170327, mae: 13.113043, mean_q: -18.787363, mean_eps: 0.050000\n",
            " 179979/200000: episode: 1124, duration: 0.602s, episode steps:  86, steps per second: 143, episode reward: -85.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.709 [0.000, 2.000],  loss: 0.960025, mae: 13.042102, mean_q: -18.728573, mean_eps: 0.050000\n",
            " 180091/200000: episode: 1125, duration: 0.818s, episode steps: 112, steps per second: 137, episode reward: -111.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.732 [0.000, 2.000],  loss: 0.983172, mae: 13.015877, mean_q: -18.672314, mean_eps: 0.050000\n",
            " 180181/200000: episode: 1126, duration: 0.628s, episode steps:  90, steps per second: 143, episode reward: -89.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.833 [0.000, 2.000],  loss: 0.993628, mae: 13.078609, mean_q: -18.757044, mean_eps: 0.050000\n",
            " 180331/200000: episode: 1127, duration: 1.074s, episode steps: 150, steps per second: 140, episode reward: -149.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.840 [0.000, 2.000],  loss: 0.943862, mae: 13.070544, mean_q: -18.769197, mean_eps: 0.050000\n",
            " 180484/200000: episode: 1128, duration: 1.122s, episode steps: 153, steps per second: 136, episode reward: -152.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.784 [0.000, 2.000],  loss: 1.148163, mae: 13.100760, mean_q: -18.757901, mean_eps: 0.050000\n",
            " 180640/200000: episode: 1129, duration: 1.142s, episode steps: 156, steps per second: 137, episode reward: -155.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.058 [0.000, 2.000],  loss: 1.032519, mae: 13.146491, mean_q: -18.873852, mean_eps: 0.050000\n",
            " 180764/200000: episode: 1130, duration: 1.350s, episode steps: 124, steps per second:  92, episode reward: -123.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.968 [0.000, 2.000],  loss: 0.980937, mae: 13.035535, mean_q: -18.730785, mean_eps: 0.050000\n",
            " 180853/200000: episode: 1131, duration: 0.947s, episode steps:  89, steps per second:  94, episode reward: -88.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.101 [0.000, 2.000],  loss: 1.021337, mae: 13.001157, mean_q: -18.650726, mean_eps: 0.050000\n",
            " 180947/200000: episode: 1132, duration: 0.944s, episode steps:  94, steps per second: 100, episode reward: -93.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.872 [0.000, 2.000],  loss: 1.159203, mae: 13.113611, mean_q: -18.792447, mean_eps: 0.050000\n",
            " 181084/200000: episode: 1133, duration: 1.111s, episode steps: 137, steps per second: 123, episode reward: -136.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.956 [0.000, 2.000],  loss: 1.035272, mae: 13.110414, mean_q: -18.806446, mean_eps: 0.050000\n",
            " 181185/200000: episode: 1134, duration: 0.716s, episode steps: 101, steps per second: 141, episode reward: -100.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.248 [0.000, 2.000],  loss: 1.000752, mae: 13.121429, mean_q: -18.817134, mean_eps: 0.050000\n",
            " 181284/200000: episode: 1135, duration: 0.682s, episode steps:  99, steps per second: 145, episode reward: -98.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.242 [0.000, 2.000],  loss: 0.938508, mae: 13.088043, mean_q: -18.776318, mean_eps: 0.050000\n",
            " 181424/200000: episode: 1136, duration: 0.972s, episode steps: 140, steps per second: 144, episode reward: -139.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.871 [0.000, 2.000],  loss: 1.109151, mae: 13.145809, mean_q: -18.817336, mean_eps: 0.050000\n",
            " 181539/200000: episode: 1137, duration: 0.785s, episode steps: 115, steps per second: 147, episode reward: -114.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.296 [0.000, 2.000],  loss: 0.905784, mae: 13.122591, mean_q: -18.849535, mean_eps: 0.050000\n",
            " 181685/200000: episode: 1138, duration: 1.004s, episode steps: 146, steps per second: 145, episode reward: -145.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.068 [0.000, 2.000],  loss: 0.928451, mae: 13.167373, mean_q: -18.908685, mean_eps: 0.050000\n",
            " 181825/200000: episode: 1139, duration: 0.979s, episode steps: 140, steps per second: 143, episode reward: -139.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.093 [0.000, 2.000],  loss: 1.029187, mae: 13.041899, mean_q: -18.694900, mean_eps: 0.050000\n",
            " 181949/200000: episode: 1140, duration: 0.872s, episode steps: 124, steps per second: 142, episode reward: -123.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 1.090379, mae: 13.102854, mean_q: -18.772168, mean_eps: 0.050000\n",
            " 182060/200000: episode: 1141, duration: 0.804s, episode steps: 111, steps per second: 138, episode reward: -110.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.036 [0.000, 2.000],  loss: 1.095313, mae: 12.891594, mean_q: -18.468677, mean_eps: 0.050000\n",
            " 182187/200000: episode: 1142, duration: 0.884s, episode steps: 127, steps per second: 144, episode reward: -126.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 1.077803, mae: 13.030288, mean_q: -18.647497, mean_eps: 0.050000\n",
            " 182340/200000: episode: 1143, duration: 1.047s, episode steps: 153, steps per second: 146, episode reward: -152.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.190 [0.000, 2.000],  loss: 1.078049, mae: 12.966285, mean_q: -18.575456, mean_eps: 0.050000\n",
            " 182437/200000: episode: 1144, duration: 0.771s, episode steps:  97, steps per second: 126, episode reward: -96.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.103 [0.000, 2.000],  loss: 0.989522, mae: 12.846142, mean_q: -18.410583, mean_eps: 0.050000\n",
            " 182537/200000: episode: 1145, duration: 0.946s, episode steps: 100, steps per second: 106, episode reward: -99.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.230 [0.000, 2.000],  loss: 1.021329, mae: 12.921320, mean_q: -18.505777, mean_eps: 0.050000\n",
            " 182652/200000: episode: 1146, duration: 1.127s, episode steps: 115, steps per second: 102, episode reward: -114.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.104 [0.000, 2.000],  loss: 0.866323, mae: 12.956191, mean_q: -18.591513, mean_eps: 0.050000\n",
            " 182779/200000: episode: 1147, duration: 1.263s, episode steps: 127, steps per second: 101, episode reward: -126.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.118 [0.000, 2.000],  loss: 1.016143, mae: 12.949086, mean_q: -18.560134, mean_eps: 0.050000\n",
            " 182880/200000: episode: 1148, duration: 0.724s, episode steps: 101, steps per second: 140, episode reward: -100.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.248 [0.000, 2.000],  loss: 1.132214, mae: 12.953019, mean_q: -18.538108, mean_eps: 0.050000\n",
            " 183037/200000: episode: 1149, duration: 1.053s, episode steps: 157, steps per second: 149, episode reward: -156.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.045 [0.000, 2.000],  loss: 1.016848, mae: 12.874964, mean_q: -18.458378, mean_eps: 0.050000\n",
            " 183164/200000: episode: 1150, duration: 0.898s, episode steps: 127, steps per second: 141, episode reward: -126.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.228 [0.000, 2.000],  loss: 0.940030, mae: 12.732589, mean_q: -18.242180, mean_eps: 0.050000\n",
            " 183273/200000: episode: 1151, duration: 0.784s, episode steps: 109, steps per second: 139, episode reward: -108.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.257 [0.000, 2.000],  loss: 0.966960, mae: 12.851692, mean_q: -18.408635, mean_eps: 0.050000\n",
            " 183392/200000: episode: 1152, duration: 0.796s, episode steps: 119, steps per second: 150, episode reward: -118.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.143 [0.000, 2.000],  loss: 1.056630, mae: 12.987988, mean_q: -18.574660, mean_eps: 0.050000\n",
            " 183508/200000: episode: 1153, duration: 0.829s, episode steps: 116, steps per second: 140, episode reward: -115.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.853 [0.000, 2.000],  loss: 1.110668, mae: 12.854415, mean_q: -18.373434, mean_eps: 0.050000\n",
            " 183638/200000: episode: 1154, duration: 0.966s, episode steps: 130, steps per second: 135, episode reward: -129.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.185 [0.000, 2.000],  loss: 1.003771, mae: 12.816695, mean_q: -18.386211, mean_eps: 0.050000\n",
            " 183836/200000: episode: 1155, duration: 1.434s, episode steps: 198, steps per second: 138, episode reward: -197.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.303 [0.000, 2.000],  loss: 0.925013, mae: 12.819924, mean_q: -18.397149, mean_eps: 0.050000\n",
            " 183929/200000: episode: 1156, duration: 0.667s, episode steps:  93, steps per second: 139, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.312 [0.000, 2.000],  loss: 0.902727, mae: 12.864422, mean_q: -18.460633, mean_eps: 0.050000\n",
            " 184027/200000: episode: 1157, duration: 0.655s, episode steps:  98, steps per second: 150, episode reward: -97.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.918 [0.000, 2.000],  loss: 1.007588, mae: 12.785670, mean_q: -18.308532, mean_eps: 0.050000\n",
            " 184203/200000: episode: 1158, duration: 1.304s, episode steps: 176, steps per second: 135, episode reward: -175.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.222 [0.000, 2.000],  loss: 0.919179, mae: 12.763512, mean_q: -18.303931, mean_eps: 0.050000\n",
            " 184360/200000: episode: 1159, duration: 1.578s, episode steps: 157, steps per second: 100, episode reward: -156.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.032 [0.000, 2.000],  loss: 0.913030, mae: 12.731296, mean_q: -18.278088, mean_eps: 0.050000\n",
            " 184491/200000: episode: 1160, duration: 1.321s, episode steps: 131, steps per second:  99, episode reward: -130.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.947 [0.000, 2.000],  loss: 1.034968, mae: 12.641685, mean_q: -18.107214, mean_eps: 0.050000\n",
            " 184616/200000: episode: 1161, duration: 1.103s, episode steps: 125, steps per second: 113, episode reward: -124.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.304 [0.000, 2.000],  loss: 1.032777, mae: 12.851215, mean_q: -18.418380, mean_eps: 0.050000\n",
            " 184740/200000: episode: 1162, duration: 0.833s, episode steps: 124, steps per second: 149, episode reward: -123.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.089 [0.000, 2.000],  loss: 1.060599, mae: 12.872886, mean_q: -18.460448, mean_eps: 0.050000\n",
            " 184839/200000: episode: 1163, duration: 0.709s, episode steps:  99, steps per second: 140, episode reward: -98.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.960 [0.000, 2.000],  loss: 0.973904, mae: 12.886399, mean_q: -18.508667, mean_eps: 0.050000\n",
            " 184998/200000: episode: 1164, duration: 1.101s, episode steps: 159, steps per second: 144, episode reward: -158.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.987 [0.000, 2.000],  loss: 0.979646, mae: 12.744661, mean_q: -18.287547, mean_eps: 0.050000\n",
            " 185108/200000: episode: 1165, duration: 0.777s, episode steps: 110, steps per second: 142, episode reward: -109.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.282 [0.000, 2.000],  loss: 1.020510, mae: 12.707809, mean_q: -18.210340, mean_eps: 0.050000\n",
            " 185252/200000: episode: 1166, duration: 0.975s, episode steps: 144, steps per second: 148, episode reward: -143.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.007 [0.000, 2.000],  loss: 0.987365, mae: 12.695104, mean_q: -18.216441, mean_eps: 0.050000\n",
            " 185361/200000: episode: 1167, duration: 0.784s, episode steps: 109, steps per second: 139, episode reward: -108.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.248 [0.000, 2.000],  loss: 0.875039, mae: 12.791472, mean_q: -18.389795, mean_eps: 0.050000\n",
            " 185486/200000: episode: 1168, duration: 0.863s, episode steps: 125, steps per second: 145, episode reward: -124.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.816 [0.000, 2.000],  loss: 0.943524, mae: 12.712712, mean_q: -18.256320, mean_eps: 0.050000\n",
            " 185598/200000: episode: 1169, duration: 0.746s, episode steps: 112, steps per second: 150, episode reward: -111.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.875 [0.000, 2.000],  loss: 0.945118, mae: 12.633679, mean_q: -18.109038, mean_eps: 0.050000\n",
            " 185720/200000: episode: 1170, duration: 0.818s, episode steps: 122, steps per second: 149, episode reward: -121.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.246 [0.000, 2.000],  loss: 1.049525, mae: 12.645487, mean_q: -18.128089, mean_eps: 0.050000\n",
            " 185818/200000: episode: 1171, duration: 0.711s, episode steps:  98, steps per second: 138, episode reward: -97.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.918 [0.000, 2.000],  loss: 0.947752, mae: 12.752655, mean_q: -18.308151, mean_eps: 0.050000\n",
            " 185930/200000: episode: 1172, duration: 0.757s, episode steps: 112, steps per second: 148, episode reward: -111.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.205 [0.000, 2.000],  loss: 0.998446, mae: 12.818984, mean_q: -18.402172, mean_eps: 0.050000\n",
            " 186057/200000: episode: 1173, duration: 1.254s, episode steps: 127, steps per second: 101, episode reward: -126.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.945 [0.000, 2.000],  loss: 0.995765, mae: 12.688881, mean_q: -18.176100, mean_eps: 0.050000\n",
            " 186183/200000: episode: 1174, duration: 1.183s, episode steps: 126, steps per second: 106, episode reward: -125.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.993112, mae: 12.786981, mean_q: -18.320468, mean_eps: 0.050000\n",
            " 186296/200000: episode: 1175, duration: 1.152s, episode steps: 113, steps per second:  98, episode reward: -112.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.088 [0.000, 2.000],  loss: 1.118077, mae: 12.708191, mean_q: -18.204284, mean_eps: 0.050000\n",
            " 186442/200000: episode: 1176, duration: 1.031s, episode steps: 146, steps per second: 142, episode reward: -145.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.062 [0.000, 2.000],  loss: 1.077139, mae: 12.672404, mean_q: -18.160242, mean_eps: 0.050000\n",
            " 186587/200000: episode: 1177, duration: 0.974s, episode steps: 145, steps per second: 149, episode reward: -144.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.848 [0.000, 2.000],  loss: 1.067439, mae: 12.696202, mean_q: -18.185366, mean_eps: 0.050000\n",
            " 186701/200000: episode: 1178, duration: 0.769s, episode steps: 114, steps per second: 148, episode reward: -113.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.895 [0.000, 2.000],  loss: 0.987496, mae: 12.566217, mean_q: -18.017484, mean_eps: 0.050000\n",
            " 186815/200000: episode: 1179, duration: 0.736s, episode steps: 114, steps per second: 155, episode reward: -113.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.175 [0.000, 2.000],  loss: 0.974228, mae: 12.680083, mean_q: -18.181911, mean_eps: 0.050000\n",
            " 186936/200000: episode: 1180, duration: 0.837s, episode steps: 121, steps per second: 145, episode reward: -120.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 1.006085, mae: 12.681140, mean_q: -18.166512, mean_eps: 0.050000\n",
            " 187082/200000: episode: 1181, duration: 0.999s, episode steps: 146, steps per second: 146, episode reward: -145.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.199 [0.000, 2.000],  loss: 1.096447, mae: 12.720312, mean_q: -18.196136, mean_eps: 0.050000\n",
            " 187227/200000: episode: 1182, duration: 0.988s, episode steps: 145, steps per second: 147, episode reward: -144.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.069 [0.000, 2.000],  loss: 1.049250, mae: 12.754472, mean_q: -18.270546, mean_eps: 0.050000\n",
            " 187362/200000: episode: 1183, duration: 0.945s, episode steps: 135, steps per second: 143, episode reward: -134.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.815 [0.000, 2.000],  loss: 0.936716, mae: 12.763516, mean_q: -18.318472, mean_eps: 0.050000\n",
            " 187483/200000: episode: 1184, duration: 0.801s, episode steps: 121, steps per second: 151, episode reward: -120.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.669 [0.000, 2.000],  loss: 1.027242, mae: 12.845777, mean_q: -18.419032, mean_eps: 0.050000\n",
            " 187578/200000: episode: 1185, duration: 0.670s, episode steps:  95, steps per second: 142, episode reward: -94.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.663 [0.000, 2.000],  loss: 1.001794, mae: 12.707842, mean_q: -18.222296, mean_eps: 0.050000\n",
            " 187713/200000: episode: 1186, duration: 0.892s, episode steps: 135, steps per second: 151, episode reward: -134.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.867 [0.000, 2.000],  loss: 1.047444, mae: 12.613649, mean_q: -18.049259, mean_eps: 0.050000\n",
            " 187815/200000: episode: 1187, duration: 0.841s, episode steps: 102, steps per second: 121, episode reward: -101.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.118 [0.000, 2.000],  loss: 1.021916, mae: 12.652155, mean_q: -18.135065, mean_eps: 0.050000\n",
            " 187911/200000: episode: 1188, duration: 1.031s, episode steps:  96, steps per second:  93, episode reward: -95.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.146 [0.000, 2.000],  loss: 1.195451, mae: 12.672788, mean_q: -18.099372, mean_eps: 0.050000\n",
            " 188047/200000: episode: 1189, duration: 1.314s, episode steps: 136, steps per second: 104, episode reward: -135.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.875 [0.000, 2.000],  loss: 1.040384, mae: 12.844500, mean_q: -18.415407, mean_eps: 0.050000\n",
            " 188264/200000: episode: 1190, duration: 1.735s, episode steps: 217, steps per second: 125, episode reward: -216.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.917 [0.000, 2.000],  loss: 0.976016, mae: 12.846487, mean_q: -18.432762, mean_eps: 0.050000\n",
            " 188394/200000: episode: 1191, duration: 1.044s, episode steps: 130, steps per second: 125, episode reward: -129.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.808 [0.000, 2.000],  loss: 1.105873, mae: 12.713006, mean_q: -18.205510, mean_eps: 0.050000\n",
            " 188552/200000: episode: 1192, duration: 1.175s, episode steps: 158, steps per second: 134, episode reward: -157.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 1.055357, mae: 12.839719, mean_q: -18.397940, mean_eps: 0.050000\n",
            " 188692/200000: episode: 1193, duration: 1.027s, episode steps: 140, steps per second: 136, episode reward: -139.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.757 [0.000, 2.000],  loss: 0.985330, mae: 12.773086, mean_q: -18.310218, mean_eps: 0.050000\n",
            " 188806/200000: episode: 1194, duration: 0.833s, episode steps: 114, steps per second: 137, episode reward: -113.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.588 [0.000, 2.000],  loss: 1.103171, mae: 12.848465, mean_q: -18.383724, mean_eps: 0.050000\n",
            " 188937/200000: episode: 1195, duration: 0.936s, episode steps: 131, steps per second: 140, episode reward: -130.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.679 [0.000, 2.000],  loss: 0.978900, mae: 12.918177, mean_q: -18.539667, mean_eps: 0.050000\n",
            " 189061/200000: episode: 1196, duration: 0.884s, episode steps: 124, steps per second: 140, episode reward: -123.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.895 [0.000, 2.000],  loss: 1.003702, mae: 12.957906, mean_q: -18.587745, mean_eps: 0.050000\n",
            " 189182/200000: episode: 1197, duration: 0.849s, episode steps: 121, steps per second: 143, episode reward: -120.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.711 [0.000, 2.000],  loss: 1.014224, mae: 12.869132, mean_q: -18.453707, mean_eps: 0.050000\n",
            " 189320/200000: episode: 1198, duration: 0.957s, episode steps: 138, steps per second: 144, episode reward: -137.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.717 [0.000, 2.000],  loss: 0.939801, mae: 12.917851, mean_q: -18.547203, mean_eps: 0.050000\n",
            " 189500/200000: episode: 1199, duration: 1.319s, episode steps: 180, steps per second: 137, episode reward: -179.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.817 [0.000, 2.000],  loss: 0.935481, mae: 12.865298, mean_q: -18.467945, mean_eps: 0.050000\n",
            " 189625/200000: episode: 1200, duration: 1.291s, episode steps: 125, steps per second:  97, episode reward: -124.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.752 [0.000, 2.000],  loss: 1.135146, mae: 12.740503, mean_q: -18.222759, mean_eps: 0.050000\n",
            " 189752/200000: episode: 1201, duration: 1.219s, episode steps: 127, steps per second: 104, episode reward: -126.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.835 [0.000, 2.000],  loss: 1.038695, mae: 12.912983, mean_q: -18.514752, mean_eps: 0.050000\n",
            " 189870/200000: episode: 1202, duration: 1.137s, episode steps: 118, steps per second: 104, episode reward: -117.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.220 [0.000, 2.000],  loss: 1.056412, mae: 13.050791, mean_q: -18.714255, mean_eps: 0.050000\n",
            " 189968/200000: episode: 1203, duration: 0.667s, episode steps:  98, steps per second: 147, episode reward: -97.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.908 [0.000, 2.000],  loss: 1.119624, mae: 12.722732, mean_q: -18.204552, mean_eps: 0.050000\n",
            " 190101/200000: episode: 1204, duration: 0.913s, episode steps: 133, steps per second: 146, episode reward: -132.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.128 [0.000, 2.000],  loss: 1.056064, mae: 12.729158, mean_q: -18.226395, mean_eps: 0.050000\n",
            " 190246/200000: episode: 1205, duration: 0.960s, episode steps: 145, steps per second: 151, episode reward: -144.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.117 [0.000, 2.000],  loss: 0.924604, mae: 12.786194, mean_q: -18.329696, mean_eps: 0.050000\n",
            " 190359/200000: episode: 1206, duration: 0.776s, episode steps: 113, steps per second: 146, episode reward: -112.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.044 [0.000, 2.000],  loss: 1.092695, mae: 12.709135, mean_q: -18.192665, mean_eps: 0.050000\n",
            " 190500/200000: episode: 1207, duration: 0.953s, episode steps: 141, steps per second: 148, episode reward: -140.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.950 [0.000, 2.000],  loss: 1.030278, mae: 12.647548, mean_q: -18.124786, mean_eps: 0.050000\n",
            " 190688/200000: episode: 1208, duration: 1.251s, episode steps: 188, steps per second: 150, episode reward: -187.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.043 [0.000, 2.000],  loss: 0.981734, mae: 12.701503, mean_q: -18.216002, mean_eps: 0.050000\n",
            " 190812/200000: episode: 1209, duration: 0.828s, episode steps: 124, steps per second: 150, episode reward: -123.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.992 [0.000, 2.000],  loss: 0.939812, mae: 12.581660, mean_q: -18.042491, mean_eps: 0.050000\n",
            " 190974/200000: episode: 1210, duration: 1.079s, episode steps: 162, steps per second: 150, episode reward: -161.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.185 [0.000, 2.000],  loss: 1.012305, mae: 12.683090, mean_q: -18.175943, mean_eps: 0.050000\n",
            " 191074/200000: episode: 1211, duration: 0.665s, episode steps: 100, steps per second: 150, episode reward: -99.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.040 [0.000, 2.000],  loss: 0.884571, mae: 12.591838, mean_q: -18.072772, mean_eps: 0.050000\n",
            " 191205/200000: episode: 1212, duration: 0.894s, episode steps: 131, steps per second: 147, episode reward: -130.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.817 [0.000, 2.000],  loss: 1.026777, mae: 12.655553, mean_q: -18.124597, mean_eps: 0.050000\n",
            " 191332/200000: episode: 1213, duration: 0.945s, episode steps: 127, steps per second: 134, episode reward: -126.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.008 [0.000, 2.000],  loss: 1.004957, mae: 12.648193, mean_q: -18.135854, mean_eps: 0.050000\n",
            " 191483/200000: episode: 1214, duration: 1.443s, episode steps: 151, steps per second: 105, episode reward: -150.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.113 [0.000, 2.000],  loss: 0.989835, mae: 12.678255, mean_q: -18.204570, mean_eps: 0.050000\n",
            " 191607/200000: episode: 1215, duration: 1.129s, episode steps: 124, steps per second: 110, episode reward: -123.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.887 [0.000, 2.000],  loss: 1.028432, mae: 12.751796, mean_q: -18.297605, mean_eps: 0.050000\n",
            " 191726/200000: episode: 1216, duration: 1.020s, episode steps: 119, steps per second: 117, episode reward: -118.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.941 [0.000, 2.000],  loss: 1.060016, mae: 12.674976, mean_q: -18.155915, mean_eps: 0.050000\n",
            " 191835/200000: episode: 1217, duration: 0.720s, episode steps: 109, steps per second: 151, episode reward: -108.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.064 [0.000, 2.000],  loss: 1.004633, mae: 12.597066, mean_q: -18.068350, mean_eps: 0.050000\n",
            " 191947/200000: episode: 1218, duration: 0.714s, episode steps: 112, steps per second: 157, episode reward: -111.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.893 [0.000, 2.000],  loss: 0.943244, mae: 12.683226, mean_q: -18.201919, mean_eps: 0.050000\n",
            " 192041/200000: episode: 1219, duration: 0.658s, episode steps:  94, steps per second: 143, episode reward: -93.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.996383, mae: 12.618738, mean_q: -18.063061, mean_eps: 0.050000\n",
            " 192145/200000: episode: 1220, duration: 0.661s, episode steps: 104, steps per second: 157, episode reward: -103.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.865 [0.000, 2.000],  loss: 0.979615, mae: 12.670911, mean_q: -18.148986, mean_eps: 0.050000\n",
            " 192294/200000: episode: 1221, duration: 0.969s, episode steps: 149, steps per second: 154, episode reward: -148.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.349 [0.000, 2.000],  loss: 0.936725, mae: 12.654864, mean_q: -18.132293, mean_eps: 0.050000\n",
            " 192418/200000: episode: 1222, duration: 0.852s, episode steps: 124, steps per second: 146, episode reward: -123.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.919 [0.000, 2.000],  loss: 1.109710, mae: 12.692096, mean_q: -18.177203, mean_eps: 0.050000\n",
            " 192603/200000: episode: 1223, duration: 1.255s, episode steps: 185, steps per second: 147, episode reward: -184.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.059 [0.000, 2.000],  loss: 1.020181, mae: 12.709175, mean_q: -18.221470, mean_eps: 0.050000\n",
            " 192711/200000: episode: 1224, duration: 0.754s, episode steps: 108, steps per second: 143, episode reward: -107.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.130 [0.000, 2.000],  loss: 1.023517, mae: 12.734284, mean_q: -18.262743, mean_eps: 0.050000\n",
            " 192856/200000: episode: 1225, duration: 0.936s, episode steps: 145, steps per second: 155, episode reward: -144.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.166 [0.000, 2.000],  loss: 1.006539, mae: 12.691243, mean_q: -18.207254, mean_eps: 0.050000\n",
            " 192997/200000: episode: 1226, duration: 0.991s, episode steps: 141, steps per second: 142, episode reward: -140.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.227 [0.000, 2.000],  loss: 1.080328, mae: 12.698531, mean_q: -18.186474, mean_eps: 0.050000\n",
            " 193121/200000: episode: 1227, duration: 0.842s, episode steps: 124, steps per second: 147, episode reward: -123.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.387 [0.000, 2.000],  loss: 0.924800, mae: 12.705295, mean_q: -18.228963, mean_eps: 0.050000\n",
            " 193275/200000: episode: 1228, duration: 1.380s, episode steps: 154, steps per second: 112, episode reward: -153.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.286 [0.000, 2.000],  loss: 1.008025, mae: 12.803846, mean_q: -18.374103, mean_eps: 0.050000\n",
            " 193491/200000: episode: 1229, duration: 2.140s, episode steps: 216, steps per second: 101, episode reward: -215.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.227 [0.000, 2.000],  loss: 0.986626, mae: 12.562560, mean_q: -18.015713, mean_eps: 0.050000\n",
            " 193649/200000: episode: 1230, duration: 1.155s, episode steps: 158, steps per second: 137, episode reward: -157.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.082 [0.000, 2.000],  loss: 1.058471, mae: 12.590204, mean_q: -18.036352, mean_eps: 0.050000\n",
            " 193790/200000: episode: 1231, duration: 1.028s, episode steps: 141, steps per second: 137, episode reward: -140.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.149 [0.000, 2.000],  loss: 1.096075, mae: 12.652217, mean_q: -18.110735, mean_eps: 0.050000\n",
            " 193910/200000: episode: 1232, duration: 0.917s, episode steps: 120, steps per second: 131, episode reward: -119.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.958 [0.000, 2.000],  loss: 0.977985, mae: 12.672047, mean_q: -18.170860, mean_eps: 0.050000\n",
            " 194064/200000: episode: 1233, duration: 1.089s, episode steps: 154, steps per second: 141, episode reward: -153.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.195 [0.000, 2.000],  loss: 1.019322, mae: 12.697611, mean_q: -18.191660, mean_eps: 0.050000\n",
            " 194199/200000: episode: 1234, duration: 0.938s, episode steps: 135, steps per second: 144, episode reward: -134.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.311 [0.000, 2.000],  loss: 0.958915, mae: 12.726385, mean_q: -18.265641, mean_eps: 0.050000\n",
            " 194322/200000: episode: 1235, duration: 0.854s, episode steps: 123, steps per second: 144, episode reward: -122.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.024 [0.000, 2.000],  loss: 0.987607, mae: 12.696372, mean_q: -18.195575, mean_eps: 0.050000\n",
            " 194447/200000: episode: 1236, duration: 0.856s, episode steps: 125, steps per second: 146, episode reward: -124.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.976 [0.000, 2.000],  loss: 0.946484, mae: 12.780901, mean_q: -18.336762, mean_eps: 0.050000\n",
            " 194584/200000: episode: 1237, duration: 0.950s, episode steps: 137, steps per second: 144, episode reward: -136.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.095 [0.000, 2.000],  loss: 0.989656, mae: 12.719533, mean_q: -18.231761, mean_eps: 0.050000\n",
            " 194728/200000: episode: 1238, duration: 0.979s, episode steps: 144, steps per second: 147, episode reward: -143.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.125 [0.000, 2.000],  loss: 1.031629, mae: 12.639333, mean_q: -18.108991, mean_eps: 0.050000\n",
            " 194869/200000: episode: 1239, duration: 0.949s, episode steps: 141, steps per second: 149, episode reward: -140.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.858 [0.000, 2.000],  loss: 0.964094, mae: 12.651360, mean_q: -18.139060, mean_eps: 0.050000\n",
            " 194992/200000: episode: 1240, duration: 1.094s, episode steps: 123, steps per second: 112, episode reward: -122.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.967 [0.000, 2.000],  loss: 0.921338, mae: 12.689373, mean_q: -18.208267, mean_eps: 0.050000\n",
            " 195116/200000: episode: 1241, duration: 1.230s, episode steps: 124, steps per second: 101, episode reward: -123.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.016 [0.000, 2.000],  loss: 0.987974, mae: 12.719402, mean_q: -18.237117, mean_eps: 0.050000\n",
            " 195290/200000: episode: 1242, duration: 1.656s, episode steps: 174, steps per second: 105, episode reward: -173.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.144 [0.000, 2.000],  loss: 1.032534, mae: 12.796642, mean_q: -18.343526, mean_eps: 0.050000\n",
            " 195398/200000: episode: 1243, duration: 0.708s, episode steps: 108, steps per second: 152, episode reward: -107.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.259 [0.000, 2.000],  loss: 0.987633, mae: 12.809180, mean_q: -18.378254, mean_eps: 0.050000\n",
            " 195502/200000: episode: 1244, duration: 0.694s, episode steps: 104, steps per second: 150, episode reward: -103.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.933 [0.000, 2.000],  loss: 1.064836, mae: 12.752096, mean_q: -18.280870, mean_eps: 0.050000\n",
            " 195666/200000: episode: 1245, duration: 1.150s, episode steps: 164, steps per second: 143, episode reward: -163.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.006 [0.000, 2.000],  loss: 0.992005, mae: 12.715687, mean_q: -18.228872, mean_eps: 0.050000\n",
            " 195783/200000: episode: 1246, duration: 0.874s, episode steps: 117, steps per second: 134, episode reward: -116.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.239 [0.000, 2.000],  loss: 0.924230, mae: 12.629525, mean_q: -18.120347, mean_eps: 0.050000\n",
            " 195920/200000: episode: 1247, duration: 0.977s, episode steps: 137, steps per second: 140, episode reward: -136.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.993 [0.000, 2.000],  loss: 0.976357, mae: 12.825685, mean_q: -18.398517, mean_eps: 0.050000\n",
            " 196087/200000: episode: 1248, duration: 1.177s, episode steps: 167, steps per second: 142, episode reward: -166.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.174 [0.000, 2.000],  loss: 1.052526, mae: 12.801195, mean_q: -18.337176, mean_eps: 0.050000\n",
            " 196230/200000: episode: 1249, duration: 0.979s, episode steps: 143, steps per second: 146, episode reward: -142.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.119 [0.000, 2.000],  loss: 1.022966, mae: 12.831166, mean_q: -18.412978, mean_eps: 0.050000\n",
            " 196390/200000: episode: 1250, duration: 1.116s, episode steps: 160, steps per second: 143, episode reward: -159.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.188 [0.000, 2.000],  loss: 1.038497, mae: 12.728948, mean_q: -18.239352, mean_eps: 0.050000\n",
            " 196527/200000: episode: 1251, duration: 0.961s, episode steps: 137, steps per second: 143, episode reward: -136.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.394 [0.000, 2.000],  loss: 0.924818, mae: 12.808906, mean_q: -18.359327, mean_eps: 0.050000\n",
            " 196667/200000: episode: 1252, duration: 0.937s, episode steps: 140, steps per second: 149, episode reward: -139.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.093 [0.000, 2.000],  loss: 0.962365, mae: 12.833961, mean_q: -18.412863, mean_eps: 0.050000\n",
            " 196835/200000: episode: 1253, duration: 1.436s, episode steps: 168, steps per second: 117, episode reward: -167.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.321 [0.000, 2.000],  loss: 1.040284, mae: 12.874367, mean_q: -18.463173, mean_eps: 0.050000\n",
            " 197033/200000: episode: 1254, duration: 1.876s, episode steps: 198, steps per second: 106, episode reward: -197.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.919 [0.000, 2.000],  loss: 1.033663, mae: 12.763295, mean_q: -18.306604, mean_eps: 0.050000\n",
            " 197204/200000: episode: 1255, duration: 1.344s, episode steps: 171, steps per second: 127, episode reward: -170.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.228 [0.000, 2.000],  loss: 0.958254, mae: 12.675400, mean_q: -18.185217, mean_eps: 0.050000\n",
            " 197353/200000: episode: 1256, duration: 0.980s, episode steps: 149, steps per second: 152, episode reward: -148.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.168 [0.000, 2.000],  loss: 0.970287, mae: 12.678955, mean_q: -18.184625, mean_eps: 0.050000\n",
            " 197505/200000: episode: 1257, duration: 1.009s, episode steps: 152, steps per second: 151, episode reward: -151.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.243 [0.000, 2.000],  loss: 0.986454, mae: 12.750614, mean_q: -18.294929, mean_eps: 0.050000\n",
            " 197645/200000: episode: 1258, duration: 0.966s, episode steps: 140, steps per second: 145, episode reward: -139.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.043 [0.000, 2.000],  loss: 1.050026, mae: 12.712293, mean_q: -18.220432, mean_eps: 0.050000\n",
            " 197816/200000: episode: 1259, duration: 1.110s, episode steps: 171, steps per second: 154, episode reward: -170.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.088 [0.000, 2.000],  loss: 1.037713, mae: 12.716521, mean_q: -18.221413, mean_eps: 0.050000\n",
            " 197933/200000: episode: 1260, duration: 0.764s, episode steps: 117, steps per second: 153, episode reward: -116.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.932 [0.000, 2.000],  loss: 0.999445, mae: 12.674669, mean_q: -18.176772, mean_eps: 0.050000\n",
            " 198042/200000: episode: 1261, duration: 0.730s, episode steps: 109, steps per second: 149, episode reward: -108.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.092 [0.000, 2.000],  loss: 0.958744, mae: 12.696453, mean_q: -18.232904, mean_eps: 0.050000\n",
            " 198142/200000: episode: 1262, duration: 0.647s, episode steps: 100, steps per second: 155, episode reward: -99.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.380 [0.000, 2.000],  loss: 0.955125, mae: 12.674858, mean_q: -18.193665, mean_eps: 0.050000\n",
            " 198267/200000: episode: 1263, duration: 0.777s, episode steps: 125, steps per second: 161, episode reward: -124.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.952 [0.000, 2.000],  loss: 1.077671, mae: 12.684824, mean_q: -18.178164, mean_eps: 0.050000\n",
            " 198407/200000: episode: 1264, duration: 0.897s, episode steps: 140, steps per second: 156, episode reward: -139.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.864 [0.000, 2.000],  loss: 1.060711, mae: 12.825138, mean_q: -18.389716, mean_eps: 0.050000\n",
            " 198491/200000: episode: 1265, duration: 0.564s, episode steps:  84, steps per second: 149, episode reward: -83.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.786 [0.000, 2.000],  loss: 0.947717, mae: 12.816759, mean_q: -18.390923, mean_eps: 0.050000\n",
            " 198625/200000: episode: 1266, duration: 1.044s, episode steps: 134, steps per second: 128, episode reward: -133.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.052 [0.000, 2.000],  loss: 1.044543, mae: 12.737607, mean_q: -18.250374, mean_eps: 0.050000\n",
            " 198751/200000: episode: 1267, duration: 1.215s, episode steps: 126, steps per second: 104, episode reward: -125.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.048 [0.000, 2.000],  loss: 1.009127, mae: 12.806251, mean_q: -18.367364, mean_eps: 0.050000\n",
            " 198895/200000: episode: 1268, duration: 1.434s, episode steps: 144, steps per second: 100, episode reward: -143.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.250 [0.000, 2.000],  loss: 1.034874, mae: 12.879387, mean_q: -18.479248, mean_eps: 0.050000\n",
            " 199017/200000: episode: 1269, duration: 0.983s, episode steps: 122, steps per second: 124, episode reward: -121.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.934 [0.000, 2.000],  loss: 0.970848, mae: 12.659945, mean_q: -18.156117, mean_eps: 0.050000\n",
            " 199141/200000: episode: 1270, duration: 0.866s, episode steps: 124, steps per second: 143, episode reward: -123.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 1.053680, mae: 12.569330, mean_q: -18.001751, mean_eps: 0.050000\n",
            " 199281/200000: episode: 1271, duration: 1.037s, episode steps: 140, steps per second: 135, episode reward: -139.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.886 [0.000, 2.000],  loss: 0.964835, mae: 12.632399, mean_q: -18.137890, mean_eps: 0.050000\n",
            " 199383/200000: episode: 1272, duration: 0.720s, episode steps: 102, steps per second: 142, episode reward: -101.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.853 [0.000, 2.000],  loss: 0.903149, mae: 12.662426, mean_q: -18.179775, mean_eps: 0.050000\n",
            " 199507/200000: episode: 1273, duration: 0.826s, episode steps: 124, steps per second: 150, episode reward: -123.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.984 [0.000, 2.000],  loss: 1.049362, mae: 12.610608, mean_q: -18.069808, mean_eps: 0.050000\n",
            " 199618/200000: episode: 1274, duration: 0.775s, episode steps: 111, steps per second: 143, episode reward: -110.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.883 [0.000, 2.000],  loss: 1.025590, mae: 12.554170, mean_q: -17.977119, mean_eps: 0.050000\n",
            " 199764/200000: episode: 1275, duration: 1.027s, episode steps: 146, steps per second: 142, episode reward: -145.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.068 [0.000, 2.000],  loss: 0.984570, mae: 12.593454, mean_q: -18.047079, mean_eps: 0.050000\n",
            " 199924/200000: episode: 1276, duration: 1.130s, episode steps: 160, steps per second: 142, episode reward: -159.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.900 [0.000, 2.000],  loss: 0.950285, mae: 12.496177, mean_q: -17.917562, mean_eps: 0.050000\n",
            "done, took 1348.565 seconds\n",
            "\n",
            "Evaluando DDQN...\n",
            "DDQN → Recompensa media: -358.35 ± 174.17\n",
            "\n",
            "Entrenando DUELING_DQN...\n",
            "Training for 200000 steps ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    500/200000: episode: 1, duration: 1.848s, episode steps: 500, steps per second: 271, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.954 [0.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   1000/200000: episode: 2, duration: 0.772s, episode steps: 500, steps per second: 648, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.986 [0.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   1500/200000: episode: 3, duration: 0.796s, episode steps: 500, steps per second: 628, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.026 [0.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   2000/200000: episode: 4, duration: 0.716s, episode steps: 500, steps per second: 699, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.034 [0.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   2500/200000: episode: 5, duration: 0.827s, episode steps: 500, steps per second: 605, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.024 [0.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   3000/200000: episode: 6, duration: 0.618s, episode steps: 500, steps per second: 809, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.036 [0.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   3500/200000: episode: 7, duration: 0.553s, episode steps: 500, steps per second: 904, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.024 [0.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   4000/200000: episode: 8, duration: 0.579s, episode steps: 500, steps per second: 864, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.978 [0.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   4500/200000: episode: 9, duration: 0.574s, episode steps: 500, steps per second: 871, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.034 [0.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   5000/200000: episode: 10, duration: 0.545s, episode steps: 500, steps per second: 917, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.976 [0.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   5500/200000: episode: 11, duration: 6.256s, episode steps: 500, steps per second:  80, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.004 [0.000, 2.000],  loss: 0.112029, mae: 0.533719, mean_q: -0.486629, mean_eps: 0.966750\n",
            "   6000/200000: episode: 12, duration: 2.325s, episode steps: 500, steps per second: 215, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.012 [0.000, 2.000],  loss: 0.017832, mae: 0.658644, mean_q: -0.860654, mean_eps: 0.963596\n",
            "   6500/200000: episode: 13, duration: 2.767s, episode steps: 500, steps per second: 181, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.068356, mae: 1.248581, mean_q: -1.682888, mean_eps: 0.960429\n",
            "   7000/200000: episode: 14, duration: 1.943s, episode steps: 500, steps per second: 257, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.038 [0.000, 2.000],  loss: 0.008924, mae: 1.277737, mean_q: -1.831594, mean_eps: 0.957263\n",
            "   7500/200000: episode: 15, duration: 2.036s, episode steps: 500, steps per second: 246, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.056841, mae: 1.897725, mean_q: -2.694598, mean_eps: 0.954096\n",
            "   8000/200000: episode: 16, duration: 2.051s, episode steps: 500, steps per second: 244, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.948 [0.000, 2.000],  loss: 0.012956, mae: 1.911652, mean_q: -2.788430, mean_eps: 0.950929\n",
            "   8500/200000: episode: 17, duration: 1.913s, episode steps: 500, steps per second: 261, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.006 [0.000, 2.000],  loss: 0.055362, mae: 2.548299, mean_q: -3.687173, mean_eps: 0.947763\n",
            "   8922/200000: episode: 18, duration: 1.658s, episode steps: 422, steps per second: 255, episode reward: -421.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.007 [0.000, 2.000],  loss: 0.018104, mae: 2.554475, mean_q: -3.748327, mean_eps: 0.944837\n",
            "   9422/200000: episode: 19, duration: 2.936s, episode steps: 500, steps per second: 170, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.054 [0.000, 2.000],  loss: 0.056533, mae: 3.060362, mean_q: -4.456813, mean_eps: 0.941911\n",
            "   9922/200000: episode: 20, duration: 2.361s, episode steps: 500, steps per second: 212, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.018 [0.000, 2.000],  loss: 0.021949, mae: 3.167919, mean_q: -4.662263, mean_eps: 0.938744\n",
            "  10422/200000: episode: 21, duration: 1.969s, episode steps: 500, steps per second: 254, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.986 [0.000, 2.000],  loss: 0.061059, mae: 3.669256, mean_q: -5.367588, mean_eps: 0.935577\n",
            "  10922/200000: episode: 22, duration: 1.914s, episode steps: 500, steps per second: 261, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.002 [0.000, 2.000],  loss: 0.030693, mae: 3.768021, mean_q: -5.560809, mean_eps: 0.932411\n",
            "  11422/200000: episode: 23, duration: 1.972s, episode steps: 500, steps per second: 254, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.016 [0.000, 2.000],  loss: 0.068736, mae: 4.268903, mean_q: -6.269646, mean_eps: 0.929244\n",
            "  11922/200000: episode: 24, duration: 1.855s, episode steps: 500, steps per second: 270, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.996 [0.000, 2.000],  loss: 0.043112, mae: 4.362325, mean_q: -6.446247, mean_eps: 0.926077\n",
            "  12422/200000: episode: 25, duration: 2.659s, episode steps: 500, steps per second: 188, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.966 [0.000, 2.000],  loss: 0.072161, mae: 4.866465, mean_q: -7.164846, mean_eps: 0.922911\n",
            "  12922/200000: episode: 26, duration: 2.598s, episode steps: 500, steps per second: 192, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.996 [0.000, 2.000],  loss: 0.039861, mae: 4.963691, mean_q: -7.346223, mean_eps: 0.919744\n",
            "  13422/200000: episode: 27, duration: 2.190s, episode steps: 500, steps per second: 228, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.984 [0.000, 2.000],  loss: 0.090963, mae: 5.470819, mean_q: -8.062075, mean_eps: 0.916577\n",
            "  13922/200000: episode: 28, duration: 2.355s, episode steps: 500, steps per second: 212, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.994 [0.000, 2.000],  loss: 0.067848, mae: 5.565644, mean_q: -8.242354, mean_eps: 0.913411\n",
            "  14422/200000: episode: 29, duration: 2.133s, episode steps: 500, steps per second: 234, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.942 [0.000, 2.000],  loss: 0.106677, mae: 6.062132, mean_q: -8.951818, mean_eps: 0.910244\n",
            "  14922/200000: episode: 30, duration: 2.162s, episode steps: 500, steps per second: 231, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.892 [0.000, 2.000],  loss: 0.104577, mae: 6.150751, mean_q: -9.112698, mean_eps: 0.907077\n",
            "  15422/200000: episode: 31, duration: 2.933s, episode steps: 500, steps per second: 170, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.004 [0.000, 2.000],  loss: 0.117019, mae: 6.638450, mean_q: -9.810895, mean_eps: 0.903911\n",
            "  15922/200000: episode: 32, duration: 2.281s, episode steps: 500, steps per second: 219, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.922 [0.000, 2.000],  loss: 0.093833, mae: 6.734204, mean_q: -9.988337, mean_eps: 0.900744\n",
            "  16422/200000: episode: 33, duration: 2.208s, episode steps: 500, steps per second: 226, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.008 [0.000, 2.000],  loss: 0.133300, mae: 7.202268, mean_q: -10.656942, mean_eps: 0.897577\n",
            "  16922/200000: episode: 34, duration: 2.132s, episode steps: 500, steps per second: 235, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.099028, mae: 7.291668, mean_q: -10.823773, mean_eps: 0.894411\n",
            "  17422/200000: episode: 35, duration: 2.150s, episode steps: 500, steps per second: 233, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.145828, mae: 7.768643, mean_q: -11.504539, mean_eps: 0.891244\n",
            "  17922/200000: episode: 36, duration: 2.300s, episode steps: 500, steps per second: 217, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.038 [0.000, 2.000],  loss: 0.117199, mae: 7.857152, mean_q: -11.669461, mean_eps: 0.888077\n",
            "  18422/200000: episode: 37, duration: 2.979s, episode steps: 500, steps per second: 168, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.006 [0.000, 2.000],  loss: 0.189316, mae: 8.315867, mean_q: -12.313228, mean_eps: 0.884911\n",
            "  18922/200000: episode: 38, duration: 2.276s, episode steps: 500, steps per second: 220, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.904 [0.000, 2.000],  loss: 0.150297, mae: 8.399891, mean_q: -12.474918, mean_eps: 0.881744\n",
            "  19422/200000: episode: 39, duration: 2.145s, episode steps: 500, steps per second: 233, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.022 [0.000, 2.000],  loss: 0.172614, mae: 8.891153, mean_q: -13.192582, mean_eps: 0.878577\n",
            "  19922/200000: episode: 40, duration: 2.265s, episode steps: 500, steps per second: 221, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.964 [0.000, 2.000],  loss: 0.184423, mae: 8.975857, mean_q: -13.335391, mean_eps: 0.875411\n",
            "  20422/200000: episode: 41, duration: 2.262s, episode steps: 500, steps per second: 221, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.187356, mae: 9.427811, mean_q: -13.994201, mean_eps: 0.872244\n",
            "  20922/200000: episode: 42, duration: 2.507s, episode steps: 500, steps per second: 199, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.048 [0.000, 2.000],  loss: 0.171530, mae: 9.510840, mean_q: -14.141063, mean_eps: 0.869077\n",
            "  21422/200000: episode: 43, duration: 2.766s, episode steps: 500, steps per second: 181, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.982 [0.000, 2.000],  loss: 0.221264, mae: 9.950580, mean_q: -14.766310, mean_eps: 0.865911\n",
            "  21922/200000: episode: 44, duration: 1.930s, episode steps: 500, steps per second: 259, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.036 [0.000, 2.000],  loss: 0.192635, mae: 10.020188, mean_q: -14.896765, mean_eps: 0.862744\n",
            "  22422/200000: episode: 45, duration: 2.059s, episode steps: 500, steps per second: 243, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.036 [0.000, 2.000],  loss: 0.242128, mae: 10.456223, mean_q: -15.521166, mean_eps: 0.859577\n",
            "  22922/200000: episode: 46, duration: 2.038s, episode steps: 500, steps per second: 245, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.016 [0.000, 2.000],  loss: 0.297319, mae: 10.536151, mean_q: -15.661442, mean_eps: 0.856411\n",
            "  23422/200000: episode: 47, duration: 2.013s, episode steps: 500, steps per second: 248, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.252991, mae: 10.982220, mean_q: -16.314281, mean_eps: 0.853244\n",
            "  23922/200000: episode: 48, duration: 2.358s, episode steps: 500, steps per second: 212, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.962 [0.000, 2.000],  loss: 0.207503, mae: 11.060025, mean_q: -16.455984, mean_eps: 0.850077\n",
            "  24422/200000: episode: 49, duration: 3.013s, episode steps: 500, steps per second: 166, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.888 [0.000, 2.000],  loss: 0.276474, mae: 11.504183, mean_q: -17.093296, mean_eps: 0.846911\n",
            "  24922/200000: episode: 50, duration: 2.215s, episode steps: 500, steps per second: 226, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.249090, mae: 11.576429, mean_q: -17.215319, mean_eps: 0.843744\n",
            "  25422/200000: episode: 51, duration: 2.171s, episode steps: 500, steps per second: 230, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.992 [0.000, 2.000],  loss: 0.310672, mae: 11.998823, mean_q: -17.825289, mean_eps: 0.840577\n",
            "  25922/200000: episode: 52, duration: 2.116s, episode steps: 500, steps per second: 236, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.253518, mae: 12.076977, mean_q: -17.959830, mean_eps: 0.837411\n",
            "  26422/200000: episode: 53, duration: 2.199s, episode steps: 500, steps per second: 227, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.922 [0.000, 2.000],  loss: 0.264373, mae: 12.517966, mean_q: -18.607668, mean_eps: 0.834244\n",
            "  26922/200000: episode: 54, duration: 2.845s, episode steps: 500, steps per second: 176, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.880 [0.000, 2.000],  loss: 0.312688, mae: 12.597121, mean_q: -18.730835, mean_eps: 0.831077\n",
            "  27422/200000: episode: 55, duration: 2.885s, episode steps: 500, steps per second: 173, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.918 [0.000, 2.000],  loss: 0.331629, mae: 12.993195, mean_q: -19.303050, mean_eps: 0.827911\n",
            "  27922/200000: episode: 56, duration: 2.098s, episode steps: 500, steps per second: 238, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.830 [0.000, 2.000],  loss: 0.347458, mae: 13.060801, mean_q: -19.424431, mean_eps: 0.824744\n",
            "  28422/200000: episode: 57, duration: 2.124s, episode steps: 500, steps per second: 235, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.002 [0.000, 2.000],  loss: 0.401027, mae: 13.442045, mean_q: -19.971030, mean_eps: 0.821577\n",
            "  28922/200000: episode: 58, duration: 2.112s, episode steps: 500, steps per second: 237, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.372699, mae: 13.508429, mean_q: -20.096660, mean_eps: 0.818411\n",
            "  29422/200000: episode: 59, duration: 2.043s, episode steps: 500, steps per second: 245, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.992 [0.000, 2.000],  loss: 0.352361, mae: 13.915582, mean_q: -20.695989, mean_eps: 0.815244\n",
            "  29922/200000: episode: 60, duration: 2.857s, episode steps: 500, steps per second: 175, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.036 [0.000, 2.000],  loss: 0.308302, mae: 13.997234, mean_q: -20.839922, mean_eps: 0.812077\n",
            "  30240/200000: episode: 61, duration: 1.887s, episode steps: 318, steps per second: 169, episode reward: -317.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.975 [0.000, 2.000],  loss: 0.383583, mae: 14.338679, mean_q: -21.308969, mean_eps: 0.809493\n",
            "  30740/200000: episode: 62, duration: 2.384s, episode steps: 500, steps per second: 210, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.004 [0.000, 2.000],  loss: 0.499916, mae: 14.437672, mean_q: -21.473605, mean_eps: 0.806909\n",
            "  31240/200000: episode: 63, duration: 2.273s, episode steps: 500, steps per second: 220, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.944 [0.000, 2.000],  loss: 0.554443, mae: 14.682765, mean_q: -21.799981, mean_eps: 0.803743\n",
            "  31740/200000: episode: 64, duration: 2.432s, episode steps: 500, steps per second: 206, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.373694, mae: 14.950499, mean_q: -22.243442, mean_eps: 0.800576\n",
            "  32183/200000: episode: 65, duration: 1.965s, episode steps: 443, steps per second: 225, episode reward: -442.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.047 [0.000, 2.000],  loss: 0.456199, mae: 15.125812, mean_q: -22.472825, mean_eps: 0.797587\n",
            "  32532/200000: episode: 66, duration: 2.203s, episode steps: 349, steps per second: 158, episode reward: -348.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.937 [0.000, 2.000],  loss: 0.593100, mae: 15.391075, mean_q: -22.873412, mean_eps: 0.795079\n",
            "  33032/200000: episode: 67, duration: 2.929s, episode steps: 500, steps per second: 171, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.048 [0.000, 2.000],  loss: 0.573370, mae: 15.397331, mean_q: -22.881176, mean_eps: 0.792393\n",
            "  33523/200000: episode: 68, duration: 2.462s, episode steps: 491, steps per second: 199, episode reward: -490.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.963 [0.000, 2.000],  loss: 0.523462, mae: 15.854470, mean_q: -23.564700, mean_eps: 0.789252\n",
            "  33788/200000: episode: 69, duration: 1.369s, episode steps: 265, steps per second: 194, episode reward: -264.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.989 [0.000, 2.000],  loss: 0.478011, mae: 15.833685, mean_q: -23.536077, mean_eps: 0.786858\n",
            "  34129/200000: episode: 70, duration: 1.704s, episode steps: 341, steps per second: 200, episode reward: -340.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.003 [0.000, 2.000],  loss: 0.516063, mae: 15.992476, mean_q: -23.749359, mean_eps: 0.784933\n",
            "  34465/200000: episode: 71, duration: 1.815s, episode steps: 336, steps per second: 185, episode reward: -335.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.015 [0.000, 2.000],  loss: 0.657524, mae: 16.259468, mean_q: -24.171389, mean_eps: 0.782779\n",
            "  34965/200000: episode: 72, duration: 2.739s, episode steps: 500, steps per second: 183, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.024 [0.000, 2.000],  loss: 0.509726, mae: 16.260892, mean_q: -24.183892, mean_eps: 0.780132\n",
            "  35386/200000: episode: 73, duration: 2.570s, episode steps: 421, steps per second: 164, episode reward: -420.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.945 [0.000, 2.000],  loss: 0.838581, mae: 16.627553, mean_q: -24.692105, mean_eps: 0.777219\n",
            "  35886/200000: episode: 74, duration: 2.258s, episode steps: 500, steps per second: 221, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.038 [0.000, 2.000],  loss: 0.585176, mae: 16.671372, mean_q: -24.799047, mean_eps: 0.774305\n",
            "  36386/200000: episode: 75, duration: 2.238s, episode steps: 500, steps per second: 223, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.996 [0.000, 2.000],  loss: 0.658233, mae: 17.001916, mean_q: -25.271016, mean_eps: 0.771139\n",
            "  36886/200000: episode: 76, duration: 2.322s, episode steps: 500, steps per second: 215, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.898 [0.000, 2.000],  loss: 0.547245, mae: 17.091778, mean_q: -25.428108, mean_eps: 0.767972\n",
            "  37201/200000: episode: 77, duration: 1.484s, episode steps: 315, steps per second: 212, episode reward: -314.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.867 [0.000, 2.000],  loss: 0.497414, mae: 17.351346, mean_q: -25.799216, mean_eps: 0.765388\n",
            "  37481/200000: episode: 78, duration: 1.338s, episode steps: 280, steps per second: 209, episode reward: -279.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.025 [0.000, 2.000],  loss: 0.780794, mae: 17.488916, mean_q: -25.995458, mean_eps: 0.763501\n",
            "  37981/200000: episode: 79, duration: 3.182s, episode steps: 500, steps per second: 157, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.948 [0.000, 2.000],  loss: 0.728356, mae: 17.494119, mean_q: -26.011100, mean_eps: 0.761031\n",
            "  38481/200000: episode: 80, duration: 2.375s, episode steps: 500, steps per second: 210, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.898 [0.000, 2.000],  loss: 0.628761, mae: 17.903337, mean_q: -26.629931, mean_eps: 0.757864\n",
            "  38913/200000: episode: 81, duration: 1.859s, episode steps: 432, steps per second: 232, episode reward: -431.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.870 [0.000, 2.000],  loss: 0.697532, mae: 17.904619, mean_q: -26.619700, mean_eps: 0.754913\n",
            "  39413/200000: episode: 82, duration: 2.175s, episode steps: 500, steps per second: 230, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.938 [0.000, 2.000],  loss: 0.536773, mae: 18.244123, mean_q: -27.133233, mean_eps: 0.751961\n",
            "  39913/200000: episode: 83, duration: 2.130s, episode steps: 500, steps per second: 235, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.934 [0.000, 2.000],  loss: 0.607890, mae: 18.328182, mean_q: -27.271749, mean_eps: 0.748795\n",
            "  40279/200000: episode: 84, duration: 1.569s, episode steps: 366, steps per second: 233, episode reward: -365.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.874 [0.000, 2.000],  loss: 0.590765, mae: 18.607925, mean_q: -27.662357, mean_eps: 0.746059\n",
            "  40627/200000: episode: 85, duration: 2.128s, episode steps: 348, steps per second: 164, episode reward: -347.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.905 [0.000, 2.000],  loss: 0.668765, mae: 18.690311, mean_q: -27.810956, mean_eps: 0.743804\n",
            "  40985/200000: episode: 86, duration: 2.161s, episode steps: 358, steps per second: 166, episode reward: -357.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.913 [0.000, 2.000],  loss: 0.757644, mae: 18.681152, mean_q: -27.782404, mean_eps: 0.741562\n",
            "  41394/200000: episode: 87, duration: 1.984s, episode steps: 409, steps per second: 206, episode reward: -408.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.907 [0.000, 2.000],  loss: 0.779367, mae: 19.006044, mean_q: -28.251282, mean_eps: 0.739130\n",
            "  41894/200000: episode: 88, duration: 2.427s, episode steps: 500, steps per second: 206, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.996 [0.000, 2.000],  loss: 0.588216, mae: 19.020319, mean_q: -28.292491, mean_eps: 0.736255\n",
            "  42108/200000: episode: 89, duration: 1.120s, episode steps: 214, steps per second: 191, episode reward: -213.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.869 [0.000, 2.000],  loss: 0.551693, mae: 19.205307, mean_q: -28.541123, mean_eps: 0.734000\n",
            "  42426/200000: episode: 90, duration: 1.552s, episode steps: 318, steps per second: 205, episode reward: -317.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.006 [0.000, 2.000],  loss: 1.089672, mae: 19.377350, mean_q: -28.793272, mean_eps: 0.732315\n",
            "  42926/200000: episode: 91, duration: 2.576s, episode steps: 500, steps per second: 194, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.884 [0.000, 2.000],  loss: 0.926596, mae: 19.428285, mean_q: -28.882348, mean_eps: 0.729719\n",
            "  43272/200000: episode: 92, duration: 2.253s, episode steps: 346, steps per second: 154, episode reward: -345.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.040 [0.000, 2.000],  loss: 0.898945, mae: 19.676495, mean_q: -29.223950, mean_eps: 0.727046\n",
            "  43661/200000: episode: 93, duration: 2.172s, episode steps: 389, steps per second: 179, episode reward: -388.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.964 [0.000, 2.000],  loss: 0.846744, mae: 19.787863, mean_q: -29.414496, mean_eps: 0.724715\n",
            "  44161/200000: episode: 94, duration: 2.223s, episode steps: 500, steps per second: 225, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.978 [0.000, 2.000],  loss: 0.807744, mae: 19.871217, mean_q: -29.533199, mean_eps: 0.721891\n",
            "  44661/200000: episode: 95, duration: 2.257s, episode steps: 500, steps per second: 221, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.913849, mae: 20.088567, mean_q: -29.856941, mean_eps: 0.718724\n",
            "  45161/200000: episode: 96, duration: 2.260s, episode steps: 500, steps per second: 221, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.974 [0.000, 2.000],  loss: 0.754001, mae: 20.208238, mean_q: -30.029228, mean_eps: 0.715557\n",
            "  45661/200000: episode: 97, duration: 2.190s, episode steps: 500, steps per second: 228, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.008 [0.000, 2.000],  loss: 0.993434, mae: 20.390459, mean_q: -30.304137, mean_eps: 0.712391\n",
            "  46005/200000: episode: 98, duration: 2.202s, episode steps: 344, steps per second: 156, episode reward: -343.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.983 [0.000, 2.000],  loss: 0.902103, mae: 20.394991, mean_q: -30.323270, mean_eps: 0.709718\n",
            "  46442/200000: episode: 99, duration: 2.505s, episode steps: 437, steps per second: 174, episode reward: -436.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.975 [0.000, 2.000],  loss: 0.704099, mae: 20.756482, mean_q: -30.850233, mean_eps: 0.707248\n",
            "  46772/200000: episode: 100, duration: 1.456s, episode steps: 330, steps per second: 227, episode reward: -329.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.027 [0.000, 2.000],  loss: 0.696064, mae: 20.729847, mean_q: -30.827392, mean_eps: 0.704829\n",
            "  47095/200000: episode: 101, duration: 1.500s, episode steps: 323, steps per second: 215, episode reward: -322.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.913 [0.000, 2.000],  loss: 0.762940, mae: 20.832193, mean_q: -30.958710, mean_eps: 0.702764\n",
            "  47406/200000: episode: 102, duration: 1.378s, episode steps: 311, steps per second: 226, episode reward: -310.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.987 [0.000, 2.000],  loss: 0.856700, mae: 21.093570, mean_q: -31.366645, mean_eps: 0.700750\n",
            "  47906/200000: episode: 103, duration: 2.211s, episode steps: 500, steps per second: 226, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.914 [0.000, 2.000],  loss: 0.965253, mae: 21.079057, mean_q: -31.343579, mean_eps: 0.698179\n",
            "  48406/200000: episode: 104, duration: 2.260s, episode steps: 500, steps per second: 221, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.996 [0.000, 2.000],  loss: 0.999373, mae: 21.416899, mean_q: -31.833450, mean_eps: 0.695012\n",
            "  48703/200000: episode: 105, duration: 1.895s, episode steps: 297, steps per second: 157, episode reward: -296.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.751 [0.000, 2.000],  loss: 1.042481, mae: 21.505722, mean_q: -31.946399, mean_eps: 0.692491\n",
            "  48911/200000: episode: 106, duration: 1.391s, episode steps: 208, steps per second: 150, episode reward: -207.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.880 [0.000, 2.000],  loss: 0.613368, mae: 21.526377, mean_q: -32.018379, mean_eps: 0.690895\n",
            "  49222/200000: episode: 107, duration: 1.803s, episode steps: 311, steps per second: 172, episode reward: -310.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.949 [0.000, 2.000],  loss: 0.668011, mae: 21.707773, mean_q: -32.269193, mean_eps: 0.689249\n",
            "  49417/200000: episode: 108, duration: 0.921s, episode steps: 195, steps per second: 212, episode reward: -194.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.985 [0.000, 2.000],  loss: 0.638068, mae: 21.806033, mean_q: -32.426469, mean_eps: 0.687640\n",
            "  49691/200000: episode: 109, duration: 1.339s, episode steps: 274, steps per second: 205, episode reward: -273.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.007 [0.000, 2.000],  loss: 0.765308, mae: 21.833337, mean_q: -32.483736, mean_eps: 0.686158\n",
            "  49999/200000: episode: 110, duration: 1.524s, episode steps: 308, steps per second: 202, episode reward: -307.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.919 [0.000, 2.000],  loss: 0.843411, mae: 21.817948, mean_q: -32.460334, mean_eps: 0.684321\n",
            "  50203/200000: episode: 111, duration: 1.016s, episode steps: 204, steps per second: 201, episode reward: -203.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.907 [0.000, 2.000],  loss: 0.924774, mae: 22.164931, mean_q: -32.941517, mean_eps: 0.682700\n",
            "  50455/200000: episode: 112, duration: 1.281s, episode steps: 252, steps per second: 197, episode reward: -251.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.917 [0.000, 2.000],  loss: 0.929590, mae: 22.204045, mean_q: -33.019724, mean_eps: 0.681256\n",
            "  50685/200000: episode: 113, duration: 1.089s, episode steps: 230, steps per second: 211, episode reward: -229.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.957 [0.000, 2.000],  loss: 0.604778, mae: 22.199067, mean_q: -33.021294, mean_eps: 0.679723\n",
            "  50887/200000: episode: 114, duration: 0.970s, episode steps: 202, steps per second: 208, episode reward: -201.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.950 [0.000, 2.000],  loss: 0.969144, mae: 22.204191, mean_q: -33.008273, mean_eps: 0.678355\n",
            "  51192/200000: episode: 115, duration: 1.844s, episode steps: 305, steps per second: 165, episode reward: -304.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.948 [0.000, 2.000],  loss: 1.069508, mae: 22.374401, mean_q: -33.251185, mean_eps: 0.676759\n",
            "  51597/200000: episode: 116, duration: 2.539s, episode steps: 405, steps per second: 159, episode reward: -404.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.849 [0.000, 2.000],  loss: 1.117055, mae: 22.490480, mean_q: -33.413319, mean_eps: 0.674505\n",
            "  51835/200000: episode: 117, duration: 1.055s, episode steps: 238, steps per second: 226, episode reward: -237.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.029 [0.000, 2.000],  loss: 0.768438, mae: 22.507721, mean_q: -33.463474, mean_eps: 0.672465\n",
            "  52014/200000: episode: 118, duration: 0.807s, episode steps: 179, steps per second: 222, episode reward: -178.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.028 [0.000, 2.000],  loss: 0.891986, mae: 22.505581, mean_q: -33.441553, mean_eps: 0.671148\n",
            "  52220/200000: episode: 119, duration: 0.929s, episode steps: 206, steps per second: 222, episode reward: -205.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.835 [0.000, 2.000],  loss: 1.109126, mae: 22.798538, mean_q: -33.857516, mean_eps: 0.669932\n",
            "  52531/200000: episode: 120, duration: 1.387s, episode steps: 311, steps per second: 224, episode reward: -310.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.916 [0.000, 2.000],  loss: 1.362967, mae: 22.821930, mean_q: -33.905789, mean_eps: 0.668298\n",
            "  52915/200000: episode: 121, duration: 1.726s, episode steps: 384, steps per second: 222, episode reward: -383.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.982 [0.000, 2.000],  loss: 0.976916, mae: 22.825372, mean_q: -33.923890, mean_eps: 0.666094\n",
            "  53056/200000: episode: 122, duration: 0.655s, episode steps: 141, steps per second: 215, episode reward: -140.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.887 [0.000, 2.000],  loss: 1.355058, mae: 22.887210, mean_q: -33.961436, mean_eps: 0.664435\n",
            "  53505/200000: episode: 123, duration: 2.055s, episode steps: 449, steps per second: 219, episode reward: -448.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.958 [0.000, 2.000],  loss: 1.043543, mae: 23.107118, mean_q: -34.301756, mean_eps: 0.662560\n",
            "  54005/200000: episode: 124, duration: 2.933s, episode steps: 500, steps per second: 170, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.014 [0.000, 2.000],  loss: 0.925800, mae: 23.074817, mean_q: -34.283665, mean_eps: 0.659545\n",
            "  54152/200000: episode: 125, duration: 1.020s, episode steps: 147, steps per second: 144, episode reward: -146.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.980 [0.000, 2.000],  loss: 1.850941, mae: 23.383923, mean_q: -34.676592, mean_eps: 0.657506\n",
            "  54437/200000: episode: 126, duration: 1.569s, episode steps: 285, steps per second: 182, episode reward: -284.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.996 [0.000, 2.000],  loss: 0.830246, mae: 23.360800, mean_q: -34.711170, mean_eps: 0.656138\n",
            "  54907/200000: episode: 127, duration: 2.199s, episode steps: 470, steps per second: 214, episode reward: -469.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.015 [0.000, 2.000],  loss: 1.147274, mae: 23.325676, mean_q: -34.630220, mean_eps: 0.653744\n",
            "  55119/200000: episode: 128, duration: 1.028s, episode steps: 212, steps per second: 206, episode reward: -211.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.967 [0.000, 2.000],  loss: 0.954209, mae: 23.496385, mean_q: -34.895136, mean_eps: 0.651591\n",
            "  55530/200000: episode: 129, duration: 1.919s, episode steps: 411, steps per second: 214, episode reward: -410.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.964 [0.000, 2.000],  loss: 0.962649, mae: 23.621623, mean_q: -35.098015, mean_eps: 0.649615\n",
            "  55733/200000: episode: 130, duration: 0.922s, episode steps: 203, steps per second: 220, episode reward: -202.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.970 [0.000, 2.000],  loss: 1.226657, mae: 23.616485, mean_q: -35.087932, mean_eps: 0.647664\n",
            "  55958/200000: episode: 131, duration: 1.021s, episode steps: 225, steps per second: 220, episode reward: -224.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.942 [0.000, 2.000],  loss: 1.033188, mae: 23.627806, mean_q: -35.103733, mean_eps: 0.646309\n",
            "  56335/200000: episode: 132, duration: 1.834s, episode steps: 377, steps per second: 206, episode reward: -376.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.966 [0.000, 2.000],  loss: 1.190812, mae: 23.812694, mean_q: -35.348816, mean_eps: 0.644409\n",
            "  56588/200000: episode: 133, duration: 1.772s, episode steps: 253, steps per second: 143, episode reward: -252.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.964 [0.000, 2.000],  loss: 1.174955, mae: 23.831581, mean_q: -35.410824, mean_eps: 0.642420\n",
            "  56891/200000: episode: 134, duration: 2.101s, episode steps: 303, steps per second: 144, episode reward: -302.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.036 [0.000, 2.000],  loss: 0.904259, mae: 23.829940, mean_q: -35.410010, mean_eps: 0.640659\n",
            "  57153/200000: episode: 135, duration: 1.292s, episode steps: 262, steps per second: 203, episode reward: -261.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.905 [0.000, 2.000],  loss: 1.310647, mae: 24.026726, mean_q: -35.681244, mean_eps: 0.638861\n",
            "  57351/200000: episode: 136, duration: 0.915s, episode steps: 198, steps per second: 216, episode reward: -197.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.894 [0.000, 2.000],  loss: 1.156466, mae: 24.148306, mean_q: -35.856296, mean_eps: 0.637404\n",
            "  57590/200000: episode: 137, duration: 1.131s, episode steps: 239, steps per second: 211, episode reward: -238.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.050 [0.000, 2.000],  loss: 0.946682, mae: 24.169787, mean_q: -35.913847, mean_eps: 0.636023\n",
            "  57818/200000: episode: 138, duration: 1.044s, episode steps: 228, steps per second: 218, episode reward: -227.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.912 [0.000, 2.000],  loss: 1.144003, mae: 24.137647, mean_q: -35.837659, mean_eps: 0.634541\n",
            "  58058/200000: episode: 139, duration: 1.151s, episode steps: 240, steps per second: 209, episode reward: -239.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.942 [0.000, 2.000],  loss: 0.772924, mae: 24.174500, mean_q: -35.917574, mean_eps: 0.633059\n",
            "  58286/200000: episode: 140, duration: 1.091s, episode steps: 228, steps per second: 209, episode reward: -227.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.991 [0.000, 2.000],  loss: 1.996709, mae: 24.411000, mean_q: -36.223941, mean_eps: 0.631577\n",
            "  58465/200000: episode: 141, duration: 0.851s, episode steps: 179, steps per second: 210, episode reward: -178.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.899 [0.000, 2.000],  loss: 1.744029, mae: 24.461087, mean_q: -36.286422, mean_eps: 0.630285\n",
            "  58635/200000: episode: 142, duration: 0.800s, episode steps: 170, steps per second: 213, episode reward: -169.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.024 [0.000, 2.000],  loss: 1.282294, mae: 24.467474, mean_q: -36.340714, mean_eps: 0.629183\n",
            "  58904/200000: episode: 143, duration: 1.278s, episode steps: 269, steps per second: 210, episode reward: -268.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.007 [0.000, 2.000],  loss: 1.417351, mae: 24.440211, mean_q: -36.307608, mean_eps: 0.627803\n",
            "  59108/200000: episode: 144, duration: 1.406s, episode steps: 204, steps per second: 145, episode reward: -203.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.814 [0.000, 2.000],  loss: 1.443388, mae: 24.615996, mean_q: -36.518177, mean_eps: 0.626308\n",
            "  59334/200000: episode: 145, duration: 1.506s, episode steps: 226, steps per second: 150, episode reward: -225.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.925 [0.000, 2.000],  loss: 1.390746, mae: 24.804661, mean_q: -36.823826, mean_eps: 0.624940\n",
            "  59569/200000: episode: 146, duration: 1.346s, episode steps: 235, steps per second: 175, episode reward: -234.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.906 [0.000, 2.000],  loss: 1.197388, mae: 24.770076, mean_q: -36.804275, mean_eps: 0.623471\n",
            "  59841/200000: episode: 147, duration: 1.274s, episode steps: 272, steps per second: 214, episode reward: -271.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.846 [0.000, 2.000],  loss: 1.046260, mae: 24.791865, mean_q: -36.845282, mean_eps: 0.621862\n",
            "  60032/200000: episode: 148, duration: 0.919s, episode steps: 191, steps per second: 208, episode reward: -190.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.932 [0.000, 2.000],  loss: 0.824443, mae: 24.767321, mean_q: -36.818777, mean_eps: 0.620405\n",
            "  60188/200000: episode: 149, duration: 0.840s, episode steps: 156, steps per second: 186, episode reward: -155.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.968 [0.000, 2.000],  loss: 1.167596, mae: 25.153967, mean_q: -37.391613, mean_eps: 0.619316\n",
            "  60452/200000: episode: 150, duration: 1.275s, episode steps: 264, steps per second: 207, episode reward: -263.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.833 [0.000, 2.000],  loss: 2.065184, mae: 25.106718, mean_q: -37.251168, mean_eps: 0.617986\n",
            "  60769/200000: episode: 151, duration: 1.535s, episode steps: 317, steps per second: 207, episode reward: -316.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.965 [0.000, 2.000],  loss: 1.094726, mae: 25.151973, mean_q: -37.379291, mean_eps: 0.616137\n",
            "  61144/200000: episode: 152, duration: 1.845s, episode steps: 375, steps per second: 203, episode reward: -374.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.947 [0.000, 2.000],  loss: 1.192133, mae: 25.235633, mean_q: -37.491268, mean_eps: 0.613945\n",
            "  61326/200000: episode: 153, duration: 0.910s, episode steps: 182, steps per second: 200, episode reward: -181.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.011 [0.000, 2.000],  loss: 0.841407, mae: 25.388845, mean_q: -37.720430, mean_eps: 0.612185\n",
            "  61509/200000: episode: 154, duration: 1.069s, episode steps: 183, steps per second: 171, episode reward: -182.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.956 [0.000, 2.000],  loss: 1.690476, mae: 25.405782, mean_q: -37.729744, mean_eps: 0.611019\n",
            "  61729/200000: episode: 155, duration: 1.527s, episode steps: 220, steps per second: 144, episode reward: -219.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.895 [0.000, 2.000],  loss: 1.321503, mae: 25.453798, mean_q: -37.780076, mean_eps: 0.609740\n",
            "  62037/200000: episode: 156, duration: 1.980s, episode steps: 308, steps per second: 156, episode reward: -307.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.932 [0.000, 2.000],  loss: 1.340220, mae: 25.382812, mean_q: -37.713520, mean_eps: 0.608068\n",
            "  62271/200000: episode: 157, duration: 1.118s, episode steps: 234, steps per second: 209, episode reward: -233.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.859 [0.000, 2.000],  loss: 1.504853, mae: 25.615396, mean_q: -38.002550, mean_eps: 0.606358\n",
            "  62510/200000: episode: 158, duration: 1.158s, episode steps: 239, steps per second: 206, episode reward: -238.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.054 [0.000, 2.000],  loss: 0.965784, mae: 25.533412, mean_q: -37.931302, mean_eps: 0.604863\n",
            "  62678/200000: episode: 159, duration: 0.828s, episode steps: 168, steps per second: 203, episode reward: -167.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.970 [0.000, 2.000],  loss: 1.323970, mae: 25.556847, mean_q: -37.968985, mean_eps: 0.603571\n",
            "  63008/200000: episode: 160, duration: 1.618s, episode steps: 330, steps per second: 204, episode reward: -329.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.861 [0.000, 2.000],  loss: 1.615053, mae: 25.545924, mean_q: -37.924137, mean_eps: 0.602001\n",
            "  63253/200000: episode: 161, duration: 1.214s, episode steps: 245, steps per second: 202, episode reward: -244.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.906 [0.000, 2.000],  loss: 1.597763, mae: 25.802122, mean_q: -38.293113, mean_eps: 0.600177\n",
            "  63424/200000: episode: 162, duration: 0.882s, episode steps: 171, steps per second: 194, episode reward: -170.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.977 [0.000, 2.000],  loss: 1.331665, mae: 25.767123, mean_q: -38.274030, mean_eps: 0.598859\n",
            "  63608/200000: episode: 163, duration: 0.922s, episode steps: 184, steps per second: 200, episode reward: -183.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.832 [0.000, 2.000],  loss: 1.623669, mae: 25.749705, mean_q: -38.233144, mean_eps: 0.597745\n",
            "  63764/200000: episode: 164, duration: 0.818s, episode steps: 156, steps per second: 191, episode reward: -155.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.962 [0.000, 2.000],  loss: 1.084749, mae: 25.797822, mean_q: -38.312856, mean_eps: 0.596668\n",
            "  63883/200000: episode: 165, duration: 0.557s, episode steps: 119, steps per second: 214, episode reward: -118.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.899 [0.000, 2.000],  loss: 1.780383, mae: 25.729319, mean_q: -38.148349, mean_eps: 0.595794\n",
            "  64114/200000: episode: 166, duration: 1.406s, episode steps: 231, steps per second: 164, episode reward: -230.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.009 [0.000, 2.000],  loss: 1.411912, mae: 25.895536, mean_q: -38.434579, mean_eps: 0.594679\n",
            "  64308/200000: episode: 167, duration: 1.327s, episode steps: 194, steps per second: 146, episode reward: -193.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.067 [0.000, 2.000],  loss: 1.180377, mae: 25.999652, mean_q: -38.587382, mean_eps: 0.593337\n",
            "  64471/200000: episode: 168, duration: 1.166s, episode steps: 163, steps per second: 140, episode reward: -162.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.853 [0.000, 2.000],  loss: 1.373821, mae: 26.019167, mean_q: -38.591866, mean_eps: 0.592209\n",
            "  64648/200000: episode: 169, duration: 0.874s, episode steps: 177, steps per second: 202, episode reward: -176.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.147 [0.000, 2.000],  loss: 1.284131, mae: 26.029048, mean_q: -38.635179, mean_eps: 0.591133\n",
            "  64885/200000: episode: 170, duration: 1.138s, episode steps: 237, steps per second: 208, episode reward: -236.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.996 [0.000, 2.000],  loss: 1.287068, mae: 26.029404, mean_q: -38.650871, mean_eps: 0.589815\n",
            "  65052/200000: episode: 171, duration: 0.827s, episode steps: 167, steps per second: 202, episode reward: -166.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.964 [0.000, 2.000],  loss: 1.399016, mae: 26.100896, mean_q: -38.760673, mean_eps: 0.588536\n",
            "  65228/200000: episode: 172, duration: 0.922s, episode steps: 176, steps per second: 191, episode reward: -175.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.972 [0.000, 2.000],  loss: 1.567056, mae: 26.361099, mean_q: -39.126411, mean_eps: 0.587459\n",
            "  65488/200000: episode: 173, duration: 1.313s, episode steps: 260, steps per second: 198, episode reward: -259.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.015 [0.000, 2.000],  loss: 1.267921, mae: 26.290483, mean_q: -39.029871, mean_eps: 0.586079\n",
            "  65739/200000: episode: 174, duration: 1.309s, episode steps: 251, steps per second: 192, episode reward: -250.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.892 [0.000, 2.000],  loss: 1.773146, mae: 26.271923, mean_q: -38.982996, mean_eps: 0.584457\n",
            "  66005/200000: episode: 175, duration: 1.329s, episode steps: 266, steps per second: 200, episode reward: -265.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.914 [0.000, 2.000],  loss: 1.515001, mae: 26.268405, mean_q: -38.988964, mean_eps: 0.582811\n",
            "  66189/200000: episode: 176, duration: 0.918s, episode steps: 184, steps per second: 200, episode reward: -183.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.043 [0.000, 2.000],  loss: 1.339988, mae: 26.637127, mean_q: -39.518532, mean_eps: 0.581379\n",
            "  66455/200000: episode: 177, duration: 1.329s, episode steps: 266, steps per second: 200, episode reward: -265.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.733 [0.000, 2.000],  loss: 1.507072, mae: 26.590071, mean_q: -39.457706, mean_eps: 0.579961\n",
            "  66671/200000: episode: 178, duration: 1.572s, episode steps: 216, steps per second: 137, episode reward: -215.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.894 [0.000, 2.000],  loss: 1.763330, mae: 26.618675, mean_q: -39.485549, mean_eps: 0.578441\n",
            "  66886/200000: episode: 179, duration: 1.500s, episode steps: 215, steps per second: 143, episode reward: -214.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.963 [0.000, 2.000],  loss: 1.415251, mae: 26.601392, mean_q: -39.464014, mean_eps: 0.577073\n",
            "  67078/200000: episode: 180, duration: 1.048s, episode steps: 192, steps per second: 183, episode reward: -191.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.906 [0.000, 2.000],  loss: 1.507399, mae: 26.695566, mean_q: -39.597174, mean_eps: 0.575781\n",
            "  67236/200000: episode: 181, duration: 0.849s, episode steps: 158, steps per second: 186, episode reward: -157.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.930 [0.000, 2.000],  loss: 1.162298, mae: 26.746066, mean_q: -39.680139, mean_eps: 0.574679\n",
            "  67444/200000: episode: 182, duration: 1.247s, episode steps: 208, steps per second: 167, episode reward: -207.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.957 [0.000, 2.000],  loss: 2.015324, mae: 26.746722, mean_q: -39.645308, mean_eps: 0.573526\n",
            "  67671/200000: episode: 183, duration: 1.659s, episode steps: 227, steps per second: 137, episode reward: -226.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.930 [0.000, 2.000],  loss: 1.454097, mae: 26.719364, mean_q: -39.609644, mean_eps: 0.572145\n",
            "  67864/200000: episode: 184, duration: 1.448s, episode steps: 193, steps per second: 133, episode reward: -192.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.938 [0.000, 2.000],  loss: 1.147272, mae: 26.742324, mean_q: -39.669995, mean_eps: 0.570815\n",
            "  68013/200000: episode: 185, duration: 0.747s, episode steps: 149, steps per second: 199, episode reward: -148.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.906 [0.000, 2.000],  loss: 2.559959, mae: 26.713654, mean_q: -39.538400, mean_eps: 0.569726\n",
            "  68256/200000: episode: 186, duration: 1.229s, episode steps: 243, steps per second: 198, episode reward: -242.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.914 [0.000, 2.000],  loss: 1.432213, mae: 27.023736, mean_q: -40.079051, mean_eps: 0.568485\n",
            "  68395/200000: episode: 187, duration: 0.685s, episode steps: 139, steps per second: 203, episode reward: -138.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.942 [0.000, 2.000],  loss: 1.184007, mae: 26.921454, mean_q: -39.917731, mean_eps: 0.567281\n",
            "  68568/200000: episode: 188, duration: 0.866s, episode steps: 173, steps per second: 200, episode reward: -172.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.919 [0.000, 2.000],  loss: 1.781440, mae: 26.915047, mean_q: -39.862688, mean_eps: 0.566293\n",
            "  68893/200000: episode: 189, duration: 2.109s, episode steps: 325, steps per second: 154, episode reward: -324.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.006 [0.000, 2.000],  loss: 1.241983, mae: 26.923169, mean_q: -39.925378, mean_eps: 0.564710\n",
            "  69031/200000: episode: 190, duration: 0.996s, episode steps: 138, steps per second: 139, episode reward: -137.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 1.531057, mae: 26.840858, mean_q: -39.764004, mean_eps: 0.563241\n",
            "  69240/200000: episode: 191, duration: 1.411s, episode steps: 209, steps per second: 148, episode reward: -208.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.038 [0.000, 2.000],  loss: 1.167333, mae: 27.056013, mean_q: -40.104569, mean_eps: 0.562151\n",
            "  69445/200000: episode: 192, duration: 1.041s, episode steps: 205, steps per second: 197, episode reward: -204.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.990 [0.000, 2.000],  loss: 1.481349, mae: 27.047194, mean_q: -40.093934, mean_eps: 0.560834\n",
            "  69615/200000: episode: 193, duration: 0.878s, episode steps: 170, steps per second: 194, episode reward: -169.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.865 [0.000, 2.000],  loss: 1.860447, mae: 27.135688, mean_q: -40.190217, mean_eps: 0.559643\n",
            "  69811/200000: episode: 194, duration: 1.003s, episode steps: 196, steps per second: 195, episode reward: -195.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.990 [0.000, 2.000],  loss: 1.560554, mae: 27.114857, mean_q: -40.164983, mean_eps: 0.558491\n",
            "  69932/200000: episode: 195, duration: 0.634s, episode steps: 121, steps per second: 191, episode reward: -120.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.934 [0.000, 2.000],  loss: 1.564968, mae: 27.122152, mean_q: -40.204219, mean_eps: 0.557490\n",
            "  70136/200000: episode: 196, duration: 0.998s, episode steps: 204, steps per second: 204, episode reward: -203.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.936 [0.000, 2.000],  loss: 1.341663, mae: 27.335132, mean_q: -40.516868, mean_eps: 0.556464\n",
            "  70316/200000: episode: 197, duration: 0.911s, episode steps: 180, steps per second: 198, episode reward: -179.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.994 [0.000, 2.000],  loss: 1.276255, mae: 27.304002, mean_q: -40.485395, mean_eps: 0.555248\n",
            "  70521/200000: episode: 198, duration: 1.018s, episode steps: 205, steps per second: 201, episode reward: -204.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.820 [0.000, 2.000],  loss: 1.575075, mae: 27.374967, mean_q: -40.573795, mean_eps: 0.554019\n",
            "  70666/200000: episode: 199, duration: 0.705s, episode steps: 145, steps per second: 206, episode reward: -144.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.876 [0.000, 2.000],  loss: 1.289920, mae: 27.330977, mean_q: -40.496876, mean_eps: 0.552905\n",
            "  70873/200000: episode: 200, duration: 1.002s, episode steps: 207, steps per second: 207, episode reward: -206.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.879 [0.000, 2.000],  loss: 1.534950, mae: 27.381377, mean_q: -40.591354, mean_eps: 0.551790\n",
            "  71055/200000: episode: 201, duration: 0.922s, episode steps: 182, steps per second: 197, episode reward: -181.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.802 [0.000, 2.000],  loss: 1.616261, mae: 27.440410, mean_q: -40.681721, mean_eps: 0.550561\n",
            "  71221/200000: episode: 202, duration: 1.042s, episode steps: 166, steps per second: 159, episode reward: -165.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.886 [0.000, 2.000],  loss: 0.911595, mae: 27.574391, mean_q: -40.899346, mean_eps: 0.549459\n",
            "  71481/200000: episode: 203, duration: 1.805s, episode steps: 260, steps per second: 144, episode reward: -259.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.865 [0.000, 2.000],  loss: 1.104880, mae: 27.547629, mean_q: -40.838962, mean_eps: 0.548104\n",
            "  71637/200000: episode: 204, duration: 1.190s, episode steps: 156, steps per second: 131, episode reward: -155.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.096 [0.000, 2.000],  loss: 1.310740, mae: 27.618530, mean_q: -40.964465, mean_eps: 0.546787\n",
            "  71827/200000: episode: 205, duration: 1.010s, episode steps: 190, steps per second: 188, episode reward: -189.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.968 [0.000, 2.000],  loss: 1.715415, mae: 27.560822, mean_q: -40.871333, mean_eps: 0.545697\n",
            "  72050/200000: episode: 206, duration: 1.219s, episode steps: 223, steps per second: 183, episode reward: -222.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.874 [0.000, 2.000],  loss: 1.431642, mae: 27.532339, mean_q: -40.799950, mean_eps: 0.544393\n",
            "  72233/200000: episode: 207, duration: 0.965s, episode steps: 183, steps per second: 190, episode reward: -182.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.913 [0.000, 2.000],  loss: 1.344021, mae: 27.804575, mean_q: -41.217845, mean_eps: 0.543101\n",
            "  72445/200000: episode: 208, duration: 1.224s, episode steps: 212, steps per second: 173, episode reward: -211.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.759 [0.000, 2.000],  loss: 1.865513, mae: 27.773519, mean_q: -41.120536, mean_eps: 0.541847\n",
            "  72686/200000: episode: 209, duration: 1.309s, episode steps: 241, steps per second: 184, episode reward: -240.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.797 [0.000, 2.000],  loss: 1.775317, mae: 27.766762, mean_q: -41.118719, mean_eps: 0.540415\n",
            "  72841/200000: episode: 210, duration: 0.841s, episode steps: 155, steps per second: 184, episode reward: -154.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.897 [0.000, 2.000],  loss: 1.304209, mae: 27.853776, mean_q: -41.284674, mean_eps: 0.539161\n",
            "  73020/200000: episode: 211, duration: 0.934s, episode steps: 179, steps per second: 192, episode reward: -178.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.905 [0.000, 2.000],  loss: 1.643097, mae: 27.834072, mean_q: -41.262614, mean_eps: 0.538110\n",
            "  73251/200000: episode: 212, duration: 1.268s, episode steps: 231, steps per second: 182, episode reward: -230.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.870 [0.000, 2.000],  loss: 1.700104, mae: 28.097151, mean_q: -41.647908, mean_eps: 0.536818\n",
            "  73519/200000: episode: 213, duration: 1.570s, episode steps: 268, steps per second: 171, episode reward: -267.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.765 [0.000, 2.000],  loss: 1.548293, mae: 28.086858, mean_q: -41.624387, mean_eps: 0.535235\n",
            "  73705/200000: episode: 214, duration: 1.432s, episode steps: 186, steps per second: 130, episode reward: -185.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.930 [0.000, 2.000],  loss: 1.854684, mae: 28.027584, mean_q: -41.540206, mean_eps: 0.533791\n",
            "  73925/200000: episode: 215, duration: 1.609s, episode steps: 220, steps per second: 137, episode reward: -219.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.823 [0.000, 2.000],  loss: 0.876143, mae: 28.141202, mean_q: -41.719109, mean_eps: 0.532499\n",
            "  74117/200000: episode: 216, duration: 1.006s, episode steps: 192, steps per second: 191, episode reward: -191.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.859 [0.000, 2.000],  loss: 1.519225, mae: 28.150919, mean_q: -41.700450, mean_eps: 0.531194\n",
            "  74279/200000: episode: 217, duration: 0.844s, episode steps: 162, steps per second: 192, episode reward: -161.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.951 [0.000, 2.000],  loss: 2.186640, mae: 28.261168, mean_q: -41.869038, mean_eps: 0.530079\n",
            "  74478/200000: episode: 218, duration: 1.042s, episode steps: 199, steps per second: 191, episode reward: -198.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.910 [0.000, 2.000],  loss: 1.403400, mae: 28.343234, mean_q: -41.988367, mean_eps: 0.528939\n",
            "  74612/200000: episode: 219, duration: 0.714s, episode steps: 134, steps per second: 188, episode reward: -133.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.060 [0.000, 2.000],  loss: 1.333390, mae: 28.280736, mean_q: -41.920661, mean_eps: 0.527888\n",
            "  74745/200000: episode: 220, duration: 0.661s, episode steps: 133, steps per second: 201, episode reward: -132.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.038 [0.000, 2.000],  loss: 1.065364, mae: 28.258687, mean_q: -41.901196, mean_eps: 0.527039\n",
            "  74937/200000: episode: 221, duration: 0.963s, episode steps: 192, steps per second: 199, episode reward: -191.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.807 [0.000, 2.000],  loss: 1.777118, mae: 28.287140, mean_q: -41.916936, mean_eps: 0.526001\n",
            "  75237/200000: episode: 222, duration: 1.488s, episode steps: 300, steps per second: 202, episode reward: -299.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.123 [0.000, 2.000],  loss: 1.967585, mae: 28.383914, mean_q: -42.027317, mean_eps: 0.524443\n",
            "  75391/200000: episode: 223, duration: 0.784s, episode steps: 154, steps per second: 196, episode reward: -153.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.981 [0.000, 2.000],  loss: 0.948890, mae: 28.466446, mean_q: -42.184214, mean_eps: 0.523011\n",
            "  75529/200000: episode: 224, duration: 0.723s, episode steps: 138, steps per second: 191, episode reward: -137.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.942 [0.000, 2.000],  loss: 1.143115, mae: 28.513438, mean_q: -42.244884, mean_eps: 0.522087\n",
            "  75694/200000: episode: 225, duration: 0.873s, episode steps: 165, steps per second: 189, episode reward: -164.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.939 [0.000, 2.000],  loss: 2.118404, mae: 28.400302, mean_q: -42.052519, mean_eps: 0.521124\n",
            "  75849/200000: episode: 226, duration: 0.987s, episode steps: 155, steps per second: 157, episode reward: -154.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.890 [0.000, 2.000],  loss: 1.402112, mae: 28.453119, mean_q: -42.116284, mean_eps: 0.520111\n",
            "  76010/200000: episode: 227, duration: 1.172s, episode steps: 161, steps per second: 137, episode reward: -160.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.894 [0.000, 2.000],  loss: 1.580052, mae: 28.373007, mean_q: -41.967964, mean_eps: 0.519110\n",
            "  76201/200000: episode: 228, duration: 1.371s, episode steps: 191, steps per second: 139, episode reward: -190.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.052 [0.000, 2.000],  loss: 1.884007, mae: 28.636224, mean_q: -42.403964, mean_eps: 0.517995\n",
            "  76350/200000: episode: 229, duration: 0.978s, episode steps: 149, steps per second: 152, episode reward: -148.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.826 [0.000, 2.000],  loss: 1.677860, mae: 28.732202, mean_q: -42.536214, mean_eps: 0.516919\n",
            "  76631/200000: episode: 230, duration: 1.552s, episode steps: 281, steps per second: 181, episode reward: -280.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 1.761070, mae: 28.586786, mean_q: -42.344405, mean_eps: 0.515563\n",
            "  76770/200000: episode: 231, duration: 0.783s, episode steps: 139, steps per second: 178, episode reward: -138.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.072 [0.000, 2.000],  loss: 1.530548, mae: 28.541119, mean_q: -42.285155, mean_eps: 0.514233\n",
            "  76930/200000: episode: 232, duration: 0.875s, episode steps: 160, steps per second: 183, episode reward: -159.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.881 [0.000, 2.000],  loss: 1.639574, mae: 28.626394, mean_q: -42.388527, mean_eps: 0.513283\n",
            "  77072/200000: episode: 233, duration: 0.743s, episode steps: 142, steps per second: 191, episode reward: -141.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.951 [0.000, 2.000],  loss: 1.232123, mae: 28.672221, mean_q: -42.462142, mean_eps: 0.512333\n",
            "  77218/200000: episode: 234, duration: 0.746s, episode steps: 146, steps per second: 196, episode reward: -145.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.904 [0.000, 2.000],  loss: 1.986259, mae: 28.868246, mean_q: -42.732132, mean_eps: 0.511421\n",
            "  77401/200000: episode: 235, duration: 0.936s, episode steps: 183, steps per second: 195, episode reward: -182.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.055 [0.000, 2.000],  loss: 1.292074, mae: 28.772757, mean_q: -42.608490, mean_eps: 0.510370\n",
            "  77571/200000: episode: 236, duration: 0.824s, episode steps: 170, steps per second: 206, episode reward: -169.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.894 [0.000, 2.000],  loss: 1.460930, mae: 28.756142, mean_q: -42.552316, mean_eps: 0.509255\n",
            "  77761/200000: episode: 237, duration: 1.019s, episode steps: 190, steps per second: 186, episode reward: -189.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.958 [0.000, 2.000],  loss: 1.924660, mae: 28.748793, mean_q: -42.537424, mean_eps: 0.508115\n",
            "  77916/200000: episode: 238, duration: 0.795s, episode steps: 155, steps per second: 195, episode reward: -154.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.968 [0.000, 2.000],  loss: 1.126627, mae: 28.779422, mean_q: -42.608075, mean_eps: 0.507026\n",
            "  78053/200000: episode: 239, duration: 0.761s, episode steps: 137, steps per second: 180, episode reward: -136.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.883 [0.000, 2.000],  loss: 2.140841, mae: 28.845375, mean_q: -42.679687, mean_eps: 0.506101\n",
            "  78229/200000: episode: 240, duration: 1.044s, episode steps: 176, steps per second: 169, episode reward: -175.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.943 [0.000, 2.000],  loss: 0.818417, mae: 28.977962, mean_q: -42.947184, mean_eps: 0.505101\n",
            "  78372/200000: episode: 241, duration: 1.104s, episode steps: 143, steps per second: 130, episode reward: -142.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.972 [0.000, 2.000],  loss: 1.292023, mae: 29.003284, mean_q: -42.979376, mean_eps: 0.504100\n",
            "  78520/200000: episode: 242, duration: 1.133s, episode steps: 148, steps per second: 131, episode reward: -147.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.966 [0.000, 2.000],  loss: 2.436150, mae: 28.924905, mean_q: -42.801459, mean_eps: 0.503188\n",
            "  78727/200000: episode: 243, duration: 1.429s, episode steps: 207, steps per second: 145, episode reward: -206.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.068 [0.000, 2.000],  loss: 1.176533, mae: 29.033765, mean_q: -43.016485, mean_eps: 0.502061\n",
            "  78867/200000: episode: 244, duration: 0.779s, episode steps: 140, steps per second: 180, episode reward: -139.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.014 [0.000, 2.000],  loss: 1.653022, mae: 28.960923, mean_q: -42.897227, mean_eps: 0.500959\n",
            "  79112/200000: episode: 245, duration: 1.370s, episode steps: 245, steps per second: 179, episode reward: -244.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.159 [0.000, 2.000],  loss: 1.184889, mae: 28.974608, mean_q: -42.916430, mean_eps: 0.499743\n",
            "  79338/200000: episode: 246, duration: 1.289s, episode steps: 226, steps per second: 175, episode reward: -225.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.084 [0.000, 2.000],  loss: 1.844317, mae: 29.111344, mean_q: -43.112219, mean_eps: 0.498248\n",
            "  79460/200000: episode: 247, duration: 0.696s, episode steps: 122, steps per second: 175, episode reward: -121.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.090 [0.000, 2.000],  loss: 1.146706, mae: 29.081413, mean_q: -43.083569, mean_eps: 0.497146\n",
            "  79565/200000: episode: 248, duration: 0.635s, episode steps: 105, steps per second: 165, episode reward: -104.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.867 [0.000, 2.000],  loss: 1.546883, mae: 29.020283, mean_q: -42.976512, mean_eps: 0.496424\n",
            "  79685/200000: episode: 249, duration: 0.726s, episode steps: 120, steps per second: 165, episode reward: -119.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.983 [0.000, 2.000],  loss: 1.556829, mae: 29.123544, mean_q: -43.083871, mean_eps: 0.495702\n",
            "  79818/200000: episode: 250, duration: 0.713s, episode steps: 133, steps per second: 187, episode reward: -132.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.053 [0.000, 2.000],  loss: 1.401326, mae: 29.031089, mean_q: -42.978069, mean_eps: 0.494904\n",
            "  79999/200000: episode: 251, duration: 0.972s, episode steps: 181, steps per second: 186, episode reward: -180.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.050 [0.000, 2.000],  loss: 2.812253, mae: 29.083810, mean_q: -43.013318, mean_eps: 0.493916\n",
            "  80136/200000: episode: 252, duration: 0.743s, episode steps: 137, steps per second: 185, episode reward: -136.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.088 [0.000, 2.000],  loss: 1.311473, mae: 29.052139, mean_q: -43.021585, mean_eps: 0.492915\n",
            "  80301/200000: episode: 253, duration: 0.894s, episode steps: 165, steps per second: 185, episode reward: -164.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 1.666160, mae: 29.074612, mean_q: -43.002405, mean_eps: 0.491953\n",
            "  80416/200000: episode: 254, duration: 0.663s, episode steps: 115, steps per second: 174, episode reward: -114.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.983 [0.000, 2.000],  loss: 1.970939, mae: 29.123411, mean_q: -43.081535, mean_eps: 0.491066\n",
            "  80589/200000: episode: 255, duration: 1.300s, episode steps: 173, steps per second: 133, episode reward: -172.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.023 [0.000, 2.000],  loss: 1.337816, mae: 29.076077, mean_q: -43.043711, mean_eps: 0.490154\n",
            "  80788/200000: episode: 256, duration: 1.504s, episode steps: 199, steps per second: 132, episode reward: -198.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.045 [0.000, 2.000],  loss: 1.012950, mae: 29.091212, mean_q: -43.062375, mean_eps: 0.488976\n",
            "  80939/200000: episode: 257, duration: 1.056s, episode steps: 151, steps per second: 143, episode reward: -150.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.066 [0.000, 2.000],  loss: 2.838752, mae: 29.205420, mean_q: -43.192532, mean_eps: 0.487874\n",
            "  81125/200000: episode: 258, duration: 1.066s, episode steps: 186, steps per second: 175, episode reward: -185.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.962 [0.000, 2.000],  loss: 2.109193, mae: 29.120435, mean_q: -43.076881, mean_eps: 0.486797\n",
            "  81302/200000: episode: 259, duration: 1.104s, episode steps: 177, steps per second: 160, episode reward: -176.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.040 [0.000, 2.000],  loss: 1.616260, mae: 29.159105, mean_q: -43.146715, mean_eps: 0.485645\n",
            "  81465/200000: episode: 260, duration: 0.890s, episode steps: 163, steps per second: 183, episode reward: -162.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.896 [0.000, 2.000],  loss: 1.474806, mae: 29.156083, mean_q: -43.163009, mean_eps: 0.484568\n",
            "  81647/200000: episode: 261, duration: 0.974s, episode steps: 182, steps per second: 187, episode reward: -181.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.901 [0.000, 2.000],  loss: 1.387762, mae: 29.102161, mean_q: -43.081249, mean_eps: 0.483479\n",
            "  81830/200000: episode: 262, duration: 1.001s, episode steps: 183, steps per second: 183, episode reward: -182.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.951 [0.000, 2.000],  loss: 1.517274, mae: 29.148131, mean_q: -43.168406, mean_eps: 0.482326\n",
            "  82330/200000: episode: 263, duration: 2.838s, episode steps: 500, steps per second: 176, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.620 [0.000, 2.000],  loss: 1.793401, mae: 29.104323, mean_q: -43.061956, mean_eps: 0.480160\n",
            "  82484/200000: episode: 264, duration: 0.842s, episode steps: 154, steps per second: 183, episode reward: -153.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.955 [0.000, 2.000],  loss: 0.934540, mae: 29.114641, mean_q: -43.094255, mean_eps: 0.478095\n",
            "  82624/200000: episode: 265, duration: 0.794s, episode steps: 140, steps per second: 176, episode reward: -139.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.771 [0.000, 2.000],  loss: 1.067587, mae: 29.089781, mean_q: -43.043380, mean_eps: 0.477171\n",
            "  82809/200000: episode: 266, duration: 1.439s, episode steps: 185, steps per second: 129, episode reward: -184.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.876 [0.000, 2.000],  loss: 1.278789, mae: 29.026918, mean_q: -42.930866, mean_eps: 0.476132\n",
            "  82943/200000: episode: 267, duration: 1.053s, episode steps: 134, steps per second: 127, episode reward: -133.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.940 [0.000, 2.000],  loss: 2.042716, mae: 28.998112, mean_q: -42.890066, mean_eps: 0.475119\n",
            "  83120/200000: episode: 268, duration: 1.256s, episode steps: 177, steps per second: 141, episode reward: -176.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.802 [0.000, 2.000],  loss: 1.260099, mae: 29.147186, mean_q: -43.122063, mean_eps: 0.474143\n",
            "  83244/200000: episode: 269, duration: 0.697s, episode steps: 124, steps per second: 178, episode reward: -123.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.911 [0.000, 2.000],  loss: 1.654664, mae: 29.255177, mean_q: -43.244549, mean_eps: 0.473193\n",
            "  83395/200000: episode: 270, duration: 0.813s, episode steps: 151, steps per second: 186, episode reward: -150.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.020 [0.000, 2.000],  loss: 1.382543, mae: 29.223153, mean_q: -43.230258, mean_eps: 0.472319\n",
            "  83573/200000: episode: 271, duration: 0.983s, episode steps: 178, steps per second: 181, episode reward: -177.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.983 [0.000, 2.000],  loss: 0.894579, mae: 29.307579, mean_q: -43.388685, mean_eps: 0.471268\n",
            "  83728/200000: episode: 272, duration: 0.844s, episode steps: 155, steps per second: 184, episode reward: -154.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.052 [0.000, 2.000],  loss: 0.877220, mae: 29.255028, mean_q: -43.342180, mean_eps: 0.470217\n",
            "  83854/200000: episode: 273, duration: 0.691s, episode steps: 126, steps per second: 182, episode reward: -125.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.897 [0.000, 2.000],  loss: 1.215478, mae: 29.128490, mean_q: -43.117167, mean_eps: 0.469330\n",
            "  84029/200000: episode: 274, duration: 0.955s, episode steps: 175, steps per second: 183, episode reward: -174.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.891 [0.000, 2.000],  loss: 1.701761, mae: 29.204612, mean_q: -43.177786, mean_eps: 0.468367\n",
            "  84165/200000: episode: 275, duration: 0.715s, episode steps: 136, steps per second: 190, episode reward: -135.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.831 [0.000, 2.000],  loss: 1.349413, mae: 29.233847, mean_q: -43.220137, mean_eps: 0.467379\n",
            "  84310/200000: episode: 276, duration: 0.803s, episode steps: 145, steps per second: 180, episode reward: -144.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.848 [0.000, 2.000],  loss: 1.806564, mae: 29.296553, mean_q: -43.341653, mean_eps: 0.466493\n",
            "  84473/200000: episode: 277, duration: 0.964s, episode steps: 163, steps per second: 169, episode reward: -162.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.037 [0.000, 2.000],  loss: 1.325420, mae: 29.412857, mean_q: -43.570810, mean_eps: 0.465517\n",
            "  84577/200000: episode: 278, duration: 0.631s, episode steps: 104, steps per second: 165, episode reward: -103.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.760 [0.000, 2.000],  loss: 1.855894, mae: 29.299442, mean_q: -43.358990, mean_eps: 0.464669\n",
            "  84745/200000: episode: 279, duration: 0.958s, episode steps: 168, steps per second: 175, episode reward: -167.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.798 [0.000, 2.000],  loss: 2.134840, mae: 29.263430, mean_q: -43.269072, mean_eps: 0.463807\n",
            "  85018/200000: episode: 280, duration: 1.973s, episode steps: 273, steps per second: 138, episode reward: -272.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.707 [0.000, 2.000],  loss: 2.398688, mae: 29.242295, mean_q: -43.239848, mean_eps: 0.462414\n",
            "  85194/200000: episode: 281, duration: 1.408s, episode steps: 176, steps per second: 125, episode reward: -175.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.898 [0.000, 2.000],  loss: 1.617126, mae: 29.341675, mean_q: -43.404578, mean_eps: 0.460995\n",
            "  85379/200000: episode: 282, duration: 1.209s, episode steps: 185, steps per second: 153, episode reward: -184.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.773 [0.000, 2.000],  loss: 1.425422, mae: 29.397144, mean_q: -43.524752, mean_eps: 0.459855\n",
            "  85517/200000: episode: 283, duration: 0.777s, episode steps: 138, steps per second: 178, episode reward: -137.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.051 [0.000, 2.000],  loss: 1.602341, mae: 29.351710, mean_q: -43.430638, mean_eps: 0.458829\n",
            "  85662/200000: episode: 284, duration: 0.809s, episode steps: 145, steps per second: 179, episode reward: -144.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.793 [0.000, 2.000],  loss: 0.977351, mae: 29.413666, mean_q: -43.547967, mean_eps: 0.457930\n",
            "  85783/200000: episode: 285, duration: 0.696s, episode steps: 121, steps per second: 174, episode reward: -120.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.884 [0.000, 2.000],  loss: 2.359590, mae: 29.496898, mean_q: -43.628448, mean_eps: 0.457094\n",
            "  85941/200000: episode: 286, duration: 0.896s, episode steps: 158, steps per second: 176, episode reward: -157.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.975 [0.000, 2.000],  loss: 1.017653, mae: 29.360161, mean_q: -43.455310, mean_eps: 0.456207\n",
            "  86099/200000: episode: 287, duration: 0.852s, episode steps: 158, steps per second: 185, episode reward: -157.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.880 [0.000, 2.000],  loss: 1.472059, mae: 29.439536, mean_q: -43.521852, mean_eps: 0.455207\n",
            "  86232/200000: episode: 288, duration: 0.821s, episode steps: 133, steps per second: 162, episode reward: -132.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.714 [0.000, 2.000],  loss: 1.226598, mae: 29.477697, mean_q: -43.593358, mean_eps: 0.454295\n",
            "  86374/200000: episode: 289, duration: 0.908s, episode steps: 142, steps per second: 156, episode reward: -141.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.838 [0.000, 2.000],  loss: 2.181009, mae: 29.402817, mean_q: -43.462099, mean_eps: 0.453421\n",
            "  86501/200000: episode: 290, duration: 0.864s, episode steps: 127, steps per second: 147, episode reward: -126.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 2.152490, mae: 29.358082, mean_q: -43.393654, mean_eps: 0.452559\n",
            "  86621/200000: episode: 291, duration: 0.777s, episode steps: 120, steps per second: 154, episode reward: -119.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.917 [0.000, 2.000],  loss: 2.220588, mae: 29.481521, mean_q: -43.597361, mean_eps: 0.451774\n",
            "  86777/200000: episode: 292, duration: 0.903s, episode steps: 156, steps per second: 173, episode reward: -155.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.885 [0.000, 2.000],  loss: 1.955280, mae: 29.448226, mean_q: -43.544285, mean_eps: 0.450900\n",
            "  86954/200000: episode: 293, duration: 1.033s, episode steps: 177, steps per second: 171, episode reward: -176.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.023 [0.000, 2.000],  loss: 1.169591, mae: 29.431426, mean_q: -43.553628, mean_eps: 0.449849\n",
            "  87112/200000: episode: 294, duration: 1.256s, episode steps: 158, steps per second: 126, episode reward: -157.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.937 [0.000, 2.000],  loss: 2.053797, mae: 29.447715, mean_q: -43.509456, mean_eps: 0.448797\n",
            "  87600/200000: episode: 295, duration: 3.502s, episode steps: 488, steps per second: 139, episode reward: -487.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.631 [0.000, 2.000],  loss: 2.075657, mae: 29.523407, mean_q: -43.644557, mean_eps: 0.446758\n",
            "  87752/200000: episode: 296, duration: 0.929s, episode steps: 152, steps per second: 164, episode reward: -151.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.914 [0.000, 2.000],  loss: 1.490538, mae: 29.472159, mean_q: -43.609772, mean_eps: 0.444731\n",
            "  87879/200000: episode: 297, duration: 0.762s, episode steps: 127, steps per second: 167, episode reward: -126.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.819 [0.000, 2.000],  loss: 1.003379, mae: 29.407519, mean_q: -43.510593, mean_eps: 0.443845\n",
            "  88010/200000: episode: 298, duration: 0.794s, episode steps: 131, steps per second: 165, episode reward: -130.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.893 [0.000, 2.000],  loss: 1.507479, mae: 29.504109, mean_q: -43.619692, mean_eps: 0.443021\n",
            "  88169/200000: episode: 299, duration: 0.934s, episode steps: 159, steps per second: 170, episode reward: -158.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.862 [0.000, 2.000],  loss: 0.864579, mae: 29.623591, mean_q: -43.811121, mean_eps: 0.442097\n",
            "  88337/200000: episode: 300, duration: 0.972s, episode steps: 168, steps per second: 173, episode reward: -167.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.083 [0.000, 2.000],  loss: 1.344615, mae: 29.637470, mean_q: -43.844059, mean_eps: 0.441058\n",
            "  88528/200000: episode: 301, duration: 1.103s, episode steps: 191, steps per second: 173, episode reward: -190.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.848 [0.000, 2.000],  loss: 1.886362, mae: 29.499236, mean_q: -43.604535, mean_eps: 0.439931\n",
            "  88664/200000: episode: 302, duration: 0.765s, episode steps: 136, steps per second: 178, episode reward: -135.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.912 [0.000, 2.000],  loss: 1.686632, mae: 29.573977, mean_q: -43.706168, mean_eps: 0.438905\n",
            "  88801/200000: episode: 303, duration: 0.773s, episode steps: 137, steps per second: 177, episode reward: -136.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.971 [0.000, 2.000],  loss: 1.756720, mae: 29.685199, mean_q: -43.907095, mean_eps: 0.438031\n",
            "  88918/200000: episode: 304, duration: 0.677s, episode steps: 117, steps per second: 173, episode reward: -116.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.026 [0.000, 2.000],  loss: 2.232347, mae: 29.515526, mean_q: -43.624053, mean_eps: 0.437220\n",
            "  89048/200000: episode: 305, duration: 0.768s, episode steps: 130, steps per second: 169, episode reward: -129.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.792 [0.000, 2.000],  loss: 2.584181, mae: 29.516288, mean_q: -43.609197, mean_eps: 0.436447\n",
            "  89195/200000: episode: 306, duration: 1.108s, episode steps: 147, steps per second: 133, episode reward: -146.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.918 [0.000, 2.000],  loss: 1.826981, mae: 29.820912, mean_q: -44.099290, mean_eps: 0.435573\n",
            "  89328/200000: episode: 307, duration: 1.118s, episode steps: 133, steps per second: 119, episode reward: -132.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.030 [0.000, 2.000],  loss: 0.985853, mae: 29.725956, mean_q: -43.955747, mean_eps: 0.434687\n",
            "  89462/200000: episode: 308, duration: 1.075s, episode steps: 134, steps per second: 125, episode reward: -133.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.015 [0.000, 2.000],  loss: 2.136740, mae: 29.757613, mean_q: -44.017578, mean_eps: 0.433838\n",
            "  89593/200000: episode: 309, duration: 0.869s, episode steps: 131, steps per second: 151, episode reward: -130.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.786 [0.000, 2.000],  loss: 1.387047, mae: 29.731822, mean_q: -43.957128, mean_eps: 0.432989\n",
            "  89762/200000: episode: 310, duration: 0.917s, episode steps: 169, steps per second: 184, episode reward: -168.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.899 [0.000, 2.000],  loss: 1.299082, mae: 29.803397, mean_q: -44.086429, mean_eps: 0.432039\n",
            "  89904/200000: episode: 311, duration: 0.811s, episode steps: 142, steps per second: 175, episode reward: -141.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.007 [0.000, 2.000],  loss: 1.600214, mae: 29.625178, mean_q: -43.814016, mean_eps: 0.431064\n",
            "  90007/200000: episode: 312, duration: 0.623s, episode steps: 103, steps per second: 165, episode reward: -102.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.951 [0.000, 2.000],  loss: 1.287808, mae: 29.636777, mean_q: -43.849264, mean_eps: 0.430291\n",
            "  90120/200000: episode: 313, duration: 0.670s, episode steps: 113, steps per second: 169, episode reward: -112.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.867 [0.000, 2.000],  loss: 1.004429, mae: 29.843370, mean_q: -44.190656, mean_eps: 0.429607\n",
            "  90226/200000: episode: 314, duration: 0.596s, episode steps: 106, steps per second: 178, episode reward: -105.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.981 [0.000, 2.000],  loss: 1.831966, mae: 29.824491, mean_q: -44.088801, mean_eps: 0.428911\n",
            "  90490/200000: episode: 315, duration: 1.470s, episode steps: 264, steps per second: 180, episode reward: -263.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.667 [0.000, 2.000],  loss: 2.038862, mae: 29.767011, mean_q: -44.005030, mean_eps: 0.427733\n",
            "  90609/200000: episode: 316, duration: 0.677s, episode steps: 119, steps per second: 176, episode reward: -118.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.933 [0.000, 2.000],  loss: 1.635968, mae: 29.741326, mean_q: -43.995940, mean_eps: 0.426517\n",
            "  90751/200000: episode: 317, duration: 0.828s, episode steps: 142, steps per second: 171, episode reward: -141.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.993 [0.000, 2.000],  loss: 0.824412, mae: 29.682815, mean_q: -43.911312, mean_eps: 0.425693\n",
            "  90881/200000: episode: 318, duration: 0.777s, episode steps: 130, steps per second: 167, episode reward: -129.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.792 [0.000, 2.000],  loss: 1.479274, mae: 29.739759, mean_q: -43.976560, mean_eps: 0.424832\n",
            "  91017/200000: episode: 319, duration: 0.850s, episode steps: 136, steps per second: 160, episode reward: -135.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.904 [0.000, 2.000],  loss: 1.380240, mae: 29.714628, mean_q: -43.943062, mean_eps: 0.423983\n",
            "  91228/200000: episode: 320, duration: 1.375s, episode steps: 211, steps per second: 153, episode reward: -210.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.033 [0.000, 2.000],  loss: 1.755313, mae: 29.820683, mean_q: -44.077403, mean_eps: 0.422894\n",
            "  91361/200000: episode: 321, duration: 1.176s, episode steps: 133, steps per second: 113, episode reward: -132.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.015 [0.000, 2.000],  loss: 0.926345, mae: 29.887378, mean_q: -44.189986, mean_eps: 0.421805\n",
            "  91462/200000: episode: 322, duration: 0.832s, episode steps: 101, steps per second: 121, episode reward: -100.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.010 [0.000, 2.000],  loss: 2.677921, mae: 29.872066, mean_q: -44.089374, mean_eps: 0.421057\n",
            "  91588/200000: episode: 323, duration: 1.169s, episode steps: 126, steps per second: 108, episode reward: -125.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.095 [0.000, 2.000],  loss: 1.238411, mae: 29.947217, mean_q: -44.282493, mean_eps: 0.420348\n",
            "  91686/200000: episode: 324, duration: 0.706s, episode steps:  98, steps per second: 139, episode reward: -97.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 1.935732, mae: 29.960003, mean_q: -44.278407, mean_eps: 0.419639\n",
            "  91813/200000: episode: 325, duration: 0.843s, episode steps: 127, steps per second: 151, episode reward: -126.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.992 [0.000, 2.000],  loss: 1.458835, mae: 29.800426, mean_q: -44.048690, mean_eps: 0.418917\n",
            "  91960/200000: episode: 326, duration: 1.038s, episode steps: 147, steps per second: 142, episode reward: -146.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.973 [0.000, 2.000],  loss: 2.589227, mae: 29.889821, mean_q: -44.157491, mean_eps: 0.418055\n",
            "  92097/200000: episode: 327, duration: 0.866s, episode steps: 137, steps per second: 158, episode reward: -136.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.920 [0.000, 2.000],  loss: 2.286565, mae: 29.867790, mean_q: -44.125797, mean_eps: 0.417156\n",
            "  92240/200000: episode: 328, duration: 0.892s, episode steps: 143, steps per second: 160, episode reward: -142.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.154 [0.000, 2.000],  loss: 1.733043, mae: 29.867662, mean_q: -44.130660, mean_eps: 0.416269\n",
            "  92350/200000: episode: 329, duration: 0.666s, episode steps: 110, steps per second: 165, episode reward: -109.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.909 [0.000, 2.000],  loss: 1.661485, mae: 29.796406, mean_q: -44.008432, mean_eps: 0.415471\n",
            "  92652/200000: episode: 330, duration: 1.761s, episode steps: 302, steps per second: 171, episode reward: -301.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.086 [0.000, 2.000],  loss: 1.510519, mae: 29.847384, mean_q: -44.119095, mean_eps: 0.414167\n",
            "  92806/200000: episode: 331, duration: 1.008s, episode steps: 154, steps per second: 153, episode reward: -153.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.864 [0.000, 2.000],  loss: 1.951292, mae: 29.835909, mean_q: -44.084933, mean_eps: 0.412723\n",
            "  92974/200000: episode: 332, duration: 1.047s, episode steps: 168, steps per second: 160, episode reward: -167.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.089 [0.000, 2.000],  loss: 1.523094, mae: 29.913808, mean_q: -44.226446, mean_eps: 0.411697\n",
            "  93124/200000: episode: 333, duration: 0.944s, episode steps: 150, steps per second: 159, episode reward: -149.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.867 [0.000, 2.000],  loss: 0.997723, mae: 30.062900, mean_q: -44.460355, mean_eps: 0.410696\n",
            "  93273/200000: episode: 334, duration: 1.116s, episode steps: 149, steps per second: 133, episode reward: -148.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.725 [0.000, 2.000],  loss: 1.500488, mae: 29.823918, mean_q: -44.054754, mean_eps: 0.409746\n",
            "  93427/200000: episode: 335, duration: 1.219s, episode steps: 154, steps per second: 126, episode reward: -153.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.117 [0.000, 2.000],  loss: 1.413699, mae: 29.936750, mean_q: -44.239478, mean_eps: 0.408783\n",
            "  93564/200000: episode: 336, duration: 1.097s, episode steps: 137, steps per second: 125, episode reward: -136.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.920 [0.000, 2.000],  loss: 2.016301, mae: 29.941775, mean_q: -44.221551, mean_eps: 0.407871\n",
            "  93828/200000: episode: 337, duration: 1.677s, episode steps: 264, steps per second: 157, episode reward: -263.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.655 [0.000, 2.000],  loss: 1.859470, mae: 29.966838, mean_q: -44.276260, mean_eps: 0.406605\n",
            "  93989/200000: episode: 338, duration: 0.918s, episode steps: 161, steps per second: 175, episode reward: -160.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.062 [0.000, 2.000],  loss: 2.133978, mae: 29.915354, mean_q: -44.197009, mean_eps: 0.405249\n",
            "  94105/200000: episode: 339, duration: 0.645s, episode steps: 116, steps per second: 180, episode reward: -115.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.922 [0.000, 2.000],  loss: 1.912612, mae: 29.879156, mean_q: -44.132099, mean_eps: 0.404363\n",
            "  94254/200000: episode: 340, duration: 0.821s, episode steps: 149, steps per second: 182, episode reward: -148.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.034 [0.000, 2.000],  loss: 1.433390, mae: 29.977215, mean_q: -44.276532, mean_eps: 0.403527\n",
            "  94374/200000: episode: 341, duration: 0.656s, episode steps: 120, steps per second: 183, episode reward: -119.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.808 [0.000, 2.000],  loss: 2.782345, mae: 30.076529, mean_q: -44.382313, mean_eps: 0.402678\n",
            "  94507/200000: episode: 342, duration: 0.744s, episode steps: 133, steps per second: 179, episode reward: -132.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.850 [0.000, 2.000],  loss: 1.125395, mae: 29.980263, mean_q: -44.293765, mean_eps: 0.401880\n",
            "  94663/200000: episode: 343, duration: 0.885s, episode steps: 156, steps per second: 176, episode reward: -155.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.827 [0.000, 2.000],  loss: 2.653007, mae: 29.908035, mean_q: -44.114243, mean_eps: 0.400968\n",
            "  94819/200000: episode: 344, duration: 0.890s, episode steps: 156, steps per second: 175, episode reward: -155.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.885 [0.000, 2.000],  loss: 1.759031, mae: 30.145313, mean_q: -44.523495, mean_eps: 0.399980\n",
            "  94948/200000: episode: 345, duration: 0.740s, episode steps: 129, steps per second: 174, episode reward: -128.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.884 [0.000, 2.000],  loss: 0.988904, mae: 30.014698, mean_q: -44.344230, mean_eps: 0.399081\n",
            "  95050/200000: episode: 346, duration: 0.576s, episode steps: 102, steps per second: 177, episode reward: -101.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.725 [0.000, 2.000],  loss: 0.732273, mae: 30.036018, mean_q: -44.336663, mean_eps: 0.398346\n",
            "  95164/200000: episode: 347, duration: 0.661s, episode steps: 114, steps per second: 172, episode reward: -113.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.895 [0.000, 2.000],  loss: 2.256185, mae: 30.160412, mean_q: -44.511449, mean_eps: 0.397662\n",
            "  95307/200000: episode: 348, duration: 0.816s, episode steps: 143, steps per second: 175, episode reward: -142.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.923 [0.000, 2.000],  loss: 2.406065, mae: 30.150467, mean_q: -44.543107, mean_eps: 0.396851\n",
            "  95441/200000: episode: 349, duration: 0.945s, episode steps: 134, steps per second: 142, episode reward: -133.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.776 [0.000, 2.000],  loss: 1.441924, mae: 30.206987, mean_q: -44.583211, mean_eps: 0.395965\n",
            "  95549/200000: episode: 350, duration: 0.888s, episode steps: 108, steps per second: 122, episode reward: -107.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.861 [0.000, 2.000],  loss: 2.416983, mae: 29.936152, mean_q: -44.143902, mean_eps: 0.395192\n",
            "  95788/200000: episode: 351, duration: 1.965s, episode steps: 239, steps per second: 122, episode reward: -238.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.134 [0.000, 2.000],  loss: 1.497073, mae: 30.037610, mean_q: -44.352616, mean_eps: 0.394103\n",
            "  95913/200000: episode: 352, duration: 0.768s, episode steps: 125, steps per second: 163, episode reward: -124.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.856 [0.000, 2.000],  loss: 1.685014, mae: 30.281108, mean_q: -44.739821, mean_eps: 0.392950\n",
            "  96018/200000: episode: 353, duration: 0.632s, episode steps: 105, steps per second: 166, episode reward: -104.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.781 [0.000, 2.000],  loss: 1.069151, mae: 30.245274, mean_q: -44.712856, mean_eps: 0.392215\n",
            "  96239/200000: episode: 354, duration: 1.385s, episode steps: 221, steps per second: 160, episode reward: -220.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.167 [0.000, 2.000],  loss: 1.891542, mae: 30.322005, mean_q: -44.775248, mean_eps: 0.391189\n",
            "  96405/200000: episode: 355, duration: 0.966s, episode steps: 166, steps per second: 172, episode reward: -165.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.982 [0.000, 2.000],  loss: 2.132186, mae: 30.203837, mean_q: -44.586201, mean_eps: 0.389961\n",
            "  96528/200000: episode: 356, duration: 0.745s, episode steps: 123, steps per second: 165, episode reward: -122.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.870 [0.000, 2.000],  loss: 1.487660, mae: 30.069093, mean_q: -44.384406, mean_eps: 0.389049\n",
            "  96676/200000: episode: 357, duration: 0.859s, episode steps: 148, steps per second: 172, episode reward: -147.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.912 [0.000, 2.000],  loss: 1.904846, mae: 30.359892, mean_q: -44.838283, mean_eps: 0.388200\n",
            "  96836/200000: episode: 358, duration: 0.943s, episode steps: 160, steps per second: 170, episode reward: -159.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.644 [0.000, 2.000],  loss: 1.972624, mae: 30.306114, mean_q: -44.733490, mean_eps: 0.387225\n",
            "  96993/200000: episode: 359, duration: 0.988s, episode steps: 157, steps per second: 159, episode reward: -156.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.981 [0.000, 2.000],  loss: 1.737871, mae: 30.161027, mean_q: -44.541238, mean_eps: 0.386211\n",
            "  97127/200000: episode: 360, duration: 0.830s, episode steps: 134, steps per second: 162, episode reward: -133.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.843 [0.000, 2.000],  loss: 1.532864, mae: 30.252097, mean_q: -44.664817, mean_eps: 0.385287\n",
            "  97277/200000: episode: 361, duration: 0.931s, episode steps: 150, steps per second: 161, episode reward: -149.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.827 [0.000, 2.000],  loss: 2.337298, mae: 30.240044, mean_q: -44.602213, mean_eps: 0.384387\n",
            "  97424/200000: episode: 362, duration: 0.859s, episode steps: 147, steps per second: 171, episode reward: -146.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.939 [0.000, 2.000],  loss: 1.792870, mae: 30.339170, mean_q: -44.837001, mean_eps: 0.383450\n",
            "  97540/200000: episode: 363, duration: 1.043s, episode steps: 116, steps per second: 111, episode reward: -115.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.862 [0.000, 2.000],  loss: 1.557156, mae: 30.186644, mean_q: -44.579911, mean_eps: 0.382627\n",
            "  98040/200000: episode: 364, duration: 3.798s, episode steps: 500, steps per second: 132, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.562 [0.000, 2.000],  loss: 1.468024, mae: 30.224456, mean_q: -44.647782, mean_eps: 0.380676\n",
            "  98168/200000: episode: 365, duration: 0.837s, episode steps: 128, steps per second: 153, episode reward: -127.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.977 [0.000, 2.000],  loss: 1.228379, mae: 30.185526, mean_q: -44.561151, mean_eps: 0.378687\n",
            "  98309/200000: episode: 366, duration: 0.889s, episode steps: 141, steps per second: 159, episode reward: -140.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.738 [0.000, 2.000],  loss: 1.258712, mae: 30.297154, mean_q: -44.744446, mean_eps: 0.377826\n",
            "  98433/200000: episode: 367, duration: 0.748s, episode steps: 124, steps per second: 166, episode reward: -123.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.903 [0.000, 2.000],  loss: 1.241780, mae: 30.222216, mean_q: -44.629797, mean_eps: 0.376977\n",
            "  98570/200000: episode: 368, duration: 0.771s, episode steps: 137, steps per second: 178, episode reward: -136.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.650 [0.000, 2.000],  loss: 2.257048, mae: 30.301958, mean_q: -44.705795, mean_eps: 0.376154\n",
            "  98771/200000: episode: 369, duration: 1.135s, episode steps: 201, steps per second: 177, episode reward: -200.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.055 [0.000, 2.000],  loss: 2.165278, mae: 30.353773, mean_q: -44.799546, mean_eps: 0.375090\n",
            "  98946/200000: episode: 370, duration: 0.971s, episode steps: 175, steps per second: 180, episode reward: -174.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.920 [0.000, 2.000],  loss: 1.370887, mae: 30.300114, mean_q: -44.771346, mean_eps: 0.373899\n",
            "  99057/200000: episode: 371, duration: 0.626s, episode steps: 111, steps per second: 177, episode reward: -110.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.964 [0.000, 2.000],  loss: 0.833903, mae: 30.285864, mean_q: -44.752231, mean_eps: 0.372987\n",
            "  99246/200000: episode: 372, duration: 1.065s, episode steps: 189, steps per second: 177, episode reward: -188.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.698 [0.000, 2.000],  loss: 2.233871, mae: 30.388777, mean_q: -44.833849, mean_eps: 0.372037\n",
            "  99382/200000: episode: 373, duration: 0.773s, episode steps: 136, steps per second: 176, episode reward: -135.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.831 [0.000, 2.000],  loss: 2.202468, mae: 30.238620, mean_q: -44.621368, mean_eps: 0.371011\n",
            "  99508/200000: episode: 374, duration: 0.718s, episode steps: 126, steps per second: 175, episode reward: -125.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.730 [0.000, 2.000],  loss: 1.033439, mae: 30.359221, mean_q: -44.849506, mean_eps: 0.370188\n",
            "  99624/200000: episode: 375, duration: 0.920s, episode steps: 116, steps per second: 126, episode reward: -115.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.983 [0.000, 2.000],  loss: 1.963413, mae: 30.331380, mean_q: -44.789706, mean_eps: 0.369428\n",
            "  99778/200000: episode: 376, duration: 1.301s, episode steps: 154, steps per second: 118, episode reward: -153.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.981 [0.000, 2.000],  loss: 1.262292, mae: 30.358372, mean_q: -44.820371, mean_eps: 0.368567\n",
            "  99896/200000: episode: 377, duration: 1.101s, episode steps: 118, steps per second: 107, episode reward: -117.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.864 [0.000, 2.000],  loss: 1.337066, mae: 30.285770, mean_q: -44.691753, mean_eps: 0.367705\n",
            " 100014/200000: episode: 378, duration: 0.708s, episode steps: 118, steps per second: 167, episode reward: -117.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.102 [0.000, 2.000],  loss: 1.985859, mae: 30.311221, mean_q: -44.696609, mean_eps: 0.366958\n",
            " 100145/200000: episode: 379, duration: 0.839s, episode steps: 131, steps per second: 156, episode reward: -130.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.038 [0.000, 2.000],  loss: 1.608745, mae: 30.444483, mean_q: -44.873926, mean_eps: 0.366160\n",
            " 100252/200000: episode: 380, duration: 0.645s, episode steps: 107, steps per second: 166, episode reward: -106.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.991 [0.000, 2.000],  loss: 0.888313, mae: 30.485869, mean_q: -45.036297, mean_eps: 0.365413\n",
            " 100347/200000: episode: 381, duration: 0.568s, episode steps:  95, steps per second: 167, episode reward: -94.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.968 [0.000, 2.000],  loss: 1.363918, mae: 30.330263, mean_q: -44.749533, mean_eps: 0.364779\n",
            " 100466/200000: episode: 382, duration: 0.712s, episode steps: 119, steps per second: 167, episode reward: -118.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.025 [0.000, 2.000],  loss: 1.487269, mae: 30.412376, mean_q: -44.892230, mean_eps: 0.364095\n",
            " 100637/200000: episode: 383, duration: 1.049s, episode steps: 171, steps per second: 163, episode reward: -170.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.906 [0.000, 2.000],  loss: 1.378641, mae: 30.406449, mean_q: -44.907258, mean_eps: 0.363171\n",
            " 100767/200000: episode: 384, duration: 0.799s, episode steps: 130, steps per second: 163, episode reward: -129.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.008 [0.000, 2.000],  loss: 1.416552, mae: 30.229284, mean_q: -44.580408, mean_eps: 0.362221\n",
            " 100858/200000: episode: 385, duration: 0.582s, episode steps:  91, steps per second: 156, episode reward: -90.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.868 [0.000, 2.000],  loss: 2.584850, mae: 30.291604, mean_q: -44.667301, mean_eps: 0.361524\n",
            " 100975/200000: episode: 386, duration: 0.706s, episode steps: 117, steps per second: 166, episode reward: -116.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.923 [0.000, 2.000],  loss: 1.552879, mae: 30.396545, mean_q: -44.858127, mean_eps: 0.360865\n",
            " 101081/200000: episode: 387, duration: 0.641s, episode steps: 106, steps per second: 165, episode reward: -105.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.038 [0.000, 2.000],  loss: 1.017122, mae: 30.203608, mean_q: -44.590030, mean_eps: 0.360156\n",
            " 101179/200000: episode: 388, duration: 0.551s, episode steps:  98, steps per second: 178, episode reward: -97.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.959 [0.000, 2.000],  loss: 0.961813, mae: 30.423475, mean_q: -44.910073, mean_eps: 0.359510\n",
            " 101298/200000: episode: 389, duration: 0.753s, episode steps: 119, steps per second: 158, episode reward: -118.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.874 [0.000, 2.000],  loss: 2.467939, mae: 30.426415, mean_q: -44.870081, mean_eps: 0.358826\n",
            " 101420/200000: episode: 390, duration: 0.785s, episode steps: 122, steps per second: 155, episode reward: -121.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.844 [0.000, 2.000],  loss: 1.948282, mae: 30.512941, mean_q: -44.979970, mean_eps: 0.358066\n",
            " 101527/200000: episode: 391, duration: 0.698s, episode steps: 107, steps per second: 153, episode reward: -106.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.963 [0.000, 2.000],  loss: 1.594743, mae: 30.374187, mean_q: -44.818317, mean_eps: 0.357344\n",
            " 101653/200000: episode: 392, duration: 1.104s, episode steps: 126, steps per second: 114, episode reward: -125.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.937 [0.000, 2.000],  loss: 2.210192, mae: 30.497955, mean_q: -44.973217, mean_eps: 0.356597\n",
            " 101766/200000: episode: 393, duration: 0.971s, episode steps: 113, steps per second: 116, episode reward: -112.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.018 [0.000, 2.000],  loss: 1.955465, mae: 30.518574, mean_q: -45.044789, mean_eps: 0.355837\n",
            " 101889/200000: episode: 394, duration: 1.061s, episode steps: 123, steps per second: 116, episode reward: -122.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.837 [0.000, 2.000],  loss: 2.919965, mae: 30.202299, mean_q: -44.478241, mean_eps: 0.355089\n",
            " 102040/200000: episode: 395, duration: 1.046s, episode steps: 151, steps per second: 144, episode reward: -150.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.828 [0.000, 2.000],  loss: 1.799074, mae: 30.391382, mean_q: -44.803411, mean_eps: 0.354228\n",
            " 102129/200000: episode: 396, duration: 0.551s, episode steps:  89, steps per second: 161, episode reward: -88.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.910 [0.000, 2.000],  loss: 1.899144, mae: 30.575380, mean_q: -45.101684, mean_eps: 0.353468\n",
            " 102248/200000: episode: 397, duration: 0.726s, episode steps: 119, steps per second: 164, episode reward: -118.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.840 [0.000, 2.000],  loss: 2.182977, mae: 30.325960, mean_q: -44.690857, mean_eps: 0.352809\n",
            " 102362/200000: episode: 398, duration: 0.697s, episode steps: 114, steps per second: 164, episode reward: -113.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.930 [0.000, 2.000],  loss: 2.017084, mae: 30.383879, mean_q: -44.791697, mean_eps: 0.352075\n",
            " 102516/200000: episode: 399, duration: 0.945s, episode steps: 154, steps per second: 163, episode reward: -153.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.877 [0.000, 2.000],  loss: 1.875721, mae: 30.391524, mean_q: -44.810543, mean_eps: 0.351226\n",
            " 102678/200000: episode: 400, duration: 0.993s, episode steps: 162, steps per second: 163, episode reward: -161.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.043 [0.000, 2.000],  loss: 2.111581, mae: 30.472903, mean_q: -44.951373, mean_eps: 0.350225\n",
            " 102777/200000: episode: 401, duration: 0.616s, episode steps:  99, steps per second: 161, episode reward: -98.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.899 [0.000, 2.000],  loss: 1.681504, mae: 30.592818, mean_q: -45.129158, mean_eps: 0.349389\n",
            " 102943/200000: episode: 402, duration: 0.982s, episode steps: 166, steps per second: 169, episode reward: -165.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.753 [0.000, 2.000],  loss: 1.612608, mae: 30.432136, mean_q: -44.852875, mean_eps: 0.348553\n",
            " 103091/200000: episode: 403, duration: 0.983s, episode steps: 148, steps per second: 151, episode reward: -147.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.899 [0.000, 2.000],  loss: 2.125586, mae: 30.385892, mean_q: -44.811526, mean_eps: 0.347565\n",
            " 103206/200000: episode: 404, duration: 0.746s, episode steps: 115, steps per second: 154, episode reward: -114.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.009 [0.000, 2.000],  loss: 2.550383, mae: 30.704915, mean_q: -45.208715, mean_eps: 0.346729\n",
            " 103326/200000: episode: 405, duration: 0.796s, episode steps: 120, steps per second: 151, episode reward: -119.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.917 [0.000, 2.000],  loss: 1.630950, mae: 30.334541, mean_q: -44.688340, mean_eps: 0.345982\n",
            " 103447/200000: episode: 406, duration: 0.750s, episode steps: 121, steps per second: 161, episode reward: -120.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.983 [0.000, 2.000],  loss: 1.398883, mae: 30.575030, mean_q: -45.103596, mean_eps: 0.345222\n",
            " 103580/200000: episode: 407, duration: 0.973s, episode steps: 133, steps per second: 137, episode reward: -132.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.932 [0.000, 2.000],  loss: 1.249733, mae: 30.631954, mean_q: -45.168515, mean_eps: 0.344424\n",
            " 103669/200000: episode: 408, duration: 0.782s, episode steps:  89, steps per second: 114, episode reward: -88.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.865 [0.000, 2.000],  loss: 1.084870, mae: 30.569713, mean_q: -45.131877, mean_eps: 0.343715\n",
            " 103805/200000: episode: 409, duration: 1.144s, episode steps: 136, steps per second: 119, episode reward: -135.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.897 [0.000, 2.000],  loss: 1.977819, mae: 30.522868, mean_q: -44.965353, mean_eps: 0.342993\n",
            " 103922/200000: episode: 410, duration: 1.040s, episode steps: 117, steps per second: 113, episode reward: -116.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.915 [0.000, 2.000],  loss: 1.257393, mae: 30.672891, mean_q: -45.243809, mean_eps: 0.342195\n",
            " 104069/200000: episode: 411, duration: 0.924s, episode steps: 147, steps per second: 159, episode reward: -146.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.810 [0.000, 2.000],  loss: 1.414403, mae: 30.657861, mean_q: -45.165811, mean_eps: 0.341359\n",
            " 104180/200000: episode: 412, duration: 0.681s, episode steps: 111, steps per second: 163, episode reward: -110.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.946 [0.000, 2.000],  loss: 1.917959, mae: 30.624956, mean_q: -45.140439, mean_eps: 0.340548\n",
            " 104293/200000: episode: 413, duration: 0.721s, episode steps: 113, steps per second: 157, episode reward: -112.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.186 [0.000, 2.000],  loss: 1.628946, mae: 30.538103, mean_q: -44.995325, mean_eps: 0.339839\n",
            " 104401/200000: episode: 414, duration: 0.641s, episode steps: 108, steps per second: 169, episode reward: -107.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.833 [0.000, 2.000],  loss: 1.640379, mae: 30.735512, mean_q: -45.330690, mean_eps: 0.339129\n",
            " 104542/200000: episode: 415, duration: 0.816s, episode steps: 141, steps per second: 173, episode reward: -140.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.092 [0.000, 2.000],  loss: 1.725081, mae: 30.716940, mean_q: -45.280008, mean_eps: 0.338344\n",
            " 104640/200000: episode: 416, duration: 0.589s, episode steps:  98, steps per second: 167, episode reward: -97.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.969 [0.000, 2.000],  loss: 1.720507, mae: 30.590360, mean_q: -45.093145, mean_eps: 0.337597\n",
            " 104768/200000: episode: 417, duration: 0.772s, episode steps: 128, steps per second: 166, episode reward: -127.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.773 [0.000, 2.000],  loss: 1.400649, mae: 30.663628, mean_q: -45.191425, mean_eps: 0.336887\n",
            " 104916/200000: episode: 418, duration: 0.929s, episode steps: 148, steps per second: 159, episode reward: -147.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.926 [0.000, 2.000],  loss: 1.561751, mae: 30.635207, mean_q: -45.167886, mean_eps: 0.336013\n",
            " 105034/200000: episode: 419, duration: 0.785s, episode steps: 118, steps per second: 150, episode reward: -117.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.966 [0.000, 2.000],  loss: 1.233409, mae: 30.779893, mean_q: -45.409260, mean_eps: 0.335165\n",
            " 105140/200000: episode: 420, duration: 0.677s, episode steps: 106, steps per second: 157, episode reward: -105.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.802 [0.000, 2.000],  loss: 1.638367, mae: 30.674798, mean_q: -45.206957, mean_eps: 0.334455\n",
            " 105230/200000: episode: 421, duration: 0.591s, episode steps:  90, steps per second: 152, episode reward: -89.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.011 [0.000, 2.000],  loss: 1.505171, mae: 30.614866, mean_q: -45.147600, mean_eps: 0.333835\n",
            " 105330/200000: episode: 422, duration: 0.675s, episode steps: 100, steps per second: 148, episode reward: -99.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.920 [0.000, 2.000],  loss: 2.379919, mae: 30.573759, mean_q: -45.064865, mean_eps: 0.333227\n",
            " 105453/200000: episode: 423, duration: 0.833s, episode steps: 123, steps per second: 148, episode reward: -122.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.935 [0.000, 2.000],  loss: 1.719469, mae: 30.531061, mean_q: -44.969561, mean_eps: 0.332517\n",
            " 105576/200000: episode: 424, duration: 0.985s, episode steps: 123, steps per second: 125, episode reward: -122.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.073 [0.000, 2.000],  loss: 2.127974, mae: 30.565283, mean_q: -45.059817, mean_eps: 0.331745\n",
            " 105690/200000: episode: 425, duration: 0.981s, episode steps: 114, steps per second: 116, episode reward: -113.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.965 [0.000, 2.000],  loss: 1.831303, mae: 30.699845, mean_q: -45.278085, mean_eps: 0.330997\n",
            " 105825/200000: episode: 426, duration: 1.125s, episode steps: 135, steps per second: 120, episode reward: -134.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.859 [0.000, 2.000],  loss: 1.585573, mae: 30.538062, mean_q: -44.993418, mean_eps: 0.330199\n",
            " 105984/200000: episode: 427, duration: 1.229s, episode steps: 159, steps per second: 129, episode reward: -158.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.950 [0.000, 2.000],  loss: 1.846607, mae: 30.566594, mean_q: -45.031792, mean_eps: 0.329275\n",
            " 106082/200000: episode: 428, duration: 0.645s, episode steps:  98, steps per second: 152, episode reward: -97.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.020 [0.000, 2.000],  loss: 2.034063, mae: 30.579957, mean_q: -45.019638, mean_eps: 0.328464\n",
            " 106207/200000: episode: 429, duration: 0.828s, episode steps: 125, steps per second: 151, episode reward: -124.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.816 [0.000, 2.000],  loss: 1.288340, mae: 30.765851, mean_q: -45.339594, mean_eps: 0.327755\n",
            " 106324/200000: episode: 430, duration: 0.714s, episode steps: 117, steps per second: 164, episode reward: -116.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.026 [0.000, 2.000],  loss: 1.384696, mae: 30.586688, mean_q: -45.084446, mean_eps: 0.326995\n",
            " 106434/200000: episode: 431, duration: 0.740s, episode steps: 110, steps per second: 149, episode reward: -109.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.945 [0.000, 2.000],  loss: 1.435174, mae: 30.570488, mean_q: -45.047856, mean_eps: 0.326273\n",
            " 106555/200000: episode: 432, duration: 0.707s, episode steps: 121, steps per second: 171, episode reward: -120.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.942 [0.000, 2.000],  loss: 1.572663, mae: 30.498529, mean_q: -44.929292, mean_eps: 0.325538\n",
            " 106690/200000: episode: 433, duration: 0.827s, episode steps: 135, steps per second: 163, episode reward: -134.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.933 [0.000, 2.000],  loss: 1.529350, mae: 30.464518, mean_q: -44.904788, mean_eps: 0.324727\n",
            " 106829/200000: episode: 434, duration: 0.849s, episode steps: 139, steps per second: 164, episode reward: -138.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.971 [0.000, 2.000],  loss: 2.360882, mae: 30.459924, mean_q: -44.850378, mean_eps: 0.323853\n",
            " 106971/200000: episode: 435, duration: 0.824s, episode steps: 142, steps per second: 172, episode reward: -141.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.901 [0.000, 2.000],  loss: 1.685805, mae: 30.665642, mean_q: -45.217199, mean_eps: 0.322967\n",
            " 107072/200000: episode: 436, duration: 0.648s, episode steps: 101, steps per second: 156, episode reward: -100.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 2.650208, mae: 30.564381, mean_q: -45.048961, mean_eps: 0.322207\n",
            " 107189/200000: episode: 437, duration: 0.741s, episode steps: 117, steps per second: 158, episode reward: -116.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.017 [0.000, 2.000],  loss: 2.572280, mae: 30.606994, mean_q: -45.079388, mean_eps: 0.321510\n",
            " 107341/200000: episode: 438, duration: 0.933s, episode steps: 152, steps per second: 163, episode reward: -151.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 2.229894, mae: 30.449401, mean_q: -44.836614, mean_eps: 0.320649\n",
            " 107476/200000: episode: 439, duration: 0.827s, episode steps: 135, steps per second: 163, episode reward: -134.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.800 [0.000, 2.000],  loss: 1.182527, mae: 30.700799, mean_q: -45.256994, mean_eps: 0.319749\n",
            " 107611/200000: episode: 440, duration: 1.137s, episode steps: 135, steps per second: 119, episode reward: -134.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.970 [0.000, 2.000],  loss: 1.960283, mae: 30.566055, mean_q: -44.993672, mean_eps: 0.318901\n",
            " 107708/200000: episode: 441, duration: 0.902s, episode steps:  97, steps per second: 108, episode reward: -96.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.907 [0.000, 2.000],  loss: 1.961528, mae: 30.510450, mean_q: -44.907872, mean_eps: 0.318166\n",
            " 107822/200000: episode: 442, duration: 1.004s, episode steps: 114, steps per second: 114, episode reward: -113.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.070 [0.000, 2.000],  loss: 1.533332, mae: 30.696481, mean_q: -45.208008, mean_eps: 0.317495\n",
            " 107924/200000: episode: 443, duration: 0.827s, episode steps: 102, steps per second: 123, episode reward: -101.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.892 [0.000, 2.000],  loss: 1.660291, mae: 30.512114, mean_q: -44.899956, mean_eps: 0.316811\n",
            " 108077/200000: episode: 444, duration: 0.965s, episode steps: 153, steps per second: 159, episode reward: -152.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.052 [0.000, 2.000],  loss: 1.729521, mae: 30.578487, mean_q: -45.018704, mean_eps: 0.316000\n",
            " 108206/200000: episode: 445, duration: 0.794s, episode steps: 129, steps per second: 162, episode reward: -128.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.891 [0.000, 2.000],  loss: 1.385865, mae: 30.499742, mean_q: -44.894510, mean_eps: 0.315101\n",
            " 108311/200000: episode: 446, duration: 0.672s, episode steps: 105, steps per second: 156, episode reward: -104.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.924 [0.000, 2.000],  loss: 1.374175, mae: 30.784876, mean_q: -45.322791, mean_eps: 0.314366\n",
            " 108435/200000: episode: 447, duration: 0.754s, episode steps: 124, steps per second: 164, episode reward: -123.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.903 [0.000, 2.000],  loss: 1.222526, mae: 30.631860, mean_q: -45.128543, mean_eps: 0.313644\n",
            " 108540/200000: episode: 448, duration: 0.655s, episode steps: 105, steps per second: 160, episode reward: -104.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.914 [0.000, 2.000],  loss: 1.004764, mae: 30.507123, mean_q: -44.951137, mean_eps: 0.312922\n",
            " 108664/200000: episode: 449, duration: 0.777s, episode steps: 124, steps per second: 159, episode reward: -123.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.871 [0.000, 2.000],  loss: 1.684267, mae: 30.841030, mean_q: -45.428347, mean_eps: 0.312200\n",
            " 108788/200000: episode: 450, duration: 0.753s, episode steps: 124, steps per second: 165, episode reward: -123.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.089 [0.000, 2.000],  loss: 1.191607, mae: 30.838879, mean_q: -45.433594, mean_eps: 0.311415\n",
            " 108913/200000: episode: 451, duration: 0.791s, episode steps: 125, steps per second: 158, episode reward: -124.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.096 [0.000, 2.000],  loss: 1.863896, mae: 30.658340, mean_q: -45.140423, mean_eps: 0.310617\n",
            " 109035/200000: episode: 452, duration: 0.777s, episode steps: 122, steps per second: 157, episode reward: -121.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.820 [0.000, 2.000],  loss: 1.836794, mae: 30.586195, mean_q: -45.032604, mean_eps: 0.309831\n",
            " 109174/200000: episode: 453, duration: 0.941s, episode steps: 139, steps per second: 148, episode reward: -138.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.942 [0.000, 2.000],  loss: 2.474342, mae: 30.627725, mean_q: -45.070479, mean_eps: 0.309008\n",
            " 109275/200000: episode: 454, duration: 0.672s, episode steps: 101, steps per second: 150, episode reward: -100.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.792 [0.000, 2.000],  loss: 2.066686, mae: 30.699126, mean_q: -45.170207, mean_eps: 0.308248\n",
            " 109371/200000: episode: 455, duration: 0.593s, episode steps:  96, steps per second: 162, episode reward: -95.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.833 [0.000, 2.000],  loss: 1.475927, mae: 30.730422, mean_q: -45.247311, mean_eps: 0.307627\n",
            " 109511/200000: episode: 456, duration: 0.981s, episode steps: 140, steps per second: 143, episode reward: -139.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.929 [0.000, 2.000],  loss: 2.135427, mae: 30.521616, mean_q: -44.923383, mean_eps: 0.306880\n",
            " 109639/200000: episode: 457, duration: 1.130s, episode steps: 128, steps per second: 113, episode reward: -127.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.758 [0.000, 2.000],  loss: 1.452751, mae: 30.704097, mean_q: -45.224307, mean_eps: 0.306031\n",
            " 109864/200000: episode: 458, duration: 1.996s, episode steps: 225, steps per second: 113, episode reward: -224.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.738 [0.000, 2.000],  loss: 1.798291, mae: 30.674621, mean_q: -45.147590, mean_eps: 0.304917\n",
            " 109971/200000: episode: 459, duration: 0.706s, episode steps: 107, steps per second: 152, episode reward: -106.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.860 [0.000, 2.000],  loss: 1.089334, mae: 30.691628, mean_q: -45.205377, mean_eps: 0.303865\n",
            " 110073/200000: episode: 460, duration: 0.640s, episode steps: 102, steps per second: 159, episode reward: -101.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.765 [0.000, 2.000],  loss: 1.512435, mae: 30.727456, mean_q: -45.260278, mean_eps: 0.303194\n",
            " 110191/200000: episode: 461, duration: 0.755s, episode steps: 118, steps per second: 156, episode reward: -117.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.831 [0.000, 2.000],  loss: 1.137693, mae: 30.746988, mean_q: -45.314460, mean_eps: 0.302497\n",
            " 110290/200000: episode: 462, duration: 0.609s, episode steps:  99, steps per second: 163, episode reward: -98.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 1.810418, mae: 30.566221, mean_q: -44.971930, mean_eps: 0.301813\n",
            " 110401/200000: episode: 463, duration: 0.651s, episode steps: 111, steps per second: 171, episode reward: -110.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.856 [0.000, 2.000],  loss: 1.900666, mae: 30.607187, mean_q: -45.034240, mean_eps: 0.301142\n",
            " 110522/200000: episode: 464, duration: 0.727s, episode steps: 121, steps per second: 166, episode reward: -120.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.950 [0.000, 2.000],  loss: 1.160777, mae: 30.501014, mean_q: -44.877041, mean_eps: 0.300407\n",
            " 110611/200000: episode: 465, duration: 0.524s, episode steps:  89, steps per second: 170, episode reward: -88.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.876 [0.000, 2.000],  loss: 1.727629, mae: 30.557472, mean_q: -44.939858, mean_eps: 0.299749\n",
            " 110704/200000: episode: 466, duration: 0.554s, episode steps:  93, steps per second: 168, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.935 [0.000, 2.000],  loss: 2.153453, mae: 30.477142, mean_q: -44.797920, mean_eps: 0.299179\n",
            " 110810/200000: episode: 467, duration: 0.620s, episode steps: 106, steps per second: 171, episode reward: -105.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.849 [0.000, 2.000],  loss: 2.415304, mae: 30.724254, mean_q: -45.168790, mean_eps: 0.298545\n",
            " 110942/200000: episode: 468, duration: 0.778s, episode steps: 132, steps per second: 170, episode reward: -131.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.803 [0.000, 2.000],  loss: 1.418951, mae: 30.373085, mean_q: -44.659242, mean_eps: 0.297785\n",
            " 111084/200000: episode: 469, duration: 0.835s, episode steps: 142, steps per second: 170, episode reward: -141.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.894 [0.000, 2.000],  loss: 1.345363, mae: 30.431859, mean_q: -44.802332, mean_eps: 0.296924\n",
            " 111210/200000: episode: 470, duration: 0.762s, episode steps: 126, steps per second: 165, episode reward: -125.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.095 [0.000, 2.000],  loss: 1.368638, mae: 30.598016, mean_q: -45.008245, mean_eps: 0.296075\n",
            " 111310/200000: episode: 471, duration: 0.614s, episode steps: 100, steps per second: 163, episode reward: -99.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.780 [0.000, 2.000],  loss: 1.271347, mae: 30.874588, mean_q: -45.449817, mean_eps: 0.295353\n",
            " 111421/200000: episode: 472, duration: 0.686s, episode steps: 111, steps per second: 162, episode reward: -110.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.793 [0.000, 2.000],  loss: 2.222851, mae: 30.692827, mean_q: -45.087913, mean_eps: 0.294682\n",
            " 111551/200000: episode: 473, duration: 0.946s, episode steps: 130, steps per second: 137, episode reward: -129.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.900 [0.000, 2.000],  loss: 1.414502, mae: 30.512736, mean_q: -44.882971, mean_eps: 0.293922\n",
            " 111669/200000: episode: 474, duration: 1.017s, episode steps: 118, steps per second: 116, episode reward: -117.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.068 [0.000, 2.000],  loss: 0.995871, mae: 30.321557, mean_q: -44.635007, mean_eps: 0.293137\n",
            " 111790/200000: episode: 475, duration: 0.994s, episode steps: 121, steps per second: 122, episode reward: -120.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.992 [0.000, 2.000],  loss: 1.610629, mae: 30.364449, mean_q: -44.631816, mean_eps: 0.292377\n",
            " 111903/200000: episode: 476, duration: 0.976s, episode steps: 113, steps per second: 116, episode reward: -112.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.912 [0.000, 2.000],  loss: 1.159244, mae: 30.670024, mean_q: -45.140468, mean_eps: 0.291642\n",
            " 112012/200000: episode: 477, duration: 0.691s, episode steps: 109, steps per second: 158, episode reward: -108.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.917 [0.000, 2.000],  loss: 1.896657, mae: 30.563740, mean_q: -44.904071, mean_eps: 0.290945\n",
            " 112125/200000: episode: 478, duration: 0.710s, episode steps: 113, steps per second: 159, episode reward: -112.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.044 [0.000, 2.000],  loss: 1.907873, mae: 30.487484, mean_q: -44.787006, mean_eps: 0.290236\n",
            " 112235/200000: episode: 479, duration: 0.863s, episode steps: 110, steps per second: 127, episode reward: -109.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.936 [0.000, 2.000],  loss: 1.357351, mae: 30.633444, mean_q: -45.063118, mean_eps: 0.289527\n",
            " 112367/200000: episode: 480, duration: 1.133s, episode steps: 132, steps per second: 117, episode reward: -131.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.902 [0.000, 2.000],  loss: 1.130493, mae: 30.472746, mean_q: -44.817911, mean_eps: 0.288767\n",
            " 112473/200000: episode: 481, duration: 0.872s, episode steps: 106, steps per second: 122, episode reward: -105.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 1.626476, mae: 30.438723, mean_q: -44.737359, mean_eps: 0.288007\n",
            " 112624/200000: episode: 482, duration: 1.156s, episode steps: 151, steps per second: 131, episode reward: -150.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.788 [0.000, 2.000],  loss: 1.772361, mae: 30.316504, mean_q: -44.586542, mean_eps: 0.287196\n",
            " 112741/200000: episode: 483, duration: 0.752s, episode steps: 117, steps per second: 156, episode reward: -116.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.641 [0.000, 2.000],  loss: 1.260205, mae: 30.455549, mean_q: -44.785706, mean_eps: 0.286347\n",
            " 112875/200000: episode: 484, duration: 0.799s, episode steps: 134, steps per second: 168, episode reward: -133.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.799 [0.000, 2.000],  loss: 1.689445, mae: 30.422422, mean_q: -44.755611, mean_eps: 0.285549\n",
            " 113001/200000: episode: 485, duration: 0.758s, episode steps: 126, steps per second: 166, episode reward: -125.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.921 [0.000, 2.000],  loss: 1.259332, mae: 30.510371, mean_q: -44.872879, mean_eps: 0.284726\n",
            " 113085/200000: episode: 486, duration: 0.502s, episode steps:  84, steps per second: 167, episode reward: -83.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.976 [0.000, 2.000],  loss: 1.421928, mae: 30.555862, mean_q: -44.890928, mean_eps: 0.284055\n",
            " 113203/200000: episode: 487, duration: 0.700s, episode steps: 118, steps per second: 169, episode reward: -117.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.025 [0.000, 2.000],  loss: 1.589611, mae: 30.520757, mean_q: -44.861815, mean_eps: 0.283421\n",
            " 113288/200000: episode: 488, duration: 0.525s, episode steps:  85, steps per second: 162, episode reward: -84.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.941 [0.000, 2.000],  loss: 1.766022, mae: 30.611631, mean_q: -45.012587, mean_eps: 0.282788\n",
            " 113462/200000: episode: 489, duration: 1.375s, episode steps: 174, steps per second: 127, episode reward: -173.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.805 [0.000, 2.000],  loss: 1.114013, mae: 30.574010, mean_q: -45.001887, mean_eps: 0.281965\n",
            " 113569/200000: episode: 490, duration: 0.917s, episode steps: 107, steps per second: 117, episode reward: -106.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.037 [0.000, 2.000],  loss: 1.393554, mae: 30.381548, mean_q: -44.657451, mean_eps: 0.281065\n",
            " 113670/200000: episode: 491, duration: 0.848s, episode steps: 101, steps per second: 119, episode reward: -100.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.891 [0.000, 2.000],  loss: 1.515963, mae: 30.558003, mean_q: -44.926185, mean_eps: 0.280407\n",
            " 113782/200000: episode: 492, duration: 0.897s, episode steps: 112, steps per second: 125, episode reward: -111.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.946 [0.000, 2.000],  loss: 1.143136, mae: 30.464319, mean_q: -44.802322, mean_eps: 0.279735\n",
            " 113892/200000: episode: 493, duration: 0.681s, episode steps: 110, steps per second: 162, episode reward: -109.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.891 [0.000, 2.000],  loss: 1.310572, mae: 30.458676, mean_q: -44.816031, mean_eps: 0.279039\n",
            " 114010/200000: episode: 494, duration: 0.732s, episode steps: 118, steps per second: 161, episode reward: -117.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.034 [0.000, 2.000],  loss: 1.289909, mae: 30.464770, mean_q: -44.819382, mean_eps: 0.278317\n",
            " 114111/200000: episode: 495, duration: 0.641s, episode steps: 101, steps per second: 158, episode reward: -100.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.010 [0.000, 2.000],  loss: 2.052658, mae: 30.359761, mean_q: -44.596895, mean_eps: 0.277620\n",
            " 114207/200000: episode: 496, duration: 0.591s, episode steps:  96, steps per second: 163, episode reward: -95.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.094 [0.000, 2.000],  loss: 1.271554, mae: 30.555797, mean_q: -44.925000, mean_eps: 0.276999\n",
            " 114305/200000: episode: 497, duration: 0.615s, episode steps:  98, steps per second: 159, episode reward: -97.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.010 [0.000, 2.000],  loss: 1.346814, mae: 30.394102, mean_q: -44.683570, mean_eps: 0.276379\n",
            " 114451/200000: episode: 498, duration: 0.859s, episode steps: 146, steps per second: 170, episode reward: -145.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.740 [0.000, 2.000],  loss: 1.396919, mae: 30.752009, mean_q: -45.219076, mean_eps: 0.275606\n",
            " 114554/200000: episode: 499, duration: 0.602s, episode steps: 103, steps per second: 171, episode reward: -102.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.155 [0.000, 2.000],  loss: 1.141083, mae: 30.548207, mean_q: -44.935727, mean_eps: 0.274821\n",
            " 114655/200000: episode: 500, duration: 0.602s, episode steps: 101, steps per second: 168, episode reward: -100.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.050 [0.000, 2.000],  loss: 1.470438, mae: 30.441904, mean_q: -44.700103, mean_eps: 0.274175\n",
            " 114758/200000: episode: 501, duration: 0.660s, episode steps: 103, steps per second: 156, episode reward: -102.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.019 [0.000, 2.000],  loss: 1.226385, mae: 30.385615, mean_q: -44.648618, mean_eps: 0.273529\n",
            " 114924/200000: episode: 502, duration: 1.098s, episode steps: 166, steps per second: 151, episode reward: -165.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.295 [0.000, 2.000],  loss: 1.152295, mae: 30.416604, mean_q: -44.709273, mean_eps: 0.272680\n",
            " 115035/200000: episode: 503, duration: 0.749s, episode steps: 111, steps per second: 148, episode reward: -110.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.027 [0.000, 2.000],  loss: 1.414021, mae: 30.237676, mean_q: -44.413933, mean_eps: 0.271806\n",
            " 115126/200000: episode: 504, duration: 0.590s, episode steps:  91, steps per second: 154, episode reward: -90.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.143 [0.000, 2.000],  loss: 1.343635, mae: 30.374606, mean_q: -44.590498, mean_eps: 0.271160\n",
            " 115250/200000: episode: 505, duration: 0.784s, episode steps: 124, steps per second: 158, episode reward: -123.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.960 [0.000, 2.000],  loss: 1.085952, mae: 30.386715, mean_q: -44.630226, mean_eps: 0.270476\n",
            " 115369/200000: episode: 506, duration: 0.911s, episode steps: 119, steps per second: 131, episode reward: -118.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.950 [0.000, 2.000],  loss: 1.300047, mae: 30.469462, mean_q: -44.767118, mean_eps: 0.269703\n",
            " 115460/200000: episode: 507, duration: 0.793s, episode steps:  91, steps per second: 115, episode reward: -90.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.044 [0.000, 2.000],  loss: 1.938171, mae: 30.305530, mean_q: -44.484432, mean_eps: 0.269045\n",
            " 115574/200000: episode: 508, duration: 1.007s, episode steps: 114, steps per second: 113, episode reward: -113.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.132 [0.000, 2.000],  loss: 1.580885, mae: 30.466888, mean_q: -44.736858, mean_eps: 0.268399\n",
            " 115666/200000: episode: 509, duration: 0.829s, episode steps:  92, steps per second: 111, episode reward: -91.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.754153, mae: 30.611812, mean_q: -45.034690, mean_eps: 0.267740\n",
            " 115870/200000: episode: 510, duration: 1.350s, episode steps: 204, steps per second: 151, episode reward: -203.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.637 [0.000, 2.000],  loss: 1.593789, mae: 30.291413, mean_q: -44.510975, mean_eps: 0.266803\n",
            " 115990/200000: episode: 511, duration: 0.734s, episode steps: 120, steps per second: 163, episode reward: -119.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.992 [0.000, 2.000],  loss: 1.602789, mae: 30.262888, mean_q: -44.430112, mean_eps: 0.265777\n",
            " 116127/200000: episode: 512, duration: 0.818s, episode steps: 137, steps per second: 167, episode reward: -136.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.036 [0.000, 2.000],  loss: 1.978402, mae: 30.211112, mean_q: -44.294820, mean_eps: 0.264966\n",
            " 116226/200000: episode: 513, duration: 0.592s, episode steps:  99, steps per second: 167, episode reward: -98.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.152 [0.000, 2.000],  loss: 1.420546, mae: 30.345070, mean_q: -44.557296, mean_eps: 0.264219\n",
            " 116333/200000: episode: 514, duration: 0.670s, episode steps: 107, steps per second: 160, episode reward: -106.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 1.589697, mae: 29.977304, mean_q: -44.005424, mean_eps: 0.263560\n",
            " 116440/200000: episode: 515, duration: 0.670s, episode steps: 107, steps per second: 160, episode reward: -106.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.103 [0.000, 2.000],  loss: 2.040240, mae: 30.284988, mean_q: -44.475253, mean_eps: 0.262889\n",
            " 116546/200000: episode: 516, duration: 0.640s, episode steps: 106, steps per second: 166, episode reward: -105.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.981 [0.000, 2.000],  loss: 1.147149, mae: 30.065756, mean_q: -44.143515, mean_eps: 0.262217\n",
            " 116631/200000: episode: 517, duration: 0.489s, episode steps:  85, steps per second: 174, episode reward: -84.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.929 [0.000, 2.000],  loss: 1.824151, mae: 30.076859, mean_q: -44.133141, mean_eps: 0.261609\n",
            " 116731/200000: episode: 518, duration: 0.592s, episode steps: 100, steps per second: 169, episode reward: -99.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 1.928515, mae: 30.101606, mean_q: -44.111801, mean_eps: 0.261027\n",
            " 116824/200000: episode: 519, duration: 0.601s, episode steps:  93, steps per second: 155, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 1.722745, mae: 29.924341, mean_q: -43.905758, mean_eps: 0.260419\n",
            " 116950/200000: episode: 520, duration: 0.742s, episode steps: 126, steps per second: 170, episode reward: -125.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.794 [0.000, 2.000],  loss: 1.137716, mae: 30.047753, mean_q: -44.101350, mean_eps: 0.259722\n",
            " 117088/200000: episode: 521, duration: 0.874s, episode steps: 138, steps per second: 158, episode reward: -137.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.007 [0.000, 2.000],  loss: 1.612345, mae: 30.003037, mean_q: -44.009640, mean_eps: 0.258886\n",
            " 117259/200000: episode: 522, duration: 1.012s, episode steps: 171, steps per second: 169, episode reward: -170.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.696 [0.000, 2.000],  loss: 1.165744, mae: 29.994500, mean_q: -44.008951, mean_eps: 0.257911\n",
            " 117358/200000: episode: 523, duration: 0.585s, episode steps:  99, steps per second: 169, episode reward: -98.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.071 [0.000, 2.000],  loss: 1.080816, mae: 30.074541, mean_q: -44.153481, mean_eps: 0.257049\n",
            " 117444/200000: episode: 524, duration: 0.713s, episode steps:  86, steps per second: 121, episode reward: -85.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.081 [0.000, 2.000],  loss: 1.968668, mae: 30.196736, mean_q: -44.319128, mean_eps: 0.256467\n",
            " 117522/200000: episode: 525, duration: 0.708s, episode steps:  78, steps per second: 110, episode reward: -77.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.051 [0.000, 2.000],  loss: 1.826129, mae: 30.300612, mean_q: -44.461934, mean_eps: 0.255947\n",
            " 117653/200000: episode: 526, duration: 0.989s, episode steps: 131, steps per second: 132, episode reward: -130.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.084 [0.000, 2.000],  loss: 1.552499, mae: 30.193305, mean_q: -44.300302, mean_eps: 0.255276\n",
            " 117766/200000: episode: 527, duration: 0.948s, episode steps: 113, steps per second: 119, episode reward: -112.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.788 [0.000, 2.000],  loss: 1.189403, mae: 30.181703, mean_q: -44.298659, mean_eps: 0.254503\n",
            " 117848/200000: episode: 528, duration: 0.484s, episode steps:  82, steps per second: 169, episode reward: -81.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.976 [0.000, 2.000],  loss: 2.078371, mae: 30.264377, mean_q: -44.426353, mean_eps: 0.253895\n",
            " 117941/200000: episode: 529, duration: 0.573s, episode steps:  93, steps per second: 162, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 2.175854, mae: 30.353924, mean_q: -44.600464, mean_eps: 0.253338\n",
            " 118050/200000: episode: 530, duration: 0.640s, episode steps: 109, steps per second: 170, episode reward: -108.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.991 [0.000, 2.000],  loss: 1.370154, mae: 29.838472, mean_q: -43.731174, mean_eps: 0.252692\n",
            " 118186/200000: episode: 531, duration: 0.784s, episode steps: 136, steps per second: 174, episode reward: -135.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.103 [0.000, 2.000],  loss: 1.947521, mae: 29.810978, mean_q: -43.721315, mean_eps: 0.251919\n",
            " 118301/200000: episode: 532, duration: 0.685s, episode steps: 115, steps per second: 168, episode reward: -114.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.896 [0.000, 2.000],  loss: 1.415096, mae: 29.895874, mean_q: -43.871185, mean_eps: 0.251121\n",
            " 118413/200000: episode: 533, duration: 0.645s, episode steps: 112, steps per second: 174, episode reward: -111.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.911 [0.000, 2.000],  loss: 1.487535, mae: 30.056733, mean_q: -44.162637, mean_eps: 0.250399\n",
            " 118552/200000: episode: 534, duration: 0.804s, episode steps: 139, steps per second: 173, episode reward: -138.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.820 [0.000, 2.000],  loss: 1.525700, mae: 29.836133, mean_q: -43.804127, mean_eps: 0.249614\n",
            " 118640/200000: episode: 535, duration: 0.553s, episode steps:  88, steps per second: 159, episode reward: -87.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.989 [0.000, 2.000],  loss: 1.559533, mae: 30.020135, mean_q: -44.059712, mean_eps: 0.248905\n",
            " 118743/200000: episode: 536, duration: 0.592s, episode steps: 103, steps per second: 174, episode reward: -102.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.039 [0.000, 2.000],  loss: 1.968742, mae: 29.624545, mean_q: -43.418467, mean_eps: 0.248297\n",
            " 118832/200000: episode: 537, duration: 0.545s, episode steps:  89, steps per second: 163, episode reward: -88.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.933 [0.000, 2.000],  loss: 1.618087, mae: 30.050833, mean_q: -44.129352, mean_eps: 0.247689\n",
            " 119220/200000: episode: 538, duration: 2.377s, episode steps: 388, steps per second: 163, episode reward: -387.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.688 [0.000, 2.000],  loss: 1.573503, mae: 29.791986, mean_q: -43.696077, mean_eps: 0.246181\n",
            " 119337/200000: episode: 539, duration: 0.787s, episode steps: 117, steps per second: 149, episode reward: -116.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.906 [0.000, 2.000],  loss: 1.452752, mae: 29.541998, mean_q: -43.328824, mean_eps: 0.244573\n",
            " 119440/200000: episode: 540, duration: 0.676s, episode steps: 103, steps per second: 152, episode reward: -102.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 1.540671, mae: 29.412977, mean_q: -43.113996, mean_eps: 0.243876\n",
            " 119527/200000: episode: 541, duration: 0.787s, episode steps:  87, steps per second: 111, episode reward: -86.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.816 [0.000, 2.000],  loss: 1.205621, mae: 29.870672, mean_q: -43.824819, mean_eps: 0.243281\n",
            " 120027/200000: episode: 542, duration: 3.743s, episode steps: 500, steps per second: 134, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.586 [0.000, 2.000],  loss: 1.467670, mae: 29.584608, mean_q: -43.391381, mean_eps: 0.241419\n",
            " 120144/200000: episode: 543, duration: 0.823s, episode steps: 117, steps per second: 142, episode reward: -116.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.709 [0.000, 2.000],  loss: 1.361014, mae: 29.557012, mean_q: -43.369702, mean_eps: 0.239468\n",
            " 120270/200000: episode: 544, duration: 0.953s, episode steps: 126, steps per second: 132, episode reward: -125.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.857 [0.000, 2.000],  loss: 1.826169, mae: 29.391643, mean_q: -43.046635, mean_eps: 0.238695\n",
            " 120375/200000: episode: 545, duration: 0.624s, episode steps: 105, steps per second: 168, episode reward: -104.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.829 [0.000, 2.000],  loss: 1.352550, mae: 29.528677, mean_q: -43.300962, mean_eps: 0.237961\n",
            " 120484/200000: episode: 546, duration: 0.701s, episode steps: 109, steps per second: 155, episode reward: -108.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.917 [0.000, 2.000],  loss: 1.557567, mae: 29.656415, mean_q: -43.502590, mean_eps: 0.237289\n",
            " 120613/200000: episode: 547, duration: 0.807s, episode steps: 129, steps per second: 160, episode reward: -128.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.915 [0.000, 2.000],  loss: 1.455508, mae: 29.606735, mean_q: -43.355956, mean_eps: 0.236529\n",
            " 120729/200000: episode: 548, duration: 0.754s, episode steps: 116, steps per second: 154, episode reward: -115.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.810 [0.000, 2.000],  loss: 0.846800, mae: 29.501753, mean_q: -43.249123, mean_eps: 0.235744\n",
            " 120830/200000: episode: 549, duration: 0.640s, episode steps: 101, steps per second: 158, episode reward: -100.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.752 [0.000, 2.000],  loss: 1.128791, mae: 29.474514, mean_q: -43.241775, mean_eps: 0.235060\n",
            " 120964/200000: episode: 550, duration: 0.935s, episode steps: 134, steps per second: 143, episode reward: -133.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.746 [0.000, 2.000],  loss: 1.057332, mae: 29.559505, mean_q: -43.352627, mean_eps: 0.234325\n",
            " 121073/200000: episode: 551, duration: 0.735s, episode steps: 109, steps per second: 148, episode reward: -108.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.716 [0.000, 2.000],  loss: 1.705381, mae: 29.370596, mean_q: -43.022352, mean_eps: 0.233553\n",
            " 121191/200000: episode: 552, duration: 0.798s, episode steps: 118, steps per second: 148, episode reward: -117.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.966 [0.000, 2.000],  loss: 1.887494, mae: 29.606057, mean_q: -43.422456, mean_eps: 0.232831\n",
            " 121282/200000: episode: 553, duration: 0.699s, episode steps:  91, steps per second: 130, episode reward: -90.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.758 [0.000, 2.000],  loss: 1.385401, mae: 29.235481, mean_q: -42.809141, mean_eps: 0.232172\n",
            " 121362/200000: episode: 554, duration: 0.707s, episode steps:  80, steps per second: 113, episode reward: -79.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.875 [0.000, 2.000],  loss: 1.677796, mae: 29.466748, mean_q: -43.201386, mean_eps: 0.231627\n",
            " 121499/200000: episode: 555, duration: 1.238s, episode steps: 137, steps per second: 111, episode reward: -136.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.080 [0.000, 2.000],  loss: 1.353549, mae: 29.564949, mean_q: -43.383951, mean_eps: 0.230943\n",
            " 121585/200000: episode: 556, duration: 0.802s, episode steps:  86, steps per second: 107, episode reward: -85.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.837 [0.000, 2.000],  loss: 1.992104, mae: 29.331608, mean_q: -42.911608, mean_eps: 0.230234\n",
            " 121685/200000: episode: 557, duration: 1.019s, episode steps: 100, steps per second:  98, episode reward: -99.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.780 [0.000, 2.000],  loss: 1.665298, mae: 29.339673, mean_q: -42.928766, mean_eps: 0.229639\n",
            " 121764/200000: episode: 558, duration: 0.497s, episode steps:  79, steps per second: 159, episode reward: -78.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.886 [0.000, 2.000],  loss: 1.561289, mae: 29.269239, mean_q: -42.911518, mean_eps: 0.229081\n",
            " 121850/200000: episode: 559, duration: 0.602s, episode steps:  86, steps per second: 143, episode reward: -85.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.965 [0.000, 2.000],  loss: 0.994090, mae: 29.615287, mean_q: -43.491585, mean_eps: 0.228562\n",
            " 121960/200000: episode: 560, duration: 0.722s, episode steps: 110, steps per second: 152, episode reward: -109.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.673 [0.000, 2.000],  loss: 1.405886, mae: 29.339216, mean_q: -42.994635, mean_eps: 0.227941\n",
            " 122116/200000: episode: 561, duration: 1.006s, episode steps: 156, steps per second: 155, episode reward: -155.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.891 [0.000, 2.000],  loss: 2.469844, mae: 29.361761, mean_q: -42.931422, mean_eps: 0.227105\n",
            " 122201/200000: episode: 562, duration: 0.562s, episode steps:  85, steps per second: 151, episode reward: -84.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.894 [0.000, 2.000],  loss: 1.133171, mae: 29.640003, mean_q: -43.448736, mean_eps: 0.226333\n",
            " 122285/200000: episode: 563, duration: 0.617s, episode steps:  84, steps per second: 136, episode reward: -83.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.976 [0.000, 2.000],  loss: 1.480118, mae: 29.689284, mean_q: -43.464608, mean_eps: 0.225788\n",
            " 122391/200000: episode: 564, duration: 0.621s, episode steps: 106, steps per second: 171, episode reward: -105.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.038 [0.000, 2.000],  loss: 1.736990, mae: 29.356602, mean_q: -42.931927, mean_eps: 0.225193\n",
            " 122494/200000: episode: 565, duration: 0.667s, episode steps: 103, steps per second: 155, episode reward: -102.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.942 [0.000, 2.000],  loss: 1.418347, mae: 29.531265, mean_q: -43.284335, mean_eps: 0.224534\n",
            " 122631/200000: episode: 566, duration: 0.873s, episode steps: 137, steps per second: 157, episode reward: -136.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.051 [0.000, 2.000],  loss: 1.477220, mae: 29.339986, mean_q: -43.000217, mean_eps: 0.223774\n",
            " 122715/200000: episode: 567, duration: 0.512s, episode steps:  84, steps per second: 164, episode reward: -83.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.810 [0.000, 2.000],  loss: 1.075988, mae: 29.376370, mean_q: -43.041680, mean_eps: 0.223077\n",
            " 122851/200000: episode: 568, duration: 0.891s, episode steps: 136, steps per second: 153, episode reward: -135.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.632 [0.000, 2.000],  loss: 1.386592, mae: 29.392167, mean_q: -43.084109, mean_eps: 0.222381\n",
            " 122954/200000: episode: 569, duration: 0.684s, episode steps: 103, steps per second: 150, episode reward: -102.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.932 [0.000, 2.000],  loss: 1.906468, mae: 29.315764, mean_q: -42.909527, mean_eps: 0.221621\n",
            " 123045/200000: episode: 570, duration: 0.594s, episode steps:  91, steps per second: 153, episode reward: -90.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.835 [0.000, 2.000],  loss: 1.281057, mae: 29.378983, mean_q: -43.043594, mean_eps: 0.221000\n",
            " 123144/200000: episode: 571, duration: 0.619s, episode steps:  99, steps per second: 160, episode reward: -98.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.889 [0.000, 2.000],  loss: 1.538632, mae: 29.720093, mean_q: -43.530292, mean_eps: 0.220405\n",
            " 123252/200000: episode: 572, duration: 0.814s, episode steps: 108, steps per second: 133, episode reward: -107.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.019 [0.000, 2.000],  loss: 1.602168, mae: 29.246061, mean_q: -42.806814, mean_eps: 0.219759\n",
            " 123338/200000: episode: 573, duration: 0.712s, episode steps:  86, steps per second: 121, episode reward: -85.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.930 [0.000, 2.000],  loss: 1.015879, mae: 29.194278, mean_q: -42.724589, mean_eps: 0.219138\n",
            " 123471/200000: episode: 574, duration: 1.249s, episode steps: 133, steps per second: 107, episode reward: -132.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.820 [0.000, 2.000],  loss: 1.739250, mae: 29.281434, mean_q: -42.820682, mean_eps: 0.218441\n",
            " 123584/200000: episode: 575, duration: 1.093s, episode steps: 113, steps per second: 103, episode reward: -112.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.894 [0.000, 2.000],  loss: 1.112412, mae: 29.331578, mean_q: -42.952370, mean_eps: 0.217669\n",
            " 123705/200000: episode: 576, duration: 0.808s, episode steps: 121, steps per second: 150, episode reward: -120.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.074 [0.000, 2.000],  loss: 1.515223, mae: 29.295626, mean_q: -42.892400, mean_eps: 0.216921\n",
            " 123799/200000: episode: 577, duration: 0.548s, episode steps:  94, steps per second: 172, episode reward: -93.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.968 [0.000, 2.000],  loss: 1.384535, mae: 29.168758, mean_q: -42.687537, mean_eps: 0.216237\n",
            " 123909/200000: episode: 578, duration: 0.670s, episode steps: 110, steps per second: 164, episode reward: -109.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.118 [0.000, 2.000],  loss: 1.411987, mae: 29.410761, mean_q: -43.067554, mean_eps: 0.215591\n",
            " 124004/200000: episode: 579, duration: 0.641s, episode steps:  95, steps per second: 148, episode reward: -94.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.989 [0.000, 2.000],  loss: 1.496637, mae: 29.079635, mean_q: -42.559382, mean_eps: 0.214945\n",
            " 124103/200000: episode: 580, duration: 0.688s, episode steps:  99, steps per second: 144, episode reward: -98.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 1.125654, mae: 29.338598, mean_q: -42.919647, mean_eps: 0.214337\n",
            " 124207/200000: episode: 581, duration: 0.675s, episode steps: 104, steps per second: 154, episode reward: -103.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.846 [0.000, 2.000],  loss: 1.463725, mae: 29.585452, mean_q: -43.333122, mean_eps: 0.213691\n",
            " 124309/200000: episode: 582, duration: 0.644s, episode steps: 102, steps per second: 158, episode reward: -101.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 1.199378, mae: 29.334686, mean_q: -42.906505, mean_eps: 0.213033\n",
            " 124401/200000: episode: 583, duration: 0.625s, episode steps:  92, steps per second: 147, episode reward: -91.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.989 [0.000, 2.000],  loss: 1.571906, mae: 29.471319, mean_q: -43.075960, mean_eps: 0.212412\n",
            " 124499/200000: episode: 584, duration: 0.564s, episode steps:  98, steps per second: 174, episode reward: -97.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.888 [0.000, 2.000],  loss: 0.936793, mae: 29.393650, mean_q: -43.033573, mean_eps: 0.211817\n",
            " 124623/200000: episode: 585, duration: 0.764s, episode steps: 124, steps per second: 162, episode reward: -123.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.185 [0.000, 2.000],  loss: 1.521716, mae: 29.373065, mean_q: -42.937572, mean_eps: 0.211120\n",
            " 124708/200000: episode: 586, duration: 0.524s, episode steps:  85, steps per second: 162, episode reward: -84.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.953 [0.000, 2.000],  loss: 1.381559, mae: 29.325672, mean_q: -42.827175, mean_eps: 0.210461\n",
            " 124792/200000: episode: 587, duration: 0.542s, episode steps:  84, steps per second: 155, episode reward: -83.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.964 [0.000, 2.000],  loss: 1.416625, mae: 29.421804, mean_q: -43.094613, mean_eps: 0.209929\n",
            " 124875/200000: episode: 588, duration: 0.507s, episode steps:  83, steps per second: 164, episode reward: -82.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.988 [0.000, 2.000],  loss: 1.200751, mae: 29.031422, mean_q: -42.458192, mean_eps: 0.209397\n",
            " 124958/200000: episode: 589, duration: 0.535s, episode steps:  83, steps per second: 155, episode reward: -82.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.084 [0.000, 2.000],  loss: 1.272906, mae: 29.125841, mean_q: -42.609986, mean_eps: 0.208865\n",
            " 125064/200000: episode: 590, duration: 0.678s, episode steps: 106, steps per second: 156, episode reward: -105.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.009 [0.000, 2.000],  loss: 1.270503, mae: 29.479932, mean_q: -43.175052, mean_eps: 0.208270\n",
            " 125189/200000: episode: 591, duration: 1.061s, episode steps: 125, steps per second: 118, episode reward: -124.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.032 [0.000, 2.000],  loss: 1.211502, mae: 29.058720, mean_q: -42.499448, mean_eps: 0.207535\n",
            " 125296/200000: episode: 592, duration: 1.053s, episode steps: 107, steps per second: 102, episode reward: -106.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.093 [0.000, 2.000],  loss: 1.220841, mae: 29.469135, mean_q: -43.209669, mean_eps: 0.206801\n",
            " 125433/200000: episode: 593, duration: 1.442s, episode steps: 137, steps per second:  95, episode reward: -136.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.175 [0.000, 2.000],  loss: 1.079779, mae: 29.360944, mean_q: -43.046573, mean_eps: 0.206028\n",
            " 125553/200000: episode: 594, duration: 1.184s, episode steps: 120, steps per second: 101, episode reward: -119.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.133 [0.000, 2.000],  loss: 1.258916, mae: 29.256088, mean_q: -42.794874, mean_eps: 0.205205\n",
            " 125758/200000: episode: 595, duration: 1.314s, episode steps: 205, steps per second: 156, episode reward: -204.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.600 [0.000, 2.000],  loss: 1.270020, mae: 29.317192, mean_q: -42.897365, mean_eps: 0.204179\n",
            " 125840/200000: episode: 596, duration: 0.540s, episode steps:  82, steps per second: 152, episode reward: -81.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.976 [0.000, 2.000],  loss: 0.987784, mae: 29.455898, mean_q: -43.108835, mean_eps: 0.203279\n",
            " 125925/200000: episode: 597, duration: 0.594s, episode steps:  85, steps per second: 143, episode reward: -84.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.882 [0.000, 2.000],  loss: 1.544650, mae: 29.137770, mean_q: -42.595617, mean_eps: 0.202747\n",
            " 126032/200000: episode: 598, duration: 0.718s, episode steps: 107, steps per second: 149, episode reward: -106.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.953 [0.000, 2.000],  loss: 1.254710, mae: 29.189876, mean_q: -42.726532, mean_eps: 0.202139\n",
            " 126144/200000: episode: 599, duration: 0.728s, episode steps: 112, steps per second: 154, episode reward: -111.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.036 [0.000, 2.000],  loss: 1.158834, mae: 29.329330, mean_q: -42.885477, mean_eps: 0.201455\n",
            " 126266/200000: episode: 600, duration: 0.766s, episode steps: 122, steps per second: 159, episode reward: -121.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.008 [0.000, 2.000],  loss: 2.196954, mae: 29.235401, mean_q: -42.722067, mean_eps: 0.200708\n",
            " 126375/200000: episode: 601, duration: 0.668s, episode steps: 109, steps per second: 163, episode reward: -108.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.073 [0.000, 2.000],  loss: 1.208042, mae: 29.237589, mean_q: -42.833887, mean_eps: 0.199973\n",
            " 126476/200000: episode: 602, duration: 0.642s, episode steps: 101, steps per second: 157, episode reward: -100.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.109 [0.000, 2.000],  loss: 1.609936, mae: 29.174126, mean_q: -42.710895, mean_eps: 0.199315\n",
            " 126606/200000: episode: 603, duration: 0.908s, episode steps: 130, steps per second: 143, episode reward: -129.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.892 [0.000, 2.000],  loss: 1.203091, mae: 29.385361, mean_q: -43.041123, mean_eps: 0.198580\n",
            " 126703/200000: episode: 604, duration: 0.643s, episode steps:  97, steps per second: 151, episode reward: -96.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.887 [0.000, 2.000],  loss: 1.222022, mae: 29.182784, mean_q: -42.720565, mean_eps: 0.197858\n",
            " 126851/200000: episode: 605, duration: 0.922s, episode steps: 148, steps per second: 161, episode reward: -147.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.818 [0.000, 2.000],  loss: 1.294363, mae: 29.210866, mean_q: -42.759694, mean_eps: 0.197085\n",
            " 126955/200000: episode: 606, duration: 0.655s, episode steps: 104, steps per second: 159, episode reward: -103.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.971 [0.000, 2.000],  loss: 1.346080, mae: 29.097337, mean_q: -42.488036, mean_eps: 0.196287\n",
            " 127048/200000: episode: 607, duration: 0.615s, episode steps:  93, steps per second: 151, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.935 [0.000, 2.000],  loss: 1.280229, mae: 29.045847, mean_q: -42.501660, mean_eps: 0.195667\n",
            " 127283/200000: episode: 608, duration: 2.135s, episode steps: 235, steps per second: 110, episode reward: -234.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.430 [0.000, 2.000],  loss: 1.225080, mae: 29.117476, mean_q: -42.567192, mean_eps: 0.194628\n",
            " 127380/200000: episode: 609, duration: 0.904s, episode steps:  97, steps per second: 107, episode reward: -96.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.134 [0.000, 2.000],  loss: 2.039725, mae: 29.049425, mean_q: -42.459238, mean_eps: 0.193577\n",
            " 127456/200000: episode: 610, duration: 0.660s, episode steps:  76, steps per second: 115, episode reward: -75.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.039 [0.000, 2.000],  loss: 1.188582, mae: 29.076532, mean_q: -42.538413, mean_eps: 0.193032\n",
            " 127546/200000: episode: 611, duration: 0.553s, episode steps:  90, steps per second: 163, episode reward: -89.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.922 [0.000, 2.000],  loss: 1.329904, mae: 28.876049, mean_q: -42.206008, mean_eps: 0.192500\n",
            " 127649/200000: episode: 612, duration: 0.708s, episode steps: 103, steps per second: 146, episode reward: -102.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.922 [0.000, 2.000],  loss: 1.335688, mae: 29.131035, mean_q: -42.603761, mean_eps: 0.191879\n",
            " 127744/200000: episode: 613, duration: 0.642s, episode steps:  95, steps per second: 148, episode reward: -94.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.053 [0.000, 2.000],  loss: 1.421955, mae: 29.162238, mean_q: -42.628979, mean_eps: 0.191259\n",
            " 127852/200000: episode: 614, duration: 0.678s, episode steps: 108, steps per second: 159, episode reward: -107.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.083 [0.000, 2.000],  loss: 1.111255, mae: 28.965444, mean_q: -42.342863, mean_eps: 0.190625\n",
            " 127958/200000: episode: 615, duration: 0.724s, episode steps: 106, steps per second: 146, episode reward: -105.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.179 [0.000, 2.000],  loss: 0.970183, mae: 29.227267, mean_q: -42.726849, mean_eps: 0.189941\n",
            " 128059/200000: episode: 616, duration: 0.662s, episode steps: 101, steps per second: 152, episode reward: -100.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.891 [0.000, 2.000],  loss: 1.249871, mae: 29.098993, mean_q: -42.500481, mean_eps: 0.189283\n",
            " 128336/200000: episode: 617, duration: 1.858s, episode steps: 277, steps per second: 149, episode reward: -276.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.430 [0.000, 2.000],  loss: 1.471293, mae: 28.977635, mean_q: -42.313987, mean_eps: 0.188092\n",
            " 128836/200000: episode: 618, duration: 3.290s, episode steps: 500, steps per second: 152, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.672 [0.000, 2.000],  loss: 1.394704, mae: 28.898852, mean_q: -42.225035, mean_eps: 0.185635\n",
            " 128950/200000: episode: 619, duration: 0.720s, episode steps: 114, steps per second: 158, episode reward: -113.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.930 [0.000, 2.000],  loss: 1.051558, mae: 29.080027, mean_q: -42.537710, mean_eps: 0.183684\n",
            " 129450/200000: episode: 620, duration: 3.832s, episode steps: 500, steps per second: 130, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.336 [0.000, 2.000],  loss: 1.286401, mae: 28.793172, mean_q: -42.040159, mean_eps: 0.181733\n",
            " 129542/200000: episode: 621, duration: 0.522s, episode steps:  92, steps per second: 176, episode reward: -91.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.837 [0.000, 2.000],  loss: 1.364603, mae: 29.083023, mean_q: -42.512869, mean_eps: 0.179859\n",
            " 129655/200000: episode: 622, duration: 0.639s, episode steps: 113, steps per second: 177, episode reward: -112.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.805 [0.000, 2.000],  loss: 1.040931, mae: 28.854918, mean_q: -42.143433, mean_eps: 0.179213\n",
            " 129750/200000: episode: 623, duration: 0.534s, episode steps:  95, steps per second: 178, episode reward: -94.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.905 [0.000, 2.000],  loss: 1.615686, mae: 28.867535, mean_q: -42.195896, mean_eps: 0.178554\n",
            " 129863/200000: episode: 624, duration: 0.670s, episode steps: 113, steps per second: 169, episode reward: -112.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.938 [0.000, 2.000],  loss: 1.322502, mae: 28.819622, mean_q: -42.106197, mean_eps: 0.177895\n",
            " 129940/200000: episode: 625, duration: 0.493s, episode steps:  77, steps per second: 156, episode reward: -76.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.039 [0.000, 2.000],  loss: 1.656995, mae: 28.751269, mean_q: -41.944785, mean_eps: 0.177300\n",
            " 130033/200000: episode: 626, duration: 0.566s, episode steps:  93, steps per second: 164, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.806 [0.000, 2.000],  loss: 1.185252, mae: 28.650800, mean_q: -41.847018, mean_eps: 0.176755\n",
            " 130141/200000: episode: 627, duration: 0.634s, episode steps: 108, steps per second: 170, episode reward: -107.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.824 [0.000, 2.000],  loss: 1.632314, mae: 28.470638, mean_q: -41.561263, mean_eps: 0.176109\n",
            " 130224/200000: episode: 628, duration: 0.474s, episode steps:  83, steps per second: 175, episode reward: -82.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.867 [0.000, 2.000],  loss: 1.246825, mae: 28.727848, mean_q: -41.855709, mean_eps: 0.175514\n",
            " 130355/200000: episode: 629, duration: 0.781s, episode steps: 131, steps per second: 168, episode reward: -130.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.015 [0.000, 2.000],  loss: 1.007660, mae: 28.617054, mean_q: -41.798557, mean_eps: 0.174843\n",
            " 130434/200000: episode: 630, duration: 0.511s, episode steps:  79, steps per second: 155, episode reward: -78.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.962 [0.000, 2.000],  loss: 1.149387, mae: 28.517495, mean_q: -41.620641, mean_eps: 0.174171\n",
            " 130526/200000: episode: 631, duration: 0.585s, episode steps:  92, steps per second: 157, episode reward: -91.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.891 [0.000, 2.000],  loss: 1.296787, mae: 28.585511, mean_q: -41.741949, mean_eps: 0.173627\n",
            " 130615/200000: episode: 632, duration: 0.575s, episode steps:  89, steps per second: 155, episode reward: -88.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.944 [0.000, 2.000],  loss: 1.325060, mae: 28.599667, mean_q: -41.806467, mean_eps: 0.173057\n",
            " 130724/200000: episode: 633, duration: 0.696s, episode steps: 109, steps per second: 157, episode reward: -108.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.945 [0.000, 2.000],  loss: 0.945244, mae: 28.677202, mean_q: -41.889586, mean_eps: 0.172436\n",
            " 130887/200000: episode: 634, duration: 1.025s, episode steps: 163, steps per second: 159, episode reward: -162.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.491 [0.000, 2.000],  loss: 1.709193, mae: 28.575348, mean_q: -41.710700, mean_eps: 0.171575\n",
            " 130987/200000: episode: 635, duration: 0.732s, episode steps: 100, steps per second: 137, episode reward: -99.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.040 [0.000, 2.000],  loss: 1.044820, mae: 28.686679, mean_q: -41.907929, mean_eps: 0.170739\n",
            " 131087/200000: episode: 636, duration: 0.975s, episode steps: 100, steps per second: 103, episode reward: -99.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.080 [0.000, 2.000],  loss: 1.277360, mae: 28.362443, mean_q: -41.365628, mean_eps: 0.170105\n",
            " 131310/200000: episode: 637, duration: 2.061s, episode steps: 223, steps per second: 108, episode reward: -222.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.834 [0.000, 2.000],  loss: 1.881109, mae: 28.573714, mean_q: -41.657324, mean_eps: 0.169079\n",
            " 131397/200000: episode: 638, duration: 0.736s, episode steps:  87, steps per second: 118, episode reward: -86.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.908 [0.000, 2.000],  loss: 1.457267, mae: 28.007989, mean_q: -40.753525, mean_eps: 0.168091\n",
            " 131467/200000: episode: 639, duration: 0.527s, episode steps:  70, steps per second: 133, episode reward: -69.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.871 [0.000, 2.000],  loss: 1.933427, mae: 28.568015, mean_q: -41.657300, mean_eps: 0.167597\n",
            " 131565/200000: episode: 640, duration: 0.658s, episode steps:  98, steps per second: 149, episode reward: -97.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.806 [0.000, 2.000],  loss: 1.410330, mae: 28.370017, mean_q: -41.382415, mean_eps: 0.167065\n",
            " 131672/200000: episode: 641, duration: 0.689s, episode steps: 107, steps per second: 155, episode reward: -106.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.159 [0.000, 2.000],  loss: 1.592990, mae: 28.774465, mean_q: -41.990046, mean_eps: 0.166419\n",
            " 131765/200000: episode: 642, duration: 0.660s, episode steps:  93, steps per second: 141, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.968 [0.000, 2.000],  loss: 1.053311, mae: 28.567024, mean_q: -41.703333, mean_eps: 0.165786\n",
            " 131876/200000: episode: 643, duration: 0.666s, episode steps: 111, steps per second: 167, episode reward: -110.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.099 [0.000, 2.000],  loss: 1.287893, mae: 28.474341, mean_q: -41.490482, mean_eps: 0.165140\n",
            " 131947/200000: episode: 644, duration: 0.451s, episode steps:  71, steps per second: 158, episode reward: -70.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.958 [0.000, 2.000],  loss: 1.341597, mae: 28.652805, mean_q: -41.819304, mean_eps: 0.164570\n",
            " 132074/200000: episode: 645, duration: 0.777s, episode steps: 127, steps per second: 163, episode reward: -126.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.079 [0.000, 2.000],  loss: 1.521556, mae: 28.520085, mean_q: -41.562964, mean_eps: 0.163937\n",
            " 132193/200000: episode: 646, duration: 0.724s, episode steps: 119, steps per second: 164, episode reward: -118.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.899 [0.000, 2.000],  loss: 1.321392, mae: 28.308112, mean_q: -41.286037, mean_eps: 0.163151\n",
            " 132323/200000: episode: 647, duration: 0.830s, episode steps: 130, steps per second: 157, episode reward: -129.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.862 [0.000, 2.000],  loss: 1.325418, mae: 28.411729, mean_q: -41.456904, mean_eps: 0.162366\n",
            " 132411/200000: episode: 648, duration: 0.595s, episode steps:  88, steps per second: 148, episode reward: -87.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.977 [0.000, 2.000],  loss: 1.308999, mae: 28.659142, mean_q: -41.714595, mean_eps: 0.161682\n",
            " 132497/200000: episode: 649, duration: 0.560s, episode steps:  86, steps per second: 154, episode reward: -85.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.930 [0.000, 2.000],  loss: 1.251852, mae: 28.477016, mean_q: -41.456369, mean_eps: 0.161125\n",
            " 132586/200000: episode: 650, duration: 0.557s, episode steps:  89, steps per second: 160, episode reward: -88.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.910 [0.000, 2.000],  loss: 1.501214, mae: 28.389591, mean_q: -41.337822, mean_eps: 0.160567\n",
            " 132682/200000: episode: 651, duration: 0.653s, episode steps:  96, steps per second: 147, episode reward: -95.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 1.320808, mae: 28.546778, mean_q: -41.650514, mean_eps: 0.159985\n",
            " 132764/200000: episode: 652, duration: 0.547s, episode steps:  82, steps per second: 150, episode reward: -81.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.976 [0.000, 2.000],  loss: 1.445860, mae: 28.304251, mean_q: -41.237786, mean_eps: 0.159427\n",
            " 132883/200000: episode: 653, duration: 0.786s, episode steps: 119, steps per second: 151, episode reward: -118.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.176 [0.000, 2.000],  loss: 1.687514, mae: 28.713622, mean_q: -41.834186, mean_eps: 0.158794\n",
            " 133000/200000: episode: 654, duration: 1.055s, episode steps: 117, steps per second: 111, episode reward: -116.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.855 [0.000, 2.000],  loss: 1.175431, mae: 28.648672, mean_q: -41.773086, mean_eps: 0.158047\n",
            " 133070/200000: episode: 655, duration: 0.632s, episode steps:  70, steps per second: 111, episode reward: -69.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.057 [0.000, 2.000],  loss: 1.183724, mae: 28.865530, mean_q: -42.157350, mean_eps: 0.157451\n",
            " 133176/200000: episode: 656, duration: 0.952s, episode steps: 106, steps per second: 111, episode reward: -105.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.038 [0.000, 2.000],  loss: 1.163443, mae: 28.723628, mean_q: -41.944180, mean_eps: 0.156894\n",
            " 133257/200000: episode: 657, duration: 0.832s, episode steps:  81, steps per second:  97, episode reward: -80.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.877 [0.000, 2.000],  loss: 1.559868, mae: 28.353621, mean_q: -41.273283, mean_eps: 0.156299\n",
            " 133342/200000: episode: 658, duration: 0.583s, episode steps:  85, steps per second: 146, episode reward: -84.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.941 [0.000, 2.000],  loss: 1.471903, mae: 28.548563, mean_q: -41.596882, mean_eps: 0.155767\n",
            " 133450/200000: episode: 659, duration: 0.690s, episode steps: 108, steps per second: 156, episode reward: -107.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.028 [0.000, 2.000],  loss: 1.108971, mae: 28.694539, mean_q: -41.856929, mean_eps: 0.155159\n",
            " 133543/200000: episode: 660, duration: 0.609s, episode steps:  93, steps per second: 153, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.914 [0.000, 2.000],  loss: 1.447780, mae: 28.552069, mean_q: -41.603010, mean_eps: 0.154525\n",
            " 133629/200000: episode: 661, duration: 0.526s, episode steps:  86, steps per second: 163, episode reward: -85.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.837 [0.000, 2.000],  loss: 2.046378, mae: 28.451055, mean_q: -41.328055, mean_eps: 0.153955\n",
            " 133731/200000: episode: 662, duration: 0.600s, episode steps: 102, steps per second: 170, episode reward: -101.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.059 [0.000, 2.000],  loss: 1.093594, mae: 28.867896, mean_q: -42.075597, mean_eps: 0.153360\n",
            " 133820/200000: episode: 663, duration: 0.521s, episode steps:  89, steps per second: 171, episode reward: -88.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.955 [0.000, 2.000],  loss: 1.720860, mae: 28.469840, mean_q: -41.452119, mean_eps: 0.152765\n",
            " 134070/200000: episode: 664, duration: 1.572s, episode steps: 250, steps per second: 159, episode reward: -249.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.188 [0.000, 2.000],  loss: 1.720623, mae: 28.519269, mean_q: -41.539600, mean_eps: 0.151688\n",
            " 134172/200000: episode: 665, duration: 0.609s, episode steps: 102, steps per second: 167, episode reward: -101.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.873 [0.000, 2.000],  loss: 1.697606, mae: 28.341054, mean_q: -41.256142, mean_eps: 0.150573\n",
            " 134276/200000: episode: 666, duration: 0.670s, episode steps: 104, steps per second: 155, episode reward: -103.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.856 [0.000, 2.000],  loss: 1.088148, mae: 28.431277, mean_q: -41.457293, mean_eps: 0.149927\n",
            " 134369/200000: episode: 667, duration: 0.575s, episode steps:  93, steps per second: 162, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.892 [0.000, 2.000],  loss: 1.062262, mae: 28.300666, mean_q: -41.273942, mean_eps: 0.149294\n",
            " 134451/200000: episode: 668, duration: 0.495s, episode steps:  82, steps per second: 166, episode reward: -81.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.915 [0.000, 2.000],  loss: 1.838909, mae: 28.445044, mean_q: -41.477363, mean_eps: 0.148737\n",
            " 134567/200000: episode: 669, duration: 0.702s, episode steps: 116, steps per second: 165, episode reward: -115.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.879 [0.000, 2.000],  loss: 1.240542, mae: 28.603437, mean_q: -41.654071, mean_eps: 0.148116\n",
            " 134667/200000: episode: 670, duration: 0.604s, episode steps: 100, steps per second: 166, episode reward: -99.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.890 [0.000, 2.000],  loss: 1.742408, mae: 28.397744, mean_q: -41.430376, mean_eps: 0.147432\n",
            " 134750/200000: episode: 671, duration: 0.499s, episode steps:  83, steps per second: 166, episode reward: -82.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.964 [0.000, 2.000],  loss: 1.179412, mae: 28.836843, mean_q: -42.092744, mean_eps: 0.146849\n",
            " 134851/200000: episode: 672, duration: 0.642s, episode steps: 101, steps per second: 157, episode reward: -100.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.089 [0.000, 2.000],  loss: 1.300707, mae: 28.259347, mean_q: -41.092781, mean_eps: 0.146267\n",
            " 134940/200000: episode: 673, duration: 0.868s, episode steps:  89, steps per second: 102, episode reward: -88.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.011 [0.000, 2.000],  loss: 1.690298, mae: 28.302938, mean_q: -41.155294, mean_eps: 0.145671\n",
            " 135019/200000: episode: 674, duration: 0.743s, episode steps:  79, steps per second: 106, episode reward: -78.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.975 [0.000, 2.000],  loss: 1.496964, mae: 28.024195, mean_q: -40.794257, mean_eps: 0.145139\n",
            " 135114/200000: episode: 675, duration: 0.785s, episode steps:  95, steps per second: 121, episode reward: -94.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.916 [0.000, 2.000],  loss: 1.113836, mae: 28.486371, mean_q: -41.490283, mean_eps: 0.144582\n",
            " 135228/200000: episode: 676, duration: 1.061s, episode steps: 114, steps per second: 107, episode reward: -113.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.123 [0.000, 2.000],  loss: 1.250342, mae: 28.292735, mean_q: -41.188125, mean_eps: 0.143923\n",
            " 135319/200000: episode: 677, duration: 0.575s, episode steps:  91, steps per second: 158, episode reward: -90.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.824 [0.000, 2.000],  loss: 0.965960, mae: 28.348640, mean_q: -41.369040, mean_eps: 0.143277\n",
            " 135437/200000: episode: 678, duration: 0.755s, episode steps: 118, steps per second: 156, episode reward: -117.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.729 [0.000, 2.000],  loss: 1.247959, mae: 28.439393, mean_q: -41.448219, mean_eps: 0.142606\n",
            " 135535/200000: episode: 679, duration: 0.594s, episode steps:  98, steps per second: 165, episode reward: -97.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.092 [0.000, 2.000],  loss: 1.391679, mae: 28.237675, mean_q: -41.157356, mean_eps: 0.141922\n",
            " 135627/200000: episode: 680, duration: 0.546s, episode steps:  92, steps per second: 168, episode reward: -91.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.913 [0.000, 2.000],  loss: 1.139007, mae: 28.365617, mean_q: -41.360941, mean_eps: 0.141327\n",
            " 135709/200000: episode: 681, duration: 0.511s, episode steps:  82, steps per second: 161, episode reward: -81.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 1.867484, mae: 28.357862, mean_q: -41.363224, mean_eps: 0.140769\n",
            " 135799/200000: episode: 682, duration: 0.538s, episode steps:  90, steps per second: 167, episode reward: -89.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.011 [0.000, 2.000],  loss: 1.258363, mae: 28.228784, mean_q: -41.111700, mean_eps: 0.140225\n",
            " 135896/200000: episode: 683, duration: 0.594s, episode steps:  97, steps per second: 163, episode reward: -96.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.804 [0.000, 2.000],  loss: 1.237804, mae: 28.145295, mean_q: -40.983804, mean_eps: 0.139642\n",
            " 135997/200000: episode: 684, duration: 0.597s, episode steps: 101, steps per second: 169, episode reward: -100.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.020 [0.000, 2.000],  loss: 1.110042, mae: 28.380101, mean_q: -41.422204, mean_eps: 0.139009\n",
            " 136114/200000: episode: 685, duration: 0.720s, episode steps: 117, steps per second: 162, episode reward: -116.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.179 [0.000, 2.000],  loss: 1.403678, mae: 28.519464, mean_q: -41.583978, mean_eps: 0.138312\n",
            " 136184/200000: episode: 686, duration: 0.431s, episode steps:  70, steps per second: 162, episode reward: -69.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.829 [0.000, 2.000],  loss: 1.873009, mae: 28.130704, mean_q: -40.985468, mean_eps: 0.137729\n",
            " 136289/200000: episode: 687, duration: 0.674s, episode steps: 105, steps per second: 156, episode reward: -104.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.962 [0.000, 2.000],  loss: 1.462607, mae: 28.597453, mean_q: -41.680671, mean_eps: 0.137172\n",
            " 136365/200000: episode: 688, duration: 0.468s, episode steps:  76, steps per second: 163, episode reward: -75.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.053 [0.000, 2.000],  loss: 1.030390, mae: 28.196077, mean_q: -41.078921, mean_eps: 0.136589\n",
            " 136446/200000: episode: 689, duration: 0.536s, episode steps:  81, steps per second: 151, episode reward: -80.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.914 [0.000, 2.000],  loss: 1.350838, mae: 28.133640, mean_q: -41.047503, mean_eps: 0.136095\n",
            " 136547/200000: episode: 690, duration: 0.657s, episode steps: 101, steps per second: 154, episode reward: -100.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.980 [0.000, 2.000],  loss: 1.293146, mae: 28.243937, mean_q: -41.222368, mean_eps: 0.135525\n",
            " 136651/200000: episode: 691, duration: 0.655s, episode steps: 104, steps per second: 159, episode reward: -103.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.029 [0.000, 2.000],  loss: 1.234167, mae: 28.257627, mean_q: -41.211165, mean_eps: 0.134879\n",
            " 136750/200000: episode: 692, duration: 0.720s, episode steps:  99, steps per second: 137, episode reward: -98.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.960 [0.000, 2.000],  loss: 1.050021, mae: 28.045887, mean_q: -40.939450, mean_eps: 0.134233\n",
            " 136858/200000: episode: 693, duration: 0.809s, episode steps: 108, steps per second: 133, episode reward: -107.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 1.474667, mae: 28.525591, mean_q: -41.602745, mean_eps: 0.133575\n",
            " 136956/200000: episode: 694, duration: 0.876s, episode steps:  98, steps per second: 112, episode reward: -97.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.041 [0.000, 2.000],  loss: 1.079520, mae: 28.123133, mean_q: -40.988855, mean_eps: 0.132929\n",
            " 137040/200000: episode: 695, duration: 0.801s, episode steps:  84, steps per second: 105, episode reward: -83.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.012 [0.000, 2.000],  loss: 1.055355, mae: 28.482179, mean_q: -41.352355, mean_eps: 0.132359\n",
            " 137120/200000: episode: 696, duration: 0.686s, episode steps:  80, steps per second: 117, episode reward: -79.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.887 [0.000, 2.000],  loss: 1.176592, mae: 28.352769, mean_q: -41.257321, mean_eps: 0.131839\n",
            " 137202/200000: episode: 697, duration: 0.791s, episode steps:  82, steps per second: 104, episode reward: -81.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.927 [0.000, 2.000],  loss: 1.293187, mae: 28.291926, mean_q: -41.176090, mean_eps: 0.131320\n",
            " 137283/200000: episode: 698, duration: 0.499s, episode steps:  81, steps per second: 162, episode reward: -80.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.926 [0.000, 2.000],  loss: 1.309657, mae: 27.973851, mean_q: -40.710832, mean_eps: 0.130801\n",
            " 137365/200000: episode: 699, duration: 0.572s, episode steps:  82, steps per second: 143, episode reward: -81.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.866 [0.000, 2.000],  loss: 1.179303, mae: 27.948738, mean_q: -40.670284, mean_eps: 0.130281\n",
            " 137438/200000: episode: 700, duration: 0.437s, episode steps:  73, steps per second: 167, episode reward: -72.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.918 [0.000, 2.000],  loss: 1.783990, mae: 28.264141, mean_q: -41.025489, mean_eps: 0.129787\n",
            " 137523/200000: episode: 701, duration: 0.560s, episode steps:  85, steps per second: 152, episode reward: -84.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.059 [0.000, 2.000],  loss: 1.150888, mae: 28.132563, mean_q: -40.896492, mean_eps: 0.129293\n",
            " 137625/200000: episode: 702, duration: 0.624s, episode steps: 102, steps per second: 163, episode reward: -101.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.108 [0.000, 2.000],  loss: 1.109819, mae: 27.952996, mean_q: -40.641655, mean_eps: 0.128698\n",
            " 137815/200000: episode: 703, duration: 1.191s, episode steps: 190, steps per second: 160, episode reward: -189.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.784 [0.000, 2.000],  loss: 1.267248, mae: 28.077255, mean_q: -40.895626, mean_eps: 0.127773\n",
            " 137924/200000: episode: 704, duration: 0.769s, episode steps: 109, steps per second: 142, episode reward: -108.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.780 [0.000, 2.000],  loss: 1.046274, mae: 28.005503, mean_q: -40.771568, mean_eps: 0.126836\n",
            " 138026/200000: episode: 705, duration: 0.738s, episode steps: 102, steps per second: 138, episode reward: -101.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.657 [0.000, 2.000],  loss: 1.321429, mae: 27.938075, mean_q: -40.640393, mean_eps: 0.126165\n",
            " 138117/200000: episode: 706, duration: 0.676s, episode steps:  91, steps per second: 135, episode reward: -90.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.901 [0.000, 2.000],  loss: 1.110013, mae: 28.167369, mean_q: -41.049670, mean_eps: 0.125544\n",
            " 138216/200000: episode: 707, duration: 0.683s, episode steps:  99, steps per second: 145, episode reward: -98.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.747 [0.000, 2.000],  loss: 1.281850, mae: 28.075102, mean_q: -40.941755, mean_eps: 0.124949\n",
            " 138309/200000: episode: 708, duration: 0.620s, episode steps:  93, steps per second: 150, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.022 [0.000, 2.000],  loss: 1.091322, mae: 27.964117, mean_q: -40.720123, mean_eps: 0.124341\n",
            " 138390/200000: episode: 709, duration: 0.529s, episode steps:  81, steps per second: 153, episode reward: -80.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 1.300424, mae: 28.244121, mean_q: -41.134531, mean_eps: 0.123783\n",
            " 138466/200000: episode: 710, duration: 0.522s, episode steps:  76, steps per second: 146, episode reward: -75.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.868 [0.000, 2.000],  loss: 0.949405, mae: 28.248911, mean_q: -41.193883, mean_eps: 0.123289\n",
            " 138548/200000: episode: 711, duration: 0.570s, episode steps:  82, steps per second: 144, episode reward: -81.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.963 [0.000, 2.000],  loss: 1.670281, mae: 28.125171, mean_q: -40.963914, mean_eps: 0.122795\n",
            " 138653/200000: episode: 712, duration: 0.728s, episode steps: 105, steps per second: 144, episode reward: -104.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.771 [0.000, 2.000],  loss: 1.261174, mae: 28.000994, mean_q: -40.734739, mean_eps: 0.122200\n",
            " 138744/200000: episode: 713, duration: 0.748s, episode steps:  91, steps per second: 122, episode reward: -90.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.857 [0.000, 2.000],  loss: 1.480611, mae: 28.184963, mean_q: -41.035929, mean_eps: 0.121579\n",
            " 138827/200000: episode: 714, duration: 0.768s, episode steps:  83, steps per second: 108, episode reward: -82.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.904 [0.000, 2.000],  loss: 1.631386, mae: 27.996757, mean_q: -40.684234, mean_eps: 0.121035\n",
            " 138914/200000: episode: 715, duration: 0.835s, episode steps:  87, steps per second: 104, episode reward: -86.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.920 [0.000, 2.000],  loss: 1.230275, mae: 28.128407, mean_q: -41.003489, mean_eps: 0.120490\n",
            " 139009/200000: episode: 716, duration: 0.842s, episode steps:  95, steps per second: 113, episode reward: -94.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.874 [0.000, 2.000],  loss: 1.642798, mae: 28.220590, mean_q: -41.081842, mean_eps: 0.119907\n",
            " 139092/200000: episode: 717, duration: 0.698s, episode steps:  83, steps per second: 119, episode reward: -82.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.843 [0.000, 2.000],  loss: 1.459496, mae: 28.092833, mean_q: -40.872808, mean_eps: 0.119350\n",
            " 139196/200000: episode: 718, duration: 0.642s, episode steps: 104, steps per second: 162, episode reward: -103.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.894 [0.000, 2.000],  loss: 1.413647, mae: 28.383410, mean_q: -41.366107, mean_eps: 0.118767\n",
            " 139344/200000: episode: 719, duration: 0.893s, episode steps: 148, steps per second: 166, episode reward: -147.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.696 [0.000, 2.000],  loss: 1.268022, mae: 27.956909, mean_q: -40.695260, mean_eps: 0.117969\n",
            " 139443/200000: episode: 720, duration: 0.660s, episode steps:  99, steps per second: 150, episode reward: -98.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.939 [0.000, 2.000],  loss: 1.246648, mae: 28.051373, mean_q: -40.785112, mean_eps: 0.117184\n",
            " 139515/200000: episode: 721, duration: 0.440s, episode steps:  72, steps per second: 164, episode reward: -71.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.014 [0.000, 2.000],  loss: 0.942309, mae: 28.405991, mean_q: -41.357103, mean_eps: 0.116639\n",
            " 139595/200000: episode: 722, duration: 0.471s, episode steps:  80, steps per second: 170, episode reward: -79.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 1.232132, mae: 28.240436, mean_q: -41.041516, mean_eps: 0.116158\n",
            " 139693/200000: episode: 723, duration: 0.609s, episode steps:  98, steps per second: 161, episode reward: -97.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.857 [0.000, 2.000],  loss: 0.975026, mae: 27.997343, mean_q: -40.769151, mean_eps: 0.115588\n",
            " 139769/200000: episode: 724, duration: 0.475s, episode steps:  76, steps per second: 160, episode reward: -75.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.026 [0.000, 2.000],  loss: 0.993346, mae: 28.138616, mean_q: -40.933815, mean_eps: 0.115031\n",
            " 139847/200000: episode: 725, duration: 0.469s, episode steps:  78, steps per second: 166, episode reward: -77.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 1.365789, mae: 28.196159, mean_q: -41.031442, mean_eps: 0.114549\n",
            " 139951/200000: episode: 726, duration: 0.707s, episode steps: 104, steps per second: 147, episode reward: -103.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.058 [0.000, 2.000],  loss: 0.990962, mae: 28.108835, mean_q: -40.919983, mean_eps: 0.113979\n",
            " 140014/200000: episode: 727, duration: 0.403s, episode steps:  63, steps per second: 156, episode reward: -62.000, mean reward: -0.984 [-1.000,  0.000], mean action: 1.032 [0.000, 2.000],  loss: 1.569779, mae: 27.731424, mean_q: -40.308966, mean_eps: 0.113447\n",
            " 140096/200000: episode: 728, duration: 0.549s, episode steps:  82, steps per second: 149, episode reward: -81.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 1.324250, mae: 27.816026, mean_q: -40.394928, mean_eps: 0.112991\n",
            " 140205/200000: episode: 729, duration: 0.664s, episode steps: 109, steps per second: 164, episode reward: -108.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.688 [0.000, 2.000],  loss: 1.612062, mae: 27.578659, mean_q: -40.127256, mean_eps: 0.112383\n",
            " 140294/200000: episode: 730, duration: 0.537s, episode steps:  89, steps per second: 166, episode reward: -88.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.944 [0.000, 2.000],  loss: 0.943859, mae: 28.155691, mean_q: -40.917452, mean_eps: 0.111750\n",
            " 140403/200000: episode: 731, duration: 0.655s, episode steps: 109, steps per second: 166, episode reward: -108.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.862 [0.000, 2.000],  loss: 1.074530, mae: 27.881895, mean_q: -40.583412, mean_eps: 0.111129\n",
            " 140495/200000: episode: 732, duration: 0.551s, episode steps:  92, steps per second: 167, episode reward: -91.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.989 [0.000, 2.000],  loss: 1.305219, mae: 27.673612, mean_q: -40.190023, mean_eps: 0.110496\n",
            " 140582/200000: episode: 733, duration: 0.550s, episode steps:  87, steps per second: 158, episode reward: -86.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.713 [0.000, 2.000],  loss: 1.411624, mae: 27.902703, mean_q: -40.565849, mean_eps: 0.109926\n",
            " 140660/200000: episode: 734, duration: 0.480s, episode steps:  78, steps per second: 162, episode reward: -77.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.987 [0.000, 2.000],  loss: 1.295625, mae: 27.600930, mean_q: -40.074403, mean_eps: 0.109407\n",
            " 140763/200000: episode: 735, duration: 0.938s, episode steps: 103, steps per second: 110, episode reward: -102.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.835 [0.000, 2.000],  loss: 1.155227, mae: 28.125698, mean_q: -40.915181, mean_eps: 0.108837\n",
            " 140849/200000: episode: 736, duration: 0.835s, episode steps:  86, steps per second: 103, episode reward: -85.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.116 [0.000, 2.000],  loss: 1.332467, mae: 28.113546, mean_q: -40.905416, mean_eps: 0.108229\n",
            " 140947/200000: episode: 737, duration: 0.955s, episode steps:  98, steps per second: 103, episode reward: -97.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.745 [0.000, 2.000],  loss: 1.148897, mae: 27.906099, mean_q: -40.594967, mean_eps: 0.107646\n",
            " 141060/200000: episode: 738, duration: 1.098s, episode steps: 113, steps per second: 103, episode reward: -112.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.735 [0.000, 2.000],  loss: 1.254336, mae: 27.877182, mean_q: -40.560655, mean_eps: 0.106987\n",
            " 141152/200000: episode: 739, duration: 0.723s, episode steps:  92, steps per second: 127, episode reward: -91.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.989 [0.000, 2.000],  loss: 1.237901, mae: 27.417640, mean_q: -39.873804, mean_eps: 0.106341\n",
            " 141248/200000: episode: 740, duration: 0.698s, episode steps:  96, steps per second: 137, episode reward: -95.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.906 [0.000, 2.000],  loss: 1.325074, mae: 27.787514, mean_q: -40.438958, mean_eps: 0.105746\n",
            " 141340/200000: episode: 741, duration: 0.630s, episode steps:  92, steps per second: 146, episode reward: -91.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.054 [0.000, 2.000],  loss: 1.402563, mae: 27.690211, mean_q: -40.228241, mean_eps: 0.105151\n",
            " 141425/200000: episode: 742, duration: 0.519s, episode steps:  85, steps per second: 164, episode reward: -84.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.906 [0.000, 2.000],  loss: 1.650831, mae: 27.638467, mean_q: -40.135984, mean_eps: 0.104581\n",
            " 141568/200000: episode: 743, duration: 0.902s, episode steps: 143, steps per second: 159, episode reward: -142.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.308 [0.000, 2.000],  loss: 1.481208, mae: 27.506478, mean_q: -39.918963, mean_eps: 0.103859\n",
            " 141650/200000: episode: 744, duration: 0.505s, episode steps:  82, steps per second: 163, episode reward: -81.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.829 [0.000, 2.000],  loss: 1.512845, mae: 27.227687, mean_q: -39.470400, mean_eps: 0.103149\n",
            " 141739/200000: episode: 745, duration: 0.538s, episode steps:  89, steps per second: 165, episode reward: -88.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.910 [0.000, 2.000],  loss: 1.475449, mae: 27.799310, mean_q: -40.457516, mean_eps: 0.102605\n",
            " 141830/200000: episode: 746, duration: 0.556s, episode steps:  91, steps per second: 164, episode reward: -90.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.615 [0.000, 2.000],  loss: 0.917947, mae: 27.869585, mean_q: -40.582039, mean_eps: 0.102035\n",
            " 141992/200000: episode: 747, duration: 0.985s, episode steps: 162, steps per second: 164, episode reward: -161.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.117 [0.000, 2.000],  loss: 1.426496, mae: 27.622833, mean_q: -40.051723, mean_eps: 0.101237\n",
            " 142096/200000: episode: 748, duration: 0.658s, episode steps: 104, steps per second: 158, episode reward: -103.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.923 [0.000, 2.000],  loss: 1.390959, mae: 27.592198, mean_q: -40.078758, mean_eps: 0.100401\n",
            " 142196/200000: episode: 749, duration: 0.670s, episode steps: 100, steps per second: 149, episode reward: -99.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.820 [0.000, 2.000],  loss: 1.222191, mae: 27.447803, mean_q: -39.899175, mean_eps: 0.099755\n",
            " 142301/200000: episode: 750, duration: 0.692s, episode steps: 105, steps per second: 152, episode reward: -104.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 1.382341, mae: 27.409474, mean_q: -39.795973, mean_eps: 0.099096\n",
            " 142404/200000: episode: 751, duration: 0.735s, episode steps: 103, steps per second: 140, episode reward: -102.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.806 [0.000, 2.000],  loss: 1.177516, mae: 27.356314, mean_q: -39.775808, mean_eps: 0.098437\n",
            " 142499/200000: episode: 752, duration: 0.580s, episode steps:  95, steps per second: 164, episode reward: -94.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.084 [0.000, 2.000],  loss: 1.062459, mae: 27.201120, mean_q: -39.450647, mean_eps: 0.097817\n",
            " 142608/200000: episode: 753, duration: 0.784s, episode steps: 109, steps per second: 139, episode reward: -108.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.844 [0.000, 2.000],  loss: 1.534596, mae: 27.672506, mean_q: -40.126220, mean_eps: 0.097171\n",
            " 142683/200000: episode: 754, duration: 0.650s, episode steps:  75, steps per second: 115, episode reward: -74.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.893 [0.000, 2.000],  loss: 0.974832, mae: 27.814354, mean_q: -40.438817, mean_eps: 0.096588\n",
            " 142774/200000: episode: 755, duration: 0.814s, episode steps:  91, steps per second: 112, episode reward: -90.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.857 [0.000, 2.000],  loss: 0.998667, mae: 27.638963, mean_q: -40.159303, mean_eps: 0.096056\n",
            " 142880/200000: episode: 756, duration: 0.951s, episode steps: 106, steps per second: 111, episode reward: -105.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.717 [0.000, 2.000],  loss: 1.195039, mae: 27.365987, mean_q: -39.794747, mean_eps: 0.095435\n",
            " 142962/200000: episode: 757, duration: 0.742s, episode steps:  82, steps per second: 111, episode reward: -81.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.866 [0.000, 2.000],  loss: 1.246584, mae: 27.807536, mean_q: -40.375792, mean_eps: 0.094840\n",
            " 143055/200000: episode: 758, duration: 0.545s, episode steps:  93, steps per second: 171, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.774 [0.000, 2.000],  loss: 1.157696, mae: 27.300658, mean_q: -39.641923, mean_eps: 0.094283\n",
            " 143135/200000: episode: 759, duration: 0.494s, episode steps:  80, steps per second: 162, episode reward: -79.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.963 [0.000, 2.000],  loss: 1.401320, mae: 27.630992, mean_q: -40.070261, mean_eps: 0.093738\n",
            " 143218/200000: episode: 760, duration: 0.544s, episode steps:  83, steps per second: 153, episode reward: -82.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.723 [0.000, 2.000],  loss: 1.451649, mae: 27.821817, mean_q: -40.378394, mean_eps: 0.093219\n",
            " 143313/200000: episode: 761, duration: 0.596s, episode steps:  95, steps per second: 159, episode reward: -94.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.032 [0.000, 2.000],  loss: 1.682174, mae: 27.459621, mean_q: -39.806977, mean_eps: 0.092649\n",
            " 143415/200000: episode: 762, duration: 0.647s, episode steps: 102, steps per second: 158, episode reward: -101.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.745 [0.000, 2.000],  loss: 1.224854, mae: 27.602174, mean_q: -40.079387, mean_eps: 0.092028\n",
            " 143492/200000: episode: 763, duration: 0.476s, episode steps:  77, steps per second: 162, episode reward: -76.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.844 [0.000, 2.000],  loss: 1.104451, mae: 27.896008, mean_q: -40.525749, mean_eps: 0.091471\n",
            " 143591/200000: episode: 764, duration: 0.683s, episode steps:  99, steps per second: 145, episode reward: -98.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.808 [0.000, 2.000],  loss: 1.294677, mae: 27.780713, mean_q: -40.358952, mean_eps: 0.090913\n",
            " 143671/200000: episode: 765, duration: 0.534s, episode steps:  80, steps per second: 150, episode reward: -79.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.825 [0.000, 2.000],  loss: 1.240125, mae: 27.758592, mean_q: -40.240314, mean_eps: 0.090343\n",
            " 143739/200000: episode: 766, duration: 0.464s, episode steps:  68, steps per second: 147, episode reward: -67.000, mean reward: -0.985 [-1.000,  0.000], mean action: 0.956 [0.000, 2.000],  loss: 1.254130, mae: 27.675201, mean_q: -40.182583, mean_eps: 0.089875\n",
            " 143823/200000: episode: 767, duration: 0.615s, episode steps:  84, steps per second: 136, episode reward: -83.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.988 [0.000, 2.000],  loss: 1.159423, mae: 27.777560, mean_q: -40.257074, mean_eps: 0.089393\n",
            " 143917/200000: episode: 768, duration: 0.575s, episode steps:  94, steps per second: 164, episode reward: -93.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.915 [0.000, 2.000],  loss: 1.233150, mae: 27.504526, mean_q: -39.918840, mean_eps: 0.088823\n",
            " 144033/200000: episode: 769, duration: 0.737s, episode steps: 116, steps per second: 157, episode reward: -115.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.966 [0.000, 2.000],  loss: 1.750678, mae: 27.362299, mean_q: -39.686649, mean_eps: 0.088152\n",
            " 144126/200000: episode: 770, duration: 0.589s, episode steps:  93, steps per second: 158, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.075 [0.000, 2.000],  loss: 1.495457, mae: 27.520664, mean_q: -39.974476, mean_eps: 0.087493\n",
            " 144216/200000: episode: 771, duration: 0.608s, episode steps:  90, steps per second: 148, episode reward: -89.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.900 [0.000, 2.000],  loss: 1.471740, mae: 27.439958, mean_q: -39.880498, mean_eps: 0.086923\n",
            " 144316/200000: episode: 772, duration: 0.779s, episode steps: 100, steps per second: 128, episode reward: -99.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.950 [0.000, 2.000],  loss: 1.321856, mae: 27.457640, mean_q: -39.901000, mean_eps: 0.086328\n",
            " 144426/200000: episode: 773, duration: 0.818s, episode steps: 110, steps per second: 135, episode reward: -109.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.064 [0.000, 2.000],  loss: 1.192299, mae: 27.433350, mean_q: -39.943106, mean_eps: 0.085657\n",
            " 144502/200000: episode: 774, duration: 0.614s, episode steps:  76, steps per second: 124, episode reward: -75.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.039 [0.000, 2.000],  loss: 1.461516, mae: 28.205648, mean_q: -41.069463, mean_eps: 0.085061\n",
            " 144580/200000: episode: 775, duration: 0.737s, episode steps:  78, steps per second: 106, episode reward: -77.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.872 [0.000, 2.000],  loss: 1.409766, mae: 27.972286, mean_q: -40.732756, mean_eps: 0.084580\n",
            " 144714/200000: episode: 776, duration: 1.175s, episode steps: 134, steps per second: 114, episode reward: -133.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.851 [0.000, 2.000],  loss: 0.992197, mae: 27.696614, mean_q: -40.266494, mean_eps: 0.083909\n",
            " 144784/200000: episode: 777, duration: 0.672s, episode steps:  70, steps per second: 104, episode reward: -69.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.986 [0.000, 2.000],  loss: 0.979512, mae: 27.513866, mean_q: -40.015231, mean_eps: 0.083263\n",
            " 144885/200000: episode: 778, duration: 0.728s, episode steps: 101, steps per second: 139, episode reward: -100.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.901 [0.000, 2.000],  loss: 1.254527, mae: 27.670600, mean_q: -40.238622, mean_eps: 0.082718\n",
            " 144986/200000: episode: 779, duration: 0.641s, episode steps: 101, steps per second: 158, episode reward: -100.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.960 [0.000, 2.000],  loss: 1.348240, mae: 27.738183, mean_q: -40.339847, mean_eps: 0.082072\n",
            " 145066/200000: episode: 780, duration: 0.505s, episode steps:  80, steps per second: 158, episode reward: -79.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.912 [0.000, 2.000],  loss: 1.364390, mae: 27.525562, mean_q: -39.992427, mean_eps: 0.081502\n",
            " 145141/200000: episode: 781, duration: 0.451s, episode steps:  75, steps per second: 166, episode reward: -74.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.040 [0.000, 2.000],  loss: 1.224744, mae: 27.303929, mean_q: -39.636784, mean_eps: 0.081008\n",
            " 145234/200000: episode: 782, duration: 0.534s, episode steps:  93, steps per second: 174, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.871 [0.000, 2.000],  loss: 1.057578, mae: 27.367706, mean_q: -39.665956, mean_eps: 0.080476\n",
            " 145353/200000: episode: 783, duration: 0.714s, episode steps: 119, steps per second: 167, episode reward: -118.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.840 [0.000, 2.000],  loss: 1.314305, mae: 27.362803, mean_q: -39.702912, mean_eps: 0.079805\n",
            " 145450/200000: episode: 784, duration: 0.607s, episode steps:  97, steps per second: 160, episode reward: -96.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.041 [0.000, 2.000],  loss: 1.061871, mae: 27.851327, mean_q: -40.521016, mean_eps: 0.079121\n",
            " 145540/200000: episode: 785, duration: 0.612s, episode steps:  90, steps per second: 147, episode reward: -89.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.933 [0.000, 2.000],  loss: 1.006417, mae: 27.717823, mean_q: -40.268268, mean_eps: 0.078538\n",
            " 145660/200000: episode: 786, duration: 0.729s, episode steps: 120, steps per second: 165, episode reward: -119.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.225 [0.000, 2.000],  loss: 1.378896, mae: 27.539387, mean_q: -39.997441, mean_eps: 0.077879\n",
            " 145731/200000: episode: 787, duration: 0.452s, episode steps:  71, steps per second: 157, episode reward: -70.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.972 [0.000, 2.000],  loss: 1.202328, mae: 27.836529, mean_q: -40.461421, mean_eps: 0.077271\n",
            " 145886/200000: episode: 788, duration: 0.959s, episode steps: 155, steps per second: 162, episode reward: -154.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.910 [0.000, 2.000],  loss: 1.128274, mae: 27.691852, mean_q: -40.214542, mean_eps: 0.076549\n",
            " 145973/200000: episode: 789, duration: 0.559s, episode steps:  87, steps per second: 156, episode reward: -86.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.034 [0.000, 2.000],  loss: 1.432716, mae: 27.525418, mean_q: -39.962369, mean_eps: 0.075777\n",
            " 146051/200000: episode: 790, duration: 0.472s, episode steps:  78, steps per second: 165, episode reward: -77.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.949 [0.000, 2.000],  loss: 1.383571, mae: 27.419674, mean_q: -39.746945, mean_eps: 0.075257\n",
            " 146145/200000: episode: 791, duration: 0.597s, episode steps:  94, steps per second: 157, episode reward: -93.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.926 [0.000, 2.000],  loss: 1.348637, mae: 27.383370, mean_q: -39.716664, mean_eps: 0.074713\n",
            " 146265/200000: episode: 792, duration: 0.782s, episode steps: 120, steps per second: 154, episode reward: -119.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.758 [0.000, 2.000],  loss: 1.430592, mae: 27.090396, mean_q: -39.289096, mean_eps: 0.074029\n",
            " 146367/200000: episode: 793, duration: 0.575s, episode steps: 102, steps per second: 177, episode reward: -101.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.873 [0.000, 2.000],  loss: 1.164087, mae: 27.276129, mean_q: -39.546685, mean_eps: 0.073332\n",
            " 146449/200000: episode: 794, duration: 0.542s, episode steps:  82, steps per second: 151, episode reward: -81.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.890 [0.000, 2.000],  loss: 1.215038, mae: 27.273888, mean_q: -39.554908, mean_eps: 0.072749\n",
            " 146531/200000: episode: 795, duration: 0.783s, episode steps:  82, steps per second: 105, episode reward: -81.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.939 [0.000, 2.000],  loss: 1.293951, mae: 27.323100, mean_q: -39.696086, mean_eps: 0.072230\n",
            " 146618/200000: episode: 796, duration: 0.980s, episode steps:  87, steps per second:  89, episode reward: -86.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.080 [0.000, 2.000],  loss: 1.202965, mae: 27.104203, mean_q: -39.374997, mean_eps: 0.071698\n",
            " 146694/200000: episode: 797, duration: 0.653s, episode steps:  76, steps per second: 116, episode reward: -75.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.961 [0.000, 2.000],  loss: 1.104273, mae: 27.453689, mean_q: -39.901184, mean_eps: 0.071179\n",
            " 146774/200000: episode: 798, duration: 0.737s, episode steps:  80, steps per second: 109, episode reward: -79.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.975 [0.000, 2.000],  loss: 1.317006, mae: 27.188932, mean_q: -39.366926, mean_eps: 0.070685\n",
            " 146861/200000: episode: 799, duration: 0.585s, episode steps:  87, steps per second: 149, episode reward: -86.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.057 [0.000, 2.000],  loss: 1.323706, mae: 27.070513, mean_q: -39.310273, mean_eps: 0.070153\n",
            " 146961/200000: episode: 800, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: -99.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.720 [0.000, 2.000],  loss: 1.399262, mae: 27.169777, mean_q: -39.492918, mean_eps: 0.069557\n",
            " 147054/200000: episode: 801, duration: 0.547s, episode steps:  93, steps per second: 170, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.140 [0.000, 2.000],  loss: 1.021616, mae: 27.453220, mean_q: -39.883764, mean_eps: 0.068949\n",
            " 147138/200000: episode: 802, duration: 0.544s, episode steps:  84, steps per second: 154, episode reward: -83.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.857 [0.000, 2.000],  loss: 1.198341, mae: 27.246235, mean_q: -39.546214, mean_eps: 0.068392\n",
            " 147215/200000: episode: 803, duration: 0.450s, episode steps:  77, steps per second: 171, episode reward: -76.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.013 [0.000, 2.000],  loss: 0.997915, mae: 27.352722, mean_q: -39.698495, mean_eps: 0.067885\n",
            " 147296/200000: episode: 804, duration: 0.500s, episode steps:  81, steps per second: 162, episode reward: -80.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.037 [0.000, 2.000],  loss: 1.367515, mae: 27.240747, mean_q: -39.571453, mean_eps: 0.067391\n",
            " 147386/200000: episode: 805, duration: 0.556s, episode steps:  90, steps per second: 162, episode reward: -89.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.878 [0.000, 2.000],  loss: 1.080703, mae: 27.214641, mean_q: -39.550183, mean_eps: 0.066847\n",
            " 147462/200000: episode: 806, duration: 0.475s, episode steps:  76, steps per second: 160, episode reward: -75.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.105 [0.000, 2.000],  loss: 1.211330, mae: 27.466302, mean_q: -39.927434, mean_eps: 0.066315\n",
            " 147544/200000: episode: 807, duration: 0.499s, episode steps:  82, steps per second: 164, episode reward: -81.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.963 [0.000, 2.000],  loss: 1.160814, mae: 26.986476, mean_q: -39.146963, mean_eps: 0.065821\n",
            " 147636/200000: episode: 808, duration: 0.605s, episode steps:  92, steps per second: 152, episode reward: -91.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.033 [0.000, 2.000],  loss: 1.176709, mae: 27.125891, mean_q: -39.338520, mean_eps: 0.065276\n",
            " 147728/200000: episode: 809, duration: 0.613s, episode steps:  92, steps per second: 150, episode reward: -91.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.065 [0.000, 2.000],  loss: 1.273768, mae: 27.531507, mean_q: -40.023372, mean_eps: 0.064693\n",
            " 147830/200000: episode: 810, duration: 0.638s, episode steps: 102, steps per second: 160, episode reward: -101.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.735 [0.000, 2.000],  loss: 1.142321, mae: 27.319905, mean_q: -39.684633, mean_eps: 0.064073\n",
            " 147941/200000: episode: 811, duration: 0.683s, episode steps: 111, steps per second: 162, episode reward: -110.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.865 [0.000, 2.000],  loss: 1.195492, mae: 27.134813, mean_q: -39.340612, mean_eps: 0.063389\n",
            " 148048/200000: episode: 812, duration: 0.632s, episode steps: 107, steps per second: 169, episode reward: -106.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.159 [0.000, 2.000],  loss: 1.225039, mae: 27.287436, mean_q: -39.577998, mean_eps: 0.062705\n",
            " 148141/200000: episode: 813, duration: 0.570s, episode steps:  93, steps per second: 163, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.903 [0.000, 2.000],  loss: 1.296205, mae: 27.378659, mean_q: -39.664490, mean_eps: 0.062071\n",
            " 148241/200000: episode: 814, duration: 0.625s, episode steps: 100, steps per second: 160, episode reward: -99.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.880 [0.000, 2.000],  loss: 1.159436, mae: 27.255415, mean_q: -39.450823, mean_eps: 0.061451\n",
            " 148333/200000: episode: 815, duration: 0.573s, episode steps:  92, steps per second: 160, episode reward: -91.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.065 [0.000, 2.000],  loss: 1.387779, mae: 26.789139, mean_q: -38.801623, mean_eps: 0.060843\n",
            " 148437/200000: episode: 816, duration: 0.728s, episode steps: 104, steps per second: 143, episode reward: -103.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.048 [0.000, 2.000],  loss: 1.139830, mae: 27.102493, mean_q: -39.306155, mean_eps: 0.060222\n",
            " 148543/200000: episode: 817, duration: 0.846s, episode steps: 106, steps per second: 125, episode reward: -105.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.877 [0.000, 2.000],  loss: 1.143644, mae: 27.127747, mean_q: -39.310925, mean_eps: 0.059563\n",
            " 148623/200000: episode: 818, duration: 0.713s, episode steps:  80, steps per second: 112, episode reward: -79.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.988 [0.000, 2.000],  loss: 1.295400, mae: 26.777025, mean_q: -38.820831, mean_eps: 0.058981\n",
            " 148707/200000: episode: 819, duration: 0.781s, episode steps:  84, steps per second: 108, episode reward: -83.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.036 [0.000, 2.000],  loss: 1.291041, mae: 26.774392, mean_q: -38.742537, mean_eps: 0.058461\n",
            " 148784/200000: episode: 820, duration: 0.889s, episode steps:  77, steps per second:  87, episode reward: -76.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.987 [0.000, 2.000],  loss: 1.122199, mae: 27.380534, mean_q: -39.711670, mean_eps: 0.057955\n",
            " 148860/200000: episode: 821, duration: 0.576s, episode steps:  76, steps per second: 132, episode reward: -75.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.013 [0.000, 2.000],  loss: 1.284760, mae: 26.733321, mean_q: -38.741812, mean_eps: 0.057473\n",
            " 149019/200000: episode: 822, duration: 1.020s, episode steps: 159, steps per second: 156, episode reward: -158.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.201 [0.000, 2.000],  loss: 1.177914, mae: 26.913807, mean_q: -38.960810, mean_eps: 0.056726\n",
            " 149114/200000: episode: 823, duration: 0.580s, episode steps:  95, steps per second: 164, episode reward: -94.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.084 [0.000, 2.000],  loss: 1.241980, mae: 26.924887, mean_q: -39.027430, mean_eps: 0.055915\n",
            " 149220/200000: episode: 824, duration: 0.663s, episode steps: 106, steps per second: 160, episode reward: -105.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.123 [0.000, 2.000],  loss: 1.149365, mae: 27.077217, mean_q: -39.341864, mean_eps: 0.055282\n",
            " 149319/200000: episode: 825, duration: 0.669s, episode steps:  99, steps per second: 148, episode reward: -98.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.818 [0.000, 2.000],  loss: 1.219468, mae: 26.948463, mean_q: -39.100164, mean_eps: 0.054636\n",
            " 149418/200000: episode: 826, duration: 0.623s, episode steps:  99, steps per second: 159, episode reward: -98.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.828 [0.000, 2.000],  loss: 1.124204, mae: 26.854450, mean_q: -38.946373, mean_eps: 0.054003\n",
            " 149494/200000: episode: 827, duration: 0.478s, episode steps:  76, steps per second: 159, episode reward: -75.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.974 [0.000, 2.000],  loss: 1.313735, mae: 27.258479, mean_q: -39.539919, mean_eps: 0.053445\n",
            " 149588/200000: episode: 828, duration: 0.623s, episode steps:  94, steps per second: 151, episode reward: -93.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.128 [0.000, 2.000],  loss: 1.138599, mae: 26.712200, mean_q: -38.686642, mean_eps: 0.052913\n",
            " 149679/200000: episode: 829, duration: 0.555s, episode steps:  91, steps per second: 164, episode reward: -90.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.824 [0.000, 2.000],  loss: 1.598283, mae: 26.745903, mean_q: -38.726594, mean_eps: 0.052331\n",
            " 149750/200000: episode: 830, duration: 0.460s, episode steps:  71, steps per second: 154, episode reward: -70.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.028 [0.000, 2.000],  loss: 1.327351, mae: 26.632392, mean_q: -38.593309, mean_eps: 0.051811\n",
            " 149844/200000: episode: 831, duration: 0.605s, episode steps:  94, steps per second: 155, episode reward: -93.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.021 [0.000, 2.000],  loss: 1.181541, mae: 27.019909, mean_q: -39.218802, mean_eps: 0.051292\n",
            " 149923/200000: episode: 832, duration: 0.584s, episode steps:  79, steps per second: 135, episode reward: -78.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.038 [0.000, 2.000],  loss: 0.933427, mae: 26.941145, mean_q: -39.193824, mean_eps: 0.050747\n",
            " 150026/200000: episode: 833, duration: 0.646s, episode steps: 103, steps per second: 159, episode reward: -102.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.816 [0.000, 2.000],  loss: 1.149884, mae: 27.176150, mean_q: -39.516211, mean_eps: 0.050185\n",
            " 150264/200000: episode: 834, duration: 1.559s, episode steps: 238, steps per second: 153, episode reward: -237.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.374 [0.000, 2.000],  loss: 1.163825, mae: 26.812514, mean_q: -38.904779, mean_eps: 0.050000\n",
            " 150363/200000: episode: 835, duration: 0.695s, episode steps:  99, steps per second: 143, episode reward: -98.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.939 [0.000, 2.000],  loss: 1.077092, mae: 26.833249, mean_q: -38.920362, mean_eps: 0.050000\n",
            " 150433/200000: episode: 836, duration: 0.603s, episode steps:  70, steps per second: 116, episode reward: -69.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.057 [0.000, 2.000],  loss: 1.623959, mae: 26.832257, mean_q: -38.841192, mean_eps: 0.050000\n",
            " 150519/200000: episode: 837, duration: 0.741s, episode steps:  86, steps per second: 116, episode reward: -85.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.837 [0.000, 2.000],  loss: 1.003850, mae: 26.836062, mean_q: -38.985127, mean_eps: 0.050000\n",
            " 150610/200000: episode: 838, duration: 0.739s, episode steps:  91, steps per second: 123, episode reward: -90.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.187 [0.000, 2.000],  loss: 1.145742, mae: 26.812069, mean_q: -38.951368, mean_eps: 0.050000\n",
            " 150725/200000: episode: 839, duration: 1.040s, episode steps: 115, steps per second: 111, episode reward: -114.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.183 [0.000, 2.000],  loss: 1.284235, mae: 26.738145, mean_q: -38.782611, mean_eps: 0.050000\n",
            " 150815/200000: episode: 840, duration: 0.590s, episode steps:  90, steps per second: 153, episode reward: -89.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.911 [0.000, 2.000],  loss: 1.234376, mae: 26.797660, mean_q: -38.864325, mean_eps: 0.050000\n",
            " 150895/200000: episode: 841, duration: 0.484s, episode steps:  80, steps per second: 165, episode reward: -79.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.887 [0.000, 2.000],  loss: 1.077521, mae: 26.985826, mean_q: -39.138803, mean_eps: 0.050000\n",
            " 151003/200000: episode: 842, duration: 0.712s, episode steps: 108, steps per second: 152, episode reward: -107.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.824 [0.000, 2.000],  loss: 1.049099, mae: 27.005928, mean_q: -39.237180, mean_eps: 0.050000\n",
            " 151112/200000: episode: 843, duration: 0.700s, episode steps: 109, steps per second: 156, episode reward: -108.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.908 [0.000, 2.000],  loss: 1.099619, mae: 26.581935, mean_q: -38.517067, mean_eps: 0.050000\n",
            " 151182/200000: episode: 844, duration: 0.432s, episode steps:  70, steps per second: 162, episode reward: -69.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.986 [0.000, 2.000],  loss: 1.212133, mae: 26.702399, mean_q: -38.721963, mean_eps: 0.050000\n",
            " 151268/200000: episode: 845, duration: 0.529s, episode steps:  86, steps per second: 163, episode reward: -85.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.151 [0.000, 2.000],  loss: 1.327932, mae: 26.230871, mean_q: -37.921049, mean_eps: 0.050000\n",
            " 151339/200000: episode: 846, duration: 0.436s, episode steps:  71, steps per second: 163, episode reward: -70.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 1.242699, mae: 26.427788, mean_q: -38.302405, mean_eps: 0.050000\n",
            " 151414/200000: episode: 847, duration: 0.454s, episode steps:  75, steps per second: 165, episode reward: -74.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.027 [0.000, 2.000],  loss: 1.083532, mae: 26.530303, mean_q: -38.451889, mean_eps: 0.050000\n",
            " 151485/200000: episode: 848, duration: 0.456s, episode steps:  71, steps per second: 156, episode reward: -70.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.915 [0.000, 2.000],  loss: 0.981229, mae: 26.964674, mean_q: -39.136450, mean_eps: 0.050000\n",
            " 151651/200000: episode: 849, duration: 1.033s, episode steps: 166, steps per second: 161, episode reward: -165.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.916 [0.000, 2.000],  loss: 1.238805, mae: 26.545990, mean_q: -38.503055, mean_eps: 0.050000\n",
            " 151758/200000: episode: 850, duration: 0.705s, episode steps: 107, steps per second: 152, episode reward: -106.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.832 [0.000, 2.000],  loss: 1.122644, mae: 26.607036, mean_q: -38.534488, mean_eps: 0.050000\n",
            " 151836/200000: episode: 851, duration: 0.468s, episode steps:  78, steps per second: 166, episode reward: -77.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 1.253621, mae: 26.652753, mean_q: -38.578429, mean_eps: 0.050000\n",
            " 151933/200000: episode: 852, duration: 0.652s, episode steps:  97, steps per second: 149, episode reward: -96.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.062 [0.000, 2.000],  loss: 1.356431, mae: 26.728378, mean_q: -38.749202, mean_eps: 0.050000\n",
            " 152004/200000: episode: 853, duration: 0.466s, episode steps:  71, steps per second: 152, episode reward: -70.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.915 [0.000, 2.000],  loss: 1.226021, mae: 27.018412, mean_q: -39.218864, mean_eps: 0.050000\n",
            " 152108/200000: episode: 854, duration: 0.717s, episode steps: 104, steps per second: 145, episode reward: -103.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.577 [0.000, 2.000],  loss: 1.080149, mae: 26.718347, mean_q: -38.736068, mean_eps: 0.050000\n",
            " 152182/200000: episode: 855, duration: 0.513s, episode steps:  74, steps per second: 144, episode reward: -73.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.811 [0.000, 2.000],  loss: 1.111832, mae: 26.501408, mean_q: -38.503474, mean_eps: 0.050000\n",
            " 152257/200000: episode: 856, duration: 0.519s, episode steps:  75, steps per second: 145, episode reward: -74.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.960 [0.000, 2.000],  loss: 1.142796, mae: 26.684398, mean_q: -38.730082, mean_eps: 0.050000\n",
            " 152344/200000: episode: 857, duration: 0.762s, episode steps:  87, steps per second: 114, episode reward: -86.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.759 [0.000, 2.000],  loss: 1.295974, mae: 26.501291, mean_q: -38.338507, mean_eps: 0.050000\n",
            " 152442/200000: episode: 858, duration: 0.984s, episode steps:  98, steps per second: 100, episode reward: -97.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.980 [0.000, 2.000],  loss: 1.389228, mae: 26.431119, mean_q: -38.206531, mean_eps: 0.050000\n",
            " 152504/200000: episode: 859, duration: 0.588s, episode steps:  62, steps per second: 106, episode reward: -61.000, mean reward: -0.984 [-1.000,  0.000], mean action: 0.903 [0.000, 2.000],  loss: 1.064892, mae: 26.909515, mean_q: -39.018431, mean_eps: 0.050000\n",
            " 152582/200000: episode: 860, duration: 0.726s, episode steps:  78, steps per second: 107, episode reward: -77.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.949 [0.000, 2.000],  loss: 1.355865, mae: 26.498835, mean_q: -38.417397, mean_eps: 0.050000\n",
            " 152669/200000: episode: 861, duration: 1.049s, episode steps:  87, steps per second:  83, episode reward: -86.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.701 [0.000, 2.000],  loss: 1.099502, mae: 26.345035, mean_q: -38.120843, mean_eps: 0.050000\n",
            " 152747/200000: episode: 862, duration: 0.814s, episode steps:  78, steps per second:  96, episode reward: -77.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.974 [0.000, 2.000],  loss: 1.188535, mae: 26.664036, mean_q: -38.461044, mean_eps: 0.050000\n",
            " 152834/200000: episode: 863, duration: 0.853s, episode steps:  87, steps per second: 102, episode reward: -86.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.805 [0.000, 2.000],  loss: 1.279476, mae: 26.493315, mean_q: -38.319539, mean_eps: 0.050000\n",
            " 152916/200000: episode: 864, duration: 0.787s, episode steps:  82, steps per second: 104, episode reward: -81.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.012 [0.000, 2.000],  loss: 1.407974, mae: 26.419068, mean_q: -38.250883, mean_eps: 0.050000\n",
            " 153017/200000: episode: 865, duration: 0.989s, episode steps: 101, steps per second: 102, episode reward: -100.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.762 [0.000, 2.000],  loss: 1.055968, mae: 26.728640, mean_q: -38.742298, mean_eps: 0.050000\n",
            " 153117/200000: episode: 866, duration: 0.672s, episode steps: 100, steps per second: 149, episode reward: -99.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.200 [0.000, 2.000],  loss: 1.298061, mae: 26.811930, mean_q: -38.889624, mean_eps: 0.050000\n",
            " 153209/200000: episode: 867, duration: 0.600s, episode steps:  92, steps per second: 153, episode reward: -91.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.685 [0.000, 2.000],  loss: 1.993872, mae: 26.632799, mean_q: -38.504921, mean_eps: 0.050000\n",
            " 153286/200000: episode: 868, duration: 0.499s, episode steps:  77, steps per second: 154, episode reward: -76.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.831 [0.000, 2.000],  loss: 1.176189, mae: 26.607775, mean_q: -38.585170, mean_eps: 0.050000\n",
            " 153398/200000: episode: 869, duration: 0.745s, episode steps: 112, steps per second: 150, episode reward: -111.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.964 [0.000, 2.000],  loss: 1.110654, mae: 26.709694, mean_q: -38.757353, mean_eps: 0.050000\n",
            " 153493/200000: episode: 870, duration: 0.663s, episode steps:  95, steps per second: 143, episode reward: -94.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.684 [0.000, 2.000],  loss: 1.115677, mae: 26.373023, mean_q: -38.213391, mean_eps: 0.050000\n",
            " 153630/200000: episode: 871, duration: 0.982s, episode steps: 137, steps per second: 139, episode reward: -136.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.620 [0.000, 2.000],  loss: 1.004074, mae: 26.547088, mean_q: -38.550771, mean_eps: 0.050000\n",
            " 153729/200000: episode: 872, duration: 0.811s, episode steps:  99, steps per second: 122, episode reward: -98.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.232 [0.000, 2.000],  loss: 1.225096, mae: 26.478847, mean_q: -38.271867, mean_eps: 0.050000\n",
            " 153799/200000: episode: 873, duration: 0.489s, episode steps:  70, steps per second: 143, episode reward: -69.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.857 [0.000, 2.000],  loss: 1.297911, mae: 26.751704, mean_q: -38.832710, mean_eps: 0.050000\n",
            " 153862/200000: episode: 874, duration: 0.411s, episode steps:  63, steps per second: 153, episode reward: -62.000, mean reward: -0.984 [-1.000,  0.000], mean action: 0.889 [0.000, 2.000],  loss: 1.136456, mae: 26.643204, mean_q: -38.599436, mean_eps: 0.050000\n",
            " 153932/200000: episode: 875, duration: 0.505s, episode steps:  70, steps per second: 139, episode reward: -69.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.814 [0.000, 2.000],  loss: 1.188379, mae: 26.707819, mean_q: -38.643649, mean_eps: 0.050000\n",
            " 154006/200000: episode: 876, duration: 0.542s, episode steps:  74, steps per second: 137, episode reward: -73.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.838 [0.000, 2.000],  loss: 1.403036, mae: 26.719643, mean_q: -38.700584, mean_eps: 0.050000\n",
            " 154082/200000: episode: 877, duration: 0.730s, episode steps:  76, steps per second: 104, episode reward: -75.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.803 [0.000, 2.000],  loss: 1.019991, mae: 26.251859, mean_q: -37.987501, mean_eps: 0.050000\n",
            " 154179/200000: episode: 878, duration: 0.926s, episode steps:  97, steps per second: 105, episode reward: -96.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.670 [0.000, 2.000],  loss: 1.134908, mae: 26.454298, mean_q: -38.276656, mean_eps: 0.050000\n",
            " 154290/200000: episode: 879, duration: 1.002s, episode steps: 111, steps per second: 111, episode reward: -110.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.252 [0.000, 2.000],  loss: 1.027481, mae: 26.664232, mean_q: -38.604437, mean_eps: 0.050000\n",
            " 154353/200000: episode: 880, duration: 0.599s, episode steps:  63, steps per second: 105, episode reward: -62.000, mean reward: -0.984 [-1.000,  0.000], mean action: 0.841 [0.000, 2.000],  loss: 1.110434, mae: 26.337323, mean_q: -38.188232, mean_eps: 0.050000\n",
            " 154445/200000: episode: 881, duration: 0.711s, episode steps:  92, steps per second: 129, episode reward: -91.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.739 [0.000, 2.000],  loss: 1.176682, mae: 26.289451, mean_q: -38.132549, mean_eps: 0.050000\n",
            " 154533/200000: episode: 882, duration: 0.570s, episode steps:  88, steps per second: 154, episode reward: -87.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.909 [0.000, 2.000],  loss: 1.042929, mae: 26.751683, mean_q: -38.714967, mean_eps: 0.050000\n",
            " 154619/200000: episode: 883, duration: 0.553s, episode steps:  86, steps per second: 156, episode reward: -85.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.942 [0.000, 2.000],  loss: 1.402841, mae: 26.336371, mean_q: -38.165476, mean_eps: 0.050000\n",
            " 154712/200000: episode: 884, duration: 0.667s, episode steps:  93, steps per second: 139, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.763 [0.000, 2.000],  loss: 1.560272, mae: 26.742792, mean_q: -38.812304, mean_eps: 0.050000\n",
            " 154801/200000: episode: 885, duration: 0.629s, episode steps:  89, steps per second: 141, episode reward: -88.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.809 [0.000, 2.000],  loss: 1.017964, mae: 26.638998, mean_q: -38.579550, mean_eps: 0.050000\n",
            " 154893/200000: episode: 886, duration: 0.621s, episode steps:  92, steps per second: 148, episode reward: -91.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.957 [0.000, 2.000],  loss: 1.364507, mae: 26.465015, mean_q: -38.213784, mean_eps: 0.050000\n",
            " 154971/200000: episode: 887, duration: 0.507s, episode steps:  78, steps per second: 154, episode reward: -77.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.846 [0.000, 2.000],  loss: 1.136570, mae: 26.852780, mean_q: -38.854522, mean_eps: 0.050000\n",
            " 155064/200000: episode: 888, duration: 0.564s, episode steps:  93, steps per second: 165, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.785 [0.000, 2.000],  loss: 1.163476, mae: 26.606632, mean_q: -38.494910, mean_eps: 0.050000\n",
            " 155127/200000: episode: 889, duration: 0.375s, episode steps:  63, steps per second: 168, episode reward: -62.000, mean reward: -0.984 [-1.000,  0.000], mean action: 1.079 [0.000, 2.000],  loss: 1.153891, mae: 26.484617, mean_q: -38.374275, mean_eps: 0.050000\n",
            " 155198/200000: episode: 890, duration: 0.433s, episode steps:  71, steps per second: 164, episode reward: -70.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.915 [0.000, 2.000],  loss: 1.063541, mae: 26.693141, mean_q: -38.742274, mean_eps: 0.050000\n",
            " 155273/200000: episode: 891, duration: 0.431s, episode steps:  75, steps per second: 174, episode reward: -74.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.853 [0.000, 2.000],  loss: 1.305769, mae: 26.640635, mean_q: -38.602699, mean_eps: 0.050000\n",
            " 155348/200000: episode: 892, duration: 0.445s, episode steps:  75, steps per second: 169, episode reward: -74.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.800 [0.000, 2.000],  loss: 1.060462, mae: 26.337867, mean_q: -38.146268, mean_eps: 0.050000\n",
            " 155615/200000: episode: 893, duration: 1.552s, episode steps: 267, steps per second: 172, episode reward: -266.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.820 [0.000, 2.000],  loss: 1.360016, mae: 26.396667, mean_q: -38.258437, mean_eps: 0.050000\n",
            " 155698/200000: episode: 894, duration: 0.531s, episode steps:  83, steps per second: 156, episode reward: -82.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.145 [0.000, 2.000],  loss: 1.159542, mae: 26.376375, mean_q: -38.192355, mean_eps: 0.050000\n",
            " 155774/200000: episode: 895, duration: 0.467s, episode steps:  76, steps per second: 163, episode reward: -75.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.842 [0.000, 2.000],  loss: 1.407044, mae: 26.638838, mean_q: -38.505015, mean_eps: 0.050000\n",
            " 155848/200000: episode: 896, duration: 0.467s, episode steps:  74, steps per second: 158, episode reward: -73.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.730 [0.000, 2.000],  loss: 1.105020, mae: 26.089130, mean_q: -37.766705, mean_eps: 0.050000\n",
            " 155917/200000: episode: 897, duration: 0.460s, episode steps:  69, steps per second: 150, episode reward: -68.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.986 [0.000, 2.000],  loss: 1.296366, mae: 26.716393, mean_q: -38.736443, mean_eps: 0.050000\n",
            " 156029/200000: episode: 898, duration: 0.854s, episode steps: 112, steps per second: 131, episode reward: -111.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.679 [0.000, 2.000],  loss: 0.964561, mae: 26.366300, mean_q: -38.266751, mean_eps: 0.050000\n",
            " 156117/200000: episode: 899, duration: 0.780s, episode steps:  88, steps per second: 113, episode reward: -87.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.830 [0.000, 2.000],  loss: 1.261929, mae: 26.110717, mean_q: -37.799030, mean_eps: 0.050000\n",
            " 156221/200000: episode: 900, duration: 0.903s, episode steps: 104, steps per second: 115, episode reward: -103.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.096 [0.000, 2.000],  loss: 1.384875, mae: 26.242313, mean_q: -37.949603, mean_eps: 0.050000\n",
            " 156296/200000: episode: 901, duration: 0.630s, episode steps:  75, steps per second: 119, episode reward: -74.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.880 [0.000, 2.000],  loss: 1.075321, mae: 26.283174, mean_q: -38.077219, mean_eps: 0.050000\n",
            " 156392/200000: episode: 902, duration: 0.930s, episode steps:  96, steps per second: 103, episode reward: -95.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.021 [0.000, 2.000],  loss: 1.456753, mae: 26.526712, mean_q: -38.412915, mean_eps: 0.050000\n",
            " 156490/200000: episode: 903, duration: 0.655s, episode steps:  98, steps per second: 150, episode reward: -97.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.643 [0.000, 2.000],  loss: 1.221926, mae: 26.296639, mean_q: -38.065221, mean_eps: 0.050000\n",
            " 156574/200000: episode: 904, duration: 0.538s, episode steps:  84, steps per second: 156, episode reward: -83.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.893 [0.000, 2.000],  loss: 0.941985, mae: 26.326401, mean_q: -38.160157, mean_eps: 0.050000\n",
            " 156665/200000: episode: 905, duration: 0.642s, episode steps:  91, steps per second: 142, episode reward: -90.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.736 [0.000, 2.000],  loss: 1.129556, mae: 26.538920, mean_q: -38.453074, mean_eps: 0.050000\n",
            " 156739/200000: episode: 906, duration: 0.472s, episode steps:  74, steps per second: 157, episode reward: -73.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.068 [0.000, 2.000],  loss: 1.794417, mae: 26.016413, mean_q: -37.524383, mean_eps: 0.050000\n",
            " 156831/200000: episode: 907, duration: 0.601s, episode steps:  92, steps per second: 153, episode reward: -91.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.065 [0.000, 2.000],  loss: 1.385661, mae: 26.440757, mean_q: -38.310689, mean_eps: 0.050000\n",
            " 156924/200000: episode: 908, duration: 0.661s, episode steps:  93, steps per second: 141, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.882 [0.000, 2.000],  loss: 1.104525, mae: 26.534717, mean_q: -38.483508, mean_eps: 0.050000\n",
            " 157049/200000: episode: 909, duration: 0.989s, episode steps: 125, steps per second: 126, episode reward: -124.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.936 [0.000, 2.000],  loss: 1.084710, mae: 26.208588, mean_q: -38.011477, mean_eps: 0.050000\n",
            " 157127/200000: episode: 910, duration: 0.537s, episode steps:  78, steps per second: 145, episode reward: -77.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.103 [0.000, 2.000],  loss: 1.675168, mae: 25.872252, mean_q: -37.371933, mean_eps: 0.050000\n",
            " 157189/200000: episode: 911, duration: 0.451s, episode steps:  62, steps per second: 138, episode reward: -61.000, mean reward: -0.984 [-1.000,  0.000], mean action: 0.919 [0.000, 2.000],  loss: 1.346293, mae: 26.180930, mean_q: -37.845156, mean_eps: 0.050000\n",
            " 157285/200000: episode: 912, duration: 0.683s, episode steps:  96, steps per second: 141, episode reward: -95.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.062 [0.000, 2.000],  loss: 1.091044, mae: 26.150611, mean_q: -37.908276, mean_eps: 0.050000\n",
            " 157363/200000: episode: 913, duration: 0.549s, episode steps:  78, steps per second: 142, episode reward: -77.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.026 [0.000, 2.000],  loss: 0.911231, mae: 26.227275, mean_q: -37.987933, mean_eps: 0.050000\n",
            " 157442/200000: episode: 914, duration: 0.530s, episode steps:  79, steps per second: 149, episode reward: -78.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.911 [0.000, 2.000],  loss: 0.980929, mae: 25.884337, mean_q: -37.426314, mean_eps: 0.050000\n",
            " 157541/200000: episode: 915, duration: 0.614s, episode steps:  99, steps per second: 161, episode reward: -98.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.697 [0.000, 2.000],  loss: 1.433851, mae: 26.284940, mean_q: -38.076548, mean_eps: 0.050000\n",
            " 157627/200000: episode: 916, duration: 0.614s, episode steps:  86, steps per second: 140, episode reward: -85.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.012 [0.000, 2.000],  loss: 1.146166, mae: 26.283911, mean_q: -38.104526, mean_eps: 0.050000\n",
            " 157701/200000: episode: 917, duration: 0.465s, episode steps:  74, steps per second: 159, episode reward: -73.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.838 [0.000, 2.000],  loss: 1.006861, mae: 26.311672, mean_q: -38.054030, mean_eps: 0.050000\n",
            " 157793/200000: episode: 918, duration: 0.586s, episode steps:  92, steps per second: 157, episode reward: -91.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.793 [0.000, 2.000],  loss: 1.561295, mae: 26.336622, mean_q: -38.028221, mean_eps: 0.050000\n",
            " 157864/200000: episode: 919, duration: 0.644s, episode steps:  71, steps per second: 110, episode reward: -70.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.887 [0.000, 2.000],  loss: 1.558204, mae: 26.223456, mean_q: -38.038928, mean_eps: 0.050000\n",
            " 157944/200000: episode: 920, duration: 0.734s, episode steps:  80, steps per second: 109, episode reward: -79.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.875 [0.000, 2.000],  loss: 1.144245, mae: 26.452919, mean_q: -38.320579, mean_eps: 0.050000\n",
            " 158083/200000: episode: 921, duration: 1.174s, episode steps: 139, steps per second: 118, episode reward: -138.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.662 [0.000, 2.000],  loss: 1.101109, mae: 26.369657, mean_q: -38.197264, mean_eps: 0.050000\n",
            " 158174/200000: episode: 922, duration: 0.843s, episode steps:  91, steps per second: 108, episode reward: -90.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.890 [0.000, 2.000],  loss: 1.178424, mae: 26.074341, mean_q: -37.738399, mean_eps: 0.050000\n",
            " 158266/200000: episode: 923, duration: 0.690s, episode steps:  92, steps per second: 133, episode reward: -91.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.978 [0.000, 2.000],  loss: 1.266553, mae: 25.437513, mean_q: -36.867587, mean_eps: 0.050000\n",
            " 158341/200000: episode: 924, duration: 0.471s, episode steps:  75, steps per second: 159, episode reward: -74.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.813 [0.000, 2.000],  loss: 1.162745, mae: 26.064931, mean_q: -37.690986, mean_eps: 0.050000\n",
            " 158418/200000: episode: 925, duration: 0.485s, episode steps:  77, steps per second: 159, episode reward: -76.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.948 [0.000, 2.000],  loss: 1.322601, mae: 26.433819, mean_q: -38.309027, mean_eps: 0.050000\n",
            " 158514/200000: episode: 926, duration: 0.583s, episode steps:  96, steps per second: 165, episode reward: -95.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.875 [0.000, 2.000],  loss: 1.104401, mae: 26.282633, mean_q: -38.056205, mean_eps: 0.050000\n",
            " 158612/200000: episode: 927, duration: 0.817s, episode steps:  98, steps per second: 120, episode reward: -97.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.010 [0.000, 2.000],  loss: 1.162799, mae: 25.883090, mean_q: -37.511826, mean_eps: 0.050000\n",
            " 158728/200000: episode: 928, duration: 0.731s, episode steps: 116, steps per second: 159, episode reward: -115.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.759 [0.000, 2.000],  loss: 1.249686, mae: 25.857980, mean_q: -37.406975, mean_eps: 0.050000\n",
            " 158814/200000: episode: 929, duration: 0.533s, episode steps:  86, steps per second: 161, episode reward: -85.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.012 [0.000, 2.000],  loss: 1.096489, mae: 26.109284, mean_q: -37.839681, mean_eps: 0.050000\n",
            " 158901/200000: episode: 930, duration: 0.533s, episode steps:  87, steps per second: 163, episode reward: -86.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.770 [0.000, 2.000],  loss: 0.868230, mae: 26.364389, mean_q: -38.295325, mean_eps: 0.050000\n",
            " 159017/200000: episode: 931, duration: 0.753s, episode steps: 116, steps per second: 154, episode reward: -115.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.138 [0.000, 2.000],  loss: 1.185167, mae: 25.970962, mean_q: -37.586266, mean_eps: 0.050000\n",
            " 159119/200000: episode: 932, duration: 0.603s, episode steps: 102, steps per second: 169, episode reward: -101.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.686 [0.000, 2.000],  loss: 1.358751, mae: 25.926357, mean_q: -37.402227, mean_eps: 0.050000\n",
            " 159354/200000: episode: 933, duration: 1.470s, episode steps: 235, steps per second: 160, episode reward: -234.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.391 [0.000, 2.000],  loss: 1.177434, mae: 25.964410, mean_q: -37.584926, mean_eps: 0.050000\n",
            " 159429/200000: episode: 934, duration: 0.469s, episode steps:  75, steps per second: 160, episode reward: -74.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.867 [0.000, 2.000],  loss: 1.222837, mae: 25.963371, mean_q: -37.572515, mean_eps: 0.050000\n",
            " 159506/200000: episode: 935, duration: 0.508s, episode steps:  77, steps per second: 152, episode reward: -76.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.065 [0.000, 2.000],  loss: 1.099367, mae: 26.000459, mean_q: -37.659900, mean_eps: 0.050000\n",
            " 159598/200000: episode: 936, duration: 0.578s, episode steps:  92, steps per second: 159, episode reward: -91.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.152 [0.000, 2.000],  loss: 1.225819, mae: 25.700359, mean_q: -37.054352, mean_eps: 0.050000\n",
            " 159683/200000: episode: 937, duration: 0.504s, episode steps:  85, steps per second: 169, episode reward: -84.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.800 [0.000, 2.000],  loss: 1.301063, mae: 25.872998, mean_q: -37.270919, mean_eps: 0.050000\n",
            " 159753/200000: episode: 938, duration: 0.442s, episode steps:  70, steps per second: 158, episode reward: -69.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.914 [0.000, 2.000],  loss: 1.304223, mae: 25.912019, mean_q: -37.423293, mean_eps: 0.050000\n",
            " 159887/200000: episode: 939, duration: 1.127s, episode steps: 134, steps per second: 119, episode reward: -133.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.537 [0.000, 2.000],  loss: 1.076992, mae: 26.116185, mean_q: -37.815474, mean_eps: 0.050000\n",
            " 159966/200000: episode: 940, duration: 0.707s, episode steps:  79, steps per second: 112, episode reward: -78.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.911 [0.000, 2.000],  loss: 1.164155, mae: 25.837511, mean_q: -37.286744, mean_eps: 0.050000\n",
            " 160062/200000: episode: 941, duration: 0.727s, episode steps:  96, steps per second: 132, episode reward: -95.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.927 [0.000, 2.000],  loss: 1.192251, mae: 26.020272, mean_q: -37.563630, mean_eps: 0.050000\n",
            " 160149/200000: episode: 942, duration: 0.839s, episode steps:  87, steps per second: 104, episode reward: -86.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.724 [0.000, 2.000],  loss: 1.268758, mae: 26.171649, mean_q: -37.766218, mean_eps: 0.050000\n",
            " 160239/200000: episode: 943, duration: 0.643s, episode steps:  90, steps per second: 140, episode reward: -89.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.922 [0.000, 2.000],  loss: 1.007948, mae: 25.857569, mean_q: -37.377568, mean_eps: 0.050000\n",
            " 160338/200000: episode: 944, duration: 0.626s, episode steps:  99, steps per second: 158, episode reward: -98.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.768 [0.000, 2.000],  loss: 0.994228, mae: 26.056346, mean_q: -37.783816, mean_eps: 0.050000\n",
            " 160401/200000: episode: 945, duration: 0.431s, episode steps:  63, steps per second: 146, episode reward: -62.000, mean reward: -0.984 [-1.000,  0.000], mean action: 0.937 [0.000, 2.000],  loss: 1.340711, mae: 25.497307, mean_q: -36.814057, mean_eps: 0.050000\n",
            " 160477/200000: episode: 946, duration: 0.467s, episode steps:  76, steps per second: 163, episode reward: -75.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.013 [0.000, 2.000],  loss: 1.156605, mae: 26.045334, mean_q: -37.732089, mean_eps: 0.050000\n",
            " 160563/200000: episode: 947, duration: 0.532s, episode steps:  86, steps per second: 162, episode reward: -85.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.965 [0.000, 2.000],  loss: 1.017264, mae: 26.019948, mean_q: -37.641314, mean_eps: 0.050000\n",
            " 160627/200000: episode: 948, duration: 0.380s, episode steps:  64, steps per second: 168, episode reward: -63.000, mean reward: -0.984 [-1.000,  0.000], mean action: 0.922 [0.000, 2.000],  loss: 1.160931, mae: 26.164256, mean_q: -37.830158, mean_eps: 0.050000\n",
            " 160736/200000: episode: 949, duration: 0.707s, episode steps: 109, steps per second: 154, episode reward: -108.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.156 [0.000, 2.000],  loss: 1.014218, mae: 25.897715, mean_q: -37.502503, mean_eps: 0.050000\n",
            " 160822/200000: episode: 950, duration: 0.509s, episode steps:  86, steps per second: 169, episode reward: -85.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.012 [0.000, 2.000],  loss: 1.411292, mae: 25.974626, mean_q: -37.419255, mean_eps: 0.050000\n",
            " 160907/200000: episode: 951, duration: 0.528s, episode steps:  85, steps per second: 161, episode reward: -84.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.824 [0.000, 2.000],  loss: 1.050547, mae: 26.056920, mean_q: -37.711671, mean_eps: 0.050000\n",
            " 160993/200000: episode: 952, duration: 0.552s, episode steps:  86, steps per second: 156, episode reward: -85.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.058 [0.000, 2.000],  loss: 1.326973, mae: 25.369616, mean_q: -36.563547, mean_eps: 0.050000\n",
            " 161074/200000: episode: 953, duration: 0.486s, episode steps:  81, steps per second: 167, episode reward: -80.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.926 [0.000, 2.000],  loss: 1.045042, mae: 25.854008, mean_q: -37.344671, mean_eps: 0.050000\n",
            " 161153/200000: episode: 954, duration: 0.498s, episode steps:  79, steps per second: 159, episode reward: -78.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.937 [0.000, 2.000],  loss: 1.322312, mae: 25.554645, mean_q: -36.917033, mean_eps: 0.050000\n",
            " 161381/200000: episode: 955, duration: 1.630s, episode steps: 228, steps per second: 140, episode reward: -227.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.706 [0.000, 2.000],  loss: 1.064562, mae: 25.891735, mean_q: -37.482287, mean_eps: 0.050000\n",
            " 161459/200000: episode: 956, duration: 0.453s, episode steps:  78, steps per second: 172, episode reward: -77.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.064 [0.000, 2.000],  loss: 1.125353, mae: 25.591439, mean_q: -36.841375, mean_eps: 0.050000\n",
            " 161535/200000: episode: 957, duration: 0.476s, episode steps:  76, steps per second: 160, episode reward: -75.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.066 [0.000, 2.000],  loss: 1.395269, mae: 26.065513, mean_q: -37.724934, mean_eps: 0.050000\n",
            " 161638/200000: episode: 958, duration: 0.612s, episode steps: 103, steps per second: 168, episode reward: -102.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.204 [0.000, 2.000],  loss: 0.921533, mae: 26.113125, mean_q: -37.796954, mean_eps: 0.050000\n",
            " 161731/200000: episode: 959, duration: 0.585s, episode steps:  93, steps per second: 159, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.140 [0.000, 2.000],  loss: 1.195593, mae: 25.522209, mean_q: -36.828333, mean_eps: 0.050000\n",
            " 161857/200000: episode: 960, duration: 1.117s, episode steps: 126, steps per second: 113, episode reward: -125.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.770 [0.000, 2.000],  loss: 1.043442, mae: 25.690718, mean_q: -37.156035, mean_eps: 0.050000\n",
            " 161947/200000: episode: 961, duration: 0.940s, episode steps:  90, steps per second:  96, episode reward: -89.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.756 [0.000, 2.000],  loss: 1.308282, mae: 25.462115, mean_q: -36.724966, mean_eps: 0.050000\n",
            " 162032/200000: episode: 962, duration: 1.022s, episode steps:  85, steps per second:  83, episode reward: -84.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.188 [0.000, 2.000],  loss: 1.074855, mae: 25.829815, mean_q: -37.331273, mean_eps: 0.050000\n",
            " 162124/200000: episode: 963, duration: 1.305s, episode steps:  92, steps per second:  71, episode reward: -91.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.989 [0.000, 2.000],  loss: 1.366537, mae: 25.656996, mean_q: -37.055004, mean_eps: 0.050000\n",
            " 162210/200000: episode: 964, duration: 0.768s, episode steps:  86, steps per second: 112, episode reward: -85.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.070 [0.000, 2.000],  loss: 1.164691, mae: 26.058948, mean_q: -37.623316, mean_eps: 0.050000\n",
            " 162281/200000: episode: 965, duration: 0.621s, episode steps:  71, steps per second: 114, episode reward: -70.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.155 [0.000, 2.000],  loss: 1.003943, mae: 25.965400, mean_q: -37.437187, mean_eps: 0.050000\n",
            " 162345/200000: episode: 966, duration: 0.514s, episode steps:  64, steps per second: 125, episode reward: -63.000, mean reward: -0.984 [-1.000,  0.000], mean action: 1.062 [0.000, 2.000],  loss: 0.902208, mae: 26.301021, mean_q: -38.042224, mean_eps: 0.050000\n",
            " 162422/200000: episode: 967, duration: 0.649s, episode steps:  77, steps per second: 119, episode reward: -76.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.078 [0.000, 2.000],  loss: 0.956648, mae: 25.948373, mean_q: -37.543862, mean_eps: 0.050000\n",
            " 162502/200000: episode: 968, duration: 0.751s, episode steps:  80, steps per second: 107, episode reward: -79.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.012 [0.000, 2.000],  loss: 1.286099, mae: 25.766644, mean_q: -37.192874, mean_eps: 0.050000\n",
            " 162574/200000: episode: 969, duration: 0.594s, episode steps:  72, steps per second: 121, episode reward: -71.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.971943, mae: 25.539210, mean_q: -36.921946, mean_eps: 0.050000\n",
            " 162672/200000: episode: 970, duration: 0.892s, episode steps:  98, steps per second: 110, episode reward: -97.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.276 [0.000, 2.000],  loss: 1.030683, mae: 26.034213, mean_q: -37.629131, mean_eps: 0.050000\n",
            " 162747/200000: episode: 971, duration: 0.562s, episode steps:  75, steps per second: 134, episode reward: -74.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.173 [0.000, 2.000],  loss: 1.135489, mae: 25.876712, mean_q: -37.387344, mean_eps: 0.050000\n",
            " 162845/200000: episode: 972, duration: 0.669s, episode steps:  98, steps per second: 147, episode reward: -97.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.939 [0.000, 2.000],  loss: 1.061329, mae: 25.604857, mean_q: -36.957454, mean_eps: 0.050000\n",
            " 162929/200000: episode: 973, duration: 0.555s, episode steps:  84, steps per second: 151, episode reward: -83.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.988 [0.000, 2.000],  loss: 1.303014, mae: 25.857931, mean_q: -37.299003, mean_eps: 0.050000\n",
            " 163006/200000: episode: 974, duration: 0.550s, episode steps:  77, steps per second: 140, episode reward: -76.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.104 [0.000, 2.000],  loss: 1.196690, mae: 25.869184, mean_q: -37.356003, mean_eps: 0.050000\n",
            " 163095/200000: episode: 975, duration: 0.661s, episode steps:  89, steps per second: 135, episode reward: -88.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.045 [0.000, 2.000],  loss: 1.201902, mae: 25.789876, mean_q: -37.240039, mean_eps: 0.050000\n",
            " 163169/200000: episode: 976, duration: 0.554s, episode steps:  74, steps per second: 134, episode reward: -73.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.068 [0.000, 2.000],  loss: 1.026726, mae: 25.518825, mean_q: -36.861228, mean_eps: 0.050000\n",
            " 163241/200000: episode: 977, duration: 0.486s, episode steps:  72, steps per second: 148, episode reward: -71.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.861 [0.000, 2.000],  loss: 1.218919, mae: 25.234579, mean_q: -36.433898, mean_eps: 0.050000\n",
            " 163390/200000: episode: 978, duration: 1.103s, episode steps: 149, steps per second: 135, episode reward: -148.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.208 [0.000, 2.000],  loss: 1.024513, mae: 25.807140, mean_q: -37.339442, mean_eps: 0.050000\n",
            " 163474/200000: episode: 979, duration: 0.679s, episode steps:  84, steps per second: 124, episode reward: -83.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.012 [0.000, 2.000],  loss: 1.131237, mae: 25.522879, mean_q: -36.882168, mean_eps: 0.050000\n",
            " 163558/200000: episode: 980, duration: 0.758s, episode steps:  84, steps per second: 111, episode reward: -83.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.083 [0.000, 2.000],  loss: 1.013226, mae: 25.526712, mean_q: -36.832985, mean_eps: 0.050000\n",
            " 163640/200000: episode: 981, duration: 0.702s, episode steps:  82, steps per second: 117, episode reward: -81.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.159 [0.000, 2.000],  loss: 1.000401, mae: 25.867550, mean_q: -37.462776, mean_eps: 0.050000\n",
            " 163723/200000: episode: 982, duration: 0.752s, episode steps:  83, steps per second: 110, episode reward: -82.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.072 [0.000, 2.000],  loss: 1.507265, mae: 25.185060, mean_q: -36.390630, mean_eps: 0.050000\n",
            " 163814/200000: episode: 983, duration: 0.674s, episode steps:  91, steps per second: 135, episode reward: -90.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.011 [0.000, 2.000],  loss: 1.247014, mae: 25.601566, mean_q: -36.995651, mean_eps: 0.050000\n",
            " 163892/200000: episode: 984, duration: 0.481s, episode steps:  78, steps per second: 162, episode reward: -77.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.051 [0.000, 2.000],  loss: 1.128831, mae: 25.493756, mean_q: -36.825575, mean_eps: 0.050000\n",
            " 163969/200000: episode: 985, duration: 0.502s, episode steps:  77, steps per second: 154, episode reward: -76.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.052 [0.000, 2.000],  loss: 1.282874, mae: 25.901539, mean_q: -37.516397, mean_eps: 0.050000\n",
            " 164071/200000: episode: 986, duration: 0.649s, episode steps: 102, steps per second: 157, episode reward: -101.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.069 [0.000, 2.000],  loss: 1.129617, mae: 25.504370, mean_q: -36.878802, mean_eps: 0.050000\n",
            " 164146/200000: episode: 987, duration: 0.488s, episode steps:  75, steps per second: 154, episode reward: -74.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.107 [0.000, 2.000],  loss: 1.260343, mae: 25.389856, mean_q: -36.732034, mean_eps: 0.050000\n",
            " 164251/200000: episode: 988, duration: 0.685s, episode steps: 105, steps per second: 153, episode reward: -104.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.829 [0.000, 2.000],  loss: 1.499715, mae: 25.483016, mean_q: -36.743559, mean_eps: 0.050000\n",
            " 164328/200000: episode: 989, duration: 0.482s, episode steps:  77, steps per second: 160, episode reward: -76.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.039 [0.000, 2.000],  loss: 1.130740, mae: 25.244365, mean_q: -36.473996, mean_eps: 0.050000\n",
            " 164423/200000: episode: 990, duration: 0.640s, episode steps:  95, steps per second: 149, episode reward: -94.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.095 [0.000, 2.000],  loss: 1.056182, mae: 25.198952, mean_q: -36.408976, mean_eps: 0.050000\n",
            " 164545/200000: episode: 991, duration: 0.757s, episode steps: 122, steps per second: 161, episode reward: -121.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.934 [0.000, 2.000],  loss: 0.964583, mae: 25.553865, mean_q: -36.982457, mean_eps: 0.050000\n",
            " 164636/200000: episode: 992, duration: 0.660s, episode steps:  91, steps per second: 138, episode reward: -90.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.989 [0.000, 2.000],  loss: 1.254904, mae: 25.739965, mean_q: -37.227259, mean_eps: 0.050000\n",
            " 164706/200000: episode: 993, duration: 0.524s, episode steps:  70, steps per second: 134, episode reward: -69.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.029 [0.000, 2.000],  loss: 1.086137, mae: 25.647814, mean_q: -37.041752, mean_eps: 0.050000\n",
            " 164800/200000: episode: 994, duration: 0.599s, episode steps:  94, steps per second: 157, episode reward: -93.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.106 [0.000, 2.000],  loss: 1.227220, mae: 25.423390, mean_q: -36.681647, mean_eps: 0.050000\n",
            " 164899/200000: episode: 995, duration: 0.685s, episode steps:  99, steps per second: 144, episode reward: -98.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.081 [0.000, 2.000],  loss: 1.215002, mae: 25.580599, mean_q: -36.912680, mean_eps: 0.050000\n",
            " 164977/200000: episode: 996, duration: 0.514s, episode steps:  78, steps per second: 152, episode reward: -77.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.013 [0.000, 2.000],  loss: 1.207187, mae: 25.125220, mean_q: -36.167364, mean_eps: 0.050000\n",
            " 165071/200000: episode: 997, duration: 0.656s, episode steps:  94, steps per second: 143, episode reward: -93.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.926 [0.000, 2.000],  loss: 1.404990, mae: 25.385799, mean_q: -36.608674, mean_eps: 0.050000\n",
            " 165136/200000: episode: 998, duration: 0.469s, episode steps:  65, steps per second: 139, episode reward: -64.000, mean reward: -0.985 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 1.030874, mae: 25.588475, mean_q: -36.950813, mean_eps: 0.050000\n",
            " 165220/200000: episode: 999, duration: 0.655s, episode steps:  84, steps per second: 128, episode reward: -83.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.964 [0.000, 2.000],  loss: 1.047682, mae: 25.573216, mean_q: -36.973906, mean_eps: 0.050000\n",
            " 165314/200000: episode: 1000, duration: 0.887s, episode steps:  94, steps per second: 106, episode reward: -93.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 1.121601, mae: 25.783639, mean_q: -37.223074, mean_eps: 0.050000\n",
            " 165443/200000: episode: 1001, duration: 1.178s, episode steps: 129, steps per second: 110, episode reward: -128.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.574 [0.000, 2.000],  loss: 1.170677, mae: 25.474890, mean_q: -36.826816, mean_eps: 0.050000\n",
            " 165895/200000: episode: 1002, duration: 3.341s, episode steps: 452, steps per second: 135, episode reward: -451.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.761 [0.000, 2.000],  loss: 1.126103, mae: 25.502888, mean_q: -36.842568, mean_eps: 0.050000\n",
            " 165982/200000: episode: 1003, duration: 0.589s, episode steps:  87, steps per second: 148, episode reward: -86.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.908 [0.000, 2.000],  loss: 0.993009, mae: 25.425520, mean_q: -36.826792, mean_eps: 0.050000\n",
            " 166073/200000: episode: 1004, duration: 0.614s, episode steps:  91, steps per second: 148, episode reward: -90.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.846 [0.000, 2.000],  loss: 1.036392, mae: 25.617506, mean_q: -36.965622, mean_eps: 0.050000\n",
            " 166183/200000: episode: 1005, duration: 0.702s, episode steps: 110, steps per second: 157, episode reward: -109.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.800 [0.000, 2.000],  loss: 1.349867, mae: 25.443724, mean_q: -36.745839, mean_eps: 0.050000\n",
            " 166267/200000: episode: 1006, duration: 0.615s, episode steps:  84, steps per second: 137, episode reward: -83.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.060 [0.000, 2.000],  loss: 1.146230, mae: 25.433578, mean_q: -36.714166, mean_eps: 0.050000\n",
            " 166350/200000: episode: 1007, duration: 0.616s, episode steps:  83, steps per second: 135, episode reward: -82.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.988 [0.000, 2.000],  loss: 1.065165, mae: 25.595725, mean_q: -36.901249, mean_eps: 0.050000\n",
            " 166439/200000: episode: 1008, duration: 0.624s, episode steps:  89, steps per second: 143, episode reward: -88.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.921 [0.000, 2.000],  loss: 1.260992, mae: 25.241256, mean_q: -36.395153, mean_eps: 0.050000\n",
            " 166512/200000: episode: 1009, duration: 0.538s, episode steps:  73, steps per second: 136, episode reward: -72.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.877 [0.000, 2.000],  loss: 1.135384, mae: 25.334578, mean_q: -36.511686, mean_eps: 0.050000\n",
            " 166602/200000: episode: 1010, duration: 0.629s, episode steps:  90, steps per second: 143, episode reward: -89.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.900 [0.000, 2.000],  loss: 1.098947, mae: 25.553172, mean_q: -36.830914, mean_eps: 0.050000\n",
            " 166715/200000: episode: 1011, duration: 0.699s, episode steps: 113, steps per second: 162, episode reward: -112.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.912 [0.000, 2.000],  loss: 0.939694, mae: 25.315764, mean_q: -36.574976, mean_eps: 0.050000\n",
            " 166786/200000: episode: 1012, duration: 0.476s, episode steps:  71, steps per second: 149, episode reward: -70.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.972 [0.000, 2.000],  loss: 1.144521, mae: 25.884892, mean_q: -37.461150, mean_eps: 0.050000\n",
            " 166857/200000: episode: 1013, duration: 0.470s, episode steps:  71, steps per second: 151, episode reward: -70.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.915 [0.000, 2.000],  loss: 0.985388, mae: 25.493851, mean_q: -36.961861, mean_eps: 0.050000\n",
            " 166940/200000: episode: 1014, duration: 0.554s, episode steps:  83, steps per second: 150, episode reward: -82.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.988 [0.000, 2.000],  loss: 1.032932, mae: 25.715632, mean_q: -37.158766, mean_eps: 0.050000\n",
            " 167025/200000: episode: 1015, duration: 0.545s, episode steps:  85, steps per second: 156, episode reward: -84.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.247 [0.000, 2.000],  loss: 1.018370, mae: 25.285742, mean_q: -36.530319, mean_eps: 0.050000\n",
            " 167107/200000: episode: 1016, duration: 0.538s, episode steps:  82, steps per second: 152, episode reward: -81.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.915 [0.000, 2.000],  loss: 1.119763, mae: 25.636800, mean_q: -37.099645, mean_eps: 0.050000\n",
            " 167200/200000: episode: 1017, duration: 0.847s, episode steps:  93, steps per second: 110, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.946 [0.000, 2.000],  loss: 1.119014, mae: 25.931517, mean_q: -37.532712, mean_eps: 0.050000\n",
            " 167285/200000: episode: 1018, duration: 0.842s, episode steps:  85, steps per second: 101, episode reward: -84.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.788 [0.000, 2.000],  loss: 1.161213, mae: 25.460952, mean_q: -36.780454, mean_eps: 0.050000\n",
            " 167360/200000: episode: 1019, duration: 0.690s, episode steps:  75, steps per second: 109, episode reward: -74.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.053 [0.000, 2.000],  loss: 1.264315, mae: 25.245419, mean_q: -36.372576, mean_eps: 0.050000\n",
            " 167452/200000: episode: 1020, duration: 0.939s, episode steps:  92, steps per second:  98, episode reward: -91.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.696 [0.000, 2.000],  loss: 0.949672, mae: 25.797162, mean_q: -37.314301, mean_eps: 0.050000\n",
            " 167538/200000: episode: 1021, duration: 0.693s, episode steps:  86, steps per second: 124, episode reward: -85.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.826 [0.000, 2.000],  loss: 1.005237, mae: 25.388847, mean_q: -36.664026, mean_eps: 0.050000\n",
            " 167616/200000: episode: 1022, duration: 0.518s, episode steps:  78, steps per second: 151, episode reward: -77.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.923 [0.000, 2.000],  loss: 0.978909, mae: 25.706994, mean_q: -37.105240, mean_eps: 0.050000\n",
            " 167688/200000: episode: 1023, duration: 0.481s, episode steps:  72, steps per second: 150, episode reward: -71.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.792 [0.000, 2.000],  loss: 1.393414, mae: 25.367360, mean_q: -36.626200, mean_eps: 0.050000\n",
            " 167761/200000: episode: 1024, duration: 0.465s, episode steps:  73, steps per second: 157, episode reward: -72.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.918 [0.000, 2.000],  loss: 1.123056, mae: 25.342152, mean_q: -36.562949, mean_eps: 0.050000\n",
            " 167853/200000: episode: 1025, duration: 0.547s, episode steps:  92, steps per second: 168, episode reward: -91.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.761 [0.000, 2.000],  loss: 1.142041, mae: 25.166057, mean_q: -36.254434, mean_eps: 0.050000\n",
            " 167935/200000: episode: 1026, duration: 0.489s, episode steps:  82, steps per second: 168, episode reward: -81.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.927 [0.000, 2.000],  loss: 1.053079, mae: 25.881891, mean_q: -37.425307, mean_eps: 0.050000\n",
            " 168024/200000: episode: 1027, duration: 0.603s, episode steps:  89, steps per second: 148, episode reward: -88.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.775 [0.000, 2.000],  loss: 1.407062, mae: 25.386004, mean_q: -36.646923, mean_eps: 0.050000\n",
            " 168101/200000: episode: 1028, duration: 0.475s, episode steps:  77, steps per second: 162, episode reward: -76.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.039 [0.000, 2.000],  loss: 1.120165, mae: 25.411085, mean_q: -36.698061, mean_eps: 0.050000\n",
            " 168179/200000: episode: 1029, duration: 0.488s, episode steps:  78, steps per second: 160, episode reward: -77.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.910 [0.000, 2.000],  loss: 1.271391, mae: 25.444316, mean_q: -36.714796, mean_eps: 0.050000\n",
            " 168250/200000: episode: 1030, duration: 0.451s, episode steps:  71, steps per second: 157, episode reward: -70.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.873 [0.000, 2.000],  loss: 1.172324, mae: 25.406736, mean_q: -36.633127, mean_eps: 0.050000\n",
            " 168327/200000: episode: 1031, duration: 0.479s, episode steps:  77, steps per second: 161, episode reward: -76.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.974 [0.000, 2.000],  loss: 1.084256, mae: 25.374723, mean_q: -36.548437, mean_eps: 0.050000\n",
            " 168417/200000: episode: 1032, duration: 0.672s, episode steps:  90, steps per second: 134, episode reward: -89.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.889 [0.000, 2.000],  loss: 1.042160, mae: 25.543411, mean_q: -36.912462, mean_eps: 0.050000\n",
            " 168506/200000: episode: 1033, duration: 0.565s, episode steps:  89, steps per second: 157, episode reward: -88.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.899 [0.000, 2.000],  loss: 1.306006, mae: 25.283143, mean_q: -36.521759, mean_eps: 0.050000\n",
            " 168601/200000: episode: 1034, duration: 0.591s, episode steps:  95, steps per second: 161, episode reward: -94.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.737 [0.000, 2.000],  loss: 1.278755, mae: 25.652227, mean_q: -37.057974, mean_eps: 0.050000\n",
            " 168693/200000: episode: 1035, duration: 0.561s, episode steps:  92, steps per second: 164, episode reward: -91.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.815 [0.000, 2.000],  loss: 1.282991, mae: 25.492166, mean_q: -36.897509, mean_eps: 0.050000\n",
            " 168774/200000: episode: 1036, duration: 0.506s, episode steps:  81, steps per second: 160, episode reward: -80.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.914 [0.000, 2.000],  loss: 0.966959, mae: 25.809011, mean_q: -37.391069, mean_eps: 0.050000\n",
            " 168912/200000: episode: 1037, duration: 0.909s, episode steps: 138, steps per second: 152, episode reward: -137.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.746 [0.000, 2.000],  loss: 1.053212, mae: 25.503834, mean_q: -36.829862, mean_eps: 0.050000\n",
            " 168983/200000: episode: 1038, duration: 0.413s, episode steps:  71, steps per second: 172, episode reward: -70.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.859 [0.000, 2.000],  loss: 0.935082, mae: 25.966723, mean_q: -37.568124, mean_eps: 0.050000\n",
            " 169072/200000: episode: 1039, duration: 0.678s, episode steps:  89, steps per second: 131, episode reward: -88.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.809 [0.000, 2.000],  loss: 1.048627, mae: 25.510569, mean_q: -36.885027, mean_eps: 0.050000\n",
            " 169176/200000: episode: 1040, duration: 0.925s, episode steps: 104, steps per second: 112, episode reward: -103.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.779 [0.000, 2.000],  loss: 1.444420, mae: 25.178194, mean_q: -36.268144, mean_eps: 0.050000\n",
            " 169257/200000: episode: 1041, duration: 0.716s, episode steps:  81, steps per second: 113, episode reward: -80.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.012 [0.000, 2.000],  loss: 1.096500, mae: 25.683631, mean_q: -37.088907, mean_eps: 0.050000\n",
            " 169320/200000: episode: 1042, duration: 0.518s, episode steps:  63, steps per second: 122, episode reward: -62.000, mean reward: -0.984 [-1.000,  0.000], mean action: 0.889 [0.000, 2.000],  loss: 1.112946, mae: 25.585411, mean_q: -36.940464, mean_eps: 0.050000\n",
            " 169407/200000: episode: 1043, duration: 0.860s, episode steps:  87, steps per second: 101, episode reward: -86.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.908 [0.000, 2.000],  loss: 1.103291, mae: 25.413453, mean_q: -36.647844, mean_eps: 0.050000\n",
            " 169495/200000: episode: 1044, duration: 0.636s, episode steps:  88, steps per second: 138, episode reward: -87.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 1.269156, mae: 25.471503, mean_q: -36.732560, mean_eps: 0.050000\n",
            " 169589/200000: episode: 1045, duration: 0.627s, episode steps:  94, steps per second: 150, episode reward: -93.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.766 [0.000, 2.000],  loss: 1.014358, mae: 25.590062, mean_q: -37.016770, mean_eps: 0.050000\n",
            " 169675/200000: episode: 1046, duration: 0.552s, episode steps:  86, steps per second: 156, episode reward: -85.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.012 [0.000, 2.000],  loss: 1.383824, mae: 25.160291, mean_q: -36.319430, mean_eps: 0.050000\n",
            " 169754/200000: episode: 1047, duration: 0.526s, episode steps:  79, steps per second: 150, episode reward: -78.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.899 [0.000, 2.000],  loss: 1.235743, mae: 25.641614, mean_q: -37.065149, mean_eps: 0.050000\n",
            " 169840/200000: episode: 1048, duration: 0.595s, episode steps:  86, steps per second: 145, episode reward: -85.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.988 [0.000, 2.000],  loss: 1.110777, mae: 25.259176, mean_q: -36.522098, mean_eps: 0.050000\n",
            " 169920/200000: episode: 1049, duration: 0.549s, episode steps:  80, steps per second: 146, episode reward: -79.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.925 [0.000, 2.000],  loss: 0.922991, mae: 25.389944, mean_q: -36.652355, mean_eps: 0.050000\n",
            " 170018/200000: episode: 1050, duration: 0.705s, episode steps:  98, steps per second: 139, episode reward: -97.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.194 [0.000, 2.000],  loss: 1.136042, mae: 25.156946, mean_q: -36.292080, mean_eps: 0.050000\n",
            " 170090/200000: episode: 1051, duration: 0.437s, episode steps:  72, steps per second: 165, episode reward: -71.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.806 [0.000, 2.000],  loss: 1.053500, mae: 25.470431, mean_q: -36.793434, mean_eps: 0.050000\n",
            " 170184/200000: episode: 1052, duration: 0.657s, episode steps:  94, steps per second: 143, episode reward: -93.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.702 [0.000, 2.000],  loss: 0.863456, mae: 25.505883, mean_q: -36.845540, mean_eps: 0.050000\n",
            " 170267/200000: episode: 1053, duration: 0.559s, episode steps:  83, steps per second: 149, episode reward: -82.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.012 [0.000, 2.000],  loss: 0.919017, mae: 25.462569, mean_q: -36.737646, mean_eps: 0.050000\n",
            " 170364/200000: episode: 1054, duration: 0.616s, episode steps:  97, steps per second: 157, episode reward: -96.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.175 [0.000, 2.000],  loss: 0.997732, mae: 25.340791, mean_q: -36.617491, mean_eps: 0.050000\n",
            " 170444/200000: episode: 1055, duration: 0.550s, episode steps:  80, steps per second: 145, episode reward: -79.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.038 [0.000, 2.000],  loss: 1.009653, mae: 25.542721, mean_q: -36.956475, mean_eps: 0.050000\n",
            " 170514/200000: episode: 1056, duration: 0.504s, episode steps:  70, steps per second: 139, episode reward: -69.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.871 [0.000, 2.000],  loss: 1.050433, mae: 25.410210, mean_q: -36.786470, mean_eps: 0.050000\n",
            " 170590/200000: episode: 1057, duration: 0.531s, episode steps:  76, steps per second: 143, episode reward: -75.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.789 [0.000, 2.000],  loss: 1.156526, mae: 25.354108, mean_q: -36.626360, mean_eps: 0.050000\n",
            " 170686/200000: episode: 1058, duration: 0.649s, episode steps:  96, steps per second: 148, episode reward: -95.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.708 [0.000, 2.000],  loss: 1.101034, mae: 25.132905, mean_q: -36.320848, mean_eps: 0.050000\n",
            " 170761/200000: episode: 1059, duration: 0.506s, episode steps:  75, steps per second: 148, episode reward: -74.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.773 [0.000, 2.000],  loss: 0.914693, mae: 25.578872, mean_q: -37.075703, mean_eps: 0.050000\n",
            " 170835/200000: episode: 1060, duration: 0.482s, episode steps:  74, steps per second: 154, episode reward: -73.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.770 [0.000, 2.000],  loss: 1.239949, mae: 25.221697, mean_q: -36.368091, mean_eps: 0.050000\n",
            " 170931/200000: episode: 1061, duration: 0.786s, episode steps:  96, steps per second: 122, episode reward: -95.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.812 [0.000, 2.000],  loss: 1.066118, mae: 25.434546, mean_q: -36.735205, mean_eps: 0.050000\n",
            " 171022/200000: episode: 1062, duration: 0.857s, episode steps:  91, steps per second: 106, episode reward: -90.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.725 [0.000, 2.000],  loss: 1.147914, mae: 25.604919, mean_q: -36.999412, mean_eps: 0.050000\n",
            " 171127/200000: episode: 1063, duration: 0.939s, episode steps: 105, steps per second: 112, episode reward: -104.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.181 [0.000, 2.000],  loss: 1.048563, mae: 25.272460, mean_q: -36.437082, mean_eps: 0.050000\n",
            " 171206/200000: episode: 1064, duration: 0.637s, episode steps:  79, steps per second: 124, episode reward: -78.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 1.016465, mae: 25.097170, mean_q: -36.180329, mean_eps: 0.050000\n",
            " 171300/200000: episode: 1065, duration: 0.844s, episode steps:  94, steps per second: 111, episode reward: -93.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.138 [0.000, 2.000],  loss: 1.331785, mae: 24.959063, mean_q: -36.037646, mean_eps: 0.050000\n",
            " 171406/200000: episode: 1066, duration: 0.667s, episode steps: 106, steps per second: 159, episode reward: -105.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.057 [0.000, 2.000],  loss: 0.910852, mae: 25.748216, mean_q: -37.326038, mean_eps: 0.050000\n",
            " 171496/200000: episode: 1067, duration: 0.562s, episode steps:  90, steps per second: 160, episode reward: -89.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.867 [0.000, 2.000],  loss: 1.160521, mae: 25.182817, mean_q: -36.393894, mean_eps: 0.050000\n",
            " 171996/200000: episode: 1068, duration: 3.139s, episode steps: 500, steps per second: 159, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.770 [0.000, 2.000],  loss: 1.152604, mae: 25.286590, mean_q: -36.514351, mean_eps: 0.050000\n",
            " 172070/200000: episode: 1069, duration: 0.540s, episode steps:  74, steps per second: 137, episode reward: -73.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.865 [0.000, 2.000],  loss: 1.235098, mae: 24.759384, mean_q: -35.727011, mean_eps: 0.050000\n",
            " 172161/200000: episode: 1070, duration: 0.620s, episode steps:  91, steps per second: 147, episode reward: -90.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.033 [0.000, 2.000],  loss: 1.074478, mae: 25.166554, mean_q: -36.406501, mean_eps: 0.050000\n",
            " 172257/200000: episode: 1071, duration: 0.681s, episode steps:  96, steps per second: 141, episode reward: -95.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.031 [0.000, 2.000],  loss: 0.965638, mae: 25.332948, mean_q: -36.537556, mean_eps: 0.050000\n",
            " 172343/200000: episode: 1072, duration: 0.562s, episode steps:  86, steps per second: 153, episode reward: -85.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.907 [0.000, 2.000],  loss: 1.227766, mae: 25.155759, mean_q: -36.292909, mean_eps: 0.050000\n",
            " 172559/200000: episode: 1073, duration: 1.442s, episode steps: 216, steps per second: 150, episode reward: -215.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.602 [0.000, 2.000],  loss: 1.140490, mae: 25.166679, mean_q: -36.302978, mean_eps: 0.050000\n",
            " 172644/200000: episode: 1074, duration: 0.535s, episode steps:  85, steps per second: 159, episode reward: -84.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.929 [0.000, 2.000],  loss: 0.991213, mae: 25.199481, mean_q: -36.387625, mean_eps: 0.050000\n",
            " 172720/200000: episode: 1075, duration: 0.614s, episode steps:  76, steps per second: 124, episode reward: -75.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.079 [0.000, 2.000],  loss: 1.018820, mae: 25.374103, mean_q: -36.645673, mean_eps: 0.050000\n",
            " 172795/200000: episode: 1076, duration: 0.536s, episode steps:  75, steps per second: 140, episode reward: -74.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.053 [0.000, 2.000],  loss: 1.315533, mae: 24.996537, mean_q: -36.041570, mean_eps: 0.050000\n",
            " 172864/200000: episode: 1077, duration: 0.647s, episode steps:  69, steps per second: 107, episode reward: -68.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.986 [0.000, 2.000],  loss: 1.152279, mae: 24.935804, mean_q: -35.940148, mean_eps: 0.050000\n",
            " 172927/200000: episode: 1078, duration: 0.637s, episode steps:  63, steps per second:  99, episode reward: -62.000, mean reward: -0.984 [-1.000,  0.000], mean action: 0.952 [0.000, 2.000],  loss: 1.328266, mae: 25.145122, mean_q: -36.310164, mean_eps: 0.050000\n",
            " 173010/200000: episode: 1079, duration: 0.739s, episode steps:  83, steps per second: 112, episode reward: -82.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.060 [0.000, 2.000],  loss: 1.278047, mae: 24.812892, mean_q: -35.718833, mean_eps: 0.050000\n",
            " 173080/200000: episode: 1080, duration: 0.599s, episode steps:  70, steps per second: 117, episode reward: -69.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 1.248429, mae: 24.990142, mean_q: -36.084491, mean_eps: 0.050000\n",
            " 173143/200000: episode: 1081, duration: 0.685s, episode steps:  63, steps per second:  92, episode reward: -62.000, mean reward: -0.984 [-1.000,  0.000], mean action: 1.048 [0.000, 2.000],  loss: 1.360686, mae: 25.185604, mean_q: -36.341642, mean_eps: 0.050000\n",
            " 173224/200000: episode: 1082, duration: 0.607s, episode steps:  81, steps per second: 133, episode reward: -80.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.951 [0.000, 2.000],  loss: 1.252916, mae: 24.849310, mean_q: -35.840844, mean_eps: 0.050000\n",
            " 173302/200000: episode: 1083, duration: 0.494s, episode steps:  78, steps per second: 158, episode reward: -77.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.103 [0.000, 2.000],  loss: 1.096949, mae: 25.058872, mean_q: -36.163430, mean_eps: 0.050000\n",
            " 173389/200000: episode: 1084, duration: 0.536s, episode steps:  87, steps per second: 162, episode reward: -86.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.092 [0.000, 2.000],  loss: 1.082648, mae: 24.862636, mean_q: -35.899856, mean_eps: 0.050000\n",
            " 173496/200000: episode: 1085, duration: 0.695s, episode steps: 107, steps per second: 154, episode reward: -106.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.112 [0.000, 2.000],  loss: 1.024671, mae: 25.193957, mean_q: -36.408121, mean_eps: 0.050000\n",
            " 173571/200000: episode: 1086, duration: 0.482s, episode steps:  75, steps per second: 156, episode reward: -74.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.053 [0.000, 2.000],  loss: 1.052843, mae: 24.951480, mean_q: -36.040546, mean_eps: 0.050000\n",
            " 173654/200000: episode: 1087, duration: 0.530s, episode steps:  83, steps per second: 157, episode reward: -82.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.976 [0.000, 2.000],  loss: 0.818163, mae: 25.282257, mean_q: -36.528272, mean_eps: 0.050000\n",
            " 173749/200000: episode: 1088, duration: 0.602s, episode steps:  95, steps per second: 158, episode reward: -94.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.074 [0.000, 2.000],  loss: 0.934549, mae: 25.385346, mean_q: -36.692086, mean_eps: 0.050000\n",
            " 173846/200000: episode: 1089, duration: 0.607s, episode steps:  97, steps per second: 160, episode reward: -96.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.031 [0.000, 2.000],  loss: 1.061343, mae: 25.263734, mean_q: -36.514098, mean_eps: 0.050000\n",
            " 173933/200000: episode: 1090, duration: 0.612s, episode steps:  87, steps per second: 142, episode reward: -86.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.851 [0.000, 2.000],  loss: 1.304786, mae: 25.102494, mean_q: -36.295143, mean_eps: 0.050000\n",
            " 174018/200000: episode: 1091, duration: 0.544s, episode steps:  85, steps per second: 156, episode reward: -84.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.153 [0.000, 2.000],  loss: 1.053147, mae: 25.364024, mean_q: -36.595487, mean_eps: 0.050000\n",
            " 174104/200000: episode: 1092, duration: 0.576s, episode steps:  86, steps per second: 149, episode reward: -85.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.163 [0.000, 2.000],  loss: 1.035106, mae: 25.292425, mean_q: -36.586989, mean_eps: 0.050000\n",
            " 174192/200000: episode: 1093, duration: 0.592s, episode steps:  88, steps per second: 149, episode reward: -87.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.148 [0.000, 2.000],  loss: 1.086548, mae: 25.103399, mean_q: -36.241726, mean_eps: 0.050000\n",
            " 174283/200000: episode: 1094, duration: 0.606s, episode steps:  91, steps per second: 150, episode reward: -90.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.791 [0.000, 2.000],  loss: 1.061437, mae: 25.223628, mean_q: -36.457438, mean_eps: 0.050000\n",
            " 174376/200000: episode: 1095, duration: 0.601s, episode steps:  93, steps per second: 155, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.097 [0.000, 2.000],  loss: 0.913891, mae: 25.228876, mean_q: -36.499730, mean_eps: 0.050000\n",
            " 174451/200000: episode: 1096, duration: 0.543s, episode steps:  75, steps per second: 138, episode reward: -74.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.027 [0.000, 2.000],  loss: 1.105347, mae: 25.022743, mean_q: -36.093402, mean_eps: 0.050000\n",
            " 174551/200000: episode: 1097, duration: 0.619s, episode steps: 100, steps per second: 162, episode reward: -99.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.820 [0.000, 2.000],  loss: 1.201626, mae: 25.379323, mean_q: -36.620087, mean_eps: 0.050000\n",
            " 174631/200000: episode: 1098, duration: 0.476s, episode steps:  80, steps per second: 168, episode reward: -79.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.025 [0.000, 2.000],  loss: 1.147840, mae: 25.041579, mean_q: -36.074171, mean_eps: 0.050000\n",
            " 174727/200000: episode: 1099, duration: 0.688s, episode steps:  96, steps per second: 140, episode reward: -95.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.167 [0.000, 2.000],  loss: 1.256274, mae: 25.147403, mean_q: -36.290629, mean_eps: 0.050000\n",
            " 174819/200000: episode: 1100, duration: 0.765s, episode steps:  92, steps per second: 120, episode reward: -91.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 1.255486, mae: 25.179474, mean_q: -36.351345, mean_eps: 0.050000\n",
            " 174987/200000: episode: 1101, duration: 1.527s, episode steps: 168, steps per second: 110, episode reward: -167.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.315 [0.000, 2.000],  loss: 1.131201, mae: 25.146830, mean_q: -36.376941, mean_eps: 0.050000\n",
            " 175070/200000: episode: 1102, duration: 0.857s, episode steps:  83, steps per second:  97, episode reward: -82.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.916 [0.000, 2.000],  loss: 0.922934, mae: 25.765653, mean_q: -37.288221, mean_eps: 0.050000\n",
            " 175148/200000: episode: 1103, duration: 0.614s, episode steps:  78, steps per second: 127, episode reward: -77.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.103 [0.000, 2.000],  loss: 0.980557, mae: 25.429697, mean_q: -36.806572, mean_eps: 0.050000\n",
            " 175242/200000: episode: 1104, duration: 0.671s, episode steps:  94, steps per second: 140, episode reward: -93.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.011 [0.000, 2.000],  loss: 1.059935, mae: 25.160617, mean_q: -36.394982, mean_eps: 0.050000\n",
            " 175323/200000: episode: 1105, duration: 0.590s, episode steps:  81, steps per second: 137, episode reward: -80.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.938 [0.000, 2.000],  loss: 0.971614, mae: 25.218034, mean_q: -36.494896, mean_eps: 0.050000\n",
            " 175402/200000: episode: 1106, duration: 0.613s, episode steps:  79, steps per second: 129, episode reward: -78.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.076 [0.000, 2.000],  loss: 1.218934, mae: 24.891553, mean_q: -35.961513, mean_eps: 0.050000\n",
            " 175508/200000: episode: 1107, duration: 0.791s, episode steps: 106, steps per second: 134, episode reward: -105.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 1.329487, mae: 25.045050, mean_q: -36.156670, mean_eps: 0.050000\n",
            " 175585/200000: episode: 1108, duration: 0.567s, episode steps:  77, steps per second: 136, episode reward: -76.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.078 [0.000, 2.000],  loss: 0.984598, mae: 25.081706, mean_q: -36.214547, mean_eps: 0.050000\n",
            " 175697/200000: episode: 1109, duration: 0.781s, episode steps: 112, steps per second: 143, episode reward: -111.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.161 [0.000, 2.000],  loss: 1.051691, mae: 25.084643, mean_q: -36.270397, mean_eps: 0.050000\n",
            " 175784/200000: episode: 1110, duration: 0.615s, episode steps:  87, steps per second: 142, episode reward: -86.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.759 [0.000, 2.000],  loss: 0.948981, mae: 25.210200, mean_q: -36.422309, mean_eps: 0.050000\n",
            " 175869/200000: episode: 1111, duration: 0.551s, episode steps:  85, steps per second: 154, episode reward: -84.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.988 [0.000, 2.000],  loss: 1.015313, mae: 25.352938, mean_q: -36.627221, mean_eps: 0.050000\n",
            " 175941/200000: episode: 1112, duration: 0.502s, episode steps:  72, steps per second: 143, episode reward: -71.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.125 [0.000, 2.000],  loss: 1.130210, mae: 24.885532, mean_q: -35.819403, mean_eps: 0.050000\n",
            " 176039/200000: episode: 1113, duration: 0.657s, episode steps:  98, steps per second: 149, episode reward: -97.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.918 [0.000, 2.000],  loss: 0.965127, mae: 25.081745, mean_q: -36.257343, mean_eps: 0.050000\n",
            " 176144/200000: episode: 1114, duration: 0.701s, episode steps: 105, steps per second: 150, episode reward: -104.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.333 [0.000, 2.000],  loss: 1.134912, mae: 25.122116, mean_q: -36.285161, mean_eps: 0.050000\n",
            " 176261/200000: episode: 1115, duration: 0.807s, episode steps: 117, steps per second: 145, episode reward: -116.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.085 [0.000, 2.000],  loss: 1.095522, mae: 25.236216, mean_q: -36.434702, mean_eps: 0.050000\n",
            " 176367/200000: episode: 1116, duration: 0.680s, episode steps: 106, steps per second: 156, episode reward: -105.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.830 [0.000, 2.000],  loss: 1.013745, mae: 24.984835, mean_q: -36.076308, mean_eps: 0.050000\n",
            " 176444/200000: episode: 1117, duration: 0.508s, episode steps:  77, steps per second: 151, episode reward: -76.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.130 [0.000, 2.000],  loss: 1.017202, mae: 25.392309, mean_q: -36.719465, mean_eps: 0.050000\n",
            " 176534/200000: episode: 1118, duration: 0.674s, episode steps:  90, steps per second: 133, episode reward: -89.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.989 [0.000, 2.000],  loss: 1.097551, mae: 25.096590, mean_q: -36.336433, mean_eps: 0.050000\n",
            " 176673/200000: episode: 1119, duration: 1.321s, episode steps: 139, steps per second: 105, episode reward: -138.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.676 [0.000, 2.000],  loss: 1.087108, mae: 25.388409, mean_q: -36.638933, mean_eps: 0.050000\n",
            " 176760/200000: episode: 1120, duration: 0.822s, episode steps:  87, steps per second: 106, episode reward: -86.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.069 [0.000, 2.000],  loss: 0.984287, mae: 24.908272, mean_q: -35.926385, mean_eps: 0.050000\n",
            " 176855/200000: episode: 1121, duration: 0.896s, episode steps:  95, steps per second: 106, episode reward: -94.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.895 [0.000, 2.000],  loss: 1.013386, mae: 25.038419, mean_q: -36.139648, mean_eps: 0.050000\n",
            " 176931/200000: episode: 1122, duration: 0.626s, episode steps:  76, steps per second: 121, episode reward: -75.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.092 [0.000, 2.000],  loss: 0.947711, mae: 25.190043, mean_q: -36.386006, mean_eps: 0.050000\n",
            " 177009/200000: episode: 1123, duration: 0.556s, episode steps:  78, steps per second: 140, episode reward: -77.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.038 [0.000, 2.000],  loss: 0.946807, mae: 25.231380, mean_q: -36.426946, mean_eps: 0.050000\n",
            " 177103/200000: episode: 1124, duration: 0.660s, episode steps:  94, steps per second: 142, episode reward: -93.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.128 [0.000, 2.000],  loss: 1.201445, mae: 25.193878, mean_q: -36.378334, mean_eps: 0.050000\n",
            " 177185/200000: episode: 1125, duration: 0.502s, episode steps:  82, steps per second: 163, episode reward: -81.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.915 [0.000, 2.000],  loss: 1.138650, mae: 25.203608, mean_q: -36.368409, mean_eps: 0.050000\n",
            " 177261/200000: episode: 1126, duration: 0.467s, episode steps:  76, steps per second: 163, episode reward: -75.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.039 [0.000, 2.000],  loss: 1.343288, mae: 24.986403, mean_q: -36.003047, mean_eps: 0.050000\n",
            " 177346/200000: episode: 1127, duration: 0.554s, episode steps:  85, steps per second: 153, episode reward: -84.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.824 [0.000, 2.000],  loss: 1.221210, mae: 24.954546, mean_q: -35.986199, mean_eps: 0.050000\n",
            " 177520/200000: episode: 1128, duration: 1.068s, episode steps: 174, steps per second: 163, episode reward: -173.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.747 [0.000, 2.000],  loss: 1.066653, mae: 25.011350, mean_q: -36.115508, mean_eps: 0.050000\n",
            " 177614/200000: episode: 1129, duration: 0.628s, episode steps:  94, steps per second: 150, episode reward: -93.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.064 [0.000, 2.000],  loss: 1.014689, mae: 24.879541, mean_q: -35.952239, mean_eps: 0.050000\n",
            " 177703/200000: episode: 1130, duration: 0.616s, episode steps:  89, steps per second: 145, episode reward: -88.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.090 [0.000, 2.000],  loss: 0.990903, mae: 24.977419, mean_q: -36.069909, mean_eps: 0.050000\n",
            " 177787/200000: episode: 1131, duration: 0.496s, episode steps:  84, steps per second: 169, episode reward: -83.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.250 [0.000, 2.000],  loss: 0.928535, mae: 24.934489, mean_q: -36.015935, mean_eps: 0.050000\n",
            " 177869/200000: episode: 1132, duration: 0.521s, episode steps:  82, steps per second: 157, episode reward: -81.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.915 [0.000, 2.000],  loss: 1.110939, mae: 25.066803, mean_q: -36.215068, mean_eps: 0.050000\n",
            " 177944/200000: episode: 1133, duration: 0.487s, episode steps:  75, steps per second: 154, episode reward: -74.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.920 [0.000, 2.000],  loss: 0.986862, mae: 24.963832, mean_q: -36.085171, mean_eps: 0.050000\n",
            " 178045/200000: episode: 1134, duration: 0.721s, episode steps: 101, steps per second: 140, episode reward: -100.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.178 [0.000, 2.000],  loss: 1.121950, mae: 25.000572, mean_q: -36.133531, mean_eps: 0.050000\n",
            " 178155/200000: episode: 1135, duration: 0.728s, episode steps: 110, steps per second: 151, episode reward: -109.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.718 [0.000, 2.000],  loss: 0.995170, mae: 25.170913, mean_q: -36.355481, mean_eps: 0.050000\n",
            " 178225/200000: episode: 1136, duration: 0.462s, episode steps:  70, steps per second: 152, episode reward: -69.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.886 [0.000, 2.000],  loss: 0.943558, mae: 25.034633, mean_q: -36.256699, mean_eps: 0.050000\n",
            " 178308/200000: episode: 1137, duration: 0.546s, episode steps:  83, steps per second: 152, episode reward: -82.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.988 [0.000, 2.000],  loss: 1.038282, mae: 25.170250, mean_q: -36.393286, mean_eps: 0.050000\n",
            " 178394/200000: episode: 1138, duration: 0.561s, episode steps:  86, steps per second: 153, episode reward: -85.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.163 [0.000, 2.000],  loss: 1.030015, mae: 25.181295, mean_q: -36.366767, mean_eps: 0.050000\n",
            " 178467/200000: episode: 1139, duration: 0.620s, episode steps:  73, steps per second: 118, episode reward: -72.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.014 [0.000, 2.000],  loss: 1.021204, mae: 25.444164, mean_q: -36.720681, mean_eps: 0.050000\n",
            " 178538/200000: episode: 1140, duration: 0.632s, episode steps:  71, steps per second: 112, episode reward: -70.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.056 [0.000, 2.000],  loss: 1.106139, mae: 25.209364, mean_q: -36.401161, mean_eps: 0.050000\n",
            " 178600/200000: episode: 1141, duration: 0.594s, episode steps:  62, steps per second: 104, episode reward: -61.000, mean reward: -0.984 [-1.000,  0.000], mean action: 0.919 [0.000, 2.000],  loss: 1.202204, mae: 24.929096, mean_q: -35.976029, mean_eps: 0.050000\n",
            " 178733/200000: episode: 1142, duration: 1.212s, episode steps: 133, steps per second: 110, episode reward: -132.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.722 [0.000, 2.000],  loss: 1.019637, mae: 25.059433, mean_q: -36.209730, mean_eps: 0.050000\n",
            " 178829/200000: episode: 1143, duration: 0.843s, episode steps:  96, steps per second: 114, episode reward: -95.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.188 [0.000, 2.000],  loss: 1.029661, mae: 25.168975, mean_q: -36.370429, mean_eps: 0.050000\n",
            " 178900/200000: episode: 1144, duration: 0.471s, episode steps:  71, steps per second: 151, episode reward: -70.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.014 [0.000, 2.000],  loss: 1.188190, mae: 25.201989, mean_q: -36.334595, mean_eps: 0.050000\n",
            " 178992/200000: episode: 1145, duration: 0.614s, episode steps:  92, steps per second: 150, episode reward: -91.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.902 [0.000, 2.000],  loss: 1.392645, mae: 25.001822, mean_q: -36.139147, mean_eps: 0.050000\n",
            " 179055/200000: episode: 1146, duration: 0.397s, episode steps:  63, steps per second: 158, episode reward: -62.000, mean reward: -0.984 [-1.000,  0.000], mean action: 0.984 [0.000, 2.000],  loss: 1.039241, mae: 24.685612, mean_q: -35.530289, mean_eps: 0.050000\n",
            " 179159/200000: episode: 1147, duration: 0.654s, episode steps: 104, steps per second: 159, episode reward: -103.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.740 [0.000, 2.000],  loss: 0.950943, mae: 25.101165, mean_q: -36.263370, mean_eps: 0.050000\n",
            " 179242/200000: episode: 1148, duration: 0.490s, episode steps:  83, steps per second: 169, episode reward: -82.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.843 [0.000, 2.000],  loss: 1.009089, mae: 24.906772, mean_q: -35.916070, mean_eps: 0.050000\n",
            " 179320/200000: episode: 1149, duration: 0.496s, episode steps:  78, steps per second: 157, episode reward: -77.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.949 [0.000, 2.000],  loss: 0.849708, mae: 25.056816, mean_q: -36.202352, mean_eps: 0.050000\n",
            " 179402/200000: episode: 1150, duration: 0.506s, episode steps:  82, steps per second: 162, episode reward: -81.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.098 [0.000, 2.000],  loss: 0.929416, mae: 24.820104, mean_q: -35.842221, mean_eps: 0.050000\n",
            " 179495/200000: episode: 1151, duration: 0.590s, episode steps:  93, steps per second: 157, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.022 [0.000, 2.000],  loss: 1.156896, mae: 24.885420, mean_q: -35.784268, mean_eps: 0.050000\n",
            " 179579/200000: episode: 1152, duration: 0.509s, episode steps:  84, steps per second: 165, episode reward: -83.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 1.006762, mae: 24.748125, mean_q: -35.682401, mean_eps: 0.050000\n",
            " 179653/200000: episode: 1153, duration: 0.503s, episode steps:  74, steps per second: 147, episode reward: -73.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.851 [0.000, 2.000],  loss: 1.079162, mae: 24.624256, mean_q: -35.570295, mean_eps: 0.050000\n",
            " 179755/200000: episode: 1154, duration: 0.676s, episode steps: 102, steps per second: 151, episode reward: -101.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.863 [0.000, 2.000],  loss: 1.049784, mae: 24.697934, mean_q: -35.597439, mean_eps: 0.050000\n",
            " 179850/200000: episode: 1155, duration: 0.654s, episode steps:  95, steps per second: 145, episode reward: -94.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.916 [0.000, 2.000],  loss: 1.060054, mae: 24.862331, mean_q: -35.845637, mean_eps: 0.050000\n",
            " 179920/200000: episode: 1156, duration: 0.502s, episode steps:  70, steps per second: 140, episode reward: -69.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.043 [0.000, 2.000],  loss: 0.968483, mae: 24.955727, mean_q: -36.002986, mean_eps: 0.050000\n",
            " 180014/200000: episode: 1157, duration: 0.615s, episode steps:  94, steps per second: 153, episode reward: -93.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.745 [0.000, 2.000],  loss: 1.021910, mae: 24.769665, mean_q: -35.746053, mean_eps: 0.050000\n",
            " 180090/200000: episode: 1158, duration: 0.504s, episode steps:  76, steps per second: 151, episode reward: -75.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.789 [0.000, 2.000],  loss: 1.221178, mae: 24.605355, mean_q: -35.457920, mean_eps: 0.050000\n",
            " 180182/200000: episode: 1159, duration: 0.528s, episode steps:  92, steps per second: 174, episode reward: -91.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.793 [0.000, 2.000],  loss: 0.934007, mae: 24.891754, mean_q: -35.909230, mean_eps: 0.050000\n",
            " 180265/200000: episode: 1160, duration: 0.532s, episode steps:  83, steps per second: 156, episode reward: -82.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.855 [0.000, 2.000],  loss: 1.181640, mae: 24.750454, mean_q: -35.614205, mean_eps: 0.050000\n",
            " 180411/200000: episode: 1161, duration: 1.091s, episode steps: 146, steps per second: 134, episode reward: -145.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.068 [0.000, 2.000],  loss: 1.207142, mae: 24.821886, mean_q: -35.810899, mean_eps: 0.050000\n",
            " 180494/200000: episode: 1162, duration: 0.722s, episode steps:  83, steps per second: 115, episode reward: -82.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.892 [0.000, 2.000],  loss: 0.938108, mae: 25.049702, mean_q: -36.140505, mean_eps: 0.050000\n",
            " 180572/200000: episode: 1163, duration: 0.709s, episode steps:  78, steps per second: 110, episode reward: -77.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.872 [0.000, 2.000],  loss: 1.255762, mae: 24.748087, mean_q: -35.630656, mean_eps: 0.050000\n",
            " 180657/200000: episode: 1164, duration: 0.797s, episode steps:  85, steps per second: 107, episode reward: -84.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.871 [0.000, 2.000],  loss: 1.062754, mae: 24.832694, mean_q: -35.796225, mean_eps: 0.050000\n",
            " 180734/200000: episode: 1165, duration: 0.735s, episode steps:  77, steps per second: 105, episode reward: -76.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.883 [0.000, 2.000],  loss: 0.976647, mae: 24.932410, mean_q: -35.988222, mean_eps: 0.050000\n",
            " 180820/200000: episode: 1166, duration: 0.613s, episode steps:  86, steps per second: 140, episode reward: -85.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.884 [0.000, 2.000],  loss: 1.062106, mae: 24.940419, mean_q: -35.981233, mean_eps: 0.050000\n",
            " 180917/200000: episode: 1167, duration: 0.694s, episode steps:  97, steps per second: 140, episode reward: -96.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.649 [0.000, 2.000],  loss: 0.869824, mae: 24.907461, mean_q: -35.968508, mean_eps: 0.050000\n",
            " 181005/200000: episode: 1168, duration: 0.573s, episode steps:  88, steps per second: 154, episode reward: -87.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.807 [0.000, 2.000],  loss: 0.903149, mae: 24.822353, mean_q: -35.809202, mean_eps: 0.050000\n",
            " 181098/200000: episode: 1169, duration: 0.660s, episode steps:  93, steps per second: 141, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.796 [0.000, 2.000],  loss: 1.021022, mae: 24.678700, mean_q: -35.607430, mean_eps: 0.050000\n",
            " 181183/200000: episode: 1170, duration: 0.524s, episode steps:  85, steps per second: 162, episode reward: -84.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.800 [0.000, 2.000],  loss: 1.171413, mae: 24.778121, mean_q: -35.680552, mean_eps: 0.050000\n",
            " 181261/200000: episode: 1171, duration: 0.577s, episode steps:  78, steps per second: 135, episode reward: -77.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.910 [0.000, 2.000],  loss: 0.853914, mae: 24.610714, mean_q: -35.552439, mean_eps: 0.050000\n",
            " 181358/200000: episode: 1172, duration: 0.683s, episode steps:  97, steps per second: 142, episode reward: -96.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.948 [0.000, 2.000],  loss: 0.891477, mae: 24.516956, mean_q: -35.418042, mean_eps: 0.050000\n",
            " 181436/200000: episode: 1173, duration: 0.530s, episode steps:  78, steps per second: 147, episode reward: -77.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.930674, mae: 24.968890, mean_q: -36.109400, mean_eps: 0.050000\n",
            " 181527/200000: episode: 1174, duration: 0.647s, episode steps:  91, steps per second: 141, episode reward: -90.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.890 [0.000, 2.000],  loss: 0.882000, mae: 24.699563, mean_q: -35.692077, mean_eps: 0.050000\n",
            " 181603/200000: episode: 1175, duration: 0.526s, episode steps:  76, steps per second: 145, episode reward: -75.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.868 [0.000, 2.000],  loss: 1.063168, mae: 24.462929, mean_q: -35.303521, mean_eps: 0.050000\n",
            " 181758/200000: episode: 1176, duration: 1.093s, episode steps: 155, steps per second: 142, episode reward: -154.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.277 [0.000, 2.000],  loss: 1.147652, mae: 24.866728, mean_q: -35.877996, mean_eps: 0.050000\n",
            " 181836/200000: episode: 1177, duration: 0.644s, episode steps:  78, steps per second: 121, episode reward: -77.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.936 [0.000, 2.000],  loss: 1.198975, mae: 24.704059, mean_q: -35.580887, mean_eps: 0.050000\n",
            " 181909/200000: episode: 1178, duration: 0.596s, episode steps:  73, steps per second: 123, episode reward: -72.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.863 [0.000, 2.000],  loss: 0.874665, mae: 24.518485, mean_q: -35.277479, mean_eps: 0.050000\n",
            " 182007/200000: episode: 1179, duration: 0.683s, episode steps:  98, steps per second: 143, episode reward: -97.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.031 [0.000, 2.000],  loss: 0.893462, mae: 24.585946, mean_q: -35.471710, mean_eps: 0.050000\n",
            " 182099/200000: episode: 1180, duration: 0.660s, episode steps:  92, steps per second: 139, episode reward: -91.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.837 [0.000, 2.000],  loss: 1.056596, mae: 24.645996, mean_q: -35.618916, mean_eps: 0.050000\n",
            " 182170/200000: episode: 1181, duration: 0.668s, episode steps:  71, steps per second: 106, episode reward: -70.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.873 [0.000, 2.000],  loss: 0.923380, mae: 24.643203, mean_q: -35.562379, mean_eps: 0.050000\n",
            " 182255/200000: episode: 1182, duration: 0.792s, episode steps:  85, steps per second: 107, episode reward: -84.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.918 [0.000, 2.000],  loss: 1.292577, mae: 24.608439, mean_q: -35.370667, mean_eps: 0.050000\n",
            " 182329/200000: episode: 1183, duration: 0.717s, episode steps:  74, steps per second: 103, episode reward: -73.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.824 [0.000, 2.000],  loss: 0.967838, mae: 24.709554, mean_q: -35.618366, mean_eps: 0.050000\n",
            " 182416/200000: episode: 1184, duration: 0.782s, episode steps:  87, steps per second: 111, episode reward: -86.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.989 [0.000, 2.000],  loss: 1.131085, mae: 24.650556, mean_q: -35.604416, mean_eps: 0.050000\n",
            " 182478/200000: episode: 1185, duration: 0.652s, episode steps:  62, steps per second:  95, episode reward: -61.000, mean reward: -0.984 [-1.000,  0.000], mean action: 0.935 [0.000, 2.000],  loss: 1.176903, mae: 24.488726, mean_q: -35.347962, mean_eps: 0.050000\n",
            " 182560/200000: episode: 1186, duration: 0.556s, episode steps:  82, steps per second: 147, episode reward: -81.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.915 [0.000, 2.000],  loss: 0.903830, mae: 24.942125, mean_q: -36.024431, mean_eps: 0.050000\n",
            " 182651/200000: episode: 1187, duration: 0.584s, episode steps:  91, steps per second: 156, episode reward: -90.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.791 [0.000, 2.000],  loss: 0.972055, mae: 24.979076, mean_q: -36.132177, mean_eps: 0.050000\n",
            " 182733/200000: episode: 1188, duration: 0.509s, episode steps:  82, steps per second: 161, episode reward: -81.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.073 [0.000, 2.000],  loss: 1.012091, mae: 24.688627, mean_q: -35.610303, mean_eps: 0.050000\n",
            " 182848/200000: episode: 1189, duration: 0.676s, episode steps: 115, steps per second: 170, episode reward: -114.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.939 [0.000, 2.000],  loss: 1.142183, mae: 24.755985, mean_q: -35.669894, mean_eps: 0.050000\n",
            " 182925/200000: episode: 1190, duration: 0.534s, episode steps:  77, steps per second: 144, episode reward: -76.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.922 [0.000, 2.000],  loss: 1.319664, mae: 24.592120, mean_q: -35.352432, mean_eps: 0.050000\n",
            " 182988/200000: episode: 1191, duration: 0.402s, episode steps:  63, steps per second: 157, episode reward: -62.000, mean reward: -0.984 [-1.000,  0.000], mean action: 0.857 [0.000, 2.000],  loss: 1.156973, mae: 24.604054, mean_q: -35.396548, mean_eps: 0.050000\n",
            " 183059/200000: episode: 1192, duration: 0.473s, episode steps:  71, steps per second: 150, episode reward: -70.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.986 [0.000, 2.000],  loss: 1.075279, mae: 24.941865, mean_q: -36.038924, mean_eps: 0.050000\n",
            " 183148/200000: episode: 1193, duration: 0.562s, episode steps:  89, steps per second: 158, episode reward: -88.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.090 [0.000, 2.000],  loss: 1.113369, mae: 24.722401, mean_q: -35.631413, mean_eps: 0.050000\n",
            " 183284/200000: episode: 1194, duration: 0.856s, episode steps: 136, steps per second: 159, episode reward: -135.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.647 [0.000, 2.000],  loss: 0.981832, mae: 24.763485, mean_q: -35.706604, mean_eps: 0.050000\n",
            " 183363/200000: episode: 1195, duration: 0.494s, episode steps:  79, steps per second: 160, episode reward: -78.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.924 [0.000, 2.000],  loss: 1.012743, mae: 24.535371, mean_q: -35.357998, mean_eps: 0.050000\n",
            " 183439/200000: episode: 1196, duration: 0.466s, episode steps:  76, steps per second: 163, episode reward: -75.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.987 [0.000, 2.000],  loss: 0.879250, mae: 24.478217, mean_q: -35.307274, mean_eps: 0.050000\n",
            " 183534/200000: episode: 1197, duration: 0.632s, episode steps:  95, steps per second: 150, episode reward: -94.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.074 [0.000, 2.000],  loss: 0.959353, mae: 24.624553, mean_q: -35.497983, mean_eps: 0.050000\n",
            " 183629/200000: episode: 1198, duration: 0.568s, episode steps:  95, steps per second: 167, episode reward: -94.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.779 [0.000, 2.000],  loss: 1.236183, mae: 24.744425, mean_q: -35.742587, mean_eps: 0.050000\n",
            " 183713/200000: episode: 1199, duration: 0.526s, episode steps:  84, steps per second: 160, episode reward: -83.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.060 [0.000, 2.000],  loss: 1.179623, mae: 24.748552, mean_q: -35.700199, mean_eps: 0.050000\n",
            " 183776/200000: episode: 1200, duration: 0.392s, episode steps:  63, steps per second: 161, episode reward: -62.000, mean reward: -0.984 [-1.000,  0.000], mean action: 0.968 [0.000, 2.000],  loss: 1.095749, mae: 24.390119, mean_q: -35.142432, mean_eps: 0.050000\n",
            " 183854/200000: episode: 1201, duration: 0.502s, episode steps:  78, steps per second: 155, episode reward: -77.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.897 [0.000, 2.000],  loss: 0.962596, mae: 24.837954, mean_q: -35.889871, mean_eps: 0.050000\n",
            " 183948/200000: episode: 1202, duration: 0.585s, episode steps:  94, steps per second: 161, episode reward: -93.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.947 [0.000, 2.000],  loss: 0.936497, mae: 24.755700, mean_q: -35.770139, mean_eps: 0.050000\n",
            " 184044/200000: episode: 1203, duration: 0.630s, episode steps:  96, steps per second: 152, episode reward: -95.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.125 [0.000, 2.000],  loss: 1.092072, mae: 24.870976, mean_q: -35.895031, mean_eps: 0.050000\n",
            " 184156/200000: episode: 1204, duration: 1.019s, episode steps: 112, steps per second: 110, episode reward: -111.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.795 [0.000, 2.000],  loss: 0.948956, mae: 24.519789, mean_q: -35.291522, mean_eps: 0.050000\n",
            " 184226/200000: episode: 1205, duration: 0.652s, episode steps:  70, steps per second: 107, episode reward: -69.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.943 [0.000, 2.000],  loss: 0.902940, mae: 24.974034, mean_q: -36.023156, mean_eps: 0.050000\n",
            " 184322/200000: episode: 1206, duration: 0.806s, episode steps:  96, steps per second: 119, episode reward: -95.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.781 [0.000, 2.000],  loss: 1.038204, mae: 24.430002, mean_q: -35.234955, mean_eps: 0.050000\n",
            " 184410/200000: episode: 1207, duration: 0.794s, episode steps:  88, steps per second: 111, episode reward: -87.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 1.046216, mae: 24.528651, mean_q: -35.357832, mean_eps: 0.050000\n",
            " 184504/200000: episode: 1208, duration: 0.631s, episode steps:  94, steps per second: 149, episode reward: -93.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.787 [0.000, 2.000],  loss: 0.923425, mae: 24.719019, mean_q: -35.673022, mean_eps: 0.050000\n",
            " 184574/200000: episode: 1209, duration: 0.430s, episode steps:  70, steps per second: 163, episode reward: -69.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.014 [0.000, 2.000],  loss: 0.920922, mae: 24.815089, mean_q: -35.833263, mean_eps: 0.050000\n",
            " 184667/200000: episode: 1210, duration: 0.573s, episode steps:  93, steps per second: 162, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.817 [0.000, 2.000],  loss: 1.513016, mae: 24.573848, mean_q: -35.435524, mean_eps: 0.050000\n",
            " 184753/200000: episode: 1211, duration: 0.529s, episode steps:  86, steps per second: 162, episode reward: -85.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.988 [0.000, 2.000],  loss: 1.093922, mae: 24.618213, mean_q: -35.447131, mean_eps: 0.050000\n",
            " 184844/200000: episode: 1212, duration: 0.548s, episode steps:  91, steps per second: 166, episode reward: -90.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.725 [0.000, 2.000],  loss: 1.024850, mae: 24.447579, mean_q: -35.218214, mean_eps: 0.050000\n",
            " 184930/200000: episode: 1213, duration: 0.534s, episode steps:  86, steps per second: 161, episode reward: -85.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.837 [0.000, 2.000],  loss: 0.924367, mae: 24.337649, mean_q: -35.106933, mean_eps: 0.050000\n",
            " 185049/200000: episode: 1214, duration: 0.763s, episode steps: 119, steps per second: 156, episode reward: -118.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.950 [0.000, 2.000],  loss: 1.009105, mae: 24.669290, mean_q: -35.643595, mean_eps: 0.050000\n",
            " 185149/200000: episode: 1215, duration: 0.637s, episode steps: 100, steps per second: 157, episode reward: -99.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.940 [0.000, 2.000],  loss: 1.101939, mae: 24.632977, mean_q: -35.527645, mean_eps: 0.050000\n",
            " 185233/200000: episode: 1216, duration: 0.520s, episode steps:  84, steps per second: 162, episode reward: -83.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.940 [0.000, 2.000],  loss: 1.166116, mae: 24.804384, mean_q: -35.785415, mean_eps: 0.050000\n",
            " 185297/200000: episode: 1217, duration: 0.423s, episode steps:  64, steps per second: 151, episode reward: -63.000, mean reward: -0.984 [-1.000,  0.000], mean action: 0.922 [0.000, 2.000],  loss: 0.996629, mae: 25.027419, mean_q: -36.170325, mean_eps: 0.050000\n",
            " 185390/200000: episode: 1218, duration: 0.573s, episode steps:  93, steps per second: 162, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.086 [0.000, 2.000],  loss: 1.083655, mae: 24.633233, mean_q: -35.544976, mean_eps: 0.050000\n",
            " 185469/200000: episode: 1219, duration: 0.497s, episode steps:  79, steps per second: 159, episode reward: -78.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.025 [0.000, 2.000],  loss: 1.030899, mae: 24.592266, mean_q: -35.544225, mean_eps: 0.050000\n",
            " 185562/200000: episode: 1220, duration: 0.658s, episode steps:  93, steps per second: 141, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.129 [0.000, 2.000],  loss: 1.016719, mae: 24.871698, mean_q: -36.011134, mean_eps: 0.050000\n",
            " 185656/200000: episode: 1221, duration: 0.618s, episode steps:  94, steps per second: 152, episode reward: -93.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.734 [0.000, 2.000],  loss: 1.053074, mae: 24.510411, mean_q: -35.344727, mean_eps: 0.050000\n",
            " 185751/200000: episode: 1222, duration: 0.607s, episode steps:  95, steps per second: 157, episode reward: -94.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.853 [0.000, 2.000],  loss: 1.308233, mae: 24.739129, mean_q: -35.671506, mean_eps: 0.050000\n",
            " 185839/200000: episode: 1223, duration: 0.553s, episode steps:  88, steps per second: 159, episode reward: -87.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.807 [0.000, 2.000],  loss: 0.961527, mae: 25.110108, mean_q: -36.198034, mean_eps: 0.050000\n",
            " 185932/200000: episode: 1224, duration: 0.581s, episode steps:  93, steps per second: 160, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.097 [0.000, 2.000],  loss: 0.910719, mae: 24.595731, mean_q: -35.560383, mean_eps: 0.050000\n",
            " 186016/200000: episode: 1225, duration: 0.545s, episode steps:  84, steps per second: 154, episode reward: -83.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.917 [0.000, 2.000],  loss: 0.959884, mae: 24.542666, mean_q: -35.411566, mean_eps: 0.050000\n",
            " 186109/200000: episode: 1226, duration: 0.788s, episode steps:  93, steps per second: 118, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.860 [0.000, 2.000],  loss: 0.986213, mae: 24.880220, mean_q: -35.888794, mean_eps: 0.050000\n",
            " 186211/200000: episode: 1227, duration: 0.915s, episode steps: 102, steps per second: 111, episode reward: -101.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.961 [0.000, 2.000],  loss: 0.956953, mae: 24.640875, mean_q: -35.540661, mean_eps: 0.050000\n",
            " 186315/200000: episode: 1228, duration: 0.826s, episode steps: 104, steps per second: 126, episode reward: -103.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.962 [0.000, 2.000],  loss: 1.125264, mae: 24.512177, mean_q: -35.281724, mean_eps: 0.050000\n",
            " 186391/200000: episode: 1229, duration: 0.730s, episode steps:  76, steps per second: 104, episode reward: -75.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 1.347142, mae: 24.658283, mean_q: -35.497085, mean_eps: 0.050000\n",
            " 186503/200000: episode: 1230, duration: 0.732s, episode steps: 112, steps per second: 153, episode reward: -111.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.116 [0.000, 2.000],  loss: 0.962471, mae: 24.825561, mean_q: -35.766575, mean_eps: 0.050000\n",
            " 186581/200000: episode: 1231, duration: 0.476s, episode steps:  78, steps per second: 164, episode reward: -77.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.872 [0.000, 2.000],  loss: 1.181844, mae: 24.360524, mean_q: -35.018959, mean_eps: 0.050000\n",
            " 186674/200000: episode: 1232, duration: 0.557s, episode steps:  93, steps per second: 167, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.720 [0.000, 2.000],  loss: 1.171441, mae: 24.583646, mean_q: -35.421674, mean_eps: 0.050000\n",
            " 186760/200000: episode: 1233, duration: 0.544s, episode steps:  86, steps per second: 158, episode reward: -85.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.942 [0.000, 2.000],  loss: 1.009112, mae: 24.919953, mean_q: -35.919437, mean_eps: 0.050000\n",
            " 186836/200000: episode: 1234, duration: 0.515s, episode steps:  76, steps per second: 148, episode reward: -75.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.132 [0.000, 2.000],  loss: 0.950516, mae: 24.576382, mean_q: -35.448735, mean_eps: 0.050000\n",
            " 187039/200000: episode: 1235, duration: 1.282s, episode steps: 203, steps per second: 158, episode reward: -202.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.783 [0.000, 2.000],  loss: 1.073334, mae: 24.494301, mean_q: -35.280159, mean_eps: 0.050000\n",
            " 187116/200000: episode: 1236, duration: 0.467s, episode steps:  77, steps per second: 165, episode reward: -76.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 1.102544, mae: 24.387807, mean_q: -35.200152, mean_eps: 0.050000\n",
            " 187187/200000: episode: 1237, duration: 0.458s, episode steps:  71, steps per second: 155, episode reward: -70.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.070 [0.000, 2.000],  loss: 1.078971, mae: 24.609240, mean_q: -35.490110, mean_eps: 0.050000\n",
            " 187283/200000: episode: 1238, duration: 0.589s, episode steps:  96, steps per second: 163, episode reward: -95.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.177 [0.000, 2.000],  loss: 0.977773, mae: 24.280961, mean_q: -34.981863, mean_eps: 0.050000\n",
            " 187577/200000: episode: 1239, duration: 1.888s, episode steps: 294, steps per second: 156, episode reward: -293.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.772 [0.000, 2.000],  loss: 1.039577, mae: 24.681067, mean_q: -35.615712, mean_eps: 0.050000\n",
            " 187675/200000: episode: 1240, duration: 0.642s, episode steps:  98, steps per second: 153, episode reward: -97.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.796 [0.000, 2.000],  loss: 1.022999, mae: 24.696141, mean_q: -35.567295, mean_eps: 0.050000\n",
            " 187739/200000: episode: 1241, duration: 0.405s, episode steps:  64, steps per second: 158, episode reward: -63.000, mean reward: -0.984 [-1.000,  0.000], mean action: 1.031 [0.000, 2.000],  loss: 1.099153, mae: 24.101948, mean_q: -34.688245, mean_eps: 0.050000\n",
            " 187820/200000: episode: 1242, duration: 0.524s, episode steps:  81, steps per second: 154, episode reward: -80.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.975 [0.000, 2.000],  loss: 1.016137, mae: 24.433924, mean_q: -35.196342, mean_eps: 0.050000\n",
            " 187886/200000: episode: 1243, duration: 0.409s, episode steps:  66, steps per second: 161, episode reward: -65.000, mean reward: -0.985 [-1.000,  0.000], mean action: 1.045 [0.000, 2.000],  loss: 1.011306, mae: 24.546231, mean_q: -35.285144, mean_eps: 0.050000\n",
            " 187957/200000: episode: 1244, duration: 0.418s, episode steps:  71, steps per second: 170, episode reward: -70.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.873 [0.000, 2.000],  loss: 0.855453, mae: 24.635283, mean_q: -35.497152, mean_eps: 0.050000\n",
            " 188055/200000: episode: 1245, duration: 0.754s, episode steps:  98, steps per second: 130, episode reward: -97.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.031 [0.000, 2.000],  loss: 0.934917, mae: 24.571856, mean_q: -35.395661, mean_eps: 0.050000\n",
            " 188138/200000: episode: 1246, duration: 0.712s, episode steps:  83, steps per second: 117, episode reward: -82.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.984581, mae: 24.584665, mean_q: -35.470750, mean_eps: 0.050000\n",
            " 188215/200000: episode: 1247, duration: 0.713s, episode steps:  77, steps per second: 108, episode reward: -76.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.039 [0.000, 2.000],  loss: 0.985988, mae: 24.683531, mean_q: -35.574253, mean_eps: 0.050000\n",
            " 188296/200000: episode: 1248, duration: 0.743s, episode steps:  81, steps per second: 109, episode reward: -80.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.951 [0.000, 2.000],  loss: 0.990392, mae: 24.463172, mean_q: -35.260723, mean_eps: 0.050000\n",
            " 188401/200000: episode: 1249, duration: 1.065s, episode steps: 105, steps per second:  99, episode reward: -104.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.714 [0.000, 2.000],  loss: 1.036920, mae: 24.609142, mean_q: -35.464145, mean_eps: 0.050000\n",
            " 188495/200000: episode: 1250, duration: 0.606s, episode steps:  94, steps per second: 155, episode reward: -93.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.734 [0.000, 2.000],  loss: 1.073508, mae: 24.439571, mean_q: -35.223782, mean_eps: 0.050000\n",
            " 188578/200000: episode: 1251, duration: 0.543s, episode steps:  83, steps per second: 153, episode reward: -82.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.952 [0.000, 2.000],  loss: 1.024510, mae: 24.595280, mean_q: -35.475547, mean_eps: 0.050000\n",
            " 188683/200000: episode: 1252, duration: 0.683s, episode steps: 105, steps per second: 154, episode reward: -104.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.943 [0.000, 2.000],  loss: 1.314708, mae: 24.498180, mean_q: -35.348863, mean_eps: 0.050000\n",
            " 188760/200000: episode: 1253, duration: 0.505s, episode steps:  77, steps per second: 152, episode reward: -76.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.987 [0.000, 2.000],  loss: 0.972882, mae: 24.295103, mean_q: -35.066469, mean_eps: 0.050000\n",
            " 188838/200000: episode: 1254, duration: 0.525s, episode steps:  78, steps per second: 148, episode reward: -77.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.038 [0.000, 2.000],  loss: 1.031440, mae: 24.510757, mean_q: -35.352308, mean_eps: 0.050000\n",
            " 188917/200000: episode: 1255, duration: 0.540s, episode steps:  79, steps per second: 146, episode reward: -78.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.924 [0.000, 2.000],  loss: 0.982233, mae: 24.812990, mean_q: -35.868077, mean_eps: 0.050000\n",
            " 189011/200000: episode: 1256, duration: 0.599s, episode steps:  94, steps per second: 157, episode reward: -93.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.840 [0.000, 2.000],  loss: 1.003048, mae: 24.511981, mean_q: -35.414923, mean_eps: 0.050000\n",
            " 189092/200000: episode: 1257, duration: 0.528s, episode steps:  81, steps per second: 153, episode reward: -80.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.951 [0.000, 2.000],  loss: 1.148465, mae: 24.297029, mean_q: -35.024424, mean_eps: 0.050000\n",
            " 189166/200000: episode: 1258, duration: 0.475s, episode steps:  74, steps per second: 156, episode reward: -73.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.892 [0.000, 2.000],  loss: 1.232897, mae: 24.602738, mean_q: -35.510848, mean_eps: 0.050000\n",
            " 189303/200000: episode: 1259, duration: 0.871s, episode steps: 137, steps per second: 157, episode reward: -136.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.613 [0.000, 2.000],  loss: 0.961996, mae: 24.563390, mean_q: -35.410023, mean_eps: 0.050000\n",
            " 189390/200000: episode: 1260, duration: 0.578s, episode steps:  87, steps per second: 150, episode reward: -86.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.782 [0.000, 2.000],  loss: 1.073042, mae: 24.419864, mean_q: -35.113084, mean_eps: 0.050000\n",
            " 189484/200000: episode: 1261, duration: 0.681s, episode steps:  94, steps per second: 138, episode reward: -93.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.777 [0.000, 2.000],  loss: 1.055480, mae: 24.315347, mean_q: -35.017541, mean_eps: 0.050000\n",
            " 189557/200000: episode: 1262, duration: 0.652s, episode steps:  73, steps per second: 112, episode reward: -72.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.918 [0.000, 2.000],  loss: 0.997321, mae: 24.710204, mean_q: -35.650605, mean_eps: 0.050000\n",
            " 189626/200000: episode: 1263, duration: 0.598s, episode steps:  69, steps per second: 115, episode reward: -68.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.899 [0.000, 2.000],  loss: 0.943003, mae: 25.056595, mean_q: -36.176868, mean_eps: 0.050000\n",
            " 189701/200000: episode: 1264, duration: 0.691s, episode steps:  75, steps per second: 108, episode reward: -74.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.080 [0.000, 2.000],  loss: 1.328264, mae: 24.587172, mean_q: -35.401991, mean_eps: 0.050000\n",
            " 189777/200000: episode: 1265, duration: 0.685s, episode steps:  76, steps per second: 111, episode reward: -75.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.105 [0.000, 2.000],  loss: 0.956141, mae: 24.749024, mean_q: -35.719533, mean_eps: 0.050000\n",
            " 189886/200000: episode: 1266, duration: 1.433s, episode steps: 109, steps per second:  76, episode reward: -108.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.083 [0.000, 2.000],  loss: 1.126113, mae: 24.067524, mean_q: -34.680825, mean_eps: 0.050000\n",
            " 189955/200000: episode: 1267, duration: 0.695s, episode steps:  69, steps per second:  99, episode reward: -68.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.870 [0.000, 2.000],  loss: 1.268661, mae: 24.655827, mean_q: -35.554737, mean_eps: 0.050000\n",
            " 190029/200000: episode: 1268, duration: 0.738s, episode steps:  74, steps per second: 100, episode reward: -73.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.892 [0.000, 2.000],  loss: 0.875338, mae: 24.449925, mean_q: -35.302443, mean_eps: 0.050000\n",
            " 190103/200000: episode: 1269, duration: 0.725s, episode steps:  74, steps per second: 102, episode reward: -73.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.973 [0.000, 2.000],  loss: 1.016953, mae: 24.485640, mean_q: -35.331147, mean_eps: 0.050000\n",
            " 190185/200000: episode: 1270, duration: 0.893s, episode steps:  82, steps per second:  92, episode reward: -81.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.085 [0.000, 2.000],  loss: 1.128759, mae: 24.342752, mean_q: -35.077449, mean_eps: 0.050000\n",
            " 190268/200000: episode: 1271, duration: 0.569s, episode steps:  83, steps per second: 146, episode reward: -82.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.904 [0.000, 2.000],  loss: 1.007011, mae: 24.360897, mean_q: -35.083525, mean_eps: 0.050000\n",
            " 190353/200000: episode: 1272, duration: 0.571s, episode steps:  85, steps per second: 149, episode reward: -84.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.835 [0.000, 2.000],  loss: 1.214138, mae: 24.481109, mean_q: -35.227958, mean_eps: 0.050000\n",
            " 190433/200000: episode: 1273, duration: 0.524s, episode steps:  80, steps per second: 153, episode reward: -79.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.025 [0.000, 2.000],  loss: 1.095011, mae: 24.312133, mean_q: -35.040988, mean_eps: 0.050000\n",
            " 190510/200000: episode: 1274, duration: 0.511s, episode steps:  77, steps per second: 151, episode reward: -76.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.883 [0.000, 2.000],  loss: 1.188657, mae: 24.284643, mean_q: -35.035621, mean_eps: 0.050000\n",
            " 190605/200000: episode: 1275, duration: 0.677s, episode steps:  95, steps per second: 140, episode reward: -94.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.116 [0.000, 2.000],  loss: 0.935628, mae: 24.610257, mean_q: -35.503888, mean_eps: 0.050000\n",
            " 190667/200000: episode: 1276, duration: 0.391s, episode steps:  62, steps per second: 158, episode reward: -61.000, mean reward: -0.984 [-1.000,  0.000], mean action: 0.968 [0.000, 2.000],  loss: 1.087271, mae: 24.397655, mean_q: -35.110595, mean_eps: 0.050000\n",
            " 190849/200000: episode: 1277, duration: 1.319s, episode steps: 182, steps per second: 138, episode reward: -181.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.615 [0.000, 2.000],  loss: 1.073013, mae: 24.348624, mean_q: -34.975089, mean_eps: 0.050000\n",
            " 190941/200000: episode: 1278, duration: 0.639s, episode steps:  92, steps per second: 144, episode reward: -91.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.935 [0.000, 2.000],  loss: 1.033520, mae: 24.389560, mean_q: -35.090194, mean_eps: 0.050000\n",
            " 191036/200000: episode: 1279, duration: 0.787s, episode steps:  95, steps per second: 121, episode reward: -94.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.832 [0.000, 2.000],  loss: 0.972225, mae: 24.472207, mean_q: -35.307737, mean_eps: 0.050000\n",
            " 191112/200000: episode: 1280, duration: 0.578s, episode steps:  76, steps per second: 131, episode reward: -75.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.092 [0.000, 2.000],  loss: 0.969371, mae: 24.604020, mean_q: -35.473948, mean_eps: 0.050000\n",
            " 191210/200000: episode: 1281, duration: 0.628s, episode steps:  98, steps per second: 156, episode reward: -97.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.745 [0.000, 2.000],  loss: 0.996092, mae: 24.563535, mean_q: -35.424884, mean_eps: 0.050000\n",
            " 191377/200000: episode: 1282, duration: 1.252s, episode steps: 167, steps per second: 133, episode reward: -166.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.299 [0.000, 2.000],  loss: 1.052523, mae: 24.331459, mean_q: -35.082813, mean_eps: 0.050000\n",
            " 191461/200000: episode: 1283, duration: 0.654s, episode steps:  84, steps per second: 128, episode reward: -83.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.762 [0.000, 2.000],  loss: 0.889933, mae: 24.295685, mean_q: -35.076758, mean_eps: 0.050000\n",
            " 191552/200000: episode: 1284, duration: 0.670s, episode steps:  91, steps per second: 136, episode reward: -90.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.912 [0.000, 2.000],  loss: 1.048449, mae: 24.559360, mean_q: -35.472674, mean_eps: 0.050000\n",
            " 191633/200000: episode: 1285, duration: 0.762s, episode steps:  81, steps per second: 106, episode reward: -80.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.012 [0.000, 2.000],  loss: 0.979798, mae: 24.172751, mean_q: -34.820371, mean_eps: 0.050000\n",
            " 191696/200000: episode: 1286, duration: 0.648s, episode steps:  63, steps per second:  97, episode reward: -62.000, mean reward: -0.984 [-1.000,  0.000], mean action: 0.937 [0.000, 2.000],  loss: 1.041074, mae: 24.337203, mean_q: -35.054463, mean_eps: 0.050000\n",
            " 191802/200000: episode: 1287, duration: 1.069s, episode steps: 106, steps per second:  99, episode reward: -105.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.132 [0.000, 2.000],  loss: 0.945634, mae: 24.354749, mean_q: -35.064119, mean_eps: 0.050000\n",
            " 191884/200000: episode: 1288, duration: 0.879s, episode steps:  82, steps per second:  93, episode reward: -81.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.951 [0.000, 2.000],  loss: 1.249945, mae: 24.457919, mean_q: -35.242227, mean_eps: 0.050000\n",
            " 191978/200000: episode: 1289, duration: 0.796s, episode steps:  94, steps per second: 118, episode reward: -93.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.787 [0.000, 2.000],  loss: 0.959275, mae: 24.621970, mean_q: -35.457877, mean_eps: 0.050000\n",
            " 192070/200000: episode: 1290, duration: 0.610s, episode steps:  92, steps per second: 151, episode reward: -91.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.978 [0.000, 2.000],  loss: 0.998930, mae: 24.750234, mean_q: -35.761023, mean_eps: 0.050000\n",
            " 192160/200000: episode: 1291, duration: 0.640s, episode steps:  90, steps per second: 141, episode reward: -89.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.022 [0.000, 2.000],  loss: 0.928734, mae: 24.333880, mean_q: -35.097432, mean_eps: 0.050000\n",
            " 192255/200000: episode: 1292, duration: 0.760s, episode steps:  95, steps per second: 125, episode reward: -94.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.137 [0.000, 2.000],  loss: 0.969128, mae: 24.559352, mean_q: -35.464019, mean_eps: 0.050000\n",
            " 192361/200000: episode: 1293, duration: 0.842s, episode steps: 106, steps per second: 126, episode reward: -105.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.151 [0.000, 2.000],  loss: 1.021631, mae: 24.331539, mean_q: -35.059890, mean_eps: 0.050000\n",
            " 192450/200000: episode: 1294, duration: 0.639s, episode steps:  89, steps per second: 139, episode reward: -88.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.045 [0.000, 2.000],  loss: 0.958013, mae: 24.356082, mean_q: -35.086335, mean_eps: 0.050000\n",
            " 192576/200000: episode: 1295, duration: 0.889s, episode steps: 126, steps per second: 142, episode reward: -125.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.048 [0.000, 2.000],  loss: 1.229929, mae: 24.368011, mean_q: -35.102646, mean_eps: 0.050000\n",
            " 192650/200000: episode: 1296, duration: 0.496s, episode steps:  74, steps per second: 149, episode reward: -73.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.946 [0.000, 2.000],  loss: 0.981420, mae: 24.422241, mean_q: -35.181185, mean_eps: 0.050000\n",
            " 192733/200000: episode: 1297, duration: 0.586s, episode steps:  83, steps per second: 142, episode reward: -82.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.120 [0.000, 2.000],  loss: 0.921488, mae: 24.542634, mean_q: -35.426827, mean_eps: 0.050000\n",
            " 192819/200000: episode: 1298, duration: 0.554s, episode steps:  86, steps per second: 155, episode reward: -85.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.070 [0.000, 2.000],  loss: 1.021407, mae: 24.435518, mean_q: -35.306683, mean_eps: 0.050000\n",
            " 192911/200000: episode: 1299, duration: 0.596s, episode steps:  92, steps per second: 154, episode reward: -91.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.772 [0.000, 2.000],  loss: 1.030368, mae: 24.543987, mean_q: -35.389903, mean_eps: 0.050000\n",
            " 193004/200000: episode: 1300, duration: 0.612s, episode steps:  93, steps per second: 152, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.763 [0.000, 2.000],  loss: 1.172146, mae: 24.682499, mean_q: -35.544897, mean_eps: 0.050000\n",
            " 193089/200000: episode: 1301, duration: 0.542s, episode steps:  85, steps per second: 157, episode reward: -84.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.882 [0.000, 2.000],  loss: 0.906042, mae: 24.743495, mean_q: -35.731428, mean_eps: 0.050000\n",
            " 193172/200000: episode: 1302, duration: 0.530s, episode steps:  83, steps per second: 157, episode reward: -82.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.108 [0.000, 2.000],  loss: 0.771982, mae: 24.570695, mean_q: -35.504453, mean_eps: 0.050000\n",
            " 193250/200000: episode: 1303, duration: 0.523s, episode steps:  78, steps per second: 149, episode reward: -77.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.936 [0.000, 2.000],  loss: 0.984607, mae: 24.765749, mean_q: -35.809699, mean_eps: 0.050000\n",
            " 193338/200000: episode: 1304, duration: 0.573s, episode steps:  88, steps per second: 154, episode reward: -87.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.114 [0.000, 2.000],  loss: 1.183259, mae: 24.609814, mean_q: -35.519440, mean_eps: 0.050000\n",
            " 193433/200000: episode: 1305, duration: 0.794s, episode steps:  95, steps per second: 120, episode reward: -94.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.063 [0.000, 2.000],  loss: 1.051459, mae: 23.973673, mean_q: -34.530126, mean_eps: 0.050000\n",
            " 193517/200000: episode: 1306, duration: 0.722s, episode steps:  84, steps per second: 116, episode reward: -83.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.938864, mae: 24.506195, mean_q: -35.372676, mean_eps: 0.050000\n",
            " 193611/200000: episode: 1307, duration: 0.836s, episode steps:  94, steps per second: 113, episode reward: -93.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.043 [0.000, 2.000],  loss: 1.031825, mae: 24.290968, mean_q: -34.943738, mean_eps: 0.050000\n",
            " 193717/200000: episode: 1308, duration: 0.933s, episode steps: 106, steps per second: 114, episode reward: -105.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.113 [0.000, 2.000],  loss: 1.094623, mae: 24.338031, mean_q: -35.057167, mean_eps: 0.050000\n",
            " 193816/200000: episode: 1309, duration: 0.708s, episode steps:  99, steps per second: 140, episode reward: -98.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.778 [0.000, 2.000],  loss: 1.027093, mae: 24.337271, mean_q: -35.016626, mean_eps: 0.050000\n",
            " 193898/200000: episode: 1310, duration: 0.554s, episode steps:  82, steps per second: 148, episode reward: -81.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.073 [0.000, 2.000],  loss: 0.869398, mae: 24.522921, mean_q: -35.382039, mean_eps: 0.050000\n",
            " 193976/200000: episode: 1311, duration: 0.477s, episode steps:  78, steps per second: 164, episode reward: -77.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.013 [0.000, 2.000],  loss: 1.071010, mae: 24.406043, mean_q: -35.186428, mean_eps: 0.050000\n",
            " 194067/200000: episode: 1312, duration: 0.580s, episode steps:  91, steps per second: 157, episode reward: -90.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.879 [0.000, 2.000],  loss: 1.255670, mae: 24.165862, mean_q: -34.738457, mean_eps: 0.050000\n",
            " 194138/200000: episode: 1313, duration: 0.414s, episode steps:  71, steps per second: 171, episode reward: -70.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.197 [0.000, 2.000],  loss: 0.990160, mae: 24.475734, mean_q: -35.244904, mean_eps: 0.050000\n",
            " 194212/200000: episode: 1314, duration: 0.479s, episode steps:  74, steps per second: 154, episode reward: -73.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.946 [0.000, 2.000],  loss: 0.883698, mae: 24.365104, mean_q: -35.082324, mean_eps: 0.050000\n",
            " 194274/200000: episode: 1315, duration: 0.404s, episode steps:  62, steps per second: 153, episode reward: -61.000, mean reward: -0.984 [-1.000,  0.000], mean action: 0.952 [0.000, 2.000],  loss: 1.170895, mae: 24.570085, mean_q: -35.358598, mean_eps: 0.050000\n",
            " 194337/200000: episode: 1316, duration: 0.417s, episode steps:  63, steps per second: 151, episode reward: -62.000, mean reward: -0.984 [-1.000,  0.000], mean action: 0.921 [0.000, 2.000],  loss: 1.113930, mae: 24.367358, mean_q: -35.035127, mean_eps: 0.050000\n",
            " 194414/200000: episode: 1317, duration: 0.473s, episode steps:  77, steps per second: 163, episode reward: -76.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.026 [0.000, 2.000],  loss: 1.176175, mae: 24.664096, mean_q: -35.490632, mean_eps: 0.050000\n",
            " 194504/200000: episode: 1318, duration: 0.631s, episode steps:  90, steps per second: 143, episode reward: -89.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.678 [0.000, 2.000],  loss: 1.395612, mae: 24.419412, mean_q: -35.183785, mean_eps: 0.050000\n",
            " 194583/200000: episode: 1319, duration: 0.488s, episode steps:  79, steps per second: 162, episode reward: -78.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.975 [0.000, 2.000],  loss: 1.009291, mae: 24.214763, mean_q: -34.893152, mean_eps: 0.050000\n",
            " 194688/200000: episode: 1320, duration: 0.750s, episode steps: 105, steps per second: 140, episode reward: -104.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.048 [0.000, 2.000],  loss: 1.115004, mae: 24.324003, mean_q: -35.033168, mean_eps: 0.050000\n",
            " 194750/200000: episode: 1321, duration: 0.420s, episode steps:  62, steps per second: 148, episode reward: -61.000, mean reward: -0.984 [-1.000,  0.000], mean action: 1.032 [0.000, 2.000],  loss: 0.953404, mae: 24.173784, mean_q: -34.829142, mean_eps: 0.050000\n",
            " 194846/200000: episode: 1322, duration: 0.651s, episode steps:  96, steps per second: 147, episode reward: -95.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.792 [0.000, 2.000],  loss: 0.868738, mae: 24.672953, mean_q: -35.599286, mean_eps: 0.050000\n",
            " 194942/200000: episode: 1323, duration: 0.569s, episode steps:  96, steps per second: 169, episode reward: -95.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.062 [0.000, 2.000],  loss: 1.055606, mae: 24.061506, mean_q: -34.652697, mean_eps: 0.050000\n",
            " 195038/200000: episode: 1324, duration: 0.609s, episode steps:  96, steps per second: 158, episode reward: -95.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.083 [0.000, 2.000],  loss: 0.959924, mae: 24.237869, mean_q: -34.896864, mean_eps: 0.050000\n",
            " 195125/200000: episode: 1325, duration: 0.581s, episode steps:  87, steps per second: 150, episode reward: -86.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.011 [0.000, 2.000],  loss: 1.059076, mae: 24.225562, mean_q: -34.888351, mean_eps: 0.050000\n",
            " 195213/200000: episode: 1326, duration: 0.496s, episode steps:  88, steps per second: 177, episode reward: -87.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.898 [0.000, 2.000],  loss: 1.039500, mae: 24.016240, mean_q: -34.534185, mean_eps: 0.050000\n",
            " 195276/200000: episode: 1327, duration: 0.377s, episode steps:  63, steps per second: 167, episode reward: -62.000, mean reward: -0.984 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.995812, mae: 24.217701, mean_q: -34.858882, mean_eps: 0.050000\n",
            " 195338/200000: episode: 1328, duration: 0.488s, episode steps:  62, steps per second: 127, episode reward: -61.000, mean reward: -0.984 [-1.000,  0.000], mean action: 0.887 [0.000, 2.000],  loss: 1.093547, mae: 24.289993, mean_q: -34.969508, mean_eps: 0.050000\n",
            " 195409/200000: episode: 1329, duration: 0.586s, episode steps:  71, steps per second: 121, episode reward: -70.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.873 [0.000, 2.000],  loss: 1.116395, mae: 24.123470, mean_q: -34.676178, mean_eps: 0.050000\n",
            " 195506/200000: episode: 1330, duration: 0.888s, episode steps:  97, steps per second: 109, episode reward: -96.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.010 [0.000, 2.000],  loss: 0.974597, mae: 24.531515, mean_q: -35.386304, mean_eps: 0.050000\n",
            " 195620/200000: episode: 1331, duration: 0.997s, episode steps: 114, steps per second: 114, episode reward: -113.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.860 [0.000, 2.000],  loss: 0.949147, mae: 24.413842, mean_q: -35.208192, mean_eps: 0.050000\n",
            " 195693/200000: episode: 1332, duration: 0.724s, episode steps:  73, steps per second: 101, episode reward: -72.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.918 [0.000, 2.000],  loss: 1.027237, mae: 24.281998, mean_q: -34.985469, mean_eps: 0.050000\n",
            " 195776/200000: episode: 1333, duration: 0.538s, episode steps:  83, steps per second: 154, episode reward: -82.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.880 [0.000, 2.000],  loss: 0.940200, mae: 24.391335, mean_q: -35.138652, mean_eps: 0.050000\n",
            " 195850/200000: episode: 1334, duration: 0.475s, episode steps:  74, steps per second: 156, episode reward: -73.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.027 [0.000, 2.000],  loss: 1.195235, mae: 24.700779, mean_q: -35.601294, mean_eps: 0.050000\n",
            " 195932/200000: episode: 1335, duration: 0.486s, episode steps:  82, steps per second: 169, episode reward: -81.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.890 [0.000, 2.000],  loss: 0.962647, mae: 24.423263, mean_q: -35.170094, mean_eps: 0.050000\n",
            " 196007/200000: episode: 1336, duration: 0.520s, episode steps:  75, steps per second: 144, episode reward: -74.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.893 [0.000, 2.000],  loss: 1.085527, mae: 24.118613, mean_q: -34.723706, mean_eps: 0.050000\n",
            " 196102/200000: episode: 1337, duration: 0.645s, episode steps:  95, steps per second: 147, episode reward: -94.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.705 [0.000, 2.000],  loss: 1.088824, mae: 24.121421, mean_q: -34.714582, mean_eps: 0.050000\n",
            " 196167/200000: episode: 1338, duration: 0.447s, episode steps:  65, steps per second: 146, episode reward: -64.000, mean reward: -0.985 [-1.000,  0.000], mean action: 1.062 [0.000, 2.000],  loss: 1.054350, mae: 24.071973, mean_q: -34.690686, mean_eps: 0.050000\n",
            " 196264/200000: episode: 1339, duration: 0.683s, episode steps:  97, steps per second: 142, episode reward: -96.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.907 [0.000, 2.000],  loss: 1.088562, mae: 24.025210, mean_q: -34.637662, mean_eps: 0.050000\n",
            " 196356/200000: episode: 1340, duration: 0.643s, episode steps:  92, steps per second: 143, episode reward: -91.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.989 [0.000, 2.000],  loss: 0.999687, mae: 23.938797, mean_q: -34.494262, mean_eps: 0.050000\n",
            " 196433/200000: episode: 1341, duration: 0.535s, episode steps:  77, steps per second: 144, episode reward: -76.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.026 [0.000, 2.000],  loss: 0.972930, mae: 23.864325, mean_q: -34.308284, mean_eps: 0.050000\n",
            " 196530/200000: episode: 1342, duration: 0.612s, episode steps:  97, steps per second: 159, episode reward: -96.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.021 [0.000, 2.000],  loss: 1.143059, mae: 24.085919, mean_q: -34.679852, mean_eps: 0.050000\n",
            " 196611/200000: episode: 1343, duration: 0.523s, episode steps:  81, steps per second: 155, episode reward: -80.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.025 [0.000, 2.000],  loss: 1.014034, mae: 24.146663, mean_q: -34.824848, mean_eps: 0.050000\n",
            " 196731/200000: episode: 1344, duration: 0.721s, episode steps: 120, steps per second: 167, episode reward: -119.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.258 [0.000, 2.000],  loss: 0.921231, mae: 24.108454, mean_q: -34.765575, mean_eps: 0.050000\n",
            " 196813/200000: episode: 1345, duration: 0.533s, episode steps:  82, steps per second: 154, episode reward: -81.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.012 [0.000, 2.000],  loss: 0.891444, mae: 24.037140, mean_q: -34.597399, mean_eps: 0.050000\n",
            " 196896/200000: episode: 1346, duration: 0.507s, episode steps:  83, steps per second: 164, episode reward: -82.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.024 [0.000, 2.000],  loss: 0.972836, mae: 24.247881, mean_q: -34.947802, mean_eps: 0.050000\n",
            " 196975/200000: episode: 1347, duration: 0.486s, episode steps:  79, steps per second: 163, episode reward: -78.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.139 [0.000, 2.000],  loss: 0.885910, mae: 23.968415, mean_q: -34.496275, mean_eps: 0.050000\n",
            " 197038/200000: episode: 1348, duration: 0.367s, episode steps:  63, steps per second: 171, episode reward: -62.000, mean reward: -0.984 [-1.000,  0.000], mean action: 0.937 [0.000, 2.000],  loss: 1.003721, mae: 24.131716, mean_q: -34.670512, mean_eps: 0.050000\n",
            " 197126/200000: episode: 1349, duration: 0.528s, episode steps:  88, steps per second: 167, episode reward: -87.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.023 [0.000, 2.000],  loss: 0.868565, mae: 24.202752, mean_q: -34.883418, mean_eps: 0.050000\n",
            " 197195/200000: episode: 1350, duration: 0.415s, episode steps:  69, steps per second: 166, episode reward: -68.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.971 [0.000, 2.000],  loss: 1.052834, mae: 24.148767, mean_q: -34.807540, mean_eps: 0.050000\n",
            " 197285/200000: episode: 1351, duration: 0.732s, episode steps:  90, steps per second: 123, episode reward: -89.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.944 [0.000, 2.000],  loss: 1.061019, mae: 23.944562, mean_q: -34.517976, mean_eps: 0.050000\n",
            " 197543/200000: episode: 1352, duration: 2.262s, episode steps: 258, steps per second: 114, episode reward: -257.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.403 [0.000, 2.000],  loss: 0.982161, mae: 23.947751, mean_q: -34.514166, mean_eps: 0.050000\n",
            " 197708/200000: episode: 1353, duration: 1.354s, episode steps: 165, steps per second: 122, episode reward: -164.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.745 [0.000, 2.000],  loss: 1.010216, mae: 24.007920, mean_q: -34.533442, mean_eps: 0.050000\n",
            " 197797/200000: episode: 1354, duration: 0.615s, episode steps:  89, steps per second: 145, episode reward: -88.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.955 [0.000, 2.000],  loss: 1.001709, mae: 23.996597, mean_q: -34.465805, mean_eps: 0.050000\n",
            " 197867/200000: episode: 1355, duration: 0.505s, episode steps:  70, steps per second: 138, episode reward: -69.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.814 [0.000, 2.000],  loss: 0.807650, mae: 24.298857, mean_q: -35.000990, mean_eps: 0.050000\n",
            " 197955/200000: episode: 1356, duration: 0.630s, episode steps:  88, steps per second: 140, episode reward: -87.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.830 [0.000, 2.000],  loss: 1.031711, mae: 24.074640, mean_q: -34.701029, mean_eps: 0.050000\n",
            " 198018/200000: episode: 1357, duration: 0.390s, episode steps:  63, steps per second: 162, episode reward: -62.000, mean reward: -0.984 [-1.000,  0.000], mean action: 0.968 [0.000, 2.000],  loss: 0.934994, mae: 23.733342, mean_q: -34.217504, mean_eps: 0.050000\n",
            " 198096/200000: episode: 1358, duration: 0.507s, episode steps:  78, steps per second: 154, episode reward: -77.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.051 [0.000, 2.000],  loss: 0.998964, mae: 24.068018, mean_q: -34.681807, mean_eps: 0.050000\n",
            " 198164/200000: episode: 1359, duration: 0.471s, episode steps:  68, steps per second: 144, episode reward: -67.000, mean reward: -0.985 [-1.000,  0.000], mean action: 0.897 [0.000, 2.000],  loss: 1.078503, mae: 24.168944, mean_q: -34.827023, mean_eps: 0.050000\n",
            " 198261/200000: episode: 1360, duration: 0.660s, episode steps:  97, steps per second: 147, episode reward: -96.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.701 [0.000, 2.000],  loss: 0.994912, mae: 23.667888, mean_q: -34.072502, mean_eps: 0.050000\n",
            " 198323/200000: episode: 1361, duration: 0.381s, episode steps:  62, steps per second: 163, episode reward: -61.000, mean reward: -0.984 [-1.000,  0.000], mean action: 0.919 [0.000, 2.000],  loss: 0.959395, mae: 24.241156, mean_q: -34.964103, mean_eps: 0.050000\n",
            " 198418/200000: episode: 1362, duration: 0.617s, episode steps:  95, steps per second: 154, episode reward: -94.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.695 [0.000, 2.000],  loss: 1.037492, mae: 23.982020, mean_q: -34.591713, mean_eps: 0.050000\n",
            " 198503/200000: episode: 1363, duration: 0.564s, episode steps:  85, steps per second: 151, episode reward: -84.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.812 [0.000, 2.000],  loss: 0.791086, mae: 24.084212, mean_q: -34.774430, mean_eps: 0.050000\n",
            " 198605/200000: episode: 1364, duration: 0.649s, episode steps: 102, steps per second: 157, episode reward: -101.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.725 [0.000, 2.000],  loss: 0.997243, mae: 24.155693, mean_q: -34.841513, mean_eps: 0.050000\n",
            " 198690/200000: episode: 1365, duration: 0.534s, episode steps:  85, steps per second: 159, episode reward: -84.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.953 [0.000, 2.000],  loss: 1.026219, mae: 24.105795, mean_q: -34.785278, mean_eps: 0.050000\n",
            " 198796/200000: episode: 1366, duration: 0.639s, episode steps: 106, steps per second: 166, episode reward: -105.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.849 [0.000, 2.000],  loss: 1.030727, mae: 23.818374, mean_q: -34.367433, mean_eps: 0.050000\n",
            " 198909/200000: episode: 1367, duration: 0.721s, episode steps: 113, steps per second: 157, episode reward: -112.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.779 [0.000, 2.000],  loss: 0.967753, mae: 24.105894, mean_q: -34.790641, mean_eps: 0.050000\n",
            " 199010/200000: episode: 1368, duration: 0.628s, episode steps: 101, steps per second: 161, episode reward: -100.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.099 [0.000, 2.000],  loss: 1.006197, mae: 24.220173, mean_q: -34.893579, mean_eps: 0.050000\n",
            " 199081/200000: episode: 1369, duration: 0.463s, episode steps:  71, steps per second: 153, episode reward: -70.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.817 [0.000, 2.000],  loss: 1.114900, mae: 24.347433, mean_q: -35.111747, mean_eps: 0.050000\n",
            " 199171/200000: episode: 1370, duration: 0.711s, episode steps:  90, steps per second: 127, episode reward: -89.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.822 [0.000, 2.000],  loss: 0.878454, mae: 23.920897, mean_q: -34.507779, mean_eps: 0.050000\n",
            " 199310/200000: episode: 1371, duration: 1.211s, episode steps: 139, steps per second: 115, episode reward: -138.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.561 [0.000, 2.000],  loss: 0.915584, mae: 23.835778, mean_q: -34.344335, mean_eps: 0.050000\n",
            " 199384/200000: episode: 1372, duration: 0.663s, episode steps:  74, steps per second: 112, episode reward: -73.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.946 [0.000, 2.000],  loss: 0.898962, mae: 24.028961, mean_q: -34.621380, mean_eps: 0.050000\n",
            " 199463/200000: episode: 1373, duration: 0.696s, episode steps:  79, steps per second: 113, episode reward: -78.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.886 [0.000, 2.000],  loss: 1.033834, mae: 24.238850, mean_q: -34.928125, mean_eps: 0.050000\n",
            " 199558/200000: episode: 1374, duration: 0.823s, episode steps:  95, steps per second: 115, episode reward: -94.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.758 [0.000, 2.000],  loss: 1.123166, mae: 23.834878, mean_q: -34.251180, mean_eps: 0.050000\n",
            " 199653/200000: episode: 1375, duration: 0.594s, episode steps:  95, steps per second: 160, episode reward: -94.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.747 [0.000, 2.000],  loss: 1.054017, mae: 23.796908, mean_q: -34.221593, mean_eps: 0.050000\n",
            " 199866/200000: episode: 1376, duration: 1.314s, episode steps: 213, steps per second: 162, episode reward: -212.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.427 [0.000, 2.000],  loss: 1.038929, mae: 24.023507, mean_q: -34.524791, mean_eps: 0.050000\n",
            " 199939/200000: episode: 1377, duration: 0.439s, episode steps:  73, steps per second: 166, episode reward: -72.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.918 [0.000, 2.000],  loss: 1.056770, mae: 23.987824, mean_q: -34.558514, mean_eps: 0.050000\n",
            "done, took 1224.382 seconds\n",
            "\n",
            "Evaluando DUELING_DQN...\n",
            "DUELING_DQN → Recompensa media: -90.00 ± 16.71\n",
            "\n",
            "Entrenando BOLTZMANN_DQN...\n",
            "Training for 200000 steps ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    500/200000: episode: 1, duration: 3.234s, episode steps: 500, steps per second: 155, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.976 [0.000, 2.000],  loss: --, mae: --, mean_q: --\n",
            "   1000/200000: episode: 2, duration: 0.853s, episode steps: 500, steps per second: 586, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.032 [0.000, 2.000],  loss: --, mae: --, mean_q: --\n",
            "   1500/200000: episode: 3, duration: 0.805s, episode steps: 500, steps per second: 621, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.932 [0.000, 2.000],  loss: --, mae: --, mean_q: --\n",
            "   2000/200000: episode: 4, duration: 0.870s, episode steps: 500, steps per second: 575, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: --, mae: --, mean_q: --\n",
            "   2500/200000: episode: 5, duration: 0.741s, episode steps: 500, steps per second: 675, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.032 [0.000, 2.000],  loss: --, mae: --, mean_q: --\n",
            "   3000/200000: episode: 6, duration: 0.794s, episode steps: 500, steps per second: 630, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.038 [0.000, 2.000],  loss: --, mae: --, mean_q: --\n",
            "   3500/200000: episode: 7, duration: 0.822s, episode steps: 500, steps per second: 608, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.944 [0.000, 2.000],  loss: --, mae: --, mean_q: --\n",
            "   3831/200000: episode: 8, duration: 0.554s, episode steps: 331, steps per second: 598, episode reward: -330.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.931 [0.000, 2.000],  loss: --, mae: --, mean_q: --\n",
            "   4331/200000: episode: 9, duration: 0.762s, episode steps: 500, steps per second: 656, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: --, mae: --, mean_q: --\n",
            "   4831/200000: episode: 10, duration: 0.767s, episode steps: 500, steps per second: 652, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: --, mae: --, mean_q: --\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   5331/200000: episode: 11, duration: 8.914s, episode steps: 500, steps per second:  56, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.283497, mae: 0.729263, mean_q: -0.315651\n",
            "   5831/200000: episode: 12, duration: 2.698s, episode steps: 500, steps per second: 185, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.882 [0.000, 2.000],  loss: 0.119669, mae: 0.821699, mean_q: -0.693606\n",
            "   6331/200000: episode: 13, duration: 2.700s, episode steps: 500, steps per second: 185, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.996 [0.000, 2.000],  loss: 0.113130, mae: 1.008265, mean_q: -1.026821\n",
            "   6831/200000: episode: 14, duration: 2.457s, episode steps: 500, steps per second: 204, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.984 [0.000, 2.000],  loss: 0.082689, mae: 1.164653, mean_q: -1.322013\n",
            "   7331/200000: episode: 15, duration: 3.568s, episode steps: 500, steps per second: 140, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.068 [0.000, 2.000],  loss: 0.102998, mae: 1.393875, mean_q: -1.663079\n",
            "   7831/200000: episode: 16, duration: 2.583s, episode steps: 500, steps per second: 194, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.078724, mae: 1.566211, mean_q: -1.961083\n",
            "   8331/200000: episode: 17, duration: 2.562s, episode steps: 500, steps per second: 195, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.038 [0.000, 2.000],  loss: 0.098858, mae: 1.784536, mean_q: -2.285160\n",
            "   8831/200000: episode: 18, duration: 2.658s, episode steps: 500, steps per second: 188, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.080904, mae: 1.930360, mean_q: -2.533426\n",
            "   9331/200000: episode: 19, duration: 2.743s, episode steps: 500, steps per second: 182, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.048 [0.000, 2.000],  loss: 0.101016, mae: 2.155127, mean_q: -2.858552\n",
            "   9831/200000: episode: 20, duration: 3.634s, episode steps: 500, steps per second: 138, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.024 [0.000, 2.000],  loss: 0.084704, mae: 2.303002, mean_q: -3.099250\n",
            "  10331/200000: episode: 21, duration: 2.555s, episode steps: 500, steps per second: 196, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.998 [0.000, 2.000],  loss: 0.103473, mae: 2.517014, mean_q: -3.413565\n",
            "  10831/200000: episode: 22, duration: 2.559s, episode steps: 500, steps per second: 195, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.974 [0.000, 2.000],  loss: 0.095743, mae: 2.649884, mean_q: -3.625582\n",
            "  11331/200000: episode: 23, duration: 2.475s, episode steps: 500, steps per second: 202, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.016 [0.000, 2.000],  loss: 0.117451, mae: 2.860957, mean_q: -3.931259\n",
            "  11831/200000: episode: 24, duration: 2.585s, episode steps: 500, steps per second: 193, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.102093, mae: 2.998504, mean_q: -4.150140\n",
            "  12331/200000: episode: 25, duration: 3.493s, episode steps: 500, steps per second: 143, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.944 [0.000, 2.000],  loss: 0.123230, mae: 3.206584, mean_q: -4.447181\n",
            "  12831/200000: episode: 26, duration: 2.462s, episode steps: 500, steps per second: 203, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.972 [0.000, 2.000],  loss: 0.112594, mae: 3.341608, mean_q: -4.658834\n",
            "  13331/200000: episode: 27, duration: 2.485s, episode steps: 500, steps per second: 201, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.986 [0.000, 2.000],  loss: 0.125850, mae: 3.544987, mean_q: -4.955967\n",
            "  13831/200000: episode: 28, duration: 2.458s, episode steps: 500, steps per second: 203, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.121862, mae: 3.667353, mean_q: -5.144586\n",
            "  14331/200000: episode: 29, duration: 2.831s, episode steps: 500, steps per second: 177, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.956 [0.000, 2.000],  loss: 0.138758, mae: 3.859524, mean_q: -5.415814\n",
            "  14831/200000: episode: 30, duration: 3.489s, episode steps: 500, steps per second: 143, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.918 [0.000, 2.000],  loss: 0.134330, mae: 3.972412, mean_q: -5.586825\n",
            "  15291/200000: episode: 31, duration: 2.370s, episode steps: 460, steps per second: 194, episode reward: -459.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.980 [0.000, 2.000],  loss: 0.144739, mae: 4.155972, mean_q: -5.857353\n",
            "  15791/200000: episode: 32, duration: 2.386s, episode steps: 500, steps per second: 210, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.150055, mae: 4.301486, mean_q: -6.079198\n",
            "  16291/200000: episode: 33, duration: 2.421s, episode steps: 500, steps per second: 207, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.958 [0.000, 2.000],  loss: 0.165401, mae: 4.448102, mean_q: -6.292351\n",
            "  16791/200000: episode: 34, duration: 2.696s, episode steps: 500, steps per second: 185, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.026 [0.000, 2.000],  loss: 0.158103, mae: 4.579850, mean_q: -6.492708\n",
            "  17291/200000: episode: 35, duration: 3.471s, episode steps: 500, steps per second: 144, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.012 [0.000, 2.000],  loss: 0.187184, mae: 4.715068, mean_q: -6.684357\n",
            "  17791/200000: episode: 36, duration: 2.515s, episode steps: 500, steps per second: 199, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.008 [0.000, 2.000],  loss: 0.166774, mae: 4.838401, mean_q: -6.871227\n",
            "  18291/200000: episode: 37, duration: 2.495s, episode steps: 500, steps per second: 200, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.958 [0.000, 2.000],  loss: 0.177222, mae: 4.988585, mean_q: -7.089357\n",
            "  18782/200000: episode: 38, duration: 2.517s, episode steps: 491, steps per second: 195, episode reward: -490.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.026 [0.000, 2.000],  loss: 0.186234, mae: 5.106019, mean_q: -7.261671\n",
            "  19198/200000: episode: 39, duration: 2.200s, episode steps: 416, steps per second: 189, episode reward: -415.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.208240, mae: 5.197900, mean_q: -7.396976\n",
            "  19698/200000: episode: 40, duration: 3.687s, episode steps: 500, steps per second: 136, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.188376, mae: 5.338078, mean_q: -7.609534\n",
            "  20198/200000: episode: 41, duration: 2.721s, episode steps: 500, steps per second: 184, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.002 [0.000, 2.000],  loss: 0.200231, mae: 5.413076, mean_q: -7.707706\n",
            "  20536/200000: episode: 42, duration: 1.811s, episode steps: 338, steps per second: 187, episode reward: -337.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.003 [0.000, 2.000],  loss: 0.219131, mae: 5.569762, mean_q: -7.931365\n",
            "  21036/200000: episode: 43, duration: 2.578s, episode steps: 500, steps per second: 194, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.204742, mae: 5.560206, mean_q: -7.919274\n",
            "  21536/200000: episode: 44, duration: 2.940s, episode steps: 500, steps per second: 170, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.974 [0.000, 2.000],  loss: 0.213952, mae: 5.747775, mean_q: -8.187541\n",
            "  22036/200000: episode: 45, duration: 3.717s, episode steps: 500, steps per second: 135, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.217158, mae: 5.767483, mean_q: -8.217178\n",
            "  22536/200000: episode: 46, duration: 2.428s, episode steps: 500, steps per second: 206, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.976 [0.000, 2.000],  loss: 0.233084, mae: 5.982571, mean_q: -8.523384\n",
            "  23036/200000: episode: 47, duration: 2.557s, episode steps: 500, steps per second: 196, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.231675, mae: 5.965600, mean_q: -8.505455\n",
            "  23536/200000: episode: 48, duration: 2.544s, episode steps: 500, steps per second: 197, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.231182, mae: 6.122450, mean_q: -8.743462\n",
            "  24036/200000: episode: 49, duration: 2.773s, episode steps: 500, steps per second: 180, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.233180, mae: 6.112428, mean_q: -8.735358\n",
            "  24438/200000: episode: 50, duration: 3.015s, episode steps: 402, steps per second: 133, episode reward: -401.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.975 [0.000, 2.000],  loss: 0.234336, mae: 6.232527, mean_q: -8.895935\n",
            "  24938/200000: episode: 51, duration: 2.886s, episode steps: 500, steps per second: 173, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.972 [0.000, 2.000],  loss: 0.234803, mae: 6.201882, mean_q: -8.857647\n",
            "  25438/200000: episode: 52, duration: 2.557s, episode steps: 500, steps per second: 196, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.239551, mae: 6.344768, mean_q: -9.065500\n",
            "  25788/200000: episode: 53, duration: 1.871s, episode steps: 350, steps per second: 187, episode reward: -349.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.046 [0.000, 2.000],  loss: 0.229929, mae: 6.367546, mean_q: -9.099855\n",
            "  26288/200000: episode: 54, duration: 2.847s, episode steps: 500, steps per second: 176, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.236765, mae: 6.452254, mean_q: -9.230068\n",
            "  26624/200000: episode: 55, duration: 2.799s, episode steps: 336, steps per second: 120, episode reward: -335.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.018 [0.000, 2.000],  loss: 0.245285, mae: 6.537475, mean_q: -9.351379\n",
            "  26880/200000: episode: 56, duration: 1.521s, episode steps: 256, steps per second: 168, episode reward: -255.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.055 [0.000, 2.000],  loss: 0.262672, mae: 6.570026, mean_q: -9.393080\n",
            "  27241/200000: episode: 57, duration: 2.032s, episode steps: 361, steps per second: 178, episode reward: -360.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.003 [0.000, 2.000],  loss: 0.245147, mae: 6.645291, mean_q: -9.501538\n",
            "  27667/200000: episode: 58, duration: 2.451s, episode steps: 426, steps per second: 174, episode reward: -425.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.026 [0.000, 2.000],  loss: 0.256033, mae: 6.719001, mean_q: -9.604450\n",
            "  28167/200000: episode: 59, duration: 2.742s, episode steps: 500, steps per second: 182, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.016 [0.000, 2.000],  loss: 0.263241, mae: 6.696422, mean_q: -9.569390\n",
            "  28667/200000: episode: 60, duration: 3.244s, episode steps: 500, steps per second: 154, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.255739, mae: 6.736533, mean_q: -9.625733\n",
            "  29030/200000: episode: 61, duration: 2.481s, episode steps: 363, steps per second: 146, episode reward: -362.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.956 [0.000, 2.000],  loss: 0.263701, mae: 6.755548, mean_q: -9.652609\n",
            "  29530/200000: episode: 62, duration: 2.619s, episode steps: 500, steps per second: 191, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.986 [0.000, 2.000],  loss: 0.255850, mae: 6.827409, mean_q: -9.751027\n",
            "  30030/200000: episode: 63, duration: 2.613s, episode steps: 500, steps per second: 191, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.992 [0.000, 2.000],  loss: 0.269326, mae: 6.829277, mean_q: -9.758013\n",
            "  30405/200000: episode: 64, duration: 1.943s, episode steps: 375, steps per second: 193, episode reward: -374.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.960 [0.000, 2.000],  loss: 0.286089, mae: 6.921795, mean_q: -9.885335\n",
            "  30708/200000: episode: 65, duration: 1.725s, episode steps: 303, steps per second: 176, episode reward: -302.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.950 [0.000, 2.000],  loss: 0.282501, mae: 6.938235, mean_q: -9.905468\n",
            "  30981/200000: episode: 66, duration: 2.117s, episode steps: 273, steps per second: 129, episode reward: -272.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.897 [0.000, 2.000],  loss: 0.257608, mae: 6.923870, mean_q: -9.903734\n",
            "  31481/200000: episode: 67, duration: 3.362s, episode steps: 500, steps per second: 149, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.994 [0.000, 2.000],  loss: 0.262388, mae: 7.122202, mean_q: -10.191188\n",
            "  31981/200000: episode: 68, duration: 2.748s, episode steps: 500, steps per second: 182, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.014 [0.000, 2.000],  loss: 0.286905, mae: 7.092206, mean_q: -10.148762\n",
            "  32365/200000: episode: 69, duration: 1.992s, episode steps: 384, steps per second: 193, episode reward: -383.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.977 [0.000, 2.000],  loss: 0.304218, mae: 7.119565, mean_q: -10.173296\n",
            "  32865/200000: episode: 70, duration: 2.585s, episode steps: 500, steps per second: 193, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.978 [0.000, 2.000],  loss: 0.282156, mae: 7.123005, mean_q: -10.185262\n",
            "  33365/200000: episode: 71, duration: 3.618s, episode steps: 500, steps per second: 138, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.958 [0.000, 2.000],  loss: 0.284380, mae: 7.151064, mean_q: -10.225272\n",
            "  33778/200000: episode: 72, duration: 2.541s, episode steps: 413, steps per second: 163, episode reward: -412.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.041 [0.000, 2.000],  loss: 0.287997, mae: 7.168774, mean_q: -10.259296\n",
            "  34077/200000: episode: 73, duration: 1.627s, episode steps: 299, steps per second: 184, episode reward: -298.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.916 [0.000, 2.000],  loss: 0.271033, mae: 7.192843, mean_q: -10.295921\n",
            "  34577/200000: episode: 74, duration: 2.769s, episode steps: 500, steps per second: 181, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.022 [0.000, 2.000],  loss: 0.334279, mae: 7.359808, mean_q: -10.510770\n",
            "  35077/200000: episode: 75, duration: 2.888s, episode steps: 500, steps per second: 173, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.032 [0.000, 2.000],  loss: 0.292596, mae: 7.355742, mean_q: -10.522755\n",
            "  35476/200000: episode: 76, duration: 2.655s, episode steps: 399, steps per second: 150, episode reward: -398.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.975 [0.000, 2.000],  loss: 0.307278, mae: 7.386782, mean_q: -10.570526\n",
            "  35976/200000: episode: 77, duration: 4.180s, episode steps: 500, steps per second: 120, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.008 [0.000, 2.000],  loss: 0.308344, mae: 7.353112, mean_q: -10.519441\n",
            "  36416/200000: episode: 78, duration: 2.856s, episode steps: 440, steps per second: 154, episode reward: -439.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.998 [0.000, 2.000],  loss: 0.348732, mae: 7.508955, mean_q: -10.745947\n",
            "  36730/200000: episode: 79, duration: 1.745s, episode steps: 314, steps per second: 180, episode reward: -313.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.051 [0.000, 2.000],  loss: 0.307237, mae: 7.496072, mean_q: -10.731814\n",
            "  37230/200000: episode: 80, duration: 2.709s, episode steps: 500, steps per second: 185, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.012 [0.000, 2.000],  loss: 0.297548, mae: 7.577040, mean_q: -10.858470\n",
            "  37513/200000: episode: 81, duration: 1.746s, episode steps: 283, steps per second: 162, episode reward: -282.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.042 [0.000, 2.000],  loss: 0.321828, mae: 7.620842, mean_q: -10.899798\n",
            "  37926/200000: episode: 82, duration: 3.134s, episode steps: 413, steps per second: 132, episode reward: -412.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.995 [0.000, 2.000],  loss: 0.316273, mae: 7.612679, mean_q: -10.900816\n",
            "  38278/200000: episode: 83, duration: 1.950s, episode steps: 352, steps per second: 181, episode reward: -351.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.062 [0.000, 2.000],  loss: 0.344488, mae: 7.695627, mean_q: -11.013253\n",
            "  38710/200000: episode: 84, duration: 2.267s, episode steps: 432, steps per second: 191, episode reward: -431.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.933 [0.000, 2.000],  loss: 0.294769, mae: 7.701084, mean_q: -11.027810\n",
            "  39210/200000: episode: 85, duration: 2.635s, episode steps: 500, steps per second: 190, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.348310, mae: 7.751297, mean_q: -11.083357\n",
            "  39680/200000: episode: 86, duration: 2.466s, episode steps: 470, steps per second: 191, episode reward: -469.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.996 [0.000, 2.000],  loss: 0.370500, mae: 7.819289, mean_q: -11.173270\n",
            "  40157/200000: episode: 87, duration: 3.849s, episode steps: 477, steps per second: 124, episode reward: -476.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.971 [0.000, 2.000],  loss: 0.357000, mae: 7.755001, mean_q: -11.086820\n",
            "  40657/200000: episode: 88, duration: 2.830s, episode steps: 500, steps per second: 177, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.996 [0.000, 2.000],  loss: 0.329409, mae: 7.808129, mean_q: -11.173015\n",
            "  41157/200000: episode: 89, duration: 2.817s, episode steps: 500, steps per second: 177, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.972 [0.000, 2.000],  loss: 0.355373, mae: 7.805554, mean_q: -11.162541\n",
            "  41590/200000: episode: 90, duration: 2.486s, episode steps: 433, steps per second: 174, episode reward: -432.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.954 [0.000, 2.000],  loss: 0.344697, mae: 7.856751, mean_q: -11.248919\n",
            "  42074/200000: episode: 91, duration: 3.041s, episode steps: 484, steps per second: 159, episode reward: -483.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.089 [0.000, 2.000],  loss: 0.353074, mae: 7.852017, mean_q: -11.241474\n",
            "  42574/200000: episode: 92, duration: 3.489s, episode steps: 500, steps per second: 143, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.344691, mae: 8.014117, mean_q: -11.489606\n",
            "  42889/200000: episode: 93, duration: 1.756s, episode steps: 315, steps per second: 179, episode reward: -314.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.984 [0.000, 2.000],  loss: 0.323457, mae: 7.936556, mean_q: -11.368696\n",
            "  43231/200000: episode: 94, duration: 1.837s, episode steps: 342, steps per second: 186, episode reward: -341.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.056 [0.000, 2.000],  loss: 0.335028, mae: 7.946950, mean_q: -11.383695\n",
            "  43529/200000: episode: 95, duration: 1.629s, episode steps: 298, steps per second: 183, episode reward: -297.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.020 [0.000, 2.000],  loss: 0.331854, mae: 8.000928, mean_q: -11.468792\n",
            "  43880/200000: episode: 96, duration: 1.890s, episode steps: 351, steps per second: 186, episode reward: -350.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.048 [0.000, 2.000],  loss: 0.342806, mae: 7.985560, mean_q: -11.441528\n",
            "  44380/200000: episode: 97, duration: 3.291s, episode steps: 500, steps per second: 152, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.946 [0.000, 2.000],  loss: 0.355438, mae: 8.100711, mean_q: -11.608929\n",
            "  44633/200000: episode: 98, duration: 2.036s, episode steps: 253, steps per second: 124, episode reward: -252.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.925 [0.000, 2.000],  loss: 0.365663, mae: 8.086657, mean_q: -11.582356\n",
            "  45133/200000: episode: 99, duration: 2.902s, episode steps: 500, steps per second: 172, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.064 [0.000, 2.000],  loss: 0.310191, mae: 8.129707, mean_q: -11.655889\n",
            "  45633/200000: episode: 100, duration: 3.062s, episode steps: 500, steps per second: 163, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.026 [0.000, 2.000],  loss: 0.398770, mae: 8.152017, mean_q: -11.676832\n",
            "  46133/200000: episode: 101, duration: 2.924s, episode steps: 500, steps per second: 171, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.968 [0.000, 2.000],  loss: 0.351391, mae: 8.097488, mean_q: -11.594478\n",
            "  46606/200000: episode: 102, duration: 3.661s, episode steps: 473, steps per second: 129, episode reward: -472.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.017 [0.000, 2.000],  loss: 0.375858, mae: 8.106551, mean_q: -11.598960\n",
            "  46886/200000: episode: 103, duration: 1.969s, episode steps: 280, steps per second: 142, episode reward: -279.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.968 [0.000, 2.000],  loss: 0.413375, mae: 8.121915, mean_q: -11.600370\n",
            "  47267/200000: episode: 104, duration: 2.277s, episode steps: 381, steps per second: 167, episode reward: -380.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.063 [0.000, 2.000],  loss: 0.347742, mae: 8.118247, mean_q: -11.620556\n",
            "  47445/200000: episode: 105, duration: 1.071s, episode steps: 178, steps per second: 166, episode reward: -177.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.904 [0.000, 2.000],  loss: 0.402939, mae: 8.136358, mean_q: -11.620669\n",
            "  47743/200000: episode: 106, duration: 1.759s, episode steps: 298, steps per second: 169, episode reward: -297.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.987 [0.000, 2.000],  loss: 0.349591, mae: 8.142040, mean_q: -11.665094\n",
            "  48181/200000: episode: 107, duration: 2.428s, episode steps: 438, steps per second: 180, episode reward: -437.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.966 [0.000, 2.000],  loss: 0.378401, mae: 8.189943, mean_q: -11.722039\n",
            "  48484/200000: episode: 108, duration: 1.822s, episode steps: 303, steps per second: 166, episode reward: -302.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.010 [0.000, 2.000],  loss: 0.359321, mae: 8.238092, mean_q: -11.802688\n",
            "  48884/200000: episode: 109, duration: 3.250s, episode steps: 400, steps per second: 123, episode reward: -399.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.973 [0.000, 2.000],  loss: 0.323315, mae: 8.207056, mean_q: -11.770551\n",
            "  49326/200000: episode: 110, duration: 2.583s, episode steps: 442, steps per second: 171, episode reward: -441.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.020 [0.000, 2.000],  loss: 0.399629, mae: 8.269849, mean_q: -11.852316\n",
            "  49667/200000: episode: 111, duration: 1.968s, episode steps: 341, steps per second: 173, episode reward: -340.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.915 [0.000, 2.000],  loss: 0.345913, mae: 8.269275, mean_q: -11.854444\n",
            "  49975/200000: episode: 112, duration: 1.831s, episode steps: 308, steps per second: 168, episode reward: -307.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.955 [0.000, 2.000],  loss: 0.351806, mae: 8.345704, mean_q: -11.979354\n",
            "  50224/200000: episode: 113, duration: 1.617s, episode steps: 249, steps per second: 154, episode reward: -248.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.976 [0.000, 2.000],  loss: 0.387680, mae: 8.327432, mean_q: -11.927334\n",
            "  50506/200000: episode: 114, duration: 1.762s, episode steps: 282, steps per second: 160, episode reward: -281.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.954 [0.000, 2.000],  loss: 0.410997, mae: 8.302920, mean_q: -11.893307\n",
            "  50929/200000: episode: 115, duration: 3.590s, episode steps: 423, steps per second: 118, episode reward: -422.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.007 [0.000, 2.000],  loss: 0.356719, mae: 8.316454, mean_q: -11.940235\n",
            "  51389/200000: episode: 116, duration: 2.640s, episode steps: 460, steps per second: 174, episode reward: -459.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.002 [0.000, 2.000],  loss: 0.378378, mae: 8.341777, mean_q: -11.944692\n",
            "  51726/200000: episode: 117, duration: 1.840s, episode steps: 337, steps per second: 183, episode reward: -336.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.095 [0.000, 2.000],  loss: 0.394122, mae: 8.354348, mean_q: -11.958624\n",
            "  51942/200000: episode: 118, duration: 1.162s, episode steps: 216, steps per second: 186, episode reward: -215.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.944 [0.000, 2.000],  loss: 0.400505, mae: 8.350842, mean_q: -11.939057\n",
            "  52219/200000: episode: 119, duration: 1.506s, episode steps: 277, steps per second: 184, episode reward: -276.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.076 [0.000, 2.000],  loss: 0.376624, mae: 8.326804, mean_q: -11.924429\n",
            "  52593/200000: episode: 120, duration: 2.138s, episode steps: 374, steps per second: 175, episode reward: -373.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.011 [0.000, 2.000],  loss: 0.358163, mae: 8.391368, mean_q: -12.039233\n",
            "  52984/200000: episode: 121, duration: 2.938s, episode steps: 391, steps per second: 133, episode reward: -390.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.003 [0.000, 2.000],  loss: 0.367006, mae: 8.389599, mean_q: -12.030414\n",
            "  53339/200000: episode: 122, duration: 2.327s, episode steps: 355, steps per second: 153, episode reward: -354.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.992 [0.000, 2.000],  loss: 0.332047, mae: 8.375378, mean_q: -12.016724\n",
            "  53698/200000: episode: 123, duration: 2.026s, episode steps: 359, steps per second: 177, episode reward: -358.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.058 [0.000, 2.000],  loss: 0.368899, mae: 8.443832, mean_q: -12.100719\n",
            "  54198/200000: episode: 124, duration: 2.856s, episode steps: 500, steps per second: 175, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.996 [0.000, 2.000],  loss: 0.390569, mae: 8.415528, mean_q: -12.055520\n",
            "  54445/200000: episode: 125, duration: 1.451s, episode steps: 247, steps per second: 170, episode reward: -246.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.101 [0.000, 2.000],  loss: 0.348285, mae: 8.480077, mean_q: -12.184983\n",
            "  54911/200000: episode: 126, duration: 3.043s, episode steps: 466, steps per second: 153, episode reward: -465.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.017 [0.000, 2.000],  loss: 0.359396, mae: 8.467567, mean_q: -12.160568\n",
            "  55228/200000: episode: 127, duration: 2.667s, episode steps: 317, steps per second: 119, episode reward: -316.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.991 [0.000, 2.000],  loss: 0.358596, mae: 8.541289, mean_q: -12.269493\n",
            "  55633/200000: episode: 128, duration: 2.389s, episode steps: 405, steps per second: 170, episode reward: -404.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.044 [0.000, 2.000],  loss: 0.416270, mae: 8.592493, mean_q: -12.332785\n",
            "  55878/200000: episode: 129, duration: 1.463s, episode steps: 245, steps per second: 167, episode reward: -244.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.943 [0.000, 2.000],  loss: 0.344279, mae: 8.547605, mean_q: -12.276298\n",
            "  56378/200000: episode: 130, duration: 2.999s, episode steps: 500, steps per second: 167, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.964 [0.000, 2.000],  loss: 0.386470, mae: 8.648671, mean_q: -12.424807\n",
            "  56520/200000: episode: 131, duration: 0.888s, episode steps: 142, steps per second: 160, episode reward: -141.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.972 [0.000, 2.000],  loss: 0.380909, mae: 8.578089, mean_q: -12.309534\n",
            "  56841/200000: episode: 132, duration: 1.971s, episode steps: 321, steps per second: 163, episode reward: -320.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.075 [0.000, 2.000],  loss: 0.389644, mae: 8.621952, mean_q: -12.384237\n",
            "  57341/200000: episode: 133, duration: 4.100s, episode steps: 500, steps per second: 122, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.938 [0.000, 2.000],  loss: 0.372329, mae: 8.682009, mean_q: -12.472752\n",
            "  57831/200000: episode: 134, duration: 2.759s, episode steps: 490, steps per second: 178, episode reward: -489.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.935 [0.000, 2.000],  loss: 0.364536, mae: 8.631712, mean_q: -12.396458\n",
            "  58331/200000: episode: 135, duration: 2.891s, episode steps: 500, steps per second: 173, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.952 [0.000, 2.000],  loss: 0.376352, mae: 8.659905, mean_q: -12.441973\n",
            "  58626/200000: episode: 136, duration: 1.830s, episode steps: 295, steps per second: 161, episode reward: -294.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.983 [0.000, 2.000],  loss: 0.356338, mae: 8.680475, mean_q: -12.474739\n",
            "  58964/200000: episode: 137, duration: 2.069s, episode steps: 338, steps per second: 163, episode reward: -337.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.030 [0.000, 2.000],  loss: 0.388687, mae: 8.692851, mean_q: -12.481740\n",
            "  59280/200000: episode: 138, duration: 2.613s, episode steps: 316, steps per second: 121, episode reward: -315.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.016 [0.000, 2.000],  loss: 0.381437, mae: 8.702833, mean_q: -12.492756\n",
            "  59614/200000: episode: 139, duration: 2.514s, episode steps: 334, steps per second: 133, episode reward: -333.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.991 [0.000, 2.000],  loss: 0.401826, mae: 8.750785, mean_q: -12.566961\n",
            "  60114/200000: episode: 140, duration: 3.207s, episode steps: 500, steps per second: 156, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.393939, mae: 8.710493, mean_q: -12.511936\n",
            "  60495/200000: episode: 141, duration: 2.292s, episode steps: 381, steps per second: 166, episode reward: -380.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.008 [0.000, 2.000],  loss: 0.391342, mae: 8.753563, mean_q: -12.591659\n",
            "  60796/200000: episode: 142, duration: 1.860s, episode steps: 301, steps per second: 162, episode reward: -300.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.066 [0.000, 2.000],  loss: 0.382103, mae: 8.764546, mean_q: -12.620924\n",
            "  61120/200000: episode: 143, duration: 2.173s, episode steps: 324, steps per second: 149, episode reward: -323.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.025 [0.000, 2.000],  loss: 0.391851, mae: 8.743549, mean_q: -12.577631\n",
            "  61446/200000: episode: 144, duration: 2.758s, episode steps: 326, steps per second: 118, episode reward: -325.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.985 [0.000, 2.000],  loss: 0.402787, mae: 8.867277, mean_q: -12.754508\n",
            "  61665/200000: episode: 145, duration: 1.347s, episode steps: 219, steps per second: 163, episode reward: -218.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.027 [0.000, 2.000],  loss: 0.407943, mae: 8.782467, mean_q: -12.625557\n",
            "  62022/200000: episode: 146, duration: 2.095s, episode steps: 357, steps per second: 170, episode reward: -356.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.933 [0.000, 2.000],  loss: 0.390914, mae: 8.852324, mean_q: -12.726236\n",
            "  62522/200000: episode: 147, duration: 2.921s, episode steps: 500, steps per second: 171, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.898 [0.000, 2.000],  loss: 0.406811, mae: 8.949199, mean_q: -12.864151\n",
            "  62968/200000: episode: 148, duration: 2.518s, episode steps: 446, steps per second: 177, episode reward: -445.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.018 [0.000, 2.000],  loss: 0.403236, mae: 9.012294, mean_q: -12.959930\n",
            "  63300/200000: episode: 149, duration: 2.297s, episode steps: 332, steps per second: 145, episode reward: -331.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.111 [0.000, 2.000],  loss: 0.381422, mae: 9.084858, mean_q: -13.066599\n",
            "  63800/200000: episode: 150, duration: 3.594s, episode steps: 500, steps per second: 139, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.034 [0.000, 2.000],  loss: 0.405355, mae: 9.061634, mean_q: -13.029316\n",
            "  64102/200000: episode: 151, duration: 1.794s, episode steps: 302, steps per second: 168, episode reward: -301.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.970 [0.000, 2.000],  loss: 0.414798, mae: 9.092921, mean_q: -13.071801\n",
            "  64335/200000: episode: 152, duration: 1.325s, episode steps: 233, steps per second: 176, episode reward: -232.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.936 [0.000, 2.000],  loss: 0.407449, mae: 9.169482, mean_q: -13.187646\n",
            "  64835/200000: episode: 153, duration: 3.115s, episode steps: 500, steps per second: 161, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.958 [0.000, 2.000],  loss: 0.442145, mae: 9.207035, mean_q: -13.223190\n",
            "  65308/200000: episode: 154, duration: 3.113s, episode steps: 473, steps per second: 152, episode reward: -472.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.049 [0.000, 2.000],  loss: 0.419205, mae: 9.201901, mean_q: -13.226269\n",
            "  65556/200000: episode: 155, duration: 2.328s, episode steps: 248, steps per second: 107, episode reward: -247.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.016 [0.000, 2.000],  loss: 0.378009, mae: 9.261073, mean_q: -13.339481\n",
            "  65898/200000: episode: 156, duration: 2.272s, episode steps: 342, steps per second: 151, episode reward: -341.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.041 [0.000, 2.000],  loss: 0.411063, mae: 9.174788, mean_q: -13.181863\n",
            "  66246/200000: episode: 157, duration: 2.187s, episode steps: 348, steps per second: 159, episode reward: -347.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.968 [0.000, 2.000],  loss: 0.420636, mae: 9.311700, mean_q: -13.377256\n",
            "  66742/200000: episode: 158, duration: 3.031s, episode steps: 496, steps per second: 164, episode reward: -495.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.073 [0.000, 2.000],  loss: 0.447635, mae: 9.352920, mean_q: -13.429975\n",
            "  67053/200000: episode: 159, duration: 1.862s, episode steps: 311, steps per second: 167, episode reward: -310.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.955 [0.000, 2.000],  loss: 0.419722, mae: 9.297432, mean_q: -13.370470\n",
            "  67404/200000: episode: 160, duration: 2.557s, episode steps: 351, steps per second: 137, episode reward: -350.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.051 [0.000, 2.000],  loss: 0.425262, mae: 9.283097, mean_q: -13.360890\n",
            "  67686/200000: episode: 161, duration: 2.313s, episode steps: 282, steps per second: 122, episode reward: -281.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.961 [0.000, 2.000],  loss: 0.393438, mae: 9.298841, mean_q: -13.390728\n",
            "  68038/200000: episode: 162, duration: 2.176s, episode steps: 352, steps per second: 162, episode reward: -351.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.037 [0.000, 2.000],  loss: 0.414980, mae: 9.362845, mean_q: -13.472569\n",
            "  68371/200000: episode: 163, duration: 2.020s, episode steps: 333, steps per second: 165, episode reward: -332.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.051 [0.000, 2.000],  loss: 0.476374, mae: 9.407814, mean_q: -13.522627\n",
            "  68835/200000: episode: 164, duration: 2.992s, episode steps: 464, steps per second: 155, episode reward: -463.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.041 [0.000, 2.000],  loss: 0.444939, mae: 9.359470, mean_q: -13.460696\n",
            "  69178/200000: episode: 165, duration: 2.047s, episode steps: 343, steps per second: 168, episode reward: -342.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.965 [0.000, 2.000],  loss: 0.434069, mae: 9.342999, mean_q: -13.432476\n",
            "  69613/200000: episode: 166, duration: 3.712s, episode steps: 435, steps per second: 117, episode reward: -434.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.975 [0.000, 2.000],  loss: 0.412093, mae: 9.404330, mean_q: -13.528284\n",
            "  69974/200000: episode: 167, duration: 2.284s, episode steps: 361, steps per second: 158, episode reward: -360.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.053 [0.000, 2.000],  loss: 0.490091, mae: 9.405684, mean_q: -13.509735\n",
            "  70449/200000: episode: 168, duration: 2.867s, episode steps: 475, steps per second: 166, episode reward: -474.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.962 [0.000, 2.000],  loss: 0.418836, mae: 9.387802, mean_q: -13.499610\n",
            "  70739/200000: episode: 169, duration: 1.653s, episode steps: 290, steps per second: 175, episode reward: -289.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.986 [0.000, 2.000],  loss: 0.435325, mae: 9.393690, mean_q: -13.515727\n",
            "  71098/200000: episode: 170, duration: 2.228s, episode steps: 359, steps per second: 161, episode reward: -358.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.975 [0.000, 2.000],  loss: 0.432282, mae: 9.398493, mean_q: -13.507329\n",
            "  71598/200000: episode: 171, duration: 3.994s, episode steps: 500, steps per second: 125, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.994 [0.000, 2.000],  loss: 0.457872, mae: 9.488920, mean_q: -13.629609\n",
            "  72098/200000: episode: 172, duration: 3.341s, episode steps: 500, steps per second: 150, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.402145, mae: 9.437917, mean_q: -13.577478\n",
            "  72469/200000: episode: 173, duration: 2.388s, episode steps: 371, steps per second: 155, episode reward: -370.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.439264, mae: 9.428642, mean_q: -13.559601\n",
            "  72969/200000: episode: 174, duration: 3.100s, episode steps: 500, steps per second: 161, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.964 [0.000, 2.000],  loss: 0.440628, mae: 9.436868, mean_q: -13.572871\n",
            "  73270/200000: episode: 175, duration: 1.991s, episode steps: 301, steps per second: 151, episode reward: -300.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.056 [0.000, 2.000],  loss: 0.467010, mae: 9.386888, mean_q: -13.499741\n",
            "  73615/200000: episode: 176, duration: 3.147s, episode steps: 345, steps per second: 110, episode reward: -344.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.988 [0.000, 2.000],  loss: 0.423032, mae: 9.420340, mean_q: -13.555895\n",
            "  73785/200000: episode: 177, duration: 1.087s, episode steps: 170, steps per second: 156, episode reward: -169.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.876 [0.000, 2.000],  loss: 0.423798, mae: 9.405312, mean_q: -13.540517\n",
            "  74219/200000: episode: 178, duration: 2.739s, episode steps: 434, steps per second: 158, episode reward: -433.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.979 [0.000, 2.000],  loss: 0.441161, mae: 9.513057, mean_q: -13.687034\n",
            "  74719/200000: episode: 179, duration: 3.170s, episode steps: 500, steps per second: 158, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.012 [0.000, 2.000],  loss: 0.466102, mae: 9.558089, mean_q: -13.763044\n",
            "  74947/200000: episode: 180, duration: 1.428s, episode steps: 228, steps per second: 160, episode reward: -227.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.965 [0.000, 2.000],  loss: 0.371837, mae: 9.524531, mean_q: -13.728973\n",
            "  75268/200000: episode: 181, duration: 2.269s, episode steps: 321, steps per second: 141, episode reward: -320.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.969 [0.000, 2.000],  loss: 0.445040, mae: 9.605867, mean_q: -13.821490\n",
            "  75626/200000: episode: 182, duration: 3.926s, episode steps: 358, steps per second:  91, episode reward: -357.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.978 [0.000, 2.000],  loss: 0.456466, mae: 9.632355, mean_q: -13.852826\n",
            "  76126/200000: episode: 183, duration: 3.887s, episode steps: 500, steps per second: 129, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.982 [0.000, 2.000],  loss: 0.460985, mae: 9.646968, mean_q: -13.870296\n",
            "  76400/200000: episode: 184, duration: 1.806s, episode steps: 274, steps per second: 152, episode reward: -273.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.894 [0.000, 2.000],  loss: 0.492178, mae: 9.772352, mean_q: -14.054817\n",
            "  76759/200000: episode: 185, duration: 2.329s, episode steps: 359, steps per second: 154, episode reward: -358.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.019 [0.000, 2.000],  loss: 0.446297, mae: 9.719354, mean_q: -13.980569\n",
            "  77118/200000: episode: 186, duration: 2.352s, episode steps: 359, steps per second: 153, episode reward: -358.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.039 [0.000, 2.000],  loss: 0.468008, mae: 9.751823, mean_q: -13.999317\n",
            "  77357/200000: episode: 187, duration: 2.286s, episode steps: 239, steps per second: 105, episode reward: -238.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.033 [0.000, 2.000],  loss: 0.479147, mae: 9.650549, mean_q: -13.883774\n",
            "  77694/200000: episode: 188, duration: 2.704s, episode steps: 337, steps per second: 125, episode reward: -336.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.896 [0.000, 2.000],  loss: 0.489897, mae: 9.664633, mean_q: -13.892512\n",
            "  78026/200000: episode: 189, duration: 2.411s, episode steps: 332, steps per second: 138, episode reward: -331.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.883 [0.000, 2.000],  loss: 0.471665, mae: 9.623893, mean_q: -13.837316\n",
            "  78526/200000: episode: 190, duration: 3.533s, episode steps: 500, steps per second: 142, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.460726, mae: 9.637856, mean_q: -13.851139\n",
            "  78955/200000: episode: 191, duration: 2.818s, episode steps: 429, steps per second: 152, episode reward: -428.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.021 [0.000, 2.000],  loss: 0.463995, mae: 9.673461, mean_q: -13.919664\n",
            "  79276/200000: episode: 192, duration: 3.088s, episode steps: 321, steps per second: 104, episode reward: -320.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.981 [0.000, 2.000],  loss: 0.497041, mae: 9.680815, mean_q: -13.927994\n",
            "  79606/200000: episode: 193, duration: 2.273s, episode steps: 330, steps per second: 145, episode reward: -329.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.936 [0.000, 2.000],  loss: 0.487099, mae: 9.681642, mean_q: -13.941384\n",
            "  79978/200000: episode: 194, duration: 2.399s, episode steps: 372, steps per second: 155, episode reward: -371.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.946 [0.000, 2.000],  loss: 0.483890, mae: 9.725037, mean_q: -13.978052\n",
            "  80431/200000: episode: 195, duration: 3.047s, episode steps: 453, steps per second: 149, episode reward: -452.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.892 [0.000, 2.000],  loss: 0.453935, mae: 9.736616, mean_q: -14.021023\n",
            "  80746/200000: episode: 196, duration: 2.165s, episode steps: 315, steps per second: 146, episode reward: -314.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.054 [0.000, 2.000],  loss: 0.457440, mae: 9.746487, mean_q: -14.024284\n",
            "  81246/200000: episode: 197, duration: 4.598s, episode steps: 500, steps per second: 109, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.044 [0.000, 2.000],  loss: 0.488816, mae: 9.749915, mean_q: -14.014885\n",
            "  81641/200000: episode: 198, duration: 2.775s, episode steps: 395, steps per second: 142, episode reward: -394.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.987 [0.000, 2.000],  loss: 0.466715, mae: 9.746014, mean_q: -14.046845\n",
            "  81939/200000: episode: 199, duration: 2.086s, episode steps: 298, steps per second: 143, episode reward: -297.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.114 [0.000, 2.000],  loss: 0.462360, mae: 9.740905, mean_q: -14.038004\n",
            "  82439/200000: episode: 200, duration: 3.435s, episode steps: 500, steps per second: 146, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.054 [0.000, 2.000],  loss: 0.439179, mae: 9.860848, mean_q: -14.207932\n",
            "  82685/200000: episode: 201, duration: 1.935s, episode steps: 246, steps per second: 127, episode reward: -245.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.061 [0.000, 2.000],  loss: 0.429485, mae: 9.874029, mean_q: -14.227991\n",
            "  83185/200000: episode: 202, duration: 4.505s, episode steps: 500, steps per second: 111, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.118 [0.000, 2.000],  loss: 0.445060, mae: 9.883077, mean_q: -14.239739\n",
            "  83452/200000: episode: 203, duration: 1.839s, episode steps: 267, steps per second: 145, episode reward: -266.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.019 [0.000, 2.000],  loss: 0.498752, mae: 9.897005, mean_q: -14.240071\n",
            "  83728/200000: episode: 204, duration: 1.818s, episode steps: 276, steps per second: 152, episode reward: -275.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.127 [0.000, 2.000],  loss: 0.464411, mae: 9.917253, mean_q: -14.281013\n",
            "  84008/200000: episode: 205, duration: 1.895s, episode steps: 280, steps per second: 148, episode reward: -279.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.064 [0.000, 2.000],  loss: 0.483237, mae: 9.924166, mean_q: -14.281313\n",
            "  84409/200000: episode: 206, duration: 2.603s, episode steps: 401, steps per second: 154, episode reward: -400.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.065 [0.000, 2.000],  loss: 0.476458, mae: 9.992376, mean_q: -14.381791\n",
            "  84820/200000: episode: 207, duration: 3.711s, episode steps: 411, steps per second: 111, episode reward: -410.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.973 [0.000, 2.000],  loss: 0.436888, mae: 10.025881, mean_q: -14.455208\n",
            "  85134/200000: episode: 208, duration: 2.021s, episode steps: 314, steps per second: 155, episode reward: -313.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.048 [0.000, 2.000],  loss: 0.507650, mae: 10.042836, mean_q: -14.452110\n",
            "  85277/200000: episode: 209, duration: 1.008s, episode steps: 143, steps per second: 142, episode reward: -142.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.993 [0.000, 2.000],  loss: 0.537362, mae: 10.130417, mean_q: -14.592736\n",
            "  85692/200000: episode: 210, duration: 2.700s, episode steps: 415, steps per second: 154, episode reward: -414.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.055 [0.000, 2.000],  loss: 0.478235, mae: 10.077242, mean_q: -14.523103\n",
            "  86092/200000: episode: 211, duration: 2.798s, episode steps: 400, steps per second: 143, episode reward: -399.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.073 [0.000, 2.000],  loss: 0.488919, mae: 10.059731, mean_q: -14.480450\n",
            "  86423/200000: episode: 212, duration: 2.633s, episode steps: 331, steps per second: 126, episode reward: -330.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.891 [0.000, 2.000],  loss: 0.478352, mae: 10.013527, mean_q: -14.431030\n",
            "  86923/200000: episode: 213, duration: 3.872s, episode steps: 500, steps per second: 129, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.066 [0.000, 2.000],  loss: 0.483579, mae: 10.050838, mean_q: -14.476800\n",
            "  87423/200000: episode: 214, duration: 3.205s, episode steps: 500, steps per second: 156, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.515324, mae: 10.027635, mean_q: -14.451384\n",
            "  87923/200000: episode: 215, duration: 3.324s, episode steps: 500, steps per second: 150, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.018 [0.000, 2.000],  loss: 0.480133, mae: 10.060201, mean_q: -14.502306\n",
            "  88233/200000: episode: 216, duration: 2.316s, episode steps: 310, steps per second: 134, episode reward: -309.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.032 [0.000, 2.000],  loss: 0.523239, mae: 10.075481, mean_q: -14.521316\n",
            "  88532/200000: episode: 217, duration: 2.820s, episode steps: 299, steps per second: 106, episode reward: -298.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.040 [0.000, 2.000],  loss: 0.558188, mae: 10.192834, mean_q: -14.674401\n",
            "  88962/200000: episode: 218, duration: 2.855s, episode steps: 430, steps per second: 151, episode reward: -429.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.930 [0.000, 2.000],  loss: 0.527421, mae: 10.177091, mean_q: -14.659903\n",
            "  89185/200000: episode: 219, duration: 1.436s, episode steps: 223, steps per second: 155, episode reward: -222.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.004 [0.000, 2.000],  loss: 0.499038, mae: 10.209792, mean_q: -14.664696\n",
            "  89549/200000: episode: 220, duration: 2.560s, episode steps: 364, steps per second: 142, episode reward: -363.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.052 [0.000, 2.000],  loss: 0.501066, mae: 10.183331, mean_q: -14.645107\n",
            "  90015/200000: episode: 221, duration: 3.322s, episode steps: 466, steps per second: 140, episode reward: -465.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.946 [0.000, 2.000],  loss: 0.501912, mae: 10.172569, mean_q: -14.654388\n",
            "  90455/200000: episode: 222, duration: 4.060s, episode steps: 440, steps per second: 108, episode reward: -439.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.964 [0.000, 2.000],  loss: 0.483686, mae: 10.249530, mean_q: -14.756910\n",
            "  90915/200000: episode: 223, duration: 3.106s, episode steps: 460, steps per second: 148, episode reward: -459.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.028 [0.000, 2.000],  loss: 0.478550, mae: 10.230844, mean_q: -14.731690\n",
            "  91189/200000: episode: 224, duration: 1.886s, episode steps: 274, steps per second: 145, episode reward: -273.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.883 [0.000, 2.000],  loss: 0.476250, mae: 10.161480, mean_q: -14.638795\n",
            "  91553/200000: episode: 225, duration: 2.490s, episode steps: 364, steps per second: 146, episode reward: -363.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.080 [0.000, 2.000],  loss: 0.469998, mae: 10.201436, mean_q: -14.698423\n",
            "  91717/200000: episode: 226, duration: 1.101s, episode steps: 164, steps per second: 149, episode reward: -163.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.890 [0.000, 2.000],  loss: 0.542685, mae: 10.180979, mean_q: -14.648095\n",
            "  92018/200000: episode: 227, duration: 2.599s, episode steps: 301, steps per second: 116, episode reward: -300.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.887 [0.000, 2.000],  loss: 0.443744, mae: 10.147164, mean_q: -14.614219\n",
            "  92426/200000: episode: 228, duration: 3.105s, episode steps: 408, steps per second: 131, episode reward: -407.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.936 [0.000, 2.000],  loss: 0.488483, mae: 10.221420, mean_q: -14.733411\n",
            "  92757/200000: episode: 229, duration: 2.294s, episode steps: 331, steps per second: 144, episode reward: -330.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.094 [0.000, 2.000],  loss: 0.497625, mae: 10.245623, mean_q: -14.775633\n",
            "  93230/200000: episode: 230, duration: 3.228s, episode steps: 473, steps per second: 147, episode reward: -472.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.097 [0.000, 2.000],  loss: 0.474437, mae: 10.212933, mean_q: -14.714324\n",
            "  93639/200000: episode: 231, duration: 3.053s, episode steps: 409, steps per second: 134, episode reward: -408.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.980 [0.000, 2.000],  loss: 0.539044, mae: 10.180855, mean_q: -14.654158\n",
            "  93886/200000: episode: 232, duration: 2.366s, episode steps: 247, steps per second: 104, episode reward: -246.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.992 [0.000, 2.000],  loss: 0.528221, mae: 10.198115, mean_q: -14.690474\n",
            "  94272/200000: episode: 233, duration: 3.157s, episode steps: 386, steps per second: 122, episode reward: -385.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.049 [0.000, 2.000],  loss: 0.499505, mae: 10.246268, mean_q: -14.775776\n",
            "  94637/200000: episode: 234, duration: 2.770s, episode steps: 365, steps per second: 132, episode reward: -364.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.945 [0.000, 2.000],  loss: 0.511633, mae: 10.259199, mean_q: -14.793552\n",
            "  95059/200000: episode: 235, duration: 3.102s, episode steps: 422, steps per second: 136, episode reward: -421.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.979 [0.000, 2.000],  loss: 0.490117, mae: 10.207488, mean_q: -14.708213\n",
            "  95467/200000: episode: 236, duration: 3.284s, episode steps: 408, steps per second: 124, episode reward: -407.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.037 [0.000, 2.000],  loss: 0.465858, mae: 10.148695, mean_q: -14.622708\n",
            "  95967/200000: episode: 237, duration: 3.824s, episode steps: 500, steps per second: 131, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.028 [0.000, 2.000],  loss: 0.536933, mae: 10.149501, mean_q: -14.615767\n",
            "  96400/200000: episode: 238, duration: 2.875s, episode steps: 433, steps per second: 151, episode reward: -432.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.002 [0.000, 2.000],  loss: 0.520249, mae: 10.173694, mean_q: -14.653309\n",
            "  96842/200000: episode: 239, duration: 2.944s, episode steps: 442, steps per second: 150, episode reward: -441.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.054 [0.000, 2.000],  loss: 0.464785, mae: 10.155925, mean_q: -14.640969\n",
            "  97183/200000: episode: 240, duration: 2.459s, episode steps: 341, steps per second: 139, episode reward: -340.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.938 [0.000, 2.000],  loss: 0.459717, mae: 10.197422, mean_q: -14.704799\n",
            "  97431/200000: episode: 241, duration: 2.323s, episode steps: 248, steps per second: 107, episode reward: -247.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.052 [0.000, 2.000],  loss: 0.477871, mae: 10.194152, mean_q: -14.709321\n",
            "  97799/200000: episode: 242, duration: 2.713s, episode steps: 368, steps per second: 136, episode reward: -367.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.008 [0.000, 2.000],  loss: 0.457414, mae: 10.243125, mean_q: -14.765740\n",
            "  98229/200000: episode: 243, duration: 3.157s, episode steps: 430, steps per second: 136, episode reward: -429.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.019 [0.000, 2.000],  loss: 0.473532, mae: 10.152905, mean_q: -14.617767\n",
            "  98729/200000: episode: 244, duration: 3.620s, episode steps: 500, steps per second: 138, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.046 [0.000, 2.000],  loss: 0.446214, mae: 10.114000, mean_q: -14.572014\n",
            "  99158/200000: episode: 245, duration: 3.825s, episode steps: 429, steps per second: 112, episode reward: -428.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.019 [0.000, 2.000],  loss: 0.478687, mae: 10.155100, mean_q: -14.634545\n",
            "  99446/200000: episode: 246, duration: 2.379s, episode steps: 288, steps per second: 121, episode reward: -287.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.910 [0.000, 2.000],  loss: 0.524155, mae: 10.099496, mean_q: -14.563850\n",
            "  99877/200000: episode: 247, duration: 3.039s, episode steps: 431, steps per second: 142, episode reward: -430.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.916 [0.000, 2.000],  loss: 0.492877, mae: 10.124206, mean_q: -14.599899\n",
            " 100377/200000: episode: 248, duration: 3.534s, episode steps: 500, steps per second: 142, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.054 [0.000, 2.000],  loss: 0.527561, mae: 10.159312, mean_q: -14.636280\n",
            " 100572/200000: episode: 249, duration: 1.363s, episode steps: 195, steps per second: 143, episode reward: -194.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.021 [0.000, 2.000],  loss: 0.489417, mae: 10.205251, mean_q: -14.734085\n",
            " 100775/200000: episode: 250, duration: 1.887s, episode steps: 203, steps per second: 108, episode reward: -202.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.064 [0.000, 2.000],  loss: 0.410602, mae: 10.171346, mean_q: -14.681345\n",
            " 101275/200000: episode: 251, duration: 4.199s, episode steps: 500, steps per second: 119, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.028 [0.000, 2.000],  loss: 0.459691, mae: 10.110392, mean_q: -14.582550\n",
            " 101497/200000: episode: 252, duration: 1.633s, episode steps: 222, steps per second: 136, episode reward: -221.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.023 [0.000, 2.000],  loss: 0.455091, mae: 10.139495, mean_q: -14.622622\n",
            " 101885/200000: episode: 253, duration: 2.722s, episode steps: 388, steps per second: 143, episode reward: -387.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.982 [0.000, 2.000],  loss: 0.471089, mae: 10.114107, mean_q: -14.579017\n",
            " 102206/200000: episode: 254, duration: 2.347s, episode steps: 321, steps per second: 137, episode reward: -320.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.022 [0.000, 2.000],  loss: 0.473887, mae: 10.149713, mean_q: -14.625945\n",
            " 102534/200000: episode: 255, duration: 3.114s, episode steps: 328, steps per second: 105, episode reward: -327.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.921 [0.000, 2.000],  loss: 0.481007, mae: 10.168960, mean_q: -14.672264\n",
            " 102802/200000: episode: 256, duration: 2.324s, episode steps: 268, steps per second: 115, episode reward: -267.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.063 [0.000, 2.000],  loss: 0.433601, mae: 10.209226, mean_q: -14.723304\n",
            " 103173/200000: episode: 257, duration: 2.595s, episode steps: 371, steps per second: 143, episode reward: -370.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.898 [0.000, 2.000],  loss: 0.455010, mae: 10.139546, mean_q: -14.634082\n",
            " 103624/200000: episode: 258, duration: 3.084s, episode steps: 451, steps per second: 146, episode reward: -450.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.009 [0.000, 2.000],  loss: 0.456196, mae: 10.129439, mean_q: -14.615107\n",
            " 104124/200000: episode: 259, duration: 3.502s, episode steps: 500, steps per second: 143, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.064 [0.000, 2.000],  loss: 0.467452, mae: 10.151005, mean_q: -14.641162\n",
            " 104620/200000: episode: 260, duration: 4.583s, episode steps: 496, steps per second: 108, episode reward: -495.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.069 [0.000, 2.000],  loss: 0.485334, mae: 10.081956, mean_q: -14.549892\n",
            " 105080/200000: episode: 261, duration: 3.369s, episode steps: 460, steps per second: 137, episode reward: -459.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.948 [0.000, 2.000],  loss: 0.478333, mae: 10.069896, mean_q: -14.526199\n",
            " 105426/200000: episode: 262, duration: 2.508s, episode steps: 346, steps per second: 138, episode reward: -345.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.520949, mae: 10.091487, mean_q: -14.553464\n",
            " 105926/200000: episode: 263, duration: 3.770s, episode steps: 500, steps per second: 133, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.022 [0.000, 2.000],  loss: 0.453536, mae: 10.040039, mean_q: -14.490974\n",
            " 106301/200000: episode: 264, duration: 3.452s, episode steps: 375, steps per second: 109, episode reward: -374.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.037 [0.000, 2.000],  loss: 0.497183, mae: 10.075581, mean_q: -14.527381\n",
            " 106715/200000: episode: 265, duration: 3.051s, episode steps: 414, steps per second: 136, episode reward: -413.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.995 [0.000, 2.000],  loss: 0.482619, mae: 10.050541, mean_q: -14.489198\n",
            " 107086/200000: episode: 266, duration: 2.741s, episode steps: 371, steps per second: 135, episode reward: -370.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.995 [0.000, 2.000],  loss: 0.394300, mae: 10.013084, mean_q: -14.449294\n",
            " 107363/200000: episode: 267, duration: 1.987s, episode steps: 277, steps per second: 139, episode reward: -276.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.946 [0.000, 2.000],  loss: 0.498854, mae: 10.105895, mean_q: -14.567252\n",
            " 107863/200000: episode: 268, duration: 4.610s, episode steps: 500, steps per second: 108, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.435847, mae: 10.023743, mean_q: -14.486175\n",
            " 108176/200000: episode: 269, duration: 2.467s, episode steps: 313, steps per second: 127, episode reward: -312.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.955 [0.000, 2.000],  loss: 0.460065, mae: 10.112662, mean_q: -14.598765\n",
            " 108578/200000: episode: 270, duration: 3.100s, episode steps: 402, steps per second: 130, episode reward: -401.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.042 [0.000, 2.000],  loss: 0.464590, mae: 10.137161, mean_q: -14.620542\n",
            " 108989/200000: episode: 271, duration: 4.162s, episode steps: 411, steps per second:  99, episode reward: -410.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.039 [0.000, 2.000],  loss: 0.468684, mae: 10.165606, mean_q: -14.674668\n",
            " 109371/200000: episode: 272, duration: 4.189s, episode steps: 382, steps per second:  91, episode reward: -381.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.102 [0.000, 2.000],  loss: 0.468989, mae: 10.129779, mean_q: -14.610518\n",
            " 109871/200000: episode: 273, duration: 3.757s, episode steps: 500, steps per second: 133, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.483711, mae: 10.130414, mean_q: -14.614121\n",
            " 110336/200000: episode: 274, duration: 3.393s, episode steps: 465, steps per second: 137, episode reward: -464.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.039 [0.000, 2.000],  loss: 0.472874, mae: 10.126304, mean_q: -14.605274\n",
            " 110606/200000: episode: 275, duration: 1.967s, episode steps: 270, steps per second: 137, episode reward: -269.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.044 [0.000, 2.000],  loss: 0.470121, mae: 10.117409, mean_q: -14.590475\n",
            " 111106/200000: episode: 276, duration: 4.489s, episode steps: 500, steps per second: 111, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.972 [0.000, 2.000],  loss: 0.464229, mae: 10.072184, mean_q: -14.534561\n",
            " 111478/200000: episode: 277, duration: 2.500s, episode steps: 372, steps per second: 149, episode reward: -371.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.075 [0.000, 2.000],  loss: 0.481249, mae: 10.091974, mean_q: -14.567901\n",
            " 111775/200000: episode: 278, duration: 2.020s, episode steps: 297, steps per second: 147, episode reward: -296.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.064 [0.000, 2.000],  loss: 0.433539, mae: 10.120434, mean_q: -14.603591\n",
            " 112000/200000: episode: 279, duration: 1.600s, episode steps: 225, steps per second: 141, episode reward: -224.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.991 [0.000, 2.000],  loss: 0.487046, mae: 10.077016, mean_q: -14.508699\n",
            " 112366/200000: episode: 280, duration: 2.612s, episode steps: 366, steps per second: 140, episode reward: -365.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.940 [0.000, 2.000],  loss: 0.442901, mae: 10.126612, mean_q: -14.614687\n",
            " 112755/200000: episode: 281, duration: 3.568s, episode steps: 389, steps per second: 109, episode reward: -388.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.964 [0.000, 2.000],  loss: 0.467901, mae: 10.143532, mean_q: -14.636939\n",
            " 113032/200000: episode: 282, duration: 2.224s, episode steps: 277, steps per second: 125, episode reward: -276.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.942 [0.000, 2.000],  loss: 0.502835, mae: 10.163371, mean_q: -14.647219\n",
            " 113392/200000: episode: 283, duration: 2.685s, episode steps: 360, steps per second: 134, episode reward: -359.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.919 [0.000, 2.000],  loss: 0.491096, mae: 10.217283, mean_q: -14.730279\n",
            " 113677/200000: episode: 284, duration: 2.242s, episode steps: 285, steps per second: 127, episode reward: -284.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.007 [0.000, 2.000],  loss: 0.468524, mae: 10.250570, mean_q: -14.787776\n",
            " 113891/200000: episode: 285, duration: 1.669s, episode steps: 214, steps per second: 128, episode reward: -213.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.977 [0.000, 2.000],  loss: 0.486974, mae: 10.157607, mean_q: -14.654568\n",
            " 114143/200000: episode: 286, duration: 1.992s, episode steps: 252, steps per second: 127, episode reward: -251.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.901 [0.000, 2.000],  loss: 0.440729, mae: 10.287859, mean_q: -14.863234\n",
            " 114471/200000: episode: 287, duration: 3.394s, episode steps: 328, steps per second:  97, episode reward: -327.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.954 [0.000, 2.000],  loss: 0.485275, mae: 10.273724, mean_q: -14.829800\n",
            " 114911/200000: episode: 288, duration: 3.316s, episode steps: 440, steps per second: 133, episode reward: -439.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.982 [0.000, 2.000],  loss: 0.429404, mae: 10.341286, mean_q: -14.938523\n",
            " 115239/200000: episode: 289, duration: 2.336s, episode steps: 328, steps per second: 140, episode reward: -327.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.009 [0.000, 2.000],  loss: 0.508156, mae: 10.351747, mean_q: -14.933887\n",
            " 115694/200000: episode: 290, duration: 3.290s, episode steps: 455, steps per second: 138, episode reward: -454.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.020 [0.000, 2.000],  loss: 0.472359, mae: 10.366740, mean_q: -14.956258\n",
            " 115912/200000: episode: 291, duration: 1.889s, episode steps: 218, steps per second: 115, episode reward: -217.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.050 [0.000, 2.000],  loss: 0.489731, mae: 10.400732, mean_q: -15.015007\n",
            " 116194/200000: episode: 292, duration: 2.805s, episode steps: 282, steps per second: 101, episode reward: -281.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.469128, mae: 10.361154, mean_q: -14.971005\n",
            " 116646/200000: episode: 293, duration: 3.337s, episode steps: 452, steps per second: 135, episode reward: -451.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.027 [0.000, 2.000],  loss: 0.483748, mae: 10.370071, mean_q: -14.979279\n",
            " 117121/200000: episode: 294, duration: 3.334s, episode steps: 475, steps per second: 142, episode reward: -474.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.949 [0.000, 2.000],  loss: 0.505190, mae: 10.397935, mean_q: -15.010869\n",
            " 117395/200000: episode: 295, duration: 1.908s, episode steps: 274, steps per second: 144, episode reward: -273.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.066 [0.000, 2.000],  loss: 0.488737, mae: 10.449935, mean_q: -15.093884\n",
            " 117825/200000: episode: 296, duration: 4.053s, episode steps: 430, steps per second: 106, episode reward: -429.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.079 [0.000, 2.000],  loss: 0.436924, mae: 10.433198, mean_q: -15.071629\n",
            " 118293/200000: episode: 297, duration: 3.456s, episode steps: 468, steps per second: 135, episode reward: -467.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.019 [0.000, 2.000],  loss: 0.465587, mae: 10.435834, mean_q: -15.066835\n",
            " 118793/200000: episode: 298, duration: 3.512s, episode steps: 500, steps per second: 142, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.056 [0.000, 2.000],  loss: 0.487513, mae: 10.448967, mean_q: -15.078266\n",
            " 119000/200000: episode: 299, duration: 1.492s, episode steps: 207, steps per second: 139, episode reward: -206.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.135 [0.000, 2.000],  loss: 0.484068, mae: 10.523156, mean_q: -15.184645\n",
            " 119500/200000: episode: 300, duration: 4.369s, episode steps: 500, steps per second: 114, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.052 [0.000, 2.000],  loss: 0.460769, mae: 10.378785, mean_q: -14.978021\n",
            " 119928/200000: episode: 301, duration: 3.529s, episode steps: 428, steps per second: 121, episode reward: -427.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.026 [0.000, 2.000],  loss: 0.441614, mae: 10.375810, mean_q: -14.981436\n",
            " 120428/200000: episode: 302, duration: 3.667s, episode steps: 500, steps per second: 136, episode reward: -499.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.124 [0.000, 2.000],  loss: 0.511207, mae: 10.443755, mean_q: -15.073307\n",
            " 120854/200000: episode: 303, duration: 2.936s, episode steps: 426, steps per second: 145, episode reward: -425.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.033 [0.000, 2.000],  loss: 0.516292, mae: 10.363137, mean_q: -14.953053\n",
            " 121140/200000: episode: 304, duration: 2.526s, episode steps: 286, steps per second: 113, episode reward: -285.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.045 [0.000, 2.000],  loss: 0.450332, mae: 10.356986, mean_q: -14.969008\n",
            " 121640/200000: episode: 305, duration: 3.962s, episode steps: 500, steps per second: 126, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.954 [0.000, 2.000],  loss: 0.478314, mae: 10.336386, mean_q: -14.927983\n",
            " 121871/200000: episode: 306, duration: 1.668s, episode steps: 231, steps per second: 138, episode reward: -230.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.944 [0.000, 2.000],  loss: 0.516068, mae: 10.378236, mean_q: -14.960651\n",
            " 122304/200000: episode: 307, duration: 3.446s, episode steps: 433, steps per second: 126, episode reward: -432.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.991 [0.000, 2.000],  loss: 0.461152, mae: 10.416717, mean_q: -15.041627\n",
            " 122719/200000: episode: 308, duration: 3.320s, episode steps: 415, steps per second: 125, episode reward: -414.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.986 [0.000, 2.000],  loss: 0.472851, mae: 10.394975, mean_q: -15.005650\n",
            " 123219/200000: episode: 309, duration: 4.463s, episode steps: 500, steps per second: 112, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.002 [0.000, 2.000],  loss: 0.519924, mae: 10.447026, mean_q: -15.063535\n",
            " 123424/200000: episode: 310, duration: 1.589s, episode steps: 205, steps per second: 129, episode reward: -204.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.039 [0.000, 2.000],  loss: 0.496756, mae: 10.482766, mean_q: -15.149666\n",
            " 123736/200000: episode: 311, duration: 2.261s, episode steps: 312, steps per second: 138, episode reward: -311.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.035 [0.000, 2.000],  loss: 0.464966, mae: 10.432704, mean_q: -15.066366\n",
            " 124079/200000: episode: 312, duration: 2.574s, episode steps: 343, steps per second: 133, episode reward: -342.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.980 [0.000, 2.000],  loss: 0.485769, mae: 10.366697, mean_q: -14.972891\n",
            " 124350/200000: episode: 313, duration: 2.035s, episode steps: 271, steps per second: 133, episode reward: -270.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.055 [0.000, 2.000],  loss: 0.525585, mae: 10.447394, mean_q: -15.063198\n",
            " 124666/200000: episode: 314, duration: 3.149s, episode steps: 316, steps per second: 100, episode reward: -315.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.079 [0.000, 2.000],  loss: 0.443925, mae: 10.406476, mean_q: -15.044671\n",
            " 124938/200000: episode: 315, duration: 2.010s, episode steps: 272, steps per second: 135, episode reward: -271.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.026 [0.000, 2.000],  loss: 0.442027, mae: 10.434001, mean_q: -15.078411\n",
            " 125266/200000: episode: 316, duration: 2.447s, episode steps: 328, steps per second: 134, episode reward: -327.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.997 [0.000, 2.000],  loss: 0.521779, mae: 10.454312, mean_q: -15.086723\n",
            " 125766/200000: episode: 317, duration: 3.514s, episode steps: 500, steps per second: 142, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.036 [0.000, 2.000],  loss: 0.485138, mae: 10.433408, mean_q: -15.059351\n",
            " 126071/200000: episode: 318, duration: 2.237s, episode steps: 305, steps per second: 136, episode reward: -304.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.438014, mae: 10.503276, mean_q: -15.190110\n",
            " 126360/200000: episode: 319, duration: 3.010s, episode steps: 289, steps per second:  96, episode reward: -288.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.979 [0.000, 2.000],  loss: 0.456135, mae: 10.549332, mean_q: -15.231771\n",
            " 126671/200000: episode: 320, duration: 2.197s, episode steps: 311, steps per second: 142, episode reward: -310.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.023 [0.000, 2.000],  loss: 0.485844, mae: 10.506512, mean_q: -15.176261\n",
            " 127061/200000: episode: 321, duration: 2.777s, episode steps: 390, steps per second: 140, episode reward: -389.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.972 [0.000, 2.000],  loss: 0.441595, mae: 10.553339, mean_q: -15.269119\n",
            " 127522/200000: episode: 322, duration: 3.450s, episode steps: 461, steps per second: 134, episode reward: -460.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.952 [0.000, 2.000],  loss: 0.489056, mae: 10.676777, mean_q: -15.430613\n",
            " 127821/200000: episode: 323, duration: 2.379s, episode steps: 299, steps per second: 126, episode reward: -298.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.970 [0.000, 2.000],  loss: 0.504349, mae: 10.617811, mean_q: -15.352663\n",
            " 128007/200000: episode: 324, duration: 2.048s, episode steps: 186, steps per second:  91, episode reward: -185.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.016 [0.000, 2.000],  loss: 0.474932, mae: 10.590553, mean_q: -15.303139\n",
            " 128321/200000: episode: 325, duration: 2.494s, episode steps: 314, steps per second: 126, episode reward: -313.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.003 [0.000, 2.000],  loss: 0.537897, mae: 10.704707, mean_q: -15.468691\n",
            " 128747/200000: episode: 326, duration: 3.023s, episode steps: 426, steps per second: 141, episode reward: -425.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.505346, mae: 10.743328, mean_q: -15.512638\n",
            " 129153/200000: episode: 327, duration: 2.785s, episode steps: 406, steps per second: 146, episode reward: -405.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.012 [0.000, 2.000],  loss: 0.464697, mae: 10.693894, mean_q: -15.460324\n",
            " 129484/200000: episode: 328, duration: 2.334s, episode steps: 331, steps per second: 142, episode reward: -330.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.997 [0.000, 2.000],  loss: 0.517192, mae: 10.658830, mean_q: -15.383557\n",
            " 129827/200000: episode: 329, duration: 3.425s, episode steps: 343, steps per second: 100, episode reward: -342.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.038 [0.000, 2.000],  loss: 0.469984, mae: 10.683079, mean_q: -15.437024\n",
            " 130200/200000: episode: 330, duration: 2.692s, episode steps: 373, steps per second: 139, episode reward: -372.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.901 [0.000, 2.000],  loss: 0.512148, mae: 10.741486, mean_q: -15.508152\n",
            " 130482/200000: episode: 331, duration: 2.108s, episode steps: 282, steps per second: 134, episode reward: -281.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.050 [0.000, 2.000],  loss: 0.453534, mae: 10.720705, mean_q: -15.483511\n",
            " 130826/200000: episode: 332, duration: 2.614s, episode steps: 344, steps per second: 132, episode reward: -343.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.052 [0.000, 2.000],  loss: 0.553579, mae: 10.769177, mean_q: -15.548555\n",
            " 131052/200000: episode: 333, duration: 1.693s, episode steps: 226, steps per second: 133, episode reward: -225.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.934 [0.000, 2.000],  loss: 0.497034, mae: 10.774612, mean_q: -15.570449\n",
            " 131335/200000: episode: 334, duration: 2.622s, episode steps: 283, steps per second: 108, episode reward: -282.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.434218, mae: 10.774520, mean_q: -15.588380\n",
            " 131835/200000: episode: 335, duration: 4.099s, episode steps: 500, steps per second: 122, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.962 [0.000, 2.000],  loss: 0.494798, mae: 10.710142, mean_q: -15.473246\n",
            " 132262/200000: episode: 336, duration: 3.162s, episode steps: 427, steps per second: 135, episode reward: -426.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.012 [0.000, 2.000],  loss: 0.525013, mae: 10.798274, mean_q: -15.602818\n",
            " 132694/200000: episode: 337, duration: 3.156s, episode steps: 432, steps per second: 137, episode reward: -431.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.025 [0.000, 2.000],  loss: 0.506859, mae: 10.896449, mean_q: -15.758405\n",
            " 132982/200000: episode: 338, duration: 2.501s, episode steps: 288, steps per second: 115, episode reward: -287.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.049 [0.000, 2.000],  loss: 0.471144, mae: 10.849580, mean_q: -15.695791\n",
            " 133455/200000: episode: 339, duration: 3.813s, episode steps: 473, steps per second: 124, episode reward: -472.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.977 [0.000, 2.000],  loss: 0.536505, mae: 10.913802, mean_q: -15.773150\n",
            " 133901/200000: episode: 340, duration: 3.137s, episode steps: 446, steps per second: 142, episode reward: -445.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.031 [0.000, 2.000],  loss: 0.516569, mae: 10.924612, mean_q: -15.789949\n",
            " 134231/200000: episode: 341, duration: 2.346s, episode steps: 330, steps per second: 141, episode reward: -329.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.088 [0.000, 2.000],  loss: 0.560582, mae: 10.951775, mean_q: -15.815088\n",
            " 134731/200000: episode: 342, duration: 3.912s, episode steps: 500, steps per second: 128, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.552684, mae: 10.946953, mean_q: -15.814637\n",
            " 135038/200000: episode: 343, duration: 2.970s, episode steps: 307, steps per second: 103, episode reward: -306.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.928 [0.000, 2.000],  loss: 0.527503, mae: 10.980173, mean_q: -15.880081\n",
            " 135475/200000: episode: 344, duration: 3.219s, episode steps: 437, steps per second: 136, episode reward: -436.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.966 [0.000, 2.000],  loss: 0.482723, mae: 11.027585, mean_q: -15.965965\n",
            " 135722/200000: episode: 345, duration: 1.885s, episode steps: 247, steps per second: 131, episode reward: -246.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.073 [0.000, 2.000],  loss: 0.460621, mae: 10.976635, mean_q: -15.876115\n",
            " 136167/200000: episode: 346, duration: 3.227s, episode steps: 445, steps per second: 138, episode reward: -444.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.025 [0.000, 2.000],  loss: 0.516811, mae: 10.975865, mean_q: -15.854913\n",
            " 136436/200000: episode: 347, duration: 2.568s, episode steps: 269, steps per second: 105, episode reward: -268.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.007 [0.000, 2.000],  loss: 0.533881, mae: 11.028922, mean_q: -15.914740\n",
            " 136851/200000: episode: 348, duration: 3.368s, episode steps: 415, steps per second: 123, episode reward: -414.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.043 [0.000, 2.000],  loss: 0.511215, mae: 10.985506, mean_q: -15.884640\n",
            " 137271/200000: episode: 349, duration: 2.846s, episode steps: 420, steps per second: 148, episode reward: -419.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.121 [0.000, 2.000],  loss: 0.515624, mae: 10.896958, mean_q: -15.727957\n",
            " 137661/200000: episode: 350, duration: 2.689s, episode steps: 390, steps per second: 145, episode reward: -389.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.059 [0.000, 2.000],  loss: 0.492499, mae: 10.964965, mean_q: -15.847547\n",
            " 137997/200000: episode: 351, duration: 2.257s, episode steps: 336, steps per second: 149, episode reward: -335.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.932 [0.000, 2.000],  loss: 0.493581, mae: 11.023509, mean_q: -15.932658\n",
            " 138349/200000: episode: 352, duration: 3.167s, episode steps: 352, steps per second: 111, episode reward: -351.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.023 [0.000, 2.000],  loss: 0.525919, mae: 11.029284, mean_q: -15.942082\n",
            " 138849/200000: episode: 353, duration: 3.745s, episode steps: 500, steps per second: 134, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.052 [0.000, 2.000],  loss: 0.508528, mae: 11.040395, mean_q: -15.966072\n",
            " 139284/200000: episode: 354, duration: 3.053s, episode steps: 435, steps per second: 142, episode reward: -434.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.032 [0.000, 2.000],  loss: 0.500470, mae: 11.049357, mean_q: -15.983287\n",
            " 139552/200000: episode: 355, duration: 2.130s, episode steps: 268, steps per second: 126, episode reward: -267.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.940 [0.000, 2.000],  loss: 0.499392, mae: 11.101972, mean_q: -16.053263\n",
            " 139892/200000: episode: 356, duration: 2.746s, episode steps: 340, steps per second: 124, episode reward: -339.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.974 [0.000, 2.000],  loss: 0.482618, mae: 11.050124, mean_q: -15.988244\n",
            " 140180/200000: episode: 357, duration: 2.945s, episode steps: 288, steps per second:  98, episode reward: -287.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.024 [0.000, 2.000],  loss: 0.517738, mae: 11.035257, mean_q: -15.952033\n",
            " 140554/200000: episode: 358, duration: 3.522s, episode steps: 374, steps per second: 106, episode reward: -373.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.003 [0.000, 2.000],  loss: 0.543400, mae: 11.017593, mean_q: -15.932345\n",
            " 140806/200000: episode: 359, duration: 2.066s, episode steps: 252, steps per second: 122, episode reward: -251.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.111 [0.000, 2.000],  loss: 0.504863, mae: 11.004489, mean_q: -15.919879\n",
            " 141306/200000: episode: 360, duration: 3.598s, episode steps: 500, steps per second: 139, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.984 [0.000, 2.000],  loss: 0.498091, mae: 11.005610, mean_q: -15.922592\n",
            " 141568/200000: episode: 361, duration: 2.445s, episode steps: 262, steps per second: 107, episode reward: -261.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.950 [0.000, 2.000],  loss: 0.506379, mae: 10.971368, mean_q: -15.878427\n",
            " 142005/200000: episode: 362, duration: 3.332s, episode steps: 437, steps per second: 131, episode reward: -436.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.979 [0.000, 2.000],  loss: 0.523176, mae: 11.012794, mean_q: -15.919107\n",
            " 142329/200000: episode: 363, duration: 2.233s, episode steps: 324, steps per second: 145, episode reward: -323.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.935 [0.000, 2.000],  loss: 0.555601, mae: 10.991006, mean_q: -15.872542\n",
            " 142625/200000: episode: 364, duration: 2.072s, episode steps: 296, steps per second: 143, episode reward: -295.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.959 [0.000, 2.000],  loss: 0.533054, mae: 11.081585, mean_q: -16.024428\n",
            " 143125/200000: episode: 365, duration: 3.305s, episode steps: 500, steps per second: 151, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.016 [0.000, 2.000],  loss: 0.529758, mae: 11.119292, mean_q: -16.079720\n",
            " 143374/200000: episode: 366, duration: 2.327s, episode steps: 249, steps per second: 107, episode reward: -248.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.056 [0.000, 2.000],  loss: 0.506779, mae: 11.150865, mean_q: -16.129066\n",
            " 143729/200000: episode: 367, duration: 2.801s, episode steps: 355, steps per second: 127, episode reward: -354.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.997 [0.000, 2.000],  loss: 0.520353, mae: 11.126523, mean_q: -16.082265\n",
            " 144047/200000: episode: 368, duration: 2.284s, episode steps: 318, steps per second: 139, episode reward: -317.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.138 [0.000, 2.000],  loss: 0.482927, mae: 11.115729, mean_q: -16.091793\n",
            " 144401/200000: episode: 369, duration: 2.526s, episode steps: 354, steps per second: 140, episode reward: -353.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.124 [0.000, 2.000],  loss: 0.557469, mae: 11.067863, mean_q: -16.002685\n",
            " 144697/200000: episode: 370, duration: 2.160s, episode steps: 296, steps per second: 137, episode reward: -295.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.034 [0.000, 2.000],  loss: 0.565391, mae: 11.104301, mean_q: -16.045108\n",
            " 145036/200000: episode: 371, duration: 2.856s, episode steps: 339, steps per second: 119, episode reward: -338.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.944 [0.000, 2.000],  loss: 0.526997, mae: 11.132931, mean_q: -16.111799\n",
            " 145368/200000: episode: 372, duration: 3.043s, episode steps: 332, steps per second: 109, episode reward: -331.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.054 [0.000, 2.000],  loss: 0.613212, mae: 11.289036, mean_q: -16.316336\n",
            " 145729/200000: episode: 373, duration: 2.708s, episode steps: 361, steps per second: 133, episode reward: -360.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.036 [0.000, 2.000],  loss: 0.556395, mae: 11.285490, mean_q: -16.320761\n",
            " 146064/200000: episode: 374, duration: 2.400s, episode steps: 335, steps per second: 140, episode reward: -334.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.030 [0.000, 2.000],  loss: 0.505294, mae: 11.255760, mean_q: -16.276896\n",
            " 146418/200000: episode: 375, duration: 2.536s, episode steps: 354, steps per second: 140, episode reward: -353.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.025 [0.000, 2.000],  loss: 0.569937, mae: 11.250954, mean_q: -16.275008\n",
            " 146918/200000: episode: 376, duration: 4.819s, episode steps: 500, steps per second: 104, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.016 [0.000, 2.000],  loss: 0.525447, mae: 11.286771, mean_q: -16.331015\n",
            " 147418/200000: episode: 377, duration: 3.713s, episode steps: 500, steps per second: 135, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.976 [0.000, 2.000],  loss: 0.566385, mae: 11.304874, mean_q: -16.328907\n",
            " 147918/200000: episode: 378, duration: 3.749s, episode steps: 500, steps per second: 133, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.563843, mae: 11.328617, mean_q: -16.388280\n",
            " 148253/200000: episode: 379, duration: 2.589s, episode steps: 335, steps per second: 129, episode reward: -334.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.955 [0.000, 2.000],  loss: 0.544788, mae: 11.266288, mean_q: -16.290113\n",
            " 148570/200000: episode: 380, duration: 3.182s, episode steps: 317, steps per second: 100, episode reward: -316.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.079 [0.000, 2.000],  loss: 0.530270, mae: 11.331849, mean_q: -16.387793\n",
            " 149046/200000: episode: 381, duration: 3.292s, episode steps: 476, steps per second: 145, episode reward: -475.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.034 [0.000, 2.000],  loss: 0.511966, mae: 11.354931, mean_q: -16.442166\n",
            " 149318/200000: episode: 382, duration: 1.923s, episode steps: 272, steps per second: 141, episode reward: -271.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.026 [0.000, 2.000],  loss: 0.540709, mae: 11.272719, mean_q: -16.302970\n",
            " 149579/200000: episode: 383, duration: 1.816s, episode steps: 261, steps per second: 144, episode reward: -260.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.008 [0.000, 2.000],  loss: 0.513371, mae: 11.283028, mean_q: -16.330830\n",
            " 149998/200000: episode: 384, duration: 2.930s, episode steps: 419, steps per second: 143, episode reward: -418.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.069 [0.000, 2.000],  loss: 0.517698, mae: 11.268745, mean_q: -16.302073\n",
            " 150433/200000: episode: 385, duration: 4.282s, episode steps: 435, steps per second: 102, episode reward: -434.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.993 [0.000, 2.000],  loss: 0.530420, mae: 11.187679, mean_q: -16.160012\n",
            " 150671/200000: episode: 386, duration: 1.727s, episode steps: 238, steps per second: 138, episode reward: -237.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.025 [0.000, 2.000],  loss: 0.544145, mae: 11.169063, mean_q: -16.117185\n",
            " 150970/200000: episode: 387, duration: 2.178s, episode steps: 299, steps per second: 137, episode reward: -298.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.057 [0.000, 2.000],  loss: 0.566720, mae: 11.193471, mean_q: -16.177058\n",
            " 151226/200000: episode: 388, duration: 1.863s, episode steps: 256, steps per second: 137, episode reward: -255.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.023 [0.000, 2.000],  loss: 0.487530, mae: 11.211901, mean_q: -16.214190\n",
            " 151726/200000: episode: 389, duration: 3.586s, episode steps: 500, steps per second: 139, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.056 [0.000, 2.000],  loss: 0.543867, mae: 11.264422, mean_q: -16.278563\n",
            " 151952/200000: episode: 390, duration: 2.279s, episode steps: 226, steps per second:  99, episode reward: -225.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.093 [0.000, 2.000],  loss: 0.549824, mae: 11.223044, mean_q: -16.216837\n",
            " 152423/200000: episode: 391, duration: 3.696s, episode steps: 471, steps per second: 127, episode reward: -470.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.040 [0.000, 2.000],  loss: 0.562014, mae: 11.290005, mean_q: -16.315815\n",
            " 152923/200000: episode: 392, duration: 3.480s, episode steps: 500, steps per second: 144, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.948 [0.000, 2.000],  loss: 0.543674, mae: 11.277247, mean_q: -16.284330\n",
            " 153234/200000: episode: 393, duration: 2.128s, episode steps: 311, steps per second: 146, episode reward: -310.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.010 [0.000, 2.000],  loss: 0.510425, mae: 11.219930, mean_q: -16.226473\n",
            " 153641/200000: episode: 394, duration: 3.373s, episode steps: 407, steps per second: 121, episode reward: -406.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.956 [0.000, 2.000],  loss: 0.540164, mae: 11.316334, mean_q: -16.346226\n",
            " 153952/200000: episode: 395, duration: 2.726s, episode steps: 311, steps per second: 114, episode reward: -310.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.949 [0.000, 2.000],  loss: 0.550193, mae: 11.278546, mean_q: -16.295651\n",
            " 154308/200000: episode: 396, duration: 2.638s, episode steps: 356, steps per second: 135, episode reward: -355.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.989 [0.000, 2.000],  loss: 0.575703, mae: 11.380053, mean_q: -16.435885\n",
            " 154708/200000: episode: 397, duration: 2.837s, episode steps: 400, steps per second: 141, episode reward: -399.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.080 [0.000, 2.000],  loss: 0.546722, mae: 11.341996, mean_q: -16.393102\n",
            " 155208/200000: episode: 398, duration: 3.737s, episode steps: 500, steps per second: 134, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.018 [0.000, 2.000],  loss: 0.590076, mae: 11.351401, mean_q: -16.405208\n",
            " 155617/200000: episode: 399, duration: 3.908s, episode steps: 409, steps per second: 105, episode reward: -408.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.002 [0.000, 2.000],  loss: 0.566036, mae: 11.360825, mean_q: -16.423734\n",
            " 156010/200000: episode: 400, duration: 3.041s, episode steps: 393, steps per second: 129, episode reward: -392.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.038 [0.000, 2.000],  loss: 0.548938, mae: 11.401547, mean_q: -16.468633\n",
            " 156438/200000: episode: 401, duration: 3.021s, episode steps: 428, steps per second: 142, episode reward: -427.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.035 [0.000, 2.000],  loss: 0.537804, mae: 11.319997, mean_q: -16.366944\n",
            " 156883/200000: episode: 402, duration: 3.190s, episode steps: 445, steps per second: 139, episode reward: -444.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.083 [0.000, 2.000],  loss: 0.533265, mae: 11.290970, mean_q: -16.317228\n",
            " 157192/200000: episode: 403, duration: 3.013s, episode steps: 309, steps per second: 103, episode reward: -308.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.026 [0.000, 2.000],  loss: 0.547034, mae: 11.321764, mean_q: -16.362766\n",
            " 157584/200000: episode: 404, duration: 2.844s, episode steps: 392, steps per second: 138, episode reward: -391.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.018 [0.000, 2.000],  loss: 0.569063, mae: 11.308987, mean_q: -16.337707\n",
            " 157804/200000: episode: 405, duration: 1.520s, episode steps: 220, steps per second: 145, episode reward: -219.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.005 [0.000, 2.000],  loss: 0.604683, mae: 11.388808, mean_q: -16.444660\n",
            " 158116/200000: episode: 406, duration: 2.126s, episode steps: 312, steps per second: 147, episode reward: -311.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.913 [0.000, 2.000],  loss: 0.531883, mae: 11.365732, mean_q: -16.447235\n",
            " 158525/200000: episode: 407, duration: 2.776s, episode steps: 409, steps per second: 147, episode reward: -408.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.054 [0.000, 2.000],  loss: 0.620186, mae: 11.362231, mean_q: -16.408276\n",
            " 159008/200000: episode: 408, duration: 4.320s, episode steps: 483, steps per second: 112, episode reward: -482.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.035 [0.000, 2.000],  loss: 0.528433, mae: 11.381989, mean_q: -16.455650\n",
            " 159281/200000: episode: 409, duration: 2.032s, episode steps: 273, steps per second: 134, episode reward: -272.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.040 [0.000, 2.000],  loss: 0.619062, mae: 11.421444, mean_q: -16.499644\n",
            " 159743/200000: episode: 410, duration: 3.328s, episode steps: 462, steps per second: 139, episode reward: -461.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.026 [0.000, 2.000],  loss: 0.560018, mae: 11.398532, mean_q: -16.471822\n",
            " 160243/200000: episode: 411, duration: 3.491s, episode steps: 500, steps per second: 143, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.998 [0.000, 2.000],  loss: 0.589818, mae: 11.429055, mean_q: -16.512839\n",
            " 160743/200000: episode: 412, duration: 4.366s, episode steps: 500, steps per second: 115, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.958 [0.000, 2.000],  loss: 0.575952, mae: 11.464047, mean_q: -16.548828\n",
            " 160995/200000: episode: 413, duration: 1.848s, episode steps: 252, steps per second: 136, episode reward: -251.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.960 [0.000, 2.000],  loss: 0.520444, mae: 11.472890, mean_q: -16.582463\n",
            " 161253/200000: episode: 414, duration: 1.840s, episode steps: 258, steps per second: 140, episode reward: -257.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.992 [0.000, 2.000],  loss: 0.516261, mae: 11.363820, mean_q: -16.423810\n",
            " 161624/200000: episode: 415, duration: 2.591s, episode steps: 371, steps per second: 143, episode reward: -370.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.943 [0.000, 2.000],  loss: 0.591588, mae: 11.469556, mean_q: -16.559605\n",
            " 161919/200000: episode: 416, duration: 2.036s, episode steps: 295, steps per second: 145, episode reward: -294.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.973 [0.000, 2.000],  loss: 0.538277, mae: 11.390150, mean_q: -16.448343\n",
            " 162129/200000: episode: 417, duration: 1.455s, episode steps: 210, steps per second: 144, episode reward: -209.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.086 [0.000, 2.000],  loss: 0.542043, mae: 11.398235, mean_q: -16.474026\n",
            " 162507/200000: episode: 418, duration: 3.467s, episode steps: 378, steps per second: 109, episode reward: -377.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.005 [0.000, 2.000],  loss: 0.557104, mae: 11.428119, mean_q: -16.531144\n",
            " 162808/200000: episode: 419, duration: 2.340s, episode steps: 301, steps per second: 129, episode reward: -300.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.987 [0.000, 2.000],  loss: 0.570940, mae: 11.435338, mean_q: -16.539485\n",
            " 163044/200000: episode: 420, duration: 1.670s, episode steps: 236, steps per second: 141, episode reward: -235.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.924 [0.000, 2.000],  loss: 0.618735, mae: 11.462909, mean_q: -16.557722\n",
            " 163389/200000: episode: 421, duration: 2.570s, episode steps: 345, steps per second: 134, episode reward: -344.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.012 [0.000, 2.000],  loss: 0.535954, mae: 11.502525, mean_q: -16.648408\n",
            " 163809/200000: episode: 422, duration: 2.981s, episode steps: 420, steps per second: 141, episode reward: -419.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.976 [0.000, 2.000],  loss: 0.507835, mae: 11.520848, mean_q: -16.668781\n",
            " 164100/200000: episode: 423, duration: 2.640s, episode steps: 291, steps per second: 110, episode reward: -290.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.148 [0.000, 2.000],  loss: 0.551994, mae: 11.467949, mean_q: -16.582070\n",
            " 164471/200000: episode: 424, duration: 3.241s, episode steps: 371, steps per second: 114, episode reward: -370.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.005 [0.000, 2.000],  loss: 0.548181, mae: 11.465403, mean_q: -16.578108\n",
            " 164784/200000: episode: 425, duration: 2.373s, episode steps: 313, steps per second: 132, episode reward: -312.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.968 [0.000, 2.000],  loss: 0.555621, mae: 11.455206, mean_q: -16.549921\n",
            " 165104/200000: episode: 426, duration: 2.499s, episode steps: 320, steps per second: 128, episode reward: -319.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.919 [0.000, 2.000],  loss: 0.542109, mae: 11.471471, mean_q: -16.582433\n",
            " 165604/200000: episode: 427, duration: 4.020s, episode steps: 500, steps per second: 124, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.553867, mae: 11.452908, mean_q: -16.564671\n",
            " 165949/200000: episode: 428, duration: 3.549s, episode steps: 345, steps per second:  97, episode reward: -344.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.974 [0.000, 2.000],  loss: 0.590540, mae: 11.435234, mean_q: -16.528750\n",
            " 166216/200000: episode: 429, duration: 2.132s, episode steps: 267, steps per second: 125, episode reward: -266.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.045 [0.000, 2.000],  loss: 0.567264, mae: 11.512154, mean_q: -16.654769\n",
            " 166716/200000: episode: 430, duration: 3.760s, episode steps: 500, steps per second: 133, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.948 [0.000, 2.000],  loss: 0.570639, mae: 11.574398, mean_q: -16.745242\n",
            " 167056/200000: episode: 431, duration: 2.419s, episode steps: 340, steps per second: 141, episode reward: -339.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.513537, mae: 11.558391, mean_q: -16.727425\n",
            " 167238/200000: episode: 432, duration: 1.342s, episode steps: 182, steps per second: 136, episode reward: -181.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.978 [0.000, 2.000],  loss: 0.538878, mae: 11.594863, mean_q: -16.762667\n",
            " 167601/200000: episode: 433, duration: 3.612s, episode steps: 363, steps per second: 100, episode reward: -362.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.014 [0.000, 2.000],  loss: 0.490414, mae: 11.587761, mean_q: -16.786913\n",
            " 168081/200000: episode: 434, duration: 3.373s, episode steps: 480, steps per second: 142, episode reward: -479.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.077 [0.000, 2.000],  loss: 0.552453, mae: 11.540655, mean_q: -16.696038\n",
            " 168401/200000: episode: 435, duration: 2.248s, episode steps: 320, steps per second: 142, episode reward: -319.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.997 [0.000, 2.000],  loss: 0.560430, mae: 11.516516, mean_q: -16.651129\n",
            " 168723/200000: episode: 436, duration: 2.124s, episode steps: 322, steps per second: 152, episode reward: -321.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.925 [0.000, 2.000],  loss: 0.524662, mae: 11.519870, mean_q: -16.665854\n",
            " 169116/200000: episode: 437, duration: 3.054s, episode steps: 393, steps per second: 129, episode reward: -392.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.043 [0.000, 2.000],  loss: 0.526925, mae: 11.483496, mean_q: -16.610008\n",
            " 169616/200000: episode: 438, duration: 4.070s, episode steps: 500, steps per second: 123, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.498281, mae: 11.383659, mean_q: -16.482284\n",
            " 169957/200000: episode: 439, duration: 2.395s, episode steps: 341, steps per second: 142, episode reward: -340.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.050 [0.000, 2.000],  loss: 0.531334, mae: 11.445198, mean_q: -16.563969\n",
            " 170265/200000: episode: 440, duration: 2.108s, episode steps: 308, steps per second: 146, episode reward: -307.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.016 [0.000, 2.000],  loss: 0.561600, mae: 11.354334, mean_q: -16.424352\n",
            " 170691/200000: episode: 441, duration: 2.917s, episode steps: 426, steps per second: 146, episode reward: -425.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.045 [0.000, 2.000],  loss: 0.527791, mae: 11.327137, mean_q: -16.382517\n",
            " 171106/200000: episode: 442, duration: 4.044s, episode steps: 415, steps per second: 103, episode reward: -414.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.043 [0.000, 2.000],  loss: 0.542565, mae: 11.290984, mean_q: -16.331661\n",
            " 171337/200000: episode: 443, duration: 1.643s, episode steps: 231, steps per second: 141, episode reward: -230.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.939 [0.000, 2.000],  loss: 0.587787, mae: 11.245225, mean_q: -16.248530\n",
            " 171811/200000: episode: 444, duration: 3.208s, episode steps: 474, steps per second: 148, episode reward: -473.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.962 [0.000, 2.000],  loss: 0.498355, mae: 11.243515, mean_q: -16.278016\n",
            " 172311/200000: episode: 445, duration: 3.767s, episode steps: 500, steps per second: 133, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.036 [0.000, 2.000],  loss: 0.542126, mae: 11.228051, mean_q: -16.245203\n",
            " 172500/200000: episode: 446, duration: 1.993s, episode steps: 189, steps per second:  95, episode reward: -188.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.079 [0.000, 2.000],  loss: 0.529132, mae: 11.274670, mean_q: -16.305683\n",
            " 172849/200000: episode: 447, duration: 3.668s, episode steps: 349, steps per second:  95, episode reward: -348.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.043 [0.000, 2.000],  loss: 0.513787, mae: 11.242841, mean_q: -16.273574\n",
            " 173231/200000: episode: 448, duration: 2.625s, episode steps: 382, steps per second: 146, episode reward: -381.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.955 [0.000, 2.000],  loss: 0.534920, mae: 11.224426, mean_q: -16.221348\n",
            " 173731/200000: episode: 449, duration: 3.418s, episode steps: 500, steps per second: 146, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.504157, mae: 11.245509, mean_q: -16.270667\n",
            " 173948/200000: episode: 450, duration: 1.503s, episode steps: 217, steps per second: 144, episode reward: -216.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.991 [0.000, 2.000],  loss: 0.524828, mae: 11.268561, mean_q: -16.286874\n",
            " 174264/200000: episode: 451, duration: 2.193s, episode steps: 316, steps per second: 144, episode reward: -315.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.975 [0.000, 2.000],  loss: 0.523007, mae: 11.253209, mean_q: -16.284722\n",
            " 174515/200000: episode: 452, duration: 2.338s, episode steps: 251, steps per second: 107, episode reward: -250.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.936 [0.000, 2.000],  loss: 0.513458, mae: 11.310882, mean_q: -16.380920\n",
            " 175015/200000: episode: 453, duration: 3.780s, episode steps: 500, steps per second: 132, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.523160, mae: 11.346959, mean_q: -16.423774\n",
            " 175348/200000: episode: 454, duration: 2.407s, episode steps: 333, steps per second: 138, episode reward: -332.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.009 [0.000, 2.000],  loss: 0.528352, mae: 11.312533, mean_q: -16.361344\n",
            " 175676/200000: episode: 455, duration: 2.371s, episode steps: 328, steps per second: 138, episode reward: -327.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.015 [0.000, 2.000],  loss: 0.546637, mae: 11.265918, mean_q: -16.293301\n",
            " 176176/200000: episode: 456, duration: 3.912s, episode steps: 500, steps per second: 128, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.038 [0.000, 2.000],  loss: 0.521449, mae: 11.328233, mean_q: -16.401056\n",
            " 176509/200000: episode: 457, duration: 3.123s, episode steps: 333, steps per second: 107, episode reward: -332.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.946 [0.000, 2.000],  loss: 0.536504, mae: 11.328785, mean_q: -16.394564\n",
            " 176844/200000: episode: 458, duration: 2.416s, episode steps: 335, steps per second: 139, episode reward: -334.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.051 [0.000, 2.000],  loss: 0.524722, mae: 11.330592, mean_q: -16.402847\n",
            " 177344/200000: episode: 459, duration: 3.552s, episode steps: 500, steps per second: 141, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.946 [0.000, 2.000],  loss: 0.481114, mae: 11.336359, mean_q: -16.426788\n",
            " 177760/200000: episode: 460, duration: 2.974s, episode steps: 416, steps per second: 140, episode reward: -415.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.995 [0.000, 2.000],  loss: 0.559398, mae: 11.307956, mean_q: -16.359155\n",
            " 178150/200000: episode: 461, duration: 4.021s, episode steps: 390, steps per second:  97, episode reward: -389.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.990 [0.000, 2.000],  loss: 0.546614, mae: 11.294807, mean_q: -16.348791\n",
            " 178405/200000: episode: 462, duration: 1.748s, episode steps: 255, steps per second: 146, episode reward: -254.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.945 [0.000, 2.000],  loss: 0.536095, mae: 11.379068, mean_q: -16.482770\n",
            " 178815/200000: episode: 463, duration: 2.850s, episode steps: 410, steps per second: 144, episode reward: -409.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.034 [0.000, 2.000],  loss: 0.520609, mae: 11.381263, mean_q: -16.487172\n",
            " 179150/200000: episode: 464, duration: 2.426s, episode steps: 335, steps per second: 138, episode reward: -334.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.973 [0.000, 2.000],  loss: 0.464359, mae: 11.363836, mean_q: -16.454241\n",
            " 179373/200000: episode: 465, duration: 1.564s, episode steps: 223, steps per second: 143, episode reward: -222.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.072 [0.000, 2.000],  loss: 0.531541, mae: 11.396537, mean_q: -16.499752\n",
            " 179692/200000: episode: 466, duration: 2.752s, episode steps: 319, steps per second: 116, episode reward: -318.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.013 [0.000, 2.000],  loss: 0.521922, mae: 11.365855, mean_q: -16.462302\n",
            " 180192/200000: episode: 467, duration: 3.910s, episode steps: 500, steps per second: 128, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.978 [0.000, 2.000],  loss: 0.513076, mae: 11.387387, mean_q: -16.506596\n",
            " 180692/200000: episode: 468, duration: 3.506s, episode steps: 500, steps per second: 143, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.032 [0.000, 2.000],  loss: 0.512252, mae: 11.379951, mean_q: -16.462198\n",
            " 181010/200000: episode: 469, duration: 2.170s, episode steps: 318, steps per second: 147, episode reward: -317.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.972 [0.000, 2.000],  loss: 0.501807, mae: 11.402156, mean_q: -16.504604\n",
            " 181298/200000: episode: 470, duration: 2.022s, episode steps: 288, steps per second: 142, episode reward: -287.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.910 [0.000, 2.000],  loss: 0.537028, mae: 11.343681, mean_q: -16.411280\n",
            " 181501/200000: episode: 471, duration: 1.967s, episode steps: 203, steps per second: 103, episode reward: -202.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.094 [0.000, 2.000],  loss: 0.553131, mae: 11.381345, mean_q: -16.462303\n",
            " 181848/200000: episode: 472, duration: 2.962s, episode steps: 347, steps per second: 117, episode reward: -346.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.977 [0.000, 2.000],  loss: 0.608087, mae: 11.327850, mean_q: -16.360615\n",
            " 182158/200000: episode: 473, duration: 2.217s, episode steps: 310, steps per second: 140, episode reward: -309.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.868 [0.000, 2.000],  loss: 0.517009, mae: 11.356963, mean_q: -16.428635\n",
            " 182658/200000: episode: 474, duration: 3.393s, episode steps: 500, steps per second: 147, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.978 [0.000, 2.000],  loss: 0.499088, mae: 11.364592, mean_q: -16.457221\n",
            " 182969/200000: episode: 475, duration: 2.125s, episode steps: 311, steps per second: 146, episode reward: -310.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.013 [0.000, 2.000],  loss: 0.538095, mae: 11.319962, mean_q: -16.388973\n",
            " 183469/200000: episode: 476, duration: 4.536s, episode steps: 500, steps per second: 110, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.472991, mae: 11.451325, mean_q: -16.591781\n",
            " 183969/200000: episode: 477, duration: 3.373s, episode steps: 500, steps per second: 148, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.004 [0.000, 2.000],  loss: 0.563761, mae: 11.447776, mean_q: -16.562150\n",
            " 184423/200000: episode: 478, duration: 3.044s, episode steps: 454, steps per second: 149, episode reward: -453.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.958 [0.000, 2.000],  loss: 0.484282, mae: 11.402793, mean_q: -16.529694\n",
            " 184923/200000: episode: 479, duration: 3.309s, episode steps: 500, steps per second: 151, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.952 [0.000, 2.000],  loss: 0.536300, mae: 11.408483, mean_q: -16.523256\n",
            " 185214/200000: episode: 480, duration: 2.890s, episode steps: 291, steps per second: 101, episode reward: -290.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.045 [0.000, 2.000],  loss: 0.546654, mae: 11.435597, mean_q: -16.566192\n",
            " 185714/200000: episode: 481, duration: 3.691s, episode steps: 500, steps per second: 135, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.972 [0.000, 2.000],  loss: 0.494961, mae: 11.415378, mean_q: -16.532245\n",
            " 186111/200000: episode: 482, duration: 2.922s, episode steps: 397, steps per second: 136, episode reward: -396.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.932 [0.000, 2.000],  loss: 0.498060, mae: 11.421281, mean_q: -16.544376\n",
            " 186426/200000: episode: 483, duration: 2.238s, episode steps: 315, steps per second: 141, episode reward: -314.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.949 [0.000, 2.000],  loss: 0.477095, mae: 11.358564, mean_q: -16.454282\n",
            " 186926/200000: episode: 484, duration: 4.413s, episode steps: 500, steps per second: 113, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.872 [0.000, 2.000],  loss: 0.485624, mae: 11.333488, mean_q: -16.426924\n",
            " 187342/200000: episode: 485, duration: 3.161s, episode steps: 416, steps per second: 132, episode reward: -415.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.024 [0.000, 2.000],  loss: 0.520290, mae: 11.334069, mean_q: -16.425264\n",
            " 187729/200000: episode: 486, duration: 2.724s, episode steps: 387, steps per second: 142, episode reward: -386.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.992 [0.000, 2.000],  loss: 0.500159, mae: 11.329805, mean_q: -16.416931\n",
            " 188043/200000: episode: 487, duration: 2.341s, episode steps: 314, steps per second: 134, episode reward: -313.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.096 [0.000, 2.000],  loss: 0.459615, mae: 11.343331, mean_q: -16.426520\n",
            " 188543/200000: episode: 488, duration: 4.046s, episode steps: 500, steps per second: 124, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.986 [0.000, 2.000],  loss: 0.483319, mae: 11.401133, mean_q: -16.531103\n",
            " 189043/200000: episode: 489, duration: 4.265s, episode steps: 500, steps per second: 117, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.936 [0.000, 2.000],  loss: 0.526866, mae: 11.419670, mean_q: -16.545969\n",
            " 189415/200000: episode: 490, duration: 2.573s, episode steps: 372, steps per second: 145, episode reward: -371.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.962 [0.000, 2.000],  loss: 0.572255, mae: 11.393207, mean_q: -16.487526\n",
            " 189715/200000: episode: 491, duration: 2.137s, episode steps: 300, steps per second: 140, episode reward: -299.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.977 [0.000, 2.000],  loss: 0.521669, mae: 11.393323, mean_q: -16.510735\n",
            " 190099/200000: episode: 492, duration: 2.844s, episode steps: 384, steps per second: 135, episode reward: -383.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.964 [0.000, 2.000],  loss: 0.539314, mae: 11.420524, mean_q: -16.539659\n",
            " 190377/200000: episode: 493, duration: 2.909s, episode steps: 278, steps per second:  96, episode reward: -277.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.975 [0.000, 2.000],  loss: 0.563600, mae: 11.425494, mean_q: -16.541003\n",
            " 190877/200000: episode: 494, duration: 3.637s, episode steps: 500, steps per second: 137, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.956 [0.000, 2.000],  loss: 0.531580, mae: 11.466625, mean_q: -16.623469\n",
            " 191222/200000: episode: 495, duration: 2.481s, episode steps: 345, steps per second: 139, episode reward: -344.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.052 [0.000, 2.000],  loss: 0.492438, mae: 11.444804, mean_q: -16.590829\n",
            " 191495/200000: episode: 496, duration: 1.932s, episode steps: 273, steps per second: 141, episode reward: -272.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.051 [0.000, 2.000],  loss: 0.573982, mae: 11.539701, mean_q: -16.719825\n",
            " 191880/200000: episode: 497, duration: 3.149s, episode steps: 385, steps per second: 122, episode reward: -384.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.974 [0.000, 2.000],  loss: 0.557203, mae: 11.474972, mean_q: -16.619160\n",
            " 192291/200000: episode: 498, duration: 3.607s, episode steps: 411, steps per second: 114, episode reward: -410.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.107 [0.000, 2.000],  loss: 0.495048, mae: 11.526758, mean_q: -16.718938\n",
            " 192605/200000: episode: 499, duration: 2.160s, episode steps: 314, steps per second: 145, episode reward: -313.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.061 [0.000, 2.000],  loss: 0.521244, mae: 11.558071, mean_q: -16.753555\n",
            " 193069/200000: episode: 500, duration: 3.356s, episode steps: 464, steps per second: 138, episode reward: -463.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.966 [0.000, 2.000],  loss: 0.521432, mae: 11.548297, mean_q: -16.732330\n",
            " 193321/200000: episode: 501, duration: 1.769s, episode steps: 252, steps per second: 142, episode reward: -251.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.917 [0.000, 2.000],  loss: 0.510558, mae: 11.486247, mean_q: -16.645597\n",
            " 193821/200000: episode: 502, duration: 4.537s, episode steps: 500, steps per second: 110, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.978 [0.000, 2.000],  loss: 0.535562, mae: 11.476582, mean_q: -16.627352\n",
            " 194117/200000: episode: 503, duration: 2.131s, episode steps: 296, steps per second: 139, episode reward: -295.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.057 [0.000, 2.000],  loss: 0.542146, mae: 11.509146, mean_q: -16.668757\n",
            " 194393/200000: episode: 504, duration: 2.049s, episode steps: 276, steps per second: 135, episode reward: -275.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.986 [0.000, 2.000],  loss: 0.519324, mae: 11.548090, mean_q: -16.738237\n",
            " 194893/200000: episode: 505, duration: 3.687s, episode steps: 500, steps per second: 136, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.036 [0.000, 2.000],  loss: 0.502254, mae: 11.533746, mean_q: -16.719886\n",
            " 195333/200000: episode: 506, duration: 3.267s, episode steps: 440, steps per second: 135, episode reward: -439.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.952 [0.000, 2.000],  loss: 0.496250, mae: 11.489016, mean_q: -16.658805\n",
            " 195833/200000: episode: 507, duration: 4.199s, episode steps: 500, steps per second: 119, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.932 [0.000, 2.000],  loss: 0.484053, mae: 11.476787, mean_q: -16.643958\n",
            " 196270/200000: episode: 508, duration: 2.962s, episode steps: 437, steps per second: 148, episode reward: -436.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.484336, mae: 11.473388, mean_q: -16.642605\n",
            " 196770/200000: episode: 509, duration: 3.453s, episode steps: 500, steps per second: 145, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.507105, mae: 11.515183, mean_q: -16.695586\n",
            " 197137/200000: episode: 510, duration: 2.946s, episode steps: 367, steps per second: 125, episode reward: -366.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.986 [0.000, 2.000],  loss: 0.494748, mae: 11.461874, mean_q: -16.621437\n",
            " 197605/200000: episode: 511, duration: 4.155s, episode steps: 468, steps per second: 113, episode reward: -467.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.026 [0.000, 2.000],  loss: 0.509034, mae: 11.480750, mean_q: -16.661009\n",
            " 198105/200000: episode: 512, duration: 3.617s, episode steps: 500, steps per second: 138, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.528559, mae: 11.508074, mean_q: -16.687497\n",
            " 198429/200000: episode: 513, duration: 2.246s, episode steps: 324, steps per second: 144, episode reward: -323.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.006 [0.000, 2.000],  loss: 0.482604, mae: 11.596629, mean_q: -16.828855\n",
            " 198886/200000: episode: 514, duration: 3.552s, episode steps: 457, steps per second: 129, episode reward: -456.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.050 [0.000, 2.000],  loss: 0.507783, mae: 11.568767, mean_q: -16.788302\n",
            " 199269/200000: episode: 515, duration: 3.394s, episode steps: 383, steps per second: 113, episode reward: -382.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.029 [0.000, 2.000],  loss: 0.545676, mae: 11.498553, mean_q: -16.658721\n",
            " 199743/200000: episode: 516, duration: 3.301s, episode steps: 474, steps per second: 144, episode reward: -473.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.065 [0.000, 2.000],  loss: 0.529174, mae: 11.497701, mean_q: -16.666119\n",
            "done, took 1410.623 seconds\n",
            "\n",
            "Evaluando BOLTZMANN_DQN...\n",
            "BOLTZMANN_DQN → Recompensa media: -92.90 ± 13.81\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA14AAAIkCAYAAAAOFvrhAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQABAABJREFUeJzs3XVcldcfwPHPDbiX7lIQxQAVRUzsmvlTN6ezZ7tw5eama2PhjHVZm7O22c7N7tnYYoBFCEp33np+f1y5iqCCku68Xy9ewMMT5x6e+9zn+5xzvkcmSZKEIAiCIAiCIAiCUGbkFV0AQRAEQRAEQRCEJ50IvARBEARBEARBEMqYCLwEQRAEQRAEQRDKmAi8BEEQBEEQBEEQypgIvARBEARBEARBEMqYCLwEQRAEQRAEQRDKmAi8BEEQBEEQBEEQypgIvARBEARBEARBEMqYCLwEQRCqgF9//ZUFCxZUdDEEQRAEQXhEIvASBEF4CJlMxvTp08ts/506daJTp073/fuaNWt44403aNGiRZmV4W6//fYbMpmMiIiIcjmeIFRWNWvWZPTo0eV6zOnTpyOTySq8HIIglD4ReAmCUCXkBwP3+zp69GhFF7FMXLlyhZdeeonVq1fTtGnTii7OI9m3b1+B/5VCocDV1ZWBAwdy6dKlii6e8ITo1KnTfa8Pfn5+FV08QRAElBVdAEEQhJKYOXMmtWrVKrS8Tp06FVCa0rFjx477/u3s2bMsWbKEXr16lWOJysbrr79OixYt0Gq1nDt3jvnz57Nv3z7Onz+Pu7t7RRdPeAJ4enoya9asQsvt7OweaX9hYWHI5RX/jLqylEMQhMcjAi9BEKqUXr160bx584ouRqkyNze/798GDhxYjiUpW+3bty/wenx9fXn55ZdZtmwZU6ZMqcCSCVWBwWBAo9GgVqvvu46dnR0jRowotWOqVKpS29fjqCzlEATh8YjHJ4IgPDG0Wi2Ojo6MGTOm0N/S09NRq9W8/fbbpmXx8fGMGzcONzc31Go1AQEBLF269KHHGT16NDVr1iy0vKixGQArVqygZcuWWFpa4uDgQIcOHQq0chU1xqs4ZYuIiEAmkzFv3jwWLlxI7dq1UalUtGjRguPHjz/0dQBcuHCBLl26YGFhgaenJ59++ikGg6HIdbdu3Ur79u2xsrLCxsaG//3vf1y4cKFYxylK+/btAbh27VqB5TExMYwdOxY3NzdUKhUNGzbk119/LbR9bm4u06dPp169eqjVajw8PHj22WcL7C8rK4vJkyfj5eWFSqXC19eXefPmIUlSgX3JZDJeffVV1qxZQ4MGDbCwsKB169aEhIQAsGDBAurUqYNaraZTp06Fxr916tQJf39/Tp48SZs2bbCwsKBWrVrMnz+/ULnz8vKYNm0aderUQaVS4eXlxZQpU8jLyyuyTBs3bsTf399UF9u2bSuwXkZGBpMmTaJmzZqoVCpcXV3p1q0bp06dMq1z4MABnnvuOWrUqGE65ptvvklOTk6BfcXGxjJmzBg8PT1RqVR4eHjw9NNPP3S83+jRo7G2tub69ev06NEDKysrqlWrxsyZMwvVdUn/JytXrqRhw4aoVKpCr/1R5L9PQ0NDGTRoELa2tjg5OfHGG2+Qm5tbYN17x1ZptVpmzJhB3bp1UavVODk50a5dO3bu3Flguz179pjeK/b29jz99NNFdqs9ePAgLVq0QK1WU7t27fsm0ClqjNf169d57rnncHR0xNLSkqCgIDZv3vxolSIIQrkQLV6CIFQpaWlpJCYmFlgmk8lwcnLCzMyM/v37s379ehYsWFCgJWnjxo3k5eUxZMgQAHJycujUqRNXr17l1VdfpVatWqxZs4bRo0eTmprKG2+8USrlnTFjBtOnT6dNmzbMnDkTc3Nzjh07xp49e+jevXuR25S0bL///jsZGRm8+OKLyGQy5syZw7PPPsv169cxMzO7b9liY2Pp3LkzOp2Od999FysrKxYuXIiFhUWhdZcvX86oUaPo0aMHs2fPJjs7m59//pl27dpx+vTpIgPRh8m/mXdwcDAti4uLIygoyHTT7eLiwtatWxk3bhzp6elMmjQJAL1eT58+fdi9ezdDhgzhjTfeICMjg507d3L+/Hlq166NJEn069ePvXv3Mm7cOJo0acL27dt55513iImJ4euvvy5QngMHDrBp0yZeeeUVAGbNmkWfPn2YMmUKP/30ExMnTiQlJYU5c+YwduxY9uzZU2D7lJQUevfuzaBBgxg6dCirV6/m5ZdfxtzcnLFjxwLGVpt+/fpx8OBBXnjhBerXr09ISAhff/01ly9fZuPGjQX2efDgQdavX8/EiROxsbHhu+++Y8CAAURFReHk5ATASy+9xNq1a3n11Vdp0KABSUlJHDx4kEuXLpnGBa5Zs4bs7GxefvllnJycCA4O5vvvvyc6Opo1a9aYjjdgwAAuXLjAa6+9Rs2aNYmPj2fnzp1ERUU99H+s1+vp2bMnQUFBzJkzh23btjFt2jR0Oh0zZ84EKPH/ZM+ePaxevZpXX30VZ2fnYpXh3usDgIWFBVZWVgWWDRo0iJo1azJr1iyOHj3Kd999R0pKCsuWLbvv/qdPn86sWbMYP348LVu2JD09nRMnTnDq1Cm6desGwK5du+jVqxc+Pj5Mnz6dnJwcvv/+e9q2bcupU6dMryEkJITu3bvj4uLC9OnT0el0TJs2DTc3twe+RjC+T9q0aUN2djavv/46Tk5OLF26lH79+rF27Vr69+//0H0IglABJEEQhCpgyZIlElDkl0qlMq23fft2CZD+/vvvAtv37t1b8vHxMf3+zTffSIC0YsUK0zKNRiO1bt1asra2ltLT003LAWnatGmm30eNGiV5e3sXKuO0adOkuy+rV65ckeRyudS/f39Jr9cXWNdgMJh+7tixo9SxY8cSly08PFwCJCcnJyk5Odm07l9//VVkHdxr0qRJEiAdO3bMtCw+Pl6ys7OTACk8PFySJEnKyMiQ7O3tpQkTJhTYPjY2VrKzsyu0/F579+6VAOnXX3+VEhISpJs3b0rbtm2T6tSpI8lkMik4ONi07rhx4yQPDw8pMTGxwD6GDBki2dnZSdnZ2ZIkSdKvv/4qAdJXX31V6Hj5dbtx40YJkD799NMCfx84cKAkk8mkq1evmpbln0f5r1mSJGnBggUSILm7uxc4H957770C9SNJxv8hIH355ZemZXl5eVKTJk0kV1dXSaPRSJIkScuXL5fkcrl04MCBAmWaP3++BEiHDh0qUCZzc/MC5Tx79qwESN9//71pmZ2dnfTKK68Uqoe75dfb3WbNmiXJZDIpMjJSkiRJSklJkQBp7ty5D9xXUUaNGiUB0muvvWZaZjAYpP/973+Subm5lJCQIElSyf8ncrlcunDhQrHKkP8/KOrrxRdfNK2X/z7t169fge0nTpwoAdLZs2dNy7y9vaVRo0aZfg8ICJD+97//PbAc+f/zpKQk07KzZ89KcrlcGjlypGnZM888I6nValP9S5IkXbx4UVIoFAWuI0WVI/+9e/d5lJGRIdWqVUuqWbNmoeuNIAiVg+hqKAhClfLjjz+yc+fOAl9bt241/b1Lly44OzuzatUq07KUlBR27tzJ4MGDTcu2bNmCu7s7Q4cONS0zMzPj9ddfJzMzk/379z92WTdu3IjBYODjjz8uNDC+qC6Jj1q2wYMHF2g1yu/Cd/369QeWb8uWLQQFBdGyZUvTMhcXF4YPH15gvZ07d5KamsrQoUNJTEw0fSkUClq1asXevXsfeJx8Y8eOxcXFhWrVqtGzZ0/S0tJYvny5KU2+JEmsW7eOvn37IklSgWP16NGDtLQ0U/e5devW4ezszGuvvVboOPl1u2XLFhQKBa+//nqBv0+ePBlJkgqcNwBdu3Yt0KLSqlUrwNgKZGNjU2j5vfWrVCp58cUXTb+bm5vz4osvEh8fz8mTJwFjy1P9+vXx8/Mr8Pq6dOkCUKgun3rqKWrXrm36vXHjxtja2hY4tr29PceOHePmzZuF6iLf3a2YWVlZJCYm0qZNGyRJ4vTp06Z1zM3N2bdvHykpKffd14O8+uqrpp/zWy01Gg27du0CSv4/6dixIw0aNCj28WvWrFno+rBz505TS+nd8ls28+WfS1u2bLnv/u3t7blw4QJXrlwp8u+3bt3izJkzjB49GkdHR9Pyxo0b061bN9O+9Xo927dv55lnnqFGjRqm9erXr0+PHj0e+jq3bNlCy5YtadeunWmZtbU1L7zwAhEREVy8ePGh+xAEofyJroaCIFQpLVu2fGByDaVSyYABA/j999/Jy8tDpVKxfv16tFptgcArMjKSunXrFgqI6tevb/r747p27RpyubxEN46PUra7b9zgTte9h908R0ZGmoKIu/n6+hb4Pf8mMz84uJetre0Dj5Pv448/pn379mRmZrJhwwb+/PPPAq8xISGB1NRUFi5cyMKFC4vcR3x8PGCsW19fX5TK+3+MRUZGUq1atQJBExS/HvMz4Xl5eRW5/N76rVatWqHubPXq1QOM3SqDgoK4cuUKly5dwsXF5YGv735lAuP/9+5jz5kzh1GjRuHl5UWzZs3o3bs3I0eOxMfHx7ROVFQUH3/8MZs2bSpU7rS0NMCYwGH27NlMnjwZNzc3goKC6NOnDyNHjixW1km5XF7gmPe+fij5/6SoDKYPYmVlxVNPPVWsdevWrVvg99q1ayOXyx84nm3mzJk8/fTT1KtXD39/f3r27Mnzzz9P48aNC5T/3vcQGF/j9u3bycrKIiMjg5ycnEJlyN/2QcFf/nGKeu/eXY/+/v4P3IcgCOVPBF6CIDxxhgwZwoIFC9i6dSvPPPMMq1evxs/Pj4CAgFLZ//1aq/R6fansv6QUCkWRy6V7khU8qvxkG8uXLy/yBvxBwc/dGjVqZLopfuaZZ8jOzmbChAm0a9cOLy8v03FGjBjBqFGjitxH/g1uWbhfPZZm/RoMBho1asRXX31V5N/vDfKKc+xBgwbRvn17NmzYwI4dO5g7dy6zZ89m/fr19OrVC71eT7du3UhOTmbq1Kn4+flhZWVFTEwMo0ePLpBMZdKkSfTt25eNGzeyfft2PvroI2bNmsWePXsIDAws8et9XEWNNywrD2qFztehQweuXbvGX3/9xY4dO1i8eDFff/018+fPZ/z48eVQSkEQqjIReAmC8MTp0KEDHh4erFq1inbt2rFnzx4++OCDAut4e3tz7tw5DAZDgVaX0NBQ09/vx8HBgdTU1ELL731aX7t2bQwGAxcvXqRJkybFLv/jlK0kvL29i+wyFRYWVuD3/K5urq6uxW5NKI4vvviCDRs28NlnnzF//nxcXFywsbFBr9c/9Di1a9fm2LFjaLXa+yYQ8fb2ZteuXWRkZBRoYSntesx38+ZNsrKyCrR6Xb58GcDUhbF27dqcPXuWrl27FutGv7g8PDyYOHEiEydOJD4+nqZNm/LZZ5/Rq1cvQkJCuHz5MkuXLmXkyJGmbe7NxJevdu3aTJ48mcmTJ3PlyhWaNGnCl19+yYoVKx5YBoPBwPXr102tXFD49Zf3/+RBrly5UqBF7erVqxgMhocm8MjPnDpmzBgyMzPp0KED06dPZ/z48aby3/seAuNrdHZ2xsrKCrVajYWFRbHef0Xx9va+7zHy/y4IQuUjxngJgvDEkcvlDBw4kL///pvly5ej0+kKdDME6N27N7GxsQXGgul0Or7//nusra3p2LHjffdfu3Zt0tLSOHfunGnZrVu32LBhQ4H1nnnmGeRyOTNnziyUov1BrSWPU7aS6N27N0ePHiU4ONi0LCEhgZUrVxZYr0ePHtja2vL555+j1WoL7SchIeGRjl+7dm0GDBjAb7/9RmxsLAqFggEDBrBu3TrOnz//wOMMGDCAxMREfvjhh0Lr5ddt79690ev1hdb5+uuvkclkpT4ptU6nK5AOXKPRsGDBAlxcXGjWrBlgbJ2KiYlh0aJFhbbPyckhKyurRMfU6/WmroL5XF1dqVatmik9fX6r2d3nnCRJfPvttwW2y87OLpROvXbt2tjY2BRKdX8/d9e1JEn88MMPmJmZ0bVrV6D8/ycP8uOPPxb4/fvvvwd4YBmSkpIK/G5tbU2dOnVM9ePh4UGTJk1YunRpgYcz58+fZ8eOHfTu3Rsw/k969OjBxo0biYqKMq136dIltm/f/tCy9+7dm+DgYI4cOWJalpWVxcKFC6lZs2aJuzcLglA+RIuXIAhVytatW01Pde/Wpk2bAuNLBg8ezPfff8+0adNo1KiRaexDvhdeeIEFCxYwevRoTp48Sc2aNVm7di2HDh3im2++KTQG5W5Dhgxh6tSp9O/fn9dff92UWr1evXoF5k6qU6cOH3zwAZ988gnt27fn2WefRaVScfz4capVq8asWbOK3P/jlK0kpkyZwvLly+nZsydvvPGGKZ18fotbPltbW37++Weef/55mjZtypAhQ3BxcSEqKorNmzfTtm3bIgOg4njnnXdYvXo133zzDV988QVffPEFe/fupVWrVkyYMIEGDRqQnJzMqVOn2LVrF8nJyQCMHDmSZcuW8dZbbxEcHEz79u3Jyspi165dTJw4kaeffpq+ffvSuXNnPvjgAyIiIggICGDHjh389ddfTJo0qUDSitJQrVo1Zs+eTUREBPXq1WPVqlWcOXOGhQsXmlrlnn/+eVavXs1LL73E3r17adu2LXq9ntDQUFavXs327dtLNEF4RkYGnp6eDBw4kICAAKytrdm1axfHjx/nyy+/BMDPz4/atWvz9ttvExMTg62tLevWrSs01uvy5ct07dqVQYMG0aBBA5RKJRs2bCAuLs40DcODqNVqtm3bxqhRo2jVqhVbt25l8+bNvP/++6YxbWX9P0lLS7tvy9y9EyuHh4fTr18/evbsyZEjR1ixYgXDhg17YJfkBg0a0KlTJ5o1a4ajoyMnTpwwpfLPN3fuXHr16kXr1q0ZN26cKZ28nZ0d06dPN603Y8YMtm3bRvv27Zk4caLp4UrDhg0LvP+K8u677/LHH3/Qq1cvXn/9dRwdHVm6dCnh4eGsW7eu0PhQQRAqiQrJpSgIglBCD0onD0hLliwpsL7BYJC8vLyKTF2dLy4uThozZozk7OwsmZubS40aNSq0H0kqnE5ekiRpx44dkr+/v2Rubi75+vpKK1asKJROPt+vv/4qBQYGSiqVSnJwcJA6duwo7dy50/T3e9PJF7ds+enki0r/XVSZi3Lu3DmpY8eOklqtlqpXry598skn0i+//FIoXbokGdPC9+jRQ7Kzs5PUarVUu3ZtafTo0dKJEyceeIz8dPJr1qwp8u+dOnWSbG1tpdTUVNNrf+WVVyQvLy/JzMxMcnd3l7p27SotXLiwwHbZ2dnSBx98INWqVcu03sCBA6Vr166Z1snIyJDefPNNqVq1apKZmZlUt25dae7cuQXS+efX170p2e9Xv0W9no4dO0oNGzaUTpw4IbVu3VpSq9WSt7e39MMPPxR6vRqNRpo9e7bUsGFD0znRrFkzacaMGVJaWtoDyyRJBVOL5+XlSe+8844UEBAg2djYSFZWVlJAQID0008/Fdjm4sWL0lNPPSVZW1tLzs7O0oQJE0yp6fPPq8TEROmVV16R/Pz8JCsrK8nOzk5q1aqVtHr16kJluNeoUaMkKysr6dq1a1L37t0lS0tLyc3NTZo2bVqh1OaP8z95kAelk7/7fZn/Pr148aI0cOBAycbGRnJwcJBeffVVKScnp8A+703j/umnn0otW7aU7O3tJQsLC8nPz0/67LPPTNMF5Nu1a5fUtm1bycLCQrK1tZX69u0rXbx4sVCZ9+/fLzVr1kwyNzeXfHx8pPnz5xd5Hbm3HJIkSdeuXZMGDhwo2dvbS2q1WmrZsqX0zz//FLu+BEEofzJJKqXR14IgCILwH9WpUycSExOL7CL5XzB69GjWrl1LZmZmRRfloaZPn86MGTNISEjA2dm5oosjCMJ/iGiLFgRBEARBEARBKGMi8BIEQRAEQRAEQShjIvASBEEQBEEQBEEoY2KMlyAIgiAIgiAIQhkTLV6CIAiCIAiCIAhlTARegiAIgiAIgiAIZUwEXoIgCIIgCIIgCGVMWdEFqGoMBgM3b97ExsYGmUxW0cURBEEQBEEQBKGCSJJERkYG1apVQy5/cJuWCLxK6ObNm3h5eVV0MQRBEARBEARBqCRu3LiBp6fnA9cRgVcJ2djYAMbKtbW1reDSgFarZceOHXTv3h0zM7OKLs4TT9R3+RN1Xv5EnZcvUd/lT9R5+RN1Xr5EfZef9PR0vLy8TDHCg4jAq4Tyuxfa2tpWmsDL0tISW1tb8cYqB6K+y5+o8/In6rx8ifouf6LOy5+o8/Il6rv8FWcIkkiuIQiCIAiCIAiCUMZE4CUIgiAIgiAIglDGROAlCIIgCIIgCIJQxkTgJQiCIAiCIAiCUMZE4CUIgiAIgiAIglDGROAlCIIgCIIgCIJQxkTgJQiCIAiCIAiCUMZE4CUIgiAIgiAIglDGROAlCIIgCIIgCIJQxkTgJQiCIAiCIAiCUMZE4CUIgiAIgiAIglDGROAlCIIgCIIgCIJQxkTgJQiCIAiCIAiCUMZE4CUIgiAIgiAIglDGROAlCIIgCIIgCIJQxkTgJQiCIAiCIAiCUMZE4CUIQqUWnhZOqiG1ooshCIIgCILwWJQVXQBBEIT72RGxg3f+fQe5JMfiqgXP+T2HTCar6GIJgiAIgiCUmGjxEgShUjoQfYCpB6ZikAzo0PFJ8Ce8e+BdsrRZFV00QRAEQRCEEhOBlyAIlc7x2OO8ue9NdAYd3Wt0p7u6OwqZgi3hWxjyzxDCksMquoiCIAiCIAglIgIvQRAqlfOJ53ltz2vk6fPo6NmRT9p8Qgd1BxY9tQg3Szci0iMYtnkYay6vQZKkii6uIAiCIAhCsYjASxCESuNKyhVe2vUSWdosWrq3ZF7HeZjJzQBo4tKENX3X0L56ezQGDTOPzGTqv1PJ1GRWcKkFQRAEQRAeTgRegiBUClHpUbyw8wXS8tJo7NyY77p8h1qpLrCOg9qBH7r+wFvN3kIhU7A1YitDNg8hNDm0gkotCIIgCIJQPCLwEgShwsVmxTJhxwQScxKp51CPn576CSszqyLXlcvkjPEfw289f8Pdyp3I9EiGbx7O6rDVouuhIAiCIAiVlgi8BEGoUEk5SUzYMYGbWTfxtvVmQbcF2KnsHrpdE9cmrOmzho6eHdEYNHxy9BPe+fcd0fVQEARBEEpJljaLH8/8yKJzi8jV5VZ0cao8MY+XIAgVJi0vjRd3vkhEegQeVh4s6rYIZwvnYm9vr7bn+y7fs+ziMr45+Q3bI7ZzMekiX3b8kvpO9cuw5EJVJEkSG69u5N/of7E2t8ZB5YC92h57lfHLQe1g/K5ywMbcBoVcUdFFFgRBqBCSJLE7ajezgmcRnx0PwPor6/kg6APaVW9XwaWrukTgJQhChcjWZjNx90TCUsJwUjuxqPsiPKw9SrwfmUzGqIajaOLahHf2v8ONjBsM3zKcKS2mMNh3sJhwWQCM59uMIzPYEr6lWOvLkGGnsisYkKkdsFPZGQO2u5bn/2xjboNcJjqSCIJQtcVkxvD5sc/5N/pfADytPdEatERnRvPyrpfpUbMHU1pMwdXStYJLWvWIwEsQhHKXp8/j9T2vcy7hHLbmtizsvhBvW+/H2meASwBr+q7hw4Mfsi96H58d+4zg2GBmtJmBjblNKZVcqIrC08J5a99bXE29ikKmYFTDUdiY25CSm0JqXqrpe2peKqm5qWRoM5CQTMsi0iOKdRy5TG4KxAq1ohUVvKntsTET56ZQunJCQkj8eT4ylTlqXz9UvvVQ+/qi9PAQD6KEB9IatCy7sIz5Z+eTq89FKVcyzn8c4xuNxyAZ+PHMj6y8tJLtEds5FHOI1wJfY7DvYNE7oARE4CUIQrnSGrS8ve9tjsUew1Jpyfyn5lPPoV6p7NtOZcd3Xb5j+cXlfH3ya3ZG7uRS0iXmdZpHQ6eGpXIMoWrZEbGDjw59RLYuG2cLZ+Z1nEczt2YP3Ear15KmSSsQkN0vSEvJM/6epc3CIBlIzk0mOTe52OVTypTYqexQa9Vorml4pt4zpikUBKGkUtetJ3bGDCSNBoCMrdtMf5Pb2qKuVw+Vr68pGFPVrYvc0rKiiitUIqfjTzPzyEyupl4FoIV7Cz4M+hAfOx/TOu+0eIe+tfvyyZFPOJd4jlnBs9h0bRMft/6YBk4NKqroVYoIvAShnEmSxN4be1l0bhFNXJvwRtM3CqVNf1LpDXo+OPgB+6L3oVKo+KHrDzRyaVSqx5DJZIxsONLU9TA6M5rntzzP283fZqjfUPHE9z9Ca9Dy7clvWXpxKQDN3Joxr+O8Yo0hNFOY4WzhXKLxhhq9hrS8NGMglh+Q3f5+v+U5uhx0ko6k3CQAZh6byS8XfmGs/1ieqfMM5grzR3vxwn+OpNUSN+sLUn7/HQDrzp2xbN6M3LAw8kLDyLt+HUN6OtknTpB94sSdDWUyzGvUuBOM+fmh8vXFrFo1ZHLRbfa/IDU3lW9OfcO6K+sAcFA58HaLt+nr07fIz0s/Rz+W9VrGuivr+ObkN1xIusDQzUMZ6jeUV5u8irW5dXm/hCpFBF7CEy84PJkPN4YQ4GnPm93qUc3eosLKEp4Wzuzg2Ry6eQiA80nnOXrrKHM6zKGuQ90KK1d5kCSJT45+wtbwrShlSr7q9BUt3FuU2fEauzRmdd/VfHToI/be2Mus4FmciDvB9DbTsTW3LbPjChUvITuBt/e/zan4UwCMbjiaN5q+gVJedh955gpzXCxdcLF0KfY2efo8UnNTScxK5Le9vxFMMDGZMXxy9BMWnlvIWP+xDKg3AJVCVWblLorBIPHTvqusPhFNJ18XBjX3wr/6wzONChVDl5hI9KRJ5Jw4CYDza6/i/PLLBQInSaMhLzycvLAwckPDjN8vh6FPSEQTGYkmMpKMHTtM68utrFDVq4fKz9fYMlbPF1W9eiisi57mQ6h6JEni7+t/M+/4PFLyUgAYUHcAk5pOwl5t/8BtFXIFg3wH0aVGF+Yen8uW8C2svLSSnRE7mdpyKt28u4mHnPchAi/hiXY8IpnRS4LJ1ui5HJfJX2dvMqZNTSZ2qoOdZfl158nWZrPg3AKWXVyGzqDDTG7GgLoD2Bm5k6upVxnyzxDebvE2Q3yHPJEXK0mSmHdiHuuurEMuk/NFhy/o4NmhzI9rp7Lj287fsuLSCr46+RU7I3eash42dBZdD59Ex2OP887+d0jKTcLKzIpP237KU95PVXSxiqRSqHCzcsPR3JF26nZ81P0jNoVvYsn5JcRlxzEreBaLQxYzuuFonvN9Dgtl2T80ytboeHvNWbaExAKw7Egky45E0rCaLYNbePF0QPVyvXYKD5Zz7hzRr72OLi4OubU11ebMwaZL50LryczNUfsagyi7fneW65KSyLt8uUAwprlyFUNWFjmnT5Nz+nSB/Zh5ehqDsXq+qHx9UfvWw6xGjYe2jl1Nucqb+97EKseKJplN8HZ4vDG9wuO5nnadT49+yvHY4wDUsa/Dx60/JtA1sET7cbZwZnaH2Txd52k+O/oZURlRTN4/mfbV2/N+q/fxtPEsi+JXaTJJzDhaIunp6djZ2ZGWloatbcU/NddqtWzZsoXevXtjZiY+DO92IiKZUb8Gk6XR09rHCb0kERxuHHthZ2HGK51rM7J1TdRmxR8UWtL6liSJbRHbmHdinikda0fPjkxpMYUatjVIyknio0MfcSDmAACdvDoxs81MHNQOj/CKK6+fz/7MT2d+AmBmm5n0r9u/2NuW1jl+PvE8b+9/m5jMGJRyJW83f5thfsOeyED3cVXF64okSSy9sJRvTn2DXtJTx74OX3f6mpp2NSu6aA91b33n6fPYcGUDv5z/hdgsYwDkqHZkVMNRDPEdgqVZ2YzJuZmaw4RlJ7hwMx0zhYzXutQlLC6DnRfi0OgNAKiUcnr6uzO4uRdBPk7I5VXz/VMVz/F73T2ey9zHB88ffkDlU+ux9ytptWgiIsgNu2wKxvJCw9DFxRW5vszCAlW9uneCMb/brWO375G0ei3DtgwjNDkUMD5wGNdoHGMajvnPdLOvCEWd47m6XBaFLOLX87+iM+hQK9S83ORlnm/wPGZyMwy5ucYW0OvXybt+HU14BJrr19FERCCzsMCqVUssWwVh1ToIMy+vAp+fefo8Focs5peQX9AatKgVal4MeJFRDUZhpqia77HiKklsIAKvEhKBV9VwMjKZkb8Yg652dZxZPKo5KqWcvWHxfLE1lMtxxkl2q9mpeau7L/0Dq6Moxg1ESer7csplZh0zdm8D8LLxYmqLqXT06lhgPUmSWHlpJV+d/AqtQYurhSuft/+cVh6tHvHVVy7LLixj7om5AExtMZURDUaUaPvSPMfTNel8fOhjdkftBqBrja7MbDtTdD28R1W7rmRoMvj40MfsitoFQB+fPnwU9FGZBSil7X71rdVr+evaXywOWUxMZgwA9ip7nm/wPEP9hpZqts6TkSm8uPwkiZl5OFmZM//5ZrSo6QhASpaGDadjWH3iBqGxGaZtajhaMqi5JwObeeFuV7VuoKvaOX63QuO5unSh2pzZKKzLdmyNLiWFvMtXCgRjeVeuIOXlFbm+spoH6nq+hNhnsJHTpFS3RWPvzHVDBADVraszpcUUOnt1Fg/AysC95/jhmMN8euxTbqRHYZ8FveSNGWrRAYubyaYAS3vzJhQzLFBW88CqVRBWQa2wbNUKM3d3wDik4tOjnxIcGwwYW9M+DPrwoUmNqjIReJUhEXhVficjUxj1azCZeTra1nFi8cgWWJjfadXSGyTWnYrm652XuZVmnIXdz92Gqb386FTP5YEfAMWp73RNOj+f+Zk/Qv9AL+lRK9RMaDyBUQ1HmcZqSJKELiEBhb09cnPjAPrQ5FCm/DuF8LRwZMgY6z+WVwJfqdIZztZdXsf0I9MBeLXJq7wY8GKJ91Ha57gkSfwe+jvzTsxDZ9BR3bo6czvMLfUkH1VZVbquXE65zFv73iIyPRKlXMm7Ld5lkO+gKnUj97D61hq0bL6+mUXnFhGVEQWAjbkNI+qPYHj94dipHm/81dqT0by/PgSN3oCfuw2LRzXH06Fw0CpJEuei01h14gZ/n7lJRp4OALkMOtZzYXALL7r4uWGurPxJGarSOX634oznKk+SXo8mMoq8y2HkhoaSd7uVTHvzZpHrG8zNyavnyWHbOEJccrjmIaNOw7a82/K9KtE6XZVosrLYvWIlDdxs+ffIn2Rdu0z1JInqyTIs8u5/6y+3tUVVqxbmPj6Y+9Qy/lyrFvrkZLKOHiPr2FFyzp4DrbbAduY1a2IZ1AqroCAsWrRgW+ph5p2YZ8ry2r9Of95q9tZDx49VRSLwKkMi8KrcTkWlMPIXY9DVprYTv4wqGHTdLVer57fDEfy49yoZucYbiCAfR97rVZ8AL/sit3lQfRskA39d/YtvTn1jutB08+7G2w1fxeFmhjG7lKnrxmUM6enGfvf+/lg2a4pFYFNo5MtXVxex9vJaABo5N2J2+9l42XqVUg2Vn23h25jy7xQkJMY0HMObzd58pJvhsjrH7+16OLnZZIbXH16lbtjLSlW5rvx97W9mHplJrj4Xdyt3vuz4JY1dGld0sUqsuPWtM+jYFrGNRecWcT3tOgBWZlYM8xvG8w2eL3EXZb1BYva2UBb+a9xX9wZufD24CVaqhw//ztHo2RJyi1Unbpi6cAM4WZnzbNPqDG7hRR3X0p2jTGfQEZEWgV7SP/6+dDoOHDxA+3btUSofb7i7hdKCGrY1HrtMD1Pc8VyVgT49nYzQC/y87l2sIhMISLPHLTYXKSen0LoZagivJseqUQBBnYdjF9gcMze3Cih11SNJEvqUlEJdA/PCw9FGR4PBUPSGcrlxvN7toMoUYPn4oHB0fOjnoCE7m+xTp8k+dpSso8fIvXCh0LFUvr4oWwSyzTGaxYoj5Khl2Kvsmdx8Mk/XfvqJ+qwVgVcZEoFX5XX6dtCVkaejtY8Tv46+f9B1t9RsDT/tu8ZvhyPQ6IwXjv819uCd7r7UdC6Ywel+9X0h8QKfH/mMW9fPUTNeIiDNnk6aWlhHJqKJiip20z2Aee3aJNdzY5X5Gc6455HpasUHQR/St3bfYu+jou2/sZ9Jeyehk3QMqjeID4M+fKSLrKTVkn39OntOnqTnc8+V+jmerkln2qFppi5qXby6MLPtzMduQajqKvt1RaPXMOf4HFaFrQKgtUdrZneYXWXHRualp7N79Wq6jRpVrPrWG/TsjNrJwnMLuZJyBTDe/A/xHcLIhiOLlQY/I1fL63+cZm9YAgCvdanDm0/Ve6QxW+GJWaw+cYO1J6NJyLjT7axpDXsGt/CiT+NqxQrmihKXFcfhm4c5EHOAozePkqHNePhGFWB4/eG82/LdMtt/WY3nKkvzjs9j6cWlOFs4s6HfBixRs3vZMlo6OqK9eImc8+fJvXQRtLpC2ypdXFA3aoRFI3/U/sYvpUPVfH+XBkmrRXPjBprw8EIBliEt7b7bZavgpiNkVXOgcbOeeDYMQuVTCzNvb1Nvm+JIysxDLpPhYFX0Nvr0dLKPHyfr6DGyjx4l78qVguWXy4iuruKEZx7nvWVYNm3K+x1m4GPvU+T+qhoReJUhEXhVTmdupPL84mNk5OkI8nHk19EtsDQv2Qd9dEo2X+28zIbTMUgSKOUyhrWqwetd6+JsbewimF/fPTt0QB8eQcqF05w6vAH95Wt4JUhYaoret8LZ2TRxpdrPOADZ3McH3c2bZJ88RfbpU+ScOo3m+vVC26ZaQZinDHlAQ/739Ns4Nm6KrBL/r4NvBfPyrpfRGDT8z+d/fN7uc+Syh3eD0SUm3tUqGGoc2H3tGmi1SAoFtv/rjfPo0agblO4kjZIk8UfoH8w7MQ+tQUs1q2rM7Ti3SraclJbKfF25lXmLyfsnE5IYAsBLAS/xUuOXUMiLnySnstClpJCy8neSV6zAkJqKZccOeHzwAeY1itd6YpAM7I3ay4JzC7iUfAkAtULNc77PMabhmPumto9IzGL8shNcjc9EpZQz97kA+gVUe/zXozewNyyBVcdvsDcsHr3BeHthaa6gT2MPBreoQdMa9g/uzm3Qcib+DAdjDnIw5iCXUy4X+LuVmRWWyscfuychkZebh0qtQsbjPXlPyDEGr3M6zKFXrV6PXba7SRoNcV98QcrvfwDlN57rcZ2MO8mYbWOQkPihyw909OpY5HVF0mjIvXyZs/+u49Khf3C/kYlXAiiKuDM18/RE3cgfC/9GxmCsYcMnKr29ITcXXXw8urg4NFE30IRfJ+96OJrwcDQ3boCucIAKgEyGWbVqmPv4IPf25KAynL80J4h2ksixUjGpxZsMrT/0ka+R+WPm9ZLEG13rMb59LcwUD/5M1yUmkh0cbOqaqI2MKvB3rQKuVpdh3rI57fq9jF2TZshKEAhWNiLwKkMi8Kp8zt5IZcQvx8jI1dGyliO/jSl50HW3izfTmbM9lH1hCcgkAz6aVMZXM9BBmYr+chgpZ89inpxc9MZmZqjq1CkYZNWrh9K5eBOx6lJSyDl9muyTJ8k5dZrc8+eR7ulHLanMsQpogkXTQCybNcOiSRMUNqXbpedRnU04y4QdE8jR5dDZqzNfdvqy0Bg1g0aD5tq1QkGWPimpyH3KLNRIObmm3y2bN8dx9CisO3dGpii9m+0LSRd4e9/bRGdGo5QpmdRsEiMbjHyiukMUV2W9rhyOOczUA1NJzUvF1tyWWe1nlcu0BKVNGxND0m9LSV27tlDXK5m5OY7jxuL8wgvILYqXPl6SJP6N/pcF5xaYAlJzuTnP1n2WcY3G4W7lblr38NVEJv5+itRsLW62KhaNbE5jT/tSe2354tNzWXcqhjUnbnA9Mcu0vI6rNYObe9G/aXXTA63YrFgOxBzgUMwhjt46Spb2zvoyZPg7+9OuejvaVW9HQ6eGpRJkl+Y5/t2p71gUsggrMytW91ldat0OdYmJRL8xiZyTlWM8V3Fla7MZsGkA0ZnR9K/Tn5ltZwIPr/NcXS5LLixh+anFVLuZR91b0CXLm5o3dejuuXEHjJM/+/hg4e9vah1T+fkhV5XvvHcPI+l06JKSjEFVfDzauLjbAVb8nWXx8Q9suQKQWVqiqlnTOPaqVk1UPj7Gn729kalU7I7azazgWaYMyt1rdCcwNZDBfQY/8jl+9kYqI24/1M5Xz82az/o3MiXfKQ7tzZtkHQsm++hR0o8cQopPLPB3SW2OdfOWtxN1BKFuUL9UP9/Lmgi8ypAIvCqXc9GpDF98O+iq6ciSMS0euUuLPj3dOP7q9jisxHMXMFy7irmu6GasJBuIdJGRWcOJoHaD8GvVE/OaNUu1NcqQl0fu+fNc+3czofs24B2Zi03uPSvJZKjq1TMGYk2bYtm0Kcpq1co9YAhLDmPM9jFkaDII8gji+y7fo0hKu12nd8a35YWHF/3kTibD3Nu7QKugqp4vuLqwd+FCGlwPJ3PHDtO2Zp6eOD4/ArsBA0rt6W+GJoPph6ezI9I4kWgnr0582vbT/1zXw8p2XTFIBhadW8SPZ35EQqK+Y32+6vRVlZsjJjcsjKTFv5C+ZQvojeOUVA3qYz9mDMdu3qL+0aPkHDkCGDOGuU19F5vuxZ+IVJIkDt88zPyz8zmTcMa4H7mSZ+o8w/hG49kTomX63xfRGyQCvOxZ9HwzXG3LNhuhJEkcj0hh1fEbbAm5RY5WDzId5laR1PK6gWQRyq2ciALbOKodaVOtDe2qt6NNtTZl0oW0NM9xnUHHuO3jOBV/ivqO9Vnee/ljT3pdlcZz3evTo5+yKmwVHlYerO+3Hmtz4/W5uHUekxnD3ONzTdlnHdWOvO37Mp2ya5B3/gK5ISHknD+P7tatwhsrlajq1TW2ijXyx6JRI1R16iB7zHF8RZEkCX1qKrr4hNsBVNxdgVWCqfVKl5R0/7FW95Cp1SjdXDGvXh3zWj4Fxl4p3dyKvBbEZMYw69gs9kfvB8DT2pMPgz6kpWvLxzrHL9xMY9iiY6TlaAnyceTZQE++2BZKcpbxnmhwcy/e7eV33+6H9yNJEpqICE5tXcb1PX9R51oOtvcM/ZPb2GDZsiVWrVphGdQKVd26lfohqAi8ypAIvCqPkOg0hi8+SnqujhY1HfhtTMtiBV2STocmKoq80NAC85TobhZxEQcMZuZE2bkTZu1IdPVkor2jiXKRgb0trwe+zsB6A1HKy34u8nRNOp8cmknIqW34RUu0TXKm8U0lhujC2aOUbm6mhB0WTQNR+/qWyQdPvvC4MD5eOQaH6DSaZTjTPq8GustX0aemFrm+3NYWte/t4Mq3Hmo/P1R16hT5hP/uc5ykJFJ+/4PUVavQ3346KLeywn7gABxGjMDc6/GTkEiSxKqwVcw5PgetQYuHlQdzO84lwCXgsfddVVSm60paXhrvHXjPNNfdgLoDeK/Ve499Y1teJEki+/hxkhYvJuvfA6bllq2DcBo/Hqs2bdDpdGzZsoVevXqRu38/cbNmma5HVm1a4/bBB6hq1y7RMYNjg1lwboFpglQZCjSpTchL7Mwz/gHMerZRieYwfFwxmTHsitjPxrDdXMs4iyS7KwW5JMPFvB49anWkT90u1HeqX6zuyY+jtM/x2KxYnvv7OVLzUhniO4QPgj545H2lrltH7PQZSFptlRnPle9wzGFe3GXMXru4++IC06KUtM4PxxxmVvAsItIjAAhwCeD9Vu/TwMnY3VyXmGgcJxZynpzzIeSGnEdfRG8UmUqFun79u8aMNcK8pvcDWw4N2dmmlqg7LVNxxt/jE4wBVXw8kuY+4wvupVCgdHFB6eqK0tUFM1c3489ubrd/N/4st7EpdoChNWhZdmEZ88/OJ1efi1KuZKz/WCY0moBaqX6sc/xyXAZDFh4lOUtDM28Hlo013l+lZGmYvS2UP4/fAMDRypz3e9dnQNPqjxQYZWmz+OnUj/x7YAX1I/Q0iZLTOFqBIrvgFAUKJ6c7c4gFtTJO2l2JAjEReJUhEXhVDudj0hi+2Pgkprm3A7+NbYn1fYIuQ04OaRs3khNy3tjicvXqg+cd8fUzBgO3AwOZVzVWhq7ihzM/ojHkIEkytKktqGf2HB/2akErH6eyfKkFSJLExqsbmRU8ixxdDvYqez71m0xgnCU5p06Rffo0uRcvFmpRkltaYtEkAIumzbBsGohFQAByq5L3jZckCd2tW7dbsIwtWVmXLqKLjEJe1JVELse8Vq2CQZavL0p39+J/uBRxjhtyckj7axPJy5bdGRcnk2HdtQtOo0Zh0bz5Y1+ULyZd5O39b3Mj48Z/ruthZbmuXEi6wOR9k4nJjEGlUPFBqw9KNPl2RZIMBjJ27yZp8WJyz54zLpTLseneHafx47Hwb2ha9976NuTkkLRoMUmLFxtv7JRKHEeOxHniyyVu3d0XcYx3935FlvwiADLk/M+nNxMaT8DHruwGtufp8zgZe5IDMQc4GHPQdPOcz97cCXv8iYrxIi3ZBwzGcVutfZwY3MKLnv7uZRoYlsU5fiD6ABN3TwTgy45f0r1m9xJtX2g8V9euVJv9RaUfz5UvXZPOs389S1x2HMP8hvFeq/cK/P1R6lyr17Li0gp+PvszObocZMh4rt5zvBb4WqG05JIkobt5k5yQ8+ReOG/8fv48hszMQvuVW1ujbtgQtX9DZHKFMbBKiEd7O8gyZBQ/iYvC3v52AOWK0s3VGES5uqK8HVyZubkaMwWWYte50/GnmXlkJldTrwLQ3K05HwV9VCBZxaOe49cSMhm84CiJmXkEeNqxfHwrbNUFtz8ekcwHG0JMc6K2quXIZ/0bUcf10c7V0ORQZh6ZSUhiCHKDRNccH8ZrW2F9LpzskyeRcgt29VF6eJhaw2y6dDFN1l1RROBVhipT4KWJjiHu668517gR3YcN+88EXncHXc28HVj6gKArc/9+Ymd+gjYmpsBymYUFqnp1CwZZ9eoVevMevXWUL459wbW0awDUd2iI8kZHTsfWJEdr7DrQ1c+VKT398HUvv3FWEWkRTPl3imlA/RDfIUxuPhm1Uo0hJ4eccyHknDpJ9qnT5Jw+XfiDR6FA7euLRbPbgVjTpoXS9xqys8m7csUYZIXenjAz7PJ9P5CyLOU4+AdiXd/fFGSp6tR57P72D/rwkAwGsg4dJnnpUrIOHjQtVzWoj+PIkdj27l2izE33ytRkMv3IdLZHbAego2dHPm376RM5D8ndKkPgte7yOj4/9jkagwZPa0++7vw1fo5+FVKWkjBoNKRv2kTSL7+iCQ8HjOO27J7tj9OYMZh7exfa5n71rblxg7hZX5C5Zw9gzPTmOuUdbPv0KdYDgMtxGYxfeoKo5GysbKJp2DCYS2nGSU1lyOhZsycTGk+grkPd0njpRKVHmZJiHI89Tq7+zs2SQqYgwCWA9p7taVutLb6OvshlcvJ0enZdjGfViRscuJJgSgBrq1byTGB1BjX3wr966Xf1Latz/KuTX7Hk/BKszaxZ3Xc1XjbFa4WvquO57vbBwQ/YdG0T3rberOm7BgtlwR4Mj1PncVlxfHXyK7aEbwHATmXH64GvM6DugAeO+ZMMBjQRkeSeDzEGYiEh5F66dN+Hr3eTWVreCaLuaZkyBVYuzuU6piwtL42vT37NuivrAHBQOTC5+WT61e5X6JrwKPUdmZTFoAVHiEvPo4GHLX9MCMLOsuhttXoDiw+E8+3uy+RqDZgpZLzUsTavdK7zSA9N9AY9ay+v5dtT35KhzUAukzPUbyivNHgBReh1so4cLXIOMZ8tWyq8RVgEXmWoMgVe0W9MImP7dgxmZji//BIu48c/1k1mVXDhpjHoSs3W0rSGPUvHtsRGXfiioI2LI+7zWWRsN94wKz08sO/fH5WfL2pfX8y8vB74gRabFcvc43NNY30c1Y5MajqJ3t692bZ1Gy3ad+XH/eH8efwGeoOEXAYDmnryVvd6eNgVb0D849LoNXx36juWXlwKGGeHn9NhTqGbKEmvJ+/qVVPCjpxTp4qc3NKsenUsAgONWabCQtFG3Sg6Db5SicrHB3mdWqyXTnHKNglNTQ++e24F7tbuhdd/TMX98Mi7epXk5StI++sv09MxhYszDkOH4jB4MEqnR2uZlCSJNZfXMDt4NhqDBncrd+Z2mEsT1yaPtL+qoCIDr1xdLp8d+4yNVzcC0MmzE5+2q/zj7PSZmaT++SfJS5ehSzBmuZPb2uIwdCiOz494YIKdh9V35v79xH7+uSkzmEWzZrh/9CFqv/sHorsvxfHGn2fIzNPh5WjB4pEt8HW34ULiBRacW8DeG3tN63bz7sYLjV8ocWCbo8vheOxxDsUc4mDMQdPkzvlcLV1NSTFaebTC1vzBn5kxqTmsPRHN6hM3iEm9M+ijYTVbBrfw4umA6ve9CSypsjrHtQYtY7eN5UzCGfyd/FnWaxlmigfvv9B4rrlzsOlcNcZz5dsTtYc39r6BXCZnac+lRV4fS6POj8ceZ1bwLNM0CvUd6/N+q/dLdD2WdDryrl4lJySE3IsXkSmUppYp5V2BltzKqtL0cJAkib+v/8284/NIyUsB4Nm6z/Jm0zfv+yCwpPUdnZLN4AVHiUnNoZ6bNX9MCMLJ+uFB5Y3kbKZtusCeUGNSD28nSz552p8O9YrOqvowiTmJzDk+h63hWwFwtXBlasupdPM2jne9ew6x3MuX8Zo/v8L/TyLwKkOVKfDKDQ3l1syZ5J46DYCZdw3c338f644dK7RcZeXizXSGLT5KaraWwBr2LCsi6JL0elJWriThm28xZGeDQoHjqFG4vDKxWF3rNHoNSy8sZVHIInJ0Ochlcob4DmFik4nYqewKXciuJWQyb3sYW8/HAqBSyhndtiYTO9XBzqJ8blgPxRzig4MfkJSbhEqh4u3mbzPYd/CDUzbHxhq7Jt5OZZ8XGlbk4F+FizPqesZugmpfY6ZGlY8P2TIt47eP53zSeVwsXFjaa2mxn+yWVEk/PHQpKaSuXkPKypXo4o0fBDJzc2z79sFx5CjUvvUeqRyXki7x9v63icqIQiFTMNZ/LP3r9i+z112RKirwupF+g7f2v0VocihymZzXAl9jrP/YMh/v8zi08fGkLF9Oyh9/mlqWlW5uOI4ahf2gQcVKd12c+jZoNCQv+Y3E+fONmRDlchyGDMHljddR2N0JSiVJYsG/15m9LRRJMnYB+nlEMxzvGQAflhzGgnML2BW5CwnjbUAnz068GPAi/s7+RZZBkiQi0iM4GHOQQzGHOB57HI3hzhgXpUxJoFugKdiqa/9oA+INBonD15L483gUOy7EodEbr00qpZxe/u5M6elHNfvHe8BVluf4rcxbDPx7IOmadEbUH8HUllPvu25VHs+VLyU3hWf+eobk3GTG+o/lzWZvFrleadW5zqBjVdgqfjz9o2let361+/FmszeLNYddVXM97TqfHv3UNF6zjn0dPgr6iKZuTR+4XUnqOzYtl8ELjxCZlI2PsxV/vhiEq03xE+9IksT2C7FM33SR2HTjg88+jT34uE+DR07gczjmMJ8e+5QbGcbxZO2rt+f9Vu9XyqRKIvAqQ5Up8ALQaDQc/PxzPHfvQX/7Kat15864vfduseeCqQou3Upn2KKjpGRraeJlz7JxLQv1Oc4JOU/stGnGMU6ARUAA7jNnoPb1LdYx/o3+l9nBs01PbZu5NeO9lu/h63hn+/tdyE5FpfDFllCCI4wDe+0szHi1cx2eb+1dLgPYk3KS+PDQhxyMMXa36+zVmZltZha7S5w+M4ucs2fIPXcOmUptyipYVCtRri6Xl3e9zIm4E9ir7Pmt52/Uti/+wP+SetQPa0mrJX37DpKXLiU3JMS03LJ1EI4jR2LdsWOJu/FkajKZeWQmWyO2mpY1dGpIj5o96F6zO9Wtq5dof5VVRQRe+27s4/0D75OhzcBR7cjsDrMJ8ggql2M/irzwcJJ/XULaxo2mKR/MfXxwGjcOu759SjQnTUnqW3vrFnFz5pCxdRsACgcHXN56E/sBA8jTS7y/PoT1p41dq4e1qsH0vg0xV97/PL+acpWFIQvZFr7NFIC1rd6Wlxq/RBPXJmRrswmODTZ1IYzJLNht293K3RRoBXkEYWVWuvMqpWRp2HgmhlXHbxAaa7zJrm5vwR8Tgqjh9OjzeZX1Ob7vxj5e2/MaAN92/pYuNboU+HtVH8+VT5IkJu+fzM7IndSxr8OqPqswVxR97pd2nSflJPHtqW/ZcHUDANZm1kxsMpEhfkMKTWNSFeXqclkUsohfz/+KzqBDrVDzUsBLjGww8qGtqFD8+k7IyGPwwiNcT8iihqMlq19sjbvdowVLmXk6vtpxmd8Oh2OQwEalZEpPX4a18kbxCJOz5+pyWRyymF/O/2KqgxcDXmRUg1HFqoPyIgKvMlTZAi/ThL4dO5K6cBHJy5aBTofM3Byn8eNwmjCh2HPBVFahsekMW3SM5CwNAV72LL8n6NJnZJDwzbek/P47SBJyW1tcJ0/G/rmBxbqxvpF+gznH57Aveh9gbNae3HwyvWr1KlGfaUmS2BMaz+xtoaYBp9XtLXirWz2eCaz+SBedkjBIBlZeWsnXJ79Ga9DiauHKrPazaOnRstSOodVreWPvGxyIOYC1mTWLeyymoVPDh2/4OMd8zA9rSZLIOX2G5GXLyNixw9SyZ+7tjcPI57F/5pkSJRqRJIltEdtYd2Udx2OPY5DutBT6O/mbgrBq1o8/IW1FKc/AS2/Q8+OZH1kUsgiAxi6N+bLjlwXmnqpMckJCSFq0mIydO01dcS0CA3GaMB7rTp0eaUzOo9R31tGjxH76KZqrxvGnigYN+a7h0/yjdUQhlzGtbwOeD/IudotTeFo4i0MWs/n6ZvSSMdV9Hfs6RKZHojXcGU9hJjejmVszU7DlY+dTLt18JEniXHQab646w/XELDzs1PwxIYiazo8W6JXHOT7n+ByWX1yOjbkNa/uuNV0TnoTxXPm2XN/C1ANTUcqUrPzfSlPGwXslZebx876rqJOv8cbQXqVa5+cSzvH5sc+5kHQBMJ6377V8r1Q/+8pbabT2FOccT87SMHThUcLiMqhub8GqF4PwdHj8CcrPx6Tx/oYQzkUbsw8HeNrxWf9Gjzxe81Fb/cqLCLzKUGUNvPLfWHnXrhH32WdkHb5rLph338WmW/HngqlMwmIzGLrImNK0sacdy8e1MnXhkySJjG3biPt8lmlMhW2/vrhNmVKsCYtzdDn8EvILS84vQWPQoJQpeb7h87zY+MX7PrUtzoVMb5BYdzKar3ZeNjW5+7nb8G4vPzrWcynz/8OlpEtM+XcKEekRyJAxrtE4JjaZ+NhPAPUGPVP+ncKOyB2oFWoWdFtQLhe90rxB0sbEkLzyd1LXrDElCZHb2GD/3HM4jhiOWbWSBUtJOUnsjtrNjogdHI8rGIQ1dm5M95rd6e7dHQ9rj8cqd3krr8ArKSeJqQemcuzWMQCG+Q3j7eZvV6onmWC81mQdPEjSosVkBwebllt36oTThPFYNmv2WPt/nFbd5JUrifvue2TZ2QDs8Qki8JP3adPs0RJm3Ei/weLzi9l0dRM6yZgdtbp1ddpVb0f76u1p4d4CS7PHvzF7VPHpuQxbfIyr8Zm42ar4fUIQtV1K3kpUHue4Vq9l1LZRhCSG0NilMb/1/A3d+UtVfjxXvvjsePr/1Z90TToTm0zk5YCXi1wvIjGL0UuCiUjKxlwu8fer7fCtZl+qZTFIBtZfWc+3p74lNS8VgJ41ezK5+eRK+xAnX7Y2m4j0CMLTwolIjyAkMYRDMYcA44Pgd1u9y1M1nirxvcPDzvG0bC3DFh/lws103GxVrHqh9SM/yCiK3iCx8lgkc7eFkZGnQy6D0W1q8Vb3evdNiPYgkiTxz/V/mHdiHsm5xt5FDxvnVl5E4FWGKnvgBbcDkp07ifvii7vmgmmD24cfoPIpu/TBpe1yXAZDFx4lKUtDo+p2rBjXyjSwWnPjBrEzPyHrgHFeHHNvb9ynT8OqdeuH7leSJHZH7WbO8TncyjLWT2uP1rzb6t2HplcuyYd1rlbPkkMR/LTvKhm5xhuY1j5OvNfbj8ae9g8t5+PI1mYz5/gcU+ajxs6N+aLDF488HskgGZh2eBobr27ETG7GD11+oE31NqVZ5PsqixskQ1YWqRs3krJsOZrISONChQKbbt1wHDkSi8AmhT7kJEkiMimbw9eSOBaehJOViomda+N8e/BxYk4iuyN3sz1yOydiT5i6bYGxFaeHt7ElrLLfBED53JSeiT/D5P2Tic+Ox0JpwfTW0+nt07tMjvWoJJ2O9K3bSPrlF/JCQ40LlUrs/vc/HMeNRV3v0cYL3utx6vufczf5dPkBhp35h243TgDGpB4ur7+Ow5DBjzx/X0xmDGfjz9LAqQHetsVvOSsPCRl5jFh8jLC4DFxsVPw+vhV13UqWVba8Hi5EZ0Qz6O9BZGgz+CixDY1/O1Klx3PlkySJV3a/woGYAzRwasCK3iuKfLh3KiqF8UtPmCbdBajvbsPGV9uiUpZ+N/y0vDR+OP0Dqy+vxiAZsFBa8ELjFxjZYOR9u0CWB0mSiMuOIzwt3BRg5f8clx1XaH25TM4wv2G80uQV0wTUJfWgczwjV8uIX4I5eyMVZ2tz/nyh9SOngn+YuPRcPvnnIv+cM95vuduqmd6vAT0aFn9ambvdndlRLpPz5//+pL5T/dIudomIwKsMVYXAK59xLphFJC3+5Z65YCYWa8B3RboSZ2zpSszU4F/dlpXjjClNJY2GpF+XkPjzz0h5ecjMzHB68UWcJowvVkrX66nXmRU8i6O3jgJQzaoaU1pMoUuNLsW6ADzKh3VKloaf9l1l6eFI0yDxPo09eKeHL95OZft/2BGxg+lHppOhycDKzIoPgz6kj0+fEu1DkiRmH5/NyksrUcgUfNnxS7p6dy2jEhdWljdIksFA5v79JC9bRvaRo6bl6saNcRw5kqzW7TkSkc7ha0kcuZbIzbSCc4nYqJW81a0ezwd5o1Tc6SaUmJPIzsidbI/Yzqm4UwWCsCYuTehRswfdvLvhZlUwhX9lUaZ1Lkn8Hvo7847PQyfpqGlbk687fU0dhzqlepzHYcjJIXXdepKXLDFNRSGztMThuYE4jhpV4pbRh3mU+jYYJL7ZdZnv9hjn8elYz4V5DWRkzJlF3kXjNBMqX1/cP/oQy+bNS7W8lUFSZh7DFx8jNDYDJytzfp8QVKIpPcqzO+3uq9sJ+ehNepw2Xgeq6niuu62/sp5ph6dhLjdnVZ9VRb5/t1+I5fU/TpOnM9Couh2f9KvP8EVHyNLJGN2mJtP7lV039dDkUD4/9jmn443Jx7xtvXm35bu0q96uzI4JxjFJkemRhKeH3wmy0iKISI8gR5dz3+0c1Y7UtK1JLbta1LKrRetqrann8HgPdu53jmfl6Rj1azAnIlNwsDTjzxdal8t0OPsvJ/DRxvNEJRtb57v6uTK9X0O8HB+tBf10/GnOJ57n+QbPl2YxH4kIvMpQVQq88mmiooxzwew1pg8u6Vww5e3uoKthNVtWjm+FvaU5WcHBxM6YieaacUyDZesg3D/+GFWthz8xzNJmMf/sfFZcXIFO0mEuN2dso7GM9R9baK6RB3mcD+volGy+2nGZDWdikCRQymW0ru1EAw9b/DxsqO9hi4+z9QMHwj+KW5m3ePfAu5yKPwVAX5++fBD0QbEHwf9w+gcWnFsAwOftPqdv7b6lWr6HKa8bpNywy9z6ZQnZWzcjv50sIVFtx98+bdhaM4gMcyvMFXICa9jTqpYje8LiOR+TDhi7kk7v15CgIibTTshOMAVhp+NPFwjCmro2pXvN7nTz7oarpWuZvbaSKqs6z9ZmM/3IdFOa4G7e3fik7SelnpDhUelSUkj5/XdSVqxEn2JM2axwcMDh+RE4DhuGwt6+TI5b0vrO1uh4a9VZtl0wZlOd0L4W7/aqj0IuQ9LrSV2zhvivv8GQZhxfYdu3L65vv42ZW+U5x0pDSpaGEb8c48LNdBwszVgxvhUNqxVvDEl5XVd0CQlET3rTNJ7rr06WjJr7Nx42VXcMaExmDM/+9SzZumwmN5vMaP/Rhdb57VA4M/65iCRBFz9Xvh8aiLlcYu7KrSwMNbZ0LR7ZnKcalN3Dp/yuaV+d/IrEnETAmHhqSospj5UZT5IkEnMSi2y9upV1q8A1/m5KmRJPG09q2dWipl1NatnWMgVaZTFdRlHneK5Wz5glxzlyPQlbtZLfJwSVyTx595Or1fPj3qvM338NrV5CbSbnja71GN++FmaKqjfGMd8TGXh99tlnbN68mTNnzmBubk5qamqhdaKionj55ZfZu3cv1tbWjBo1ilmzZqG8q6vFvn37eOutt7hw4QJeXl58+OGHjB49utjlqIqBV76MffuI+3wW2qjbc8E0b4b7hw+eC6a8XY3PYMjCYyRmGifv+31CK6zzsoifM5e0DcbMRQonJ9zenVqswPFa6jX+uf4PG65sICk3CYBOXp2Y0mLKI3W7K40P64s30/liWyj/Xk4o9DczhYzaLtbU97ClvocNfu7GoMzFWvVYQbLOoGNRyCLmn52PQTLgZePFnA5z7ps2Ot+S80v46uRXAHzQ6gOG+A155DI8qrK8QUrP1RJ8PZnD15I4fC2R0NgM7PIy6B1+lD7hh3HMM44D05mZo+vaC5+XxmLnZ3wKqTdIrDp+gznbQ0nNNgZq/QKq8X7v+vfNCBWXFceuqF2mICyfDBmBroGmljAXy0eb/6S0lEWdX0+7zlt73+Ja2jWUMiVvNX+LEfVHVIqHP9qYGJJ+W0rq2rXGVO2AmacnjmPHYN+/f5knKCpJfUenZDNh2Uku3UrHXCHn82cbMbBZ4ZtIXUoKCd98S+rq1cakQ5aWOL/yCo7PjyhRxsXKLi1by8hfj3E2Og07CzNWjGtFI8+H30iWR+CVc/Ys0a+/YRrPtfw5J/5yj6Gpa1N+6fELSvmjdQOtSAbJwIQdEwiODSbQNZAlPZYUmMDYYJCYtfUSiw4YJw8f1qoGM/s1RKmQm+r8ND78diQKB0sztr7R4ZEz6BVXpiaT+Wfns/LSyhI9eNXoNUSlR5laryLSbgdY6eFkabPuu52tuS0+dj7G4Mqulqkly9PGs1yzLRbKAaDTM2HZSf69nIC1SsnycS0JrOFQbuW529X4DD7YcJ5j4caxWr5uNnzW35/mNR0rpDyP64kMvKZNm4a9vT3R0dH88ssvhQIvvV5PkyZNcHd3Z+7cudy6dYuRI0cyYcIEPv/8cwDCw8Px9/fnpZdeYvz48ezevZtJkyaxefNmevToUaxyVOXAC8CQl2ecC2bBgjtzwQwdisvrrxWYC6YiXI3PZMjCoyRm5lHfw5aVY1sg37GZ+Dlz0d9+cms/eDCub735wLIm5iSy5foW/rn+D5eSL5mW17CpwdSWU+ng2eGRy1iaH9bnY9I4G51K6K0MQmPTCb2VQUaersh1nazMqe9hi5+7DX63v9dxtS5xqvpTcad498C73Mq6hVKm5NXAVxnjP6bIeZJWh63mk6OfADCp6STGNRpX8hdZCkqzznM0ek5E5gdaSYREp2K45wro525Dm9rOtKlhg//lYLJ/X0nepTvnkVX79jiOHIlVu7bIZDJSsjR8uTOMlceikCSwNFfwWpe6jGtX64Gtl7FZsaaWsLMJZ03LZcho5taMHjV78JT3UxUyL01p35TuiNjBR4c+IluXjYuFC/M6zqsU2ahywy6T9Mti0jdvAb0xk5+qfn2cxo/DtkePRx4fVVLFre8TEcm8tOIkiZkanK3NWfB8M5p5P/hGJef8BeI++YScs8ZzzNzHB7cP3se6bdtSfQ0VKT1Xy6hfgzkdlYqNWsnyca1o4mX/wG3KOvAqan6uOGcFg/4ZRJY2iwmNJvB609dL/bhlbeWllXwR/AUWSgvW9l1LDds709bkavVMXn2WzSHGsTxTevrycsfapocr+XXetXtPBi8K5sLNdFrVcuT3CUFlnvUXjA9iZwXPMiXzyR9q0MS1SYGWq/zvMZkxBRIm3U0uk+Np7VkgsMpvyXJQOVSOB0p3neOSTMHElSfZdSkeCzMFy8a1pEUFBzmSJLHuVAyfb7lkGgM4pIUX7/byw96yaj0ceiIDr3y//fYbkyZNKhR4bd26lT59+nDz5k3c3IxN1/Pnz2fq1KkkJCRgbm7O1KlT2bx5M+fPnzdtN2TIEFJTU9m2bVuxjl/VAy/TdkXMBeM6+S3snn22QlLaXkswBl0JGXn4uduwrIsz2XM+J+eEsXuGytcX9+nTsAwMLHL7bG02e27s4Z9r/3Dk1hHTxVIpU9LOsx19fPrQ2avzYw+uLeuxL9EpOYTGZhB6K51Lt4Ox8KQsinqXKuQyartYmVrF6rvbUt/DFjfbB7eOpeWlMfPITHZE7gCglUcrPm/3eYGubv9c/4f3D7yPhFThNwiPU+canYEzN1I5fC2Rw9eSOB2VglZfsDJ9nK1oXduJNrWdCfJxxMm64FhBSZLIOXGCpKVLydy9x5RCXFW3Dk4vvoRtr57IFArOx6Tx8V/nORWVatrvtH4N6Vjv4a1XsVmx7IjYwfbI7ZxLOGdaLpfJae7WnO7e3enq3bXcgrDSOs+1Bi3fnPyGZReXAdDCvQVzOsyp0ElO8/+fiYsXk7X/X9Nyy6AgnMaPx6ptm3K/aSpOfa8+cYMPNoSg1Us08LBl0ajmVC/mJMKSwUDaxr+I//JL9EnGln+bbt1we3cqZtWfjLnnMnK1jFlynBORKdiolPw2tiXNvO//NL+sruWSRkPsrFmk/vEnUHg817aIbbyz/x1kyJj/1PxyS1JUGiLSInju7+fI1ecW6gGRkqVhwrITnIhMwUwhY95zATzdpOC5dXedR6dp6PPdAbI0et58qh5vPPVoWThLSpIkdkbuZO6JucRmxT50fRszmztdA+8KsrxsvCo0WUdx5Nd39x49mbzuPFtCYlEp5SwZ3YI2dSrPRNMpWRq+2BrKqhPG1PmOVuZ80Ls+zzatXikC2OL4TwZeH3/8MZs2beLMmTOmZeHh4fj4+HDq1CkCAwPp0KEDTZs25ZtvvjGts2TJEiZNmkTa7RaVe+Xl5ZGXl2f6PT09HS8vLxITEytN4LVz5066dev2SB8e2ceOkTjrC9O4KZW/Py4fvI/a/8Fd0EpTeGIWI349QXxGHv6O5nyvP03uyuXG+cgs1Di+8gr2w4Yhu+f16Q16guOC2RK+hT3RewoMXG3k1Ij/1fof3Wp0w0Fdek3pj1vfjyJHo+dKfCZhcRlcis0kLDaDsLgM0nKKbh2ztzDD190aXzcbYwuZuzV172kdkySJv67/xZwTc8jV52Kvsmdaq2l09OzIvuh9vHPgHfSSnsH1BjOl2ZQKvfiVpM71BokLN9M5cj2Zo+HJnIxMIUdb8Imlh52a1j6OtPZxpFUtRzxK0M1FeyOa1N9/J33DBqQsY1cTs5o1cZgwHpvevZHkCv46e4s5Oy6TmGl8gtetvivv9aqHVzHnRrmZdZPdUbvZGbWT80l3HhLJZXKauTajW41udPXqWqrn9b3uV+e5ulxS81JJ06SRmpdKam4qKXkpBZfd85WnN14/R9UfxSsBr5R79yq9Ts+1E+eJ+vcomrNncLgeikumMfiQZDISA9ugHDaC2u1a4GhVMTdSDzrHdXoDc3ZcYclhY/bNHg1cmTPAH0vzktejPj2d5J9/Ju2PP0GvR6ZW4zBuHPZjRhcrOVFll5Wn44UVpwmOSMHKXMGi55vSombR75OyuJbrEhOJnTyZ3FPGbsSOEyfi8OILhR5mfh78OWuvrsVB5cCfvf6s8K7FxaE36Bm7cywhSSEEuQfxY+cfTZ8LUcnZTFh+iuuJ2diolfw8rAmtahVuTbm3zjeeuck7684jl8GKsS3u+78qCzm6HJZcWMKyS8vQGrRUs6pGTduaeNt6G4Mr21p423rjpHaqMjf/99JqtWzfsZNdmdXZfD4OM4WM+cMD6VC38gRddzsRmcLHmy5yJd742dqqlgMz+jagtkvlGAP8IOnp6Tg7O/+3Aq8XXniByMhItm/fblqWnZ2NlZUVW7ZsoVevXtSrV48xY8bw3nvvmdbZsmUL//vf/8jOzsaiiH7806dPZ8aMGYWW//7771haVtxcJgCZhky0khZ7uf3jXRj0euwPH8Zp5y4UeXlIMhnpzZuT2LMH+jLOuhSfAz9cUJCmldE99RITz25AlWLs85vZoD7x/Z5G52BvWl+SJG7pb3FWe5azmrNkSpmmvznKHQkwC6CJeROcFIWTHDxJJAnSNBCTLeNmNtzMknEzW0Z8DhgofC7IkHBRQzUrieqWEtUsjT/rFAmsyVnNLb2xa0gjs0Zc1F5Ej55As0D6W/YvshtiZWGQIDYbLqfLuJIm41q6jBx9wddvrZSoaydR7/aXkwoe93NUnpOD/eHDOBw4iOL2mCCNoyPJnTuT3jSQHJRsi5bz7y0ZBmSYySS6VjfQtZqEeQl6h6boU7igvUCINoQYfcyd4yOnlrIW/mb+NDBrgJW85B9MWklLjpRDlpRFtiGbbMn4dffvWVKWcR1DFtlSNlq0D9/xPSxkFvS36E8D86InVi1t2lwt6VdvILsaie2NSGrERWKtLZhNTCNXsqtGc9bV6chN6zs3vXZmEtWsbr8/LI0/u6mhosZ8Z+tg6WU5oWnGAvT0NNDD08Dj9soyvxWL66a/sLxuHIejcXQkoW8fsurXf/w3RwXL08OiUDlX0uWYyyVe8DNQ167sb3PUUVF4LF+BWXo6epWK2KFDjPVZBK2kZUHGAmINsdRS1mKMVdHdvSuTf3P/ZUfuDlSoeM32Nezl9gBEZsLCUAWZWhn25hIv1dfjUYJboxVX5BxPlGNvLjGlsR6rcp7CTysZr2lmsso1d2BpMEjw5zU5xxLkyGUSY+sZaORYuW/5dQbYd0vGtmg5WoMMhUziqWoST1U3lOizs7xlZ2czbNiwyh94vfvuu8yePfuB61y6dAm/u5I/lHfgVZlbvH67+BvfnfkOW5ktLau3pKlbUwJcAqhrX/eRnirrEhNJ+vprMjb9DRgnlnV87TXsnhtYJmMdIpOyGf7rcXRx8Uy+vJnAcGPWPaW7O87vvYd1lzuTSsZmxbI1YitbIrZwLe2aabmduR3dvbvTu2ZvGjs3LvMnUxXR4lUSeVo9VxOyCI3NICzO2Dp2KTaDlOyib5ht1Erquqkw2G/hmmaraXk7j8583OITLMzMMVPIMVPIKuyp3911rlQqiUrO4cj1ZI5cT+JoeDLJWQVfm41aSauaDgTdbtWq62pdZmU3ZGaS9ucqUpYtw3A7C56yWjUcxo3F9plnuJqiYebmSxwNN/7N017NB7396OpX8om0YzJj2Bm1k51ROwuMXVTIFLRwa0G3Gt1o4NSAdE36nRan3NvfNYVboh6U2vhBlDIl9ip77FX22KnsTD/fvcxB5WD63cXCpUy75CREx3F592HSgk+gvnwBj7hIzCR9gXVyFebcqlYbXX1/HIOa49SiGdeyJMLiMgiNzSQ0NoMbKUXXR37CGz83a/w8bG63JFub5m8rDUVdVyKSsnhxxWmuJ2ajNpMz51l/evmX3hxwkiSRuX07iXPnoY+PB8CyfXucp07B3Nu71I5TEXK1el7+/QwHryahNpMzf3ggbWsXfBhXmtfy9A0biP/kU9BqMatVC49vv8H8Idl2I9IjGL5tODm6HCb4T+DlxkVPPlwZXEm9wohtI9AatMwImkFfH2Nm2z1hCUxadZYcrYH67jYsej4QN9v79yAoqs4z83Q889NRIpOz6VbflR+HBlTZFqbKRJIkPt50gT9P3EQug28GNS7V60dZu5GSzYx/Qtl/2ZiNsoajBTP6NqBdncr5UL3KtHglJCSQdLu/+f34+PhgflcGpvLuanivyjTGa97xeaYsPXezVFrS2KUxga6BBLoG0tilcYnSNWefOkXsJ5+aEgqUxVwwEYlZDJ1/mBZndzMmdDtqbS4oFDiOHInLq68gt7IiQ5PBrshd/H397wIT0prLzeno1ZG+Pn1pV70dZoqKyxJUFUiSREJGHpfyx47dSic0NoOr8Zno7sosobAKQ+2+CX1OdXJvDQKpYLBtrpRjrpCbvpspZcbvCjkqpXG5maLgd5Xizs93LzdXyO7aT8H9FlhPKUdmMLB21yGybbw4dj250FxaFmYKWtRypE1tJ9rUdqJhNbtyGah9N0N2NimrVpP0yy/oE40fFEo3N5zGj8du4AC2Xknhs82XuHW77B3ruTCtbwN8XB6tRflG+g22R25nR8SOAkFYSSlkijtBk9reGDCp7bE1s+XWtVu0btIaJysn03J7lT3WZmUXyD6MwWDg+umLhO85TPbp09hdvYhbenyh9VItbEmo6YeycRO8OgTh27Yp5uoHB0qZeTrCYo2Jbi7dSr+d9CaDzPskvHG2vivhze1xlnVcrR9pUth7rysHriTwyspTpOfq8LBTs2hk8zJL+WzIyiJx/nySflsKWi0yMzMcx47F+cUXkFdwr47HkavVM3HlKfaExqNSylk4snmB8ZaPey2XdDoydu0ieekyck4buxaWdH6uf67/w3sH3kOGjIXdFxLkEVTicpQ1rV7L8C3DuZR8iU5enfiu83fIZDJWHI3k47/OY5CgQz0XfhreFGvVgx/Q3q/OQ6LTePbnQ2j1EjOfbsjI1jXL+FU92SRJ4pN/LvHroXBkSMwd2JiBzWs8fMNKRpIktp2PZfrfF4hLNzaA9Auoxod96uNqU7aZMEvqPznGKz+5xq1bt3B1NSYJWLhwIe+88w7x8fGoVCqmTp3Kli1bCAkJMW03bNgwkpOTq2xyjfScdH7d8isqHxXnks5xNv4sGdqMAuvIZXJ8HXxNgViga+BDJ26V9HpSV68m/ptvS30umMikLN79fBVDDv1BvdRoANQBjfGYMQNFvdocijnEP9f/Yd+NfabxIQDN3ZrTx6cP3Wp2w9a8Yuq+KgZe96PRGbiWkHn7RjODS7fSCYvNID1Xi0ZnKJTtr7IwU8gIrOFAm9pOtK3jTICnfanPffaoDLm5pK5eQ9LixehutyIoXJxxGjsOVf9n+fnYTRb9G45Gb8BMIWN8ex9e7VwHq4fcsDxIVHoUOyJ3sCNiB3HZcaZAykHtUOhnB7XDnRYptT02ZjZFBlGV5TzPzc7h0v7j3Dx4FEPIWVwjw7DNK5zG+ZZDNTLqNMCqeVPqdG5LDf86yEshSVB+wpv8hxWhD0l4o5Tfbh27PRVE/dvz87naPDjhTX599+rVi9+Px/DJ5kvoDRKBNexZ8HyzcrnJyLseTtznn5N18KDxtbi74zZ1CjY9elRIwqXSkKfT8+rvp9l5MQ5zhZz5zzeli5/xs+9Rz3F9ejqpa9aQvGIlulvGLtqYmeH88ks4v/RSietq2uFprL+yHie1E2v7ra3QxDNF+fHMj8w/Ox97lT0bnt6Ao8qJOdvDmL/f2PNkcHMvPu3vX6w5mB5U578cDOeTfy5irpSzcWJbGlSr+PurqkiSJGZvu/P/GVpbz8zRvar0/UpGrpavdl5m6eEIDJKxV8uUHr4Ma+Vd7g9Z7+eJDLyioqJITk5m06ZNzJ07lwMHDgBQp04drK2tTenkq1Wrxpw5c4iNjeX5559n/PjxhdLJv/LKK4wdO5Y9e/bw+uuvV9l08tcSMrkWl0586HEGP228kBkkA1dTr3I67jSnE05zOu40N7NuFtq2mlU1At0CCXQJpIlrE+rY1ykwF0c+XUoKCV9/Q+qaNaUyF0xkZBx/v/ExnUMPIEdCZm2N6+S3uNHFj3/Ct7AtYhupeamm9X3sfOhbuy+9a/WmmnXFTzhZWW5Iy4NOb0Crl9DoDGj0t790BrS3v+f/XuQyvQFtgd+lguvd/jnvnvW0BdbXG5drDailXHoG+tCungvNvR2xqMydvTFO25C2fj2Jixahu2m8OVM4OuI4ZjTpPZ5m5u4I9oUZ53Fzt1Xz/v/q07exR6XpYlNR53nKrUQu7jpA0tETmF8KwSM2HHNDwRYnjVzJTQ8ftH7+OLVuQf2ubXH0KN/kBNkaHZfjMgm9HZBdvJVO6K100nOLbh1zsDS7HYjdyUBa1+1OwhutVsumf7YQrPdm1QnjWL5nm1bn8/6NSjxlxOOQJInMPXuM8z3GGMth5umJ3bP9se/fHzMPj3IrS2nR6Ay8/sdptl2IxUwh48dhTene0L3E53heeDgpy1eQunEjUnY2cHty7aFDcBg6FKXLo52DObochm0extXUqwR5BDH/qflFfhZXhAuJFxi+ZTh6Sc+8jvPo5PkU76w5x6azxnuKyd3q8WqXOsW+bj2oziVJYtzSE+wJjae2ixV/v9bukRLI/Nd9s+sy3+y6AsD0vvVxSAx5Yu5Xzsek8f6GEM5FGxsDArzs+ewZ/3KdAPp+nsjAa/To0SxdurTQ8r1799KpUycAIiMjefnll9m3bx9WVlaMGjWKL774otAEym+++SYXL17E09OTjz76qMpOoPzVjjC+23MVMGZqC/C0p7GXHQGe9vhXt8POwvhGi8uKMwVhp+NPE5YSVmhuChszGxq7Nqapa1MCXQPxd/YvMLFgzvkLxH4yk9yzxlTXJZ0LRpIkItZtIvazWdjnGN80+q4dOfZcHdYn7yUqI8q0rpPaid4+venj04f6jvUrzc0o/LcCr8qiKte5pNGQ+tdfJC1chPaGMVWuws4Oh1GjONeiO9P3RnIj2Ti2qFUtR2Y+7Y+vu01FFhkonzo3GAxEnb/KtT2HyDx5CpurF/BIuVVovXSVFfHevsgbN6Fa21bU79gCtWXZTmj8KCRJ4lZaboEW5NDYDK4nZBbZeiyXgY+LNX7uNtRzteKvY1e4liFDJoP3evkxob1PxXXpzM0lafEvJP/2G4bM2wmM5HKs2rbFfsAAbLp0rlKTMGv1BiatOsPmc7dQymV8NzSQbn7ODz3HJUki++hRkpcuI3PfPtNyVb16OI4aiW2fPqWSDfJa6jWGbh5Kji6HV5u8yosBLz72Ph9Xri6Xwf8M5nradXrV7MUHLT7jheUnOBaejFIuY/aAxgwoYuLuB3nYdSUpM49e3x4gPiOPwc29mD2wcWm9nP+En/ZdZc62MAA+/F99RgV5VdnPzvvRGyRWHI1k7vYwMvN0yGWwcnwQrWtX7NivJzLwqiwqU+C1+MB1fj8WRXhiJlIRmex8nK1o5GlHY097AjztaFjNDgtzBVnaLM4lnON0vDEQO5twttBAe6VMSX2n+jRxbUJT16Y0cW2Ck8rROBfMvHnok42ZB226d8dt6pQHzgWjuXGDiI+moz96GIA4O2u2D3LjH+dI0zoWSgu61OhCX5++tPJoVe4pp4urKgcBVdWTUOeSTkfaP/+Q9PN8NJHG815uY4Pt8OFs8GnPt8Fx5OkMKOQyRrb2ZtJT9UwPTipCWdS5NjeP0IMnuXHgGPqzp3GODMM+J73QenF2rqTVboBl06bU6twan8AGpdJtsKLkavVcics0zcuXP4asqIQ31iol3w8NpLPf43XnLi2GnBwydu4kdc1aso8fNy1XODhg168f9gMHoKpbPvMvPS6d3sDkNWf568xNFHIZXw5shOzGqSLPcUNeHul//03y0mXkXbliWm7dqROOo0ZiGRRU6kHxxqsb+ejQR8hlchZ3X0wL9xaluv+Smnd8HksvLsXZwpkfO/7O6ysvczU+E2uVkvkjmtHuEVKSF+e6cvhaIsMXH0OS4LuhgfQLqPieLlVBfldNME5cPbFTnSfis/N+4tJz+eSfi1xPyGLTq21RVlTq2dtE4FWGKlPgBcYL2fpNW6jeKIgLtzI5F53G2ehUoovI0KWQy6jrak2Apz2NPI0tY77uNsjlBi6nXDYFYqfjThOfU3jAeg2bGjRxbUJzy/o0/Os8+rX/mOaCcX7xBRzHji3w9E/SaEj6dQkJP/0EGg1auYyNreVsbANapQy5TE6QRxB9fPrQtUZXLM0q/0DuJ/lCVlk9SXUu6fWkb91G4s8/m+bOk1tZoRgwiB+cWrIx3Dh+ydnanKk9/RjQ1BN5BfRhL406T4xLJmzPYZKPHUdx4RweN6+j1msKHkem4JabN3l+jbBv2Ry/p9riWqPqdWcrKUmSiM/IM7WKXYxJ40ZMDJ8Na0eD6uU3l1FJaCIjSV23nrQNG9AlJJiWWwQEYDdwALa9eqOwrtzz7egNElPWnmPdqWjkMhhWW8+0UXfGv2jj40n9809S/lxlergos7TEvn9/HEYMR/WQTIWP64ODH7Dp2iZcLVxZ028NjurCc2GVh5NxJxmzbQwSEu8EzOb7zeYkZOThbqtmyZgW1Pd4tHuf4l5XvtwRxvd7rmKtUrLl9fbUcKr89wYVafnRSD7aaJzz8Y2udXmzWz3gyfrsvJ/MPN1Dk7qUBxF4laHKGHgV9cZKyszjXEwaIdFpnItO5Wx0GgkZeYW2N1fKqe9hS8DtlrHGnnb4OFsRl3PLFISdTjjN1ZSrpqyC+RqkWjFhl4zqV1IBUHp54v7e+9h06UxmcDCRH72HPNLYFzzEW8biHnJuOcnwc/Sjj08fetXqhatl5XiyW1z/hQtZZfMk1rlkMJCxYweJP88nL8zYNURmaUlWj37MtAzkbJZxjEdgDXtm9vOnkWf59mEvaZ1n5Go5f/oyNw8eQ3f2NA7XL1Et+SaKe64ZmWYWxNWoB/4BuLdpSYMuQVjalPPNemoUxJwCn45gUTmCnKp0jks6HZkHDpC6bh2Z+/aDzjiuTWZpiW3PntgPHIBFYGCl6iJ+N4NB4r31Iaw6cQMZErP6+/OMTTbJS5eRtmULaI0tkcpqHjgOH4H9wAEo7Mrn/ZetzWbI5iGEp4XTtnpbfur6U7nP75WtzWbApgFEZ0YT5NKTQ0e7kq3R4+duw5IxLfCwe/RuvsU9z3V6A0MWHuVEZAoBXvasfal1sZJ3/BetPn6DKeuMQ0Be6libqT19Te+9qnRdqepKEhtUfJgolAknaxWdfV3p7GsMbCRJIjY9l7M3jIHYudsBWXqujrM3Ujl7IxUwdoGyMlfQsLodAZ4+NPFsyqi29thZawt0TwxJDOGifRZvDpBoc0nOyN0GHG9EEz1xIqmedthHpyEH0ixhWVc5h/wcGVT/aQbVf5q6DlWja4oglBWZXI5tz57YdO9O5p49JP70M7kXL2K54U9mqzdyo013Zlg15XQU9PvxIENa1OCdHr44WlX8uJpcrZ4LN1K4euwM6cdPoQ4LocbNq7jmpGJ/z7qJNk6k1KqPqmkgNTu1oWnzRigeId16qfrrFQj/F5RqqN8XAp+Hmu2hCndnLE8ypRKbzp2x6dwZXWIiaX/9ReradWjCw0lbv5609esx9/HBfsCz2D39NErnypWlTy6XMevZRigkPeF/70D+1s+EJ103/d0iMBDHUSOxeeqpMpm/8kEszSyZ13EewzYP41DMIZacX8K4RuPKtQxfnfyK6MxobJUu7DncGr1OT7s6zvw0oim26vK5eVcq5HwzpAm9vz3A2RupzNsRxnu9ip6M+r/srzMxTF1vDLrGtq1VIOgSKi8ReP1HyGQyPOws8LCzoOftSfQkSSIyKZuzdwVi52PSydLoCQ5PJjg82bS9vaUZjarbEeDZg6Feg/gkyIpkbbgxEPM+zYyGJ+m8J4k+wRL2tzPO7ApQ8lvTJqhU7dgyYATV7St3NxRBKG8yuRybp57CumtXsv79l4SffiL37Dk892xikfk2LgR0ZLZDS/4Ihq3nb/F2d1+GtqxRbil0tXoDYfFpnL8WR+zRE0ghZ3GNDMUvOZLGuoIt6HqZnER3b7T1jd0G63RpQ/0a9x/7WWESjC2M6HIhZI3xy6EmNBkBTYaBXSUscyWldHbGadw4HMeOJef0aVLXriN961Y0168TP3ce8V9/g3WnjtgPGIB1+/blHsgURZ+ZSdq6dYxZthzd7cyNOpmctFYdafbWS1g0rtiEDvUc6vFey/eYfmQ635/+nqZuxoRX5eFwzGFWha0CIPba0+h1Kp5tWp0vnm1c7lN2eDpYMmdgY15acYoF+6/TtrYzHe6ah+2/bkvILd5afRZJghFBNfioT+VKRCbcX8VfBYUKI5PJqOlsRU1nK55uYrzZ0BskrsZn3g7GjAHZpVvppGZrOXAlkQNXEk3bu9mqaFS9IQGebXimlR2O3dOJCt1L3l87WK+uxUHa4C23488xQY/VPUEQnnQymQzrjh2x6tCBrMOHSfzpZ3JOnqTh8Z0sVe7laN0gFni258ONWv4IjmLm0w1p5l264z8MBonriZmcvZFG6PlrJB68QNiizfgmhuOfdpOAezKh5pmryfDxQxUYiGeH1ri3bIrcqpI/XNHrIPP2+NWhq+DyNji/DlIiYO+nsO9zqPOUsRWsXk9QVnwLY1Ugk8mwbNoUy6ZNcXv/PdK3bCF13Tpyz54jc9duMnftRunqit0zz2A/4FnMvb3LvYyaGzdIWbGC1LXrMGQZx1LqLS253KoHn5s3JNHCnukZNowu95IV9mzdZwmODWZL+Bbe2f8Oa/uuxV5tX6bHTNek8/HhjwHQJLdBn12H17vW5c2n6lbYDX1Pfw+Gt6rBymNRvLX6DFveaF/pJs6tCLsuxvH6H6fRGySea+bJzH7+IuiqQkTgJRSgkMvwdbfB192GQc29AOMklKG3MjgXk8a5G8Zg7Ep8BnHpecSlx7HrUpxp+xqOdchzqUVceh41HC35Y4IIugShuGQyGdZt22LVpg3ZwcdJ/Oknso8do/WlgwSFHWa/d3OWZ3ZmwM/pPNu0Ou/28nukG5H8iYHPRqcSEpXCzXOXUFwMoXbcNRomRfBsdlKhbXLsnTA0bIxT65a4t2mJ2tcXmaJyzDdUbFkJgAQyBdTtBr49ocfncPEvOL0cIg/BlR3GL0tnCBgCTUeCi29Fl7zKUFhb4zBoEA6DBpF35Qqpa9eRtmkTuvh4khYuJGnhQixbtsR+4ABsundHri67G2lJksg5cYKkpUvJ3L2H/FmvzWvXxm74cA6bKenz9NOE7b7Ggv3Xmf73RXQGifHtfcqsTMUhk8n4uPXHXEi6QGR6JB8e+pDvu3xfpjfXnx75grjsOAx5zugSezF7QCMGt6hRZscrro/6NOBERAphcRlMXn2WpWNaVkjCocpi/+UEJq48hc4g8XSTanwxoPF/uj6qIhF4CQ+lUioI8LInwMsegoxPKrPydFy4mV5gvFhEUjZRycaJJb0cLfjjhSCq2YugSxBKSiaTYdWqJVatWpJ98iSJP/1M1qFDdAoPpkPEcfZVD+TPjK50uRDHpKfqMqpNzQcOPo9Pz73TnTgigZyQ83jFXKZBUgRdkyOw0RbMgirJZKQ6u+PYpjXu7Vpj1awpZtWegLTOGbfnCbN2hfxJas0toclQ41fSNWMAduZ3yIyDIz8YvzxbQtPnoeGzoLKuuPJXMaq6dXF7711cJ79Fxp69pK5bR9bBg2QHB5MdHIz8k0+x7fM/7AcMRN2wQakFFgaNhvQtW0hetoy8i5dMy63at8dx5Eis2rZBp9cjbdmCTCbj3Z5+mMnl/LD3Kp9uvoRWL/Fyp9qlUpZHZWVmxbyO8xi+eTj7o/ez7OIyRjUcVSbHWntpG1sj/kaSZJAwmF9HtaVjJenWpzZT8MOwQPr+cJADVxJZeOA6L3Ws2P9NRTl8LZEXlp1AozfQy9+dL58LKLdu50LpEYGX8EisVEpa1nKkZa073Z3SsrWci0nlWnwmvRt54GorugQIwuOybNaMGr8sJufsWRJ/+pnM/fvpEn2KTtGnOVC9MSvin2LV8brM6NeQNnWcSc3WEBKTZpxa4kYq4VejcYoMo2FSOA2SI3gqJRozSV/gGAZzFfKG/ji2aoFVs2YoGzZg+4EDtHjSsmFl3m6dt3Ev+u9OteGp6dD5Q2Or1+nlcHk7RAcbv7a+C/79IXAkeLUE0b2nWGTm5tj27IFtzx5ob90idcMG0tatRxsTQ+off5L6x5+o/PywHzAAu759UNjbP9JxdElJpKxaRcoff6BPMHaLl6nV2D39NI7Pj0BVp86dlfV33gMymYy3e/hippDz9a7LzN4Wik5v4LWuFZsIys/RjyktpvDpsU/55uQ3BLoG0tildMegHY2IYsaRGaAAs4zOLB89FP/q5ZtF9WHqutkwrW9D3lsfwrztYbSq5UhgjcqRkbS8HI9IZtxvJ8jTGXiqvivfDgms8LmrhEcjAi+h1NhZmtG+rgvt61aOJ2WC8CSxCAjAa8F8cs5fIHH+z2Tu2k3HmLN0jDnLoVB/PrrcjZwaPshuRhuDrKQInksOxzMzodC+JEcnrJo1xaZ5MyyaNkXt54fsrgBLqy08ue8TISPW+N36PoFXPoUS/HobvzJi4ewfcGo5JF+D0yuMX86+EDgCAoaCtbjmFZeZhwcuEyfi/NJLZB87RuradWTs3EleaChxn31G/Ny52Dz1FPYDBxgnKi5GtsncsMskL1tK+t//IGmMc8Up3dxwGD4c++cGonQo3k36G0/VRamQMXd7GF/uvIzWIFXoGCeAQb6DCI4NZkfkDt7Z/w6r+67GTlU6gdG/l+OZuPM9ZNaZKHUerB82k1pOlSvoyjekhRcHrySyOeQWr/95ms2vty+3LIsV7cyNVMYsOU6OVk+Hei78OLxpuSc7EUqPCLwEQRCqEAv/hnj98AO5YWEk/jyfjO3baXvrPG1vnSfDzKJQt0EAM5/aWDVvhkXTQCybNcPM0/O/ORg7P/CycSv+Njbu0O5NaDsJoo4YA7CLGyExDHZ+BLtngG8vYytYna53ujAKDySTy7Fq3Rqr1q3Rp6aS9s9mUteuJS80lPQtW0jfsgWz6tWxe7Y/9s8+i5lHwYm1JYOBzP37SV62jOwjR03L1Y0a4ThqFLY9uhd4mFBcr3Sug1IuY9bWUL7bfQWd3sA7PSouTbdMJmN6m+lcTLpIdGY0Hx/6mG86f/PY5Vlz4gYf7lyOebVzIMlZ0HNepQ26wFgPnz/biDM3UrmRnMP760P4fmjlnS+utJyPSWPkL8fIzNPR2seJBSOaoaroKTmExyICL0EQhCpI7euL5zdfk3f1KonzF5C+ZYsx6DI3x6JxIywDm2LRrCmWTZo8ctetJ05mfuDl8eD1iiKTgXcb41ev2cZsiKeXQ8xJuPS38cummjElfeAIcKxVumV/gins7XEcMRyH4cPIvXCR1HVrSf9nM9qYGBK//4HEH37Eqm1b7AcOwCooiLTNm0lZthxNpHHuSeRybLp3x3HUSCyaNHnsm/EXO9ZGIZfx6eZL/LTvGjqDxHu9/CrsJt/G3IZ5HecxYusI9tzYw++hvzO8/vBH2pckSXy7+wrf7juJlc9GAF4MeJGW1Ss2jX5x2FmY8f2wQJ6bf4R/zt2ifV3nSpEApKyExWbw/C/HSM/V0dzbgcWjmmNhLoKuqk4EXoIgCFWYqk4dqs+bi+ubk9AlJaHy80NuLtKgF8nU1bAELV5FUdtC8zHGr7gLxq6HZ/+EjJtwYJ7xq2Z7Y0bE+n3BTCQZKg6ZTIaFf0Ms/BviNnUqGTt2kLp2HdnBwWQdPEjWwYMF1pfb2GA/6Dkchw3DrHrpzr82vr0PZgo50zZdYOG/19HqDXzcp/QSgJRUQ+eGvN38bb4I/oJ5J+bRxKUJDZ0blmgfWr2B99eHsObkDSw81yNT5NDAsQEvBkwoo1KXvqY1HJjcvR5ztoUxbdMFmnk7UMfVpqKLVequxmcyfPFRUrK1BHjZs2RMC6xU4pb9SSA6iQqCIDwBzKpXx6JxYxF0PUjGY7R43Y9bQ+g5CyaHwsAlULsLIIOIA7B+AnzpC5snw80zpXfM/wC5Wo1dv354L1tK7e3bcHrxRZSurgCYe3vj9tGH1N23F7d33in1oCvfqDY1+ay/PwBLDkUwbdMFDAapTI5VHMP8htG1Rld0Bh1v73+bDE1GsbfNyNUy9rfjrDkZjbn9CZQ2oZjLzfms3WeYyavWWKmXOtSmXR1ncrUGXv39NLla/cM3qkIiErMYvvgoiZkaGnjYsmxMS2z+I+PZ/gtE4CUIgiD8NzzKGK/iUqrA/1l4fgNMOged3gO7GpCbBscXw8KOML8dBC+CnJTSP/4TzNzbG9c3J1Fnz25q79qJz9YtOA4fXi4Tdg9v5c3sAY2QyWDZkUg+2Hi+woIvmUzGjDYzqG5dnejMaKYfno4kPbwscem5DFpwlANXErGwSMO2+lYAXgt8jToOdR6ydeUjl8v4alAATlbmhMZm8NnmSw/fqAqQJIkdF2IZsvAocel51HOzZsX4VthZiqDrSSICL0EQBOHJZ9BDVrzx59Js8SqKfQ3o9C68cdYYiDV8FhTmEBsCW96Geb6wbjxc3w8GQ9mW5QkiUyox9/QsVqbD0jS4RQ3mDgxAJoM/gqN4d/059BUUfNmp7JjTYQ5KmZIdkTtYHbb6geuHxWbQ/8dDXLqVjpO1ksaB28kzZBPoGsjzDZ4vp1KXPldbNV8OCgBg+dFItp2PreASPZ7rCZmMXnKcF5afJDY9l9ouVqwcH4SjlejB8KQRgZcgCILw5MtKAMkAMjlYlVP6d7nc2PXwuSUwOQx6zgY3f9DnQcgaWNYPvmuC/OCXqDXJ5VMm4ZEMbObJN4ObIJfB6hPRvLPmbIUFX41dGjOp2SQA5hyfQ2hyaJHrHb6ayMD5h7mZlouPixVje93iYsopLJQWfNr2UxRVPANnJ19XXujgA8DUdeeISS2c0bWyy8rTMXtbKD2++Zf9lxMwV8h5pXNt/n6tHS42qoounlAGROAlCIIgPPnyuxlauVZMyndLRwh6CV46CBP2QvOxoLKF1EgU+2fR7cJbyLe8daecQqXzdJPqfDc0EIVcxvrTMby56gw6fcW0WI5sMJKOnh3RGDS8vf9tsrRZAOTp9CRnaVhz4gajlgSTkaujRU0HvhlRnd8u/QjAW83eoobtk5EN8O3uvgR42pGWo2XSn6cr7P9RUpIk8ffZm3T9cj8/77uGVi/RydeF7W924J0efliai0QaTyrxnxUEQRCefJlxxu9lMb6rJGQyqN7U+NX9M7j4F4ZTS5FHHYHTy+D8Wmj9CrR53Zg9UahU+jSuhkIm47U/TrPp7E10BgPfDgnETPFoz7ElSSJboycrT0dGno6sPB2ZeTqy8gouu7Pc+D0zT096Xj8U6nNEpkfSetHL5MQM5t48E/9r7MGcAf68sHsMufpcWnu0ZrDv4FKoicrBXCnnu6GB/O+7gxyPSOG7PVd5q1u9ii7WA4XFZjBt03mOXje2ctdwtOTjPg3oWt/1iZ+XTBCBlyAIgvBfkHHL+N3avWLLcTdzS2gyFH3DgRxa/TVts7cjjzkB/86FE79ChynGljGlGOdRmfRq5MHPCjkTV55kS0gsOv0pRgR53xUU5QdIelPQVDiA0hu/a3QUIz/GfckthmDpvQCsToNNLUhtCYC1SsmoNt5M7ubLkgu/ci7hHNZm1sxsO/OJu7n3drLis/7+vPHnGb7fc4XWPk60ru1U0cUqJD1Xyzc7r7D0SAR6g4TaTM4rneowoYMParOq3e1TKD4ReAmCIAhPvoz8Fq9KFHjdJdnaF/1zk5Bf3Qa7Z0DSVdg2FY79DF0+MiboKOekEsL9dWvgxsLnm/PiipPsuBjHjotxj7U/mQyszZVYq5VYqYxf1ioFVreXWZuW3f2zAitVS/bFGlh9fQG21TezaNRgGrv6obzdAnc55TI/njF2MXy35bu4W1XO8/9xPd2kOgevJLLmZDSTVp1m6xsdKk1iCoNBYt2paGZvCyUxUwNAL393PvhffTwdLCu4dEJ5E4GXIAiC8OTLb/GqpIEXYLz7btAPfHvB6eWw7wtIiYB14+Dw99BtBvh0quhSCrd19nPl11EtmLs9lDyd4QHBkfFnG7USK/O71lErsVIpsFYpsTBTPHJLVNs6E4nJO8+hmENMP/Yuf/7vT5QKS7R6LR8e/BCtQUsnr070q92vlGugcpnxdENORqVwPSGLKWvPsmhk8wpv3QuJTuPjTec5HZUKgI+LFTP6NaR93XJK8CNUOiLwEkqXXgdRh6F6MzAv+zlWBEEQiiWzcrd4FaAwM3YxbDwYjvwIh76FW2dg2dNQu6sxAHNvVNGlFIB2dZ1pV7ddhZZBLpPzebvPeW7Tc4SnhfPZ0c/4zKsXCxODuZR8CXuVPdNaT6vwIKSsWZor+X5oIP1/PMyuS/H8djiCMW1rVUhZUrI0zN0Rxh/BUUgSWJkreOOpuoxuUwtzpWi5/i8T/32hdJ1bBUv7wt7PK7okgiAId1TGMV4PY24FHafA62eg5YsgN4Nru2F+e1j/AqREVnQJhUrCUe3I7A6zkcvkbLq+ibmbhrPowm8AfBj0Ic4WzhVbwHLSsJod7/f2A2DWllDOx6SV6/H1BokVRyPp/OU+fj9mDLqeaVKNPW934oUOtUXQJYjASyhl8Rdvf38yZpIXBOEJUcnHeD2QtQv0ngOvBhvHeiEZH3L90By2fwDZYg4wAZq7N2diwEQAltnZokeiV81e9KjZo4JLVr5GtanJU/Xd0OgNvPbHabLydOVy3JORyfT74SAfbjxParYWP3cbVr0QxDdDAnGzVZdLGYTKTwReQulKv2n8nv90WRAEoaIZ9FWrq+H9OPoYJ2OesBdqdQC9Bo78AN82gQNfgbbqTSArlK7xjcYTpHQAwNkg8X6r9yu4ROVPJpMxd2Bj3G3VhCdm8fFfF8r0ePEZuby1+gwDfj7ChZvp2KqVzOjXkH9ea0crn8qXXVGoWCLwEkpX/uSfIvASBKGyyE4CSQ/IjBMoV3XVm8LITTB8Hbj5Q16aMRPid03h1HJjoCn8JynkCubgwoi0dL6PS8Te/L85F5yDlTnfDmmCXAbrTkWz8XRMqR9Dqzew+MB1uszbz/pTxv0Pbu7Fnrc7MapNTVNmSUG4mzgrhNKVcbvFKydFPH0VBKFyyH8gZOUCiickp5RMBnWfghcPQP8FYOdlvP5uehV+bgNhW3msCaKEKsshI56pyan45+ZAdmJFF6fCtPJx4rUudQH4YEMIEYlZpbbvw9cS6f3tAT7dfInMPB2NPe3Y+EpbZg9sjLO1qtSOIzx5ROAllB5JgvS7WrpEq5cgCJVBfuBl41ax5SgLcjkEDIFXT0D3z0BtDwmh8McQWNIbbhyv6BIK5e3uz9787v//Ua91qUPLmo5kafS89sdpNDrDY+3vZmoOr/x+imGLjnElPhNHK3O+eLYRGye2pYmXfekUWniiicBLKD05KaDPu/N7ugi8BEGoBDLzAy+Pii1HWTJTQ5tX4Y2z0HYSKNXGqT1+eQpWjYDEKxVdQqE86DSQlXDn9//4A1ClQs43Q5pgZ2FGSEwac7aFPtJ+8nR6fvw/e+cdH1WV/v/PnZ42KRCSAKFJrwooAisiIKCrwq6iIitEEV1/gqBYQFaEtSKCouzadhFdxQIi+nUViYCuCmIBRBBp0oSEUJLMpE2m3N8fd86dmdQpd+a25/168eJmcmfmzJ2bc87TPs/mgxi55Ev8d1cRDBwwZXB7bJ49HDde1A4Gg7al+gnpIMOLkI66njWdT/gEQSgEFvFK1WDEqy5JGUKfrxnbgQv+AnAGYO//Af8YBHx8T0DdkdAmzMnA0HnECwBaZyRh8XV9AQD/+vowNu8riej5m38twZhn/4fFn+1DtduLCztk4uMZl2DhuN5ITzbHY8iEhiHDi5COuoYWTfgEQSgBpw4iXnVJbwOM+wfw12+ArmMFcZEfVgDPnw9sehxwOeUeIREP6maakAMUADC6Vy6mDG4PALjvvZ9Q4qhp9jnHzlbhtte/xy0rv8eRs1XITrPi2Rv64b07BqNna32KlhCxQ4YXIR0U8SIIQoloucarOXJ6Aje9CxR8ArQZCLirgP89LUjQb3tFSE0jtIOzzjpMKf8ic6/sgR55dpytrMWsd3fC62tYfKa61oulG/Zh1LNf4vO9JTAZOEy7pCM2zb4Uf7qgLTiO0gqJ6CHDi5AOZmhx/tuKIl4EQSgBPdR4NUeHocBtnwPXvwFknSeo3X16P/CPi4Dd75MColZw1FmH6xpiOsZmNuKFiRcgyWzElkNn8dKXh0J+z/M81u8uwqilX+L5TQdR6/FhaOcWWD/rEsz7Y0+k2SitkIgdMrwI6WCGVnZ34X+KeBEEoQRYXVOqipsnSwHHAT3HAXdtA/64VOhpVnoYWHMr8OplwOH/yT1CIlacddZhiniF0LlVKhZe0wsAsLRwP348eg4AcLCkApNXfIe/vrkdJ8qq0Trdhhcn9cebUwehc6s0OYdMaAwyvAjpYOk8rfv7f6YJnyAImfH5giJeOkw1bAijGbhwKnD3DmD4Q4AlFTi5A3j9auDNa4Hi3XKPkIgWWoebZcLAtrimX2t4fTzufnsnHv/vLxj73P/w1YEzsBgNmDGiMz6ffSmu6JNHaYWE5JDhRUgH87S1YRN+MaWvEAQhL9XnAJ9HONaDqmEkWFOB4Q8Cd+8ELrodMJiAg58DL/0B+OCvQNlxuUdIRAqLcLF1uKYMcFfLNhwlwnEcHv9Tb7TLSsaJsmq8+tVheHw8RnZvhQ33DMPs0d2QbNFIo3VCcZDhRUgHm/BbXyD8760Fqs7KNx6CIAjm8U9uKUR6iPqkZgNXLgbu+g7o9ScAPPDT28ALA4Bj2+QeHREJwamG5mThmOqt65FmM+OFiRcgzWpC+xbJ+PeUgfh3wYXo0DJF7qERGocML0IaPC6hWBsAMtoDKdnCMU34BEHICavv0rOwRri0OA+YsBKYtglo1QvwuoDfNss9KiJceD7gALXnBe55SjdskH75Gfhu3ihsnj0cI3tQNJxIDGR4EdLA8sqNViA5iyZ8giCUAZuDqL4rfNoMEEQ4AMBxQt6xEOFTUwZ4/GmFaXmAvbVwTAIbjZJkMcJgoDouInGQ4UVIg7i5yRWUs8QJnyJeBEHIiCisoXNFw0ihOVx9MAMrKRMwJwU5QOk7JAilQIYXIQ1scWaLNUW8CIJQAiwar3cp+Ughw0t9MAMrzf/d2f3rMEW8CEIxkOFFSIMY8fJP9LRoEwShBJwU8YoKexvhf0o1VA/B9V1AwACjiBdBKAYyvAhpoIgXQRBKpIKJa5DhFRFsLq8pB1wV8o6FCI96DlCKeBGE0iDDi5CGul5l0fAqlmc8BEEQAKUaRovNDljShGNyoKmDuoYXrcMEoTjI8CKkoVFPG6U4EAQhEzxPqYaxIKaMU7qhKqiXahiUeeLzyTMmgiBCIMOLkIbGUg2rzwHuGnnGRBCEvqk6B/jcwnEqyclHDNXqqou64hppuQA44W+g6qxswyIIIgAZXkTs8Hz9iFdSJmCyCceUpkIQhBwwKfnkFoDJIu9Y1AgJbKiLuhEvoxlIyRaOSWCDIBQBGV5E7FSXAh5/VIsZXhxHAhsEQcgLm3uovis6KOKlHrxuoPK0cMwiXgAJbBCEwiDDi4gdtrlJygLMtsDjtGgTBCEnTlI0jAmaw9WDsxgADxjMQoSXQZLyBKEoyPAiYkdMb2gd+jhFvAiCkBMxBZoMr6igVEP1EJzubwja2lHEiyAUBRleROyIBb15oY/ThE8QhJxQD6/YoIiXehAFruqswxTxIghFQYYXETt1C3oZNOETBCEn1MMrNpjhVXWW1GmVTl2BKwY5QAlCUZDhRcROYxM+8zJT80aCIORA7OFFUvJRkZQJmJKEY3KgKRtahwlCFZDhRcROo542SlMhCEJGmJx83bmJCA+Oo3lcLVDmCUGoAjK8iNip2zyZIYprFAu9vogANQ7greuBnW/LPRKC0CY8H5RqSBGvqCHDSx2IDtA66zAzxKpLAXd1YsdEEEQ9yPAiYqfRFAf/z14XUHUusWNSOoc2Agc+A75ZJvdICEKbVJcC3lrhmMQ1ooeUDdVBY+IatoygdFGq8yIIuSHDi4gNT22gaWPdiJfJAiS3FI4pzSGU8t/9/x+naCBBxAMW7UrKBExWeceiZijipXx4vnEHKMeRwAZBKAgyvIjYYDUURkto00YGTfgNwwyv2gqgplzesRCEFqH6LmlIZxEvMrwUS0054K4Sjus6QIGgOi9ahwlCbsjwImLDEdSglOPq/54KexuGGV4ApfAQRDyg+i5poFRD5cMMKlsGYE6q/3vRAUrrMEHIDRleRGyIzZMb8LIBFPFqjGDDK/iYIAhpcFLESxIo1VD5NCZwxRCFrmgdJgi5IcOLiI3GJGwZFPFqmBDD67h84yAIrVJxSvifenjFBot4VZQINb2E8misvotBxjNBKAYyvIjYaG7Cp+aN9XFXA1VnAj+XUwoPQUgOm5tSSdEwJpJbCDW84AN1c4SyoHWYIFQDGV5EbITtaaMUB5G6XkdKNSQI6XGyiBcZXjFBTZSVD2WeEIRqIMOLiA1xwm8ut5wmfJG6hhYVrROE9DiDhH+I2CCBDWXTrAOUrcPF1L6EIGSGDC8iNkRxjWYiXlVnAY8rMWNSOszwstr9P1ONF0FICs8H1XiR4RUzFPFSNs2Ja7B0W2+tsBYTBCEbZHgR0cPzzac4JGUCRn/zUlJUEmCGV9sLhf8dJwGfV77xEITWqCkDPDXCMdV4xQ4ZXsqmuYiXyQKkZAvH9B0ShKyQ4UVET00Z4KkWjhub8DmOJOXr4vAbXm0GAJwR8HkExTCCIKSB1XfZMgCzTdahaAJKNVQuXndg/Wgs4gWQpDwRG0U/ASV75R6FJiDDi4geZkglZTbctJFBhb2hsIhXZofAQkkCGwQhHVTfJS0U8VIuFacA8IDBDCS3bPw8+g6JaHE5gX+PAVaMBbweuUejesjwIqKnuebJDIp4hcKMrPQ2AU8y1XkRhHSw+q5U6uElCbRpVy6OICeDoYktHUW8iGg5d1jIbqopIwe6BJDhRURPc/VdDJrwA/B8oG9Xej6Q3lY4phQedXGgEDj4udyjIBqD9StqLAWaiAzmIHIWk8dbaTQncMVgvyfjmYiU4Iwcys6JGTK8iOgRNzfNpPPQhB+guhRwVwrH9tZC1AugyUxNuJzA2xOBt28CaqvkHg3REOLcRBEvSUjJBgwmgPcClVSPqijCXYeDJeUJIhKCM3LKKDsnVsjwIqIn0lRDmvADka3klkJdXHq+8DMZXuqh7BjgcwNeF6WIKpUKinhJisEYuJblFJ1XFM1JyTPEWmvKPCEipOxY4Lj8WOPnEWFBhhcRPWGnGpK4hohY39U29H8yvNRDGXn/FA9z8lCNl3SIdV5keCmK5qTkGXbKPCGihCJekkKGFxE90Yhr8Hx8x6R06hpedko1VB3BixB5/5QJ1XhJDwlsKJOwI17+v4Xqc4C7Jr5jIrRFsLFFWR4xQ4YXET2Rimt4XUKNk55pLOJVdQZwV8szJiIygtMuyPunPHiearziAfXyUibhRrySMgGTLfQ5BBEOFPGSFDK8iOjwuoHK08JxcxEvkxVIbiEc691bygwvtolJygTMycKx3q+NWggxvCjipThcjkBj91Tq4yUZFPFSHjwf5ABtZh3mOFIYJiLHXR3Y6wHCHkbvmUsxQoYXER3OYgSaNrZo/nwq7BVg3mIW6eI4qvNSG+WUdqFonP4eXtZ0wJIs71i0BBleysPlCKjkhpNWS98hESlMTMeUJPzvqQaqzso3Hg1AhhcRHcHpDU01bWRQYa+AmGqYH3iMDC91QeIaykacmyjNUFLEVEOdz+FKgkW7bGE6GSjiRUQKq2PObB/IIKBMj5ggw4uIDrGgN8zidZrwAZ83cN1Y/y6ABDbUhLsmtI+Rswjw1Mo3HqI+Ff6IV3N9jYjIsAep0/p88o6FEAhX4IrB/iYcOl6HichgzsX0fCCDtb8hh2MskOFFREe4TRsZlOIgXDPeKzQiDZa5ZtEvBxleiocZx+YUf6E6T2IDSoM5d6i+S1pScwDOAPg8oTUfhHywdThcB6idUv6JCGFGVnrbQHYOZXrEBBleRHRE62nTcxNlUVijtdCQlEGphuqBpV1ktAv63mgRUhROinjFBaM54DAiZ4MyYI7McNsmUOYJESlsX5KRH3AS014lJsjwIqIjXCl5BjVRDmzQ7W1DH0+nVEPVwDx9wYsQef+UhVjjRYaX5FDmgrIIV0qeQd8fESliqmE7weEIkLMxRsjwIqJDnPDDjHgFN1HWK3UVDRmiF+kEybQqnXLKd1c8VOMVP2jjriwidoCyiFcxrTVEeIhZHsHORhLXiAUyvIjoiFhcw79gV50BPK74jEnpiIqGbUIfZ5sZdyU1mFY6IRGvdqGPEcqAarziBzVRVhYRp/z712uvC6g6F58xEdohRBCMnI1SoQrD68iRI5g6dSo6duyIpKQknHfeeXjkkUdQWxuqJrZr1y5ccsklsNlsyM/Px9NPP13vtVavXo3u3bvDZrOhT58++OSTTxL1MbQDz0ee4pCcBRitwrFe67zKG4l4mZOA5JbCMW1olE2DES/y/ikKqvGKHxTxUhaRRrxMlsBao+e0fyI8nEWCmI7BJMynbO9SXQq4KuQdm4pRheH166+/wufz4eWXX8aePXvw7LPP4qWXXsJDDz0knuNwODB69Gi0b98eP/74IxYvXowFCxbglVdeEc/ZsmULJk6ciKlTp2LHjh0YP348xo8fj927d8vxsdRLTTngrhKO7WF62jguSGBDp+mGwZv2upDAhjooCxbXoLQLxeFyBjWUJcNLcqiXl3LwegKtLcKNeAGU9k+ET11BMFu60JgeoKhXDKjC8Bo7dixee+01jB49Gp06dcI111yD++67D2vXrhXPeeutt1BbW4sVK1agV69euPHGG3H33Xdj6dKl4jnLli3D2LFjcf/996NHjx549NFH0b9/fyxfvlyOj6VemOFkyxCiNeGid2+pOIm1qf87MryUj9fTSNrFCeprpBRYNN2SBlhS5B2LFhHncIrMy07FKYD3CdGIlOzwn0dCV0S4BAtrMDJI2TBWVGF4NUR5eTmysrLEn7du3Yphw4bBYrGIj40ZMwb79u1DaWmpeM6oUaNCXmfMmDHYunVrYgatFSKVsGXoWcq2tgqo9ufU1001DH6MJjPl4jwp9GEzWgRZ7bTWAGcEfG6gQqfps0oj0v6CRGQEO89InEFegmsZDRFs5aiJMhEuwcIaDMr0iBmT3AOIhoMHD+KFF17AM888Iz5WXFyMjh07hpyXk5Mj/i4zMxPFxcXiY8HnFBc3vmlyuVxwuQJiEA6HAwDgdrvhdrtj/iyxwsaQyLFwZb/DBMCXlgtvBO9rSM2BEYC37Hf4FHDtoiHq633uCMwAeEsqPMZkoM7zDal5MALwlR2L6JrqATnu8Ybgzh6GCQBvbwOP1wsAMKXlgXP8Ds/Zw+CTIvA6KxylXPNIEeem1Faq+jtSzfW2tYQZALwuuB2ngOQWco8oalRzzRuBK41yHU4R1mFf+YmE/42o/ZqrjVivt+HcUWHPltZa3LMZ7G2Ex0qPqnYfFw8iucayGl5z5szBokWLmjxn79696N69u/jziRMnMHbsWEyYMAHTpk2L9xDx5JNPYuHChfUe37BhA5KTk+P+/uFSWFiYsPfqWvw/9ABwvMyDnRGIk5x3qgy9ARTt344fa9UtahLp9c527MYQAE5DOjZ/+mm937cuLcGFAEqP7MbXJPjSIIm8xxui7blvMADAGXcStvi/o6G+FLQEsPOL/8OJrDOyji8eyH3NI+W8U1+iN4ATDh+2q/DvSA3Xe4zJDpvHga8/eQ+O5PZyDydm1HDNG6Lj6c/RF0BxBfB9BPd6uzNncAGAkt92YZtMfyNqveZqJdrrffHBHcgBsOtYGY7575XzTlUI+7i92/Bjtfrm2HhRVVUV9rmyGl6zZ89GQUFBk+d06tRJPD558iQuu+wyDBkyJEQ0AwByc3Nx6tSpkMfYz7m5uU2ew37fEHPnzsW9994r/uxwOJCfn4/Ro0fDbrc3OfZE4Ha7UVhYiMsvvxxmszkh72n4dDNQBLTtcRFaD78y7Odxe2qAdW+jdRqHnCvDf56SiPZ6cztLgUNAauvuuLKBz86daAWs/AeyjFUN/l7PyHGPN4Th673AUaDFeReI35Hxo4+Bn/fhgk4t0W+Idr43pVzzSDF8vhU4CbTuegFyR6nn+1DT9TYVdQCKd+GSfp3Adxkj93CiRk3XvCEMm38EfgdyulyAK8dEsA4ftADvrkCOzZvwtUbt11xtxHq9TS8/BjiBPkOvQO9OwwEA3C+1wAfvoHWKT7X7uHjAsuHCQVbDKzs7G9nZ4aXnnDhxApdddhkGDBiA1157DYY6Oc2DBw/GvHnz4Ha7xRussLAQ3bp1Q2ZmpnjOxo0bMWvWLPF5hYWFGDx4cKPva7VaYbVa6z1uNpsVNXEkdDyVgvFqzGgDYyTvmSnkBhucxTAo6NpFQ8TXu0LIpzdktG34s2cJnmPOWQSz0SAoCBEhyP435xQEBQyZ7QPfYabwvRmdJyL7W1AJsl/zSKk8DQAwpkc4NykEVVzv9HygeBdMlacApY81DFRxzRuiIrZ1mKsoku1zq/aaq5SorjfPiy1wTC06Bv7WWwglPQbHCdXv46QkkuurCnGNEydOYPjw4WjXrh2eeeYZnD59GsXFxSG1WTfddBMsFgumTp2KPXv24N1338WyZctColUzZ87E+vXrsWTJEvz6669YsGABfvjhB0yfPl2Oj6VexObJEUjYAqHiGnorzHaw5skNSMkDgliDwSSIN+i1z5nSCW6ezBALjUlaVxGQuEb80bs6rVKItHkyg31/VWcBj6vpcwn9Ul0aaM0RLAjG1jxnEeClGq9oUIXhVVhYiIMHD2Ljxo1o27Yt8vLyxH+M9PR0bNiwAYcPH8aAAQMwe/ZszJ8/H7fffrt4zpAhQ7Bq1Sq88sor6NevH9asWYN169ahd+/ecnws9RJp82QGO99TI/xR6wmmVtiQoiEgRLjYgkjKhsqkoT5sorQuGV6KoIIMr7hDhpcyiLR5MiMpEzD6s3j0qDBMhAdTLUxpBZhtgcdTsoX7h/dRW4koUYWqYUFBQbO1YADQt29ffPXVV02eM2HCBEyYMEGikekQrxuo8DdtjDTiZbYBSVmCrLqzCEjOav45WkE0vBro4cWwtxUmu/LjAAYlZFhEmPh8jUS8/P1Nyo4LUVyOS/zYiABOfw1vKhlecUNsokybLlkRHaARrsMcJxhrpUcE4y2zg9QjI7RAeQPrHSC0LkhvC5w7JKx7dP9EjCoiXoSCqDgFgBfS4pJbRv580VuqI09bUK50oxGv4N/RhkZ5VJ4GvC6AM4Q2wGbfmbsSqDonz9gIAVcFUOsUjtNymj6XiB6KeMlPjQOorRCOI414AdREmWgesXlyU31HKdMjGsjwIiLDEWXTRoZY56WjCb/qHOCpFo7tTUS8WDSMUg2VB1tg0vIAY1ARrdkmpGIAgWaThDz4xQZgSQWsafKORctQE2X5YdEuazpgSYn8+dREmWiO8ibq0sUUe9qrRAMZXkRkMIMpGi9b8PP0NOEzYY2UVoCpvkKmiOhFooiX4mD57k0tQiSwIS9sM5pK0a64wgwvdyVQUy7vWPSKI9Z1mEW8dLQOE5HBHIkZ7er/TkyxJ2djNJDhRUSGqBoW5YQfrGyoF5oT1mCkk1CDYmks3x2g700pxDo3EeFhThJqdQFKN5SLWNU79bgOE5FR1oCYFINEpWKCDC8iMqKVkmfoccIPR1gDCKQhUvheeYjCGg14/zLahZ5DyIO4GaWIV9wRBTbI8JKFaKXkGXrMPCEiIxxnI615UUGGFxEZ0UrJM/RYmN1UrnQwLCJWfQ6orYrvmIjIaEhKnsEML/L+yUsFRbwShjiPU1q0LEQrJc8gcQ2iKWorhT5vQDPiGr8Lir9ERJDhRUQGRbwiJ9xUQ1s6YPGLAtCGRlk0JCXPEL1/lO8uKyziRTVe8UePDjQlEbMDNCjiRQIpRF1YnbklDbBl1P+9vQ0ATlD6rTydyJFpAjK8iMiQKuJVeRrw1EozJqXDDK+mFA0Bob8KKRsqEzHi1VCqIeW7K4JY616I8KFeXvIilQPU6wKqS6UZE6EdRGGN/IZ7U5osgXuI9ioRQ4YXET48H5TiEOWEn9wCMFqEY5YapHXY5qS5VEMgNIRPKIPqMsDlEI4bTLvwf6/VpUIvKUIemJw8GV7xhyJe8hKrA9RkFdZigL5Doj5NCWswRIcjZXpEChleRPi4HIKEMBD95obj9NVDxOsJLJLNiWsAJLChRFgKYXJLwJJc//c2u5AmClDUS07EVEMyvOIOGV7y4fUEnAzROkCBoDovHazDRGQ0JazBIIGNqCHDiwgfR4xNGxl6Kux1FgG8DzCYA412m4JNZg4yvBRDWIsQ9TWRldrKQFSSIl7xh1IN5aOyRFhTOCOQkh3964gOUB2sw0RkRBTxIsMrUsjwIsIn1ubJDD1J2QZLyRvC+HOjVEPlEckiRIaXPLBolzkZsKbJOxY9wOZwlwOoccg7Fr3B1s20XMBgjP517DoUuiLCQ6xpbkIQjP2OIl4RQ4YXET5SNSjVk7KhKKzRjKIhg8Q1lEd5Ez28GNREWV6C67saKgYnpMWaJmQ+APqYx5WEM8jwigVKNSQag+0/mlzz2oWeS4QNGV5E+MSqpMTQk+HlCFNKniFGvE6QzK9SYFGsphYhaqIsL2wuofquxEG9vOQhVmENhp4yT4jw8XoCez0S14gLZHgR4SPZhM8WbB1M+OH28GKw2glPNVB1Lj5jIiKjqebJDMp3lxcnKRomHBLYkAfJHKA6qrUmwsd5EuC9gvp0Uz0R2XpYU07pxhFChhcRPqKUvFSphjqY8FkjwnAUDQFB5peJcJDAhjJoqnkygxSe5EWq9CsifMjwkgeKeBHxhK1h9mbq0q2pQFKmcEwOx4ggw4sIH2YopcXoaQue8LWeTidGvMLo4cUggQ3lUFsFVJ0RjpuMePlTDSuKAY8r/uMiQmE1Xk15aAlpIWVDeZA64lV1huYsIkA4whoMEtiICjK8iPCROuLlqQZqymJ7LaVTHuQ9ChcS2FAO7Duw2oGkjMbPS24BmJJCn0MkDqmEf4jwoYiXPEgV8UrOAoxW/2sWx/ZahHYIR0yKIQpskOEVCWR4EeHh9Qj9Q4DYI17mpECIWstpDq6KgGEZbo0XEKSQRxt42WHCGs1FLDmO6rzkRDS8KOKVMMSIFxleCUV0gMa4DnNcIDVXD0JXRHiE0z6FQWteVJDhRYRHxamgpo0tY389PRT2shQcazpgs4f/PEo1VA5Msamp+i5GOvXyko0KinglHFI1TDwuJ1DrFI6luNfZa5DxTDDKw6hpZlBtc1SQ4UWEh1Oipo0MPRT2irnSEaQZAgFPMhle8hON948WocTirhaUtQCq8UokzPCqLhVqIYn4w9ZLq10QN4gVaqJM1IUiXnGHDC8iPKTKK2foIcVBVDSMIM0QCEx45EmWn2i8f7QIJRaWZmhKAmzp8o5FT9jSAXOKcKzleVxJSK3eSU2UiWB4Pqh5MkW84gUZXkR4SCWswdDDhM8msEiENYBAhMxZJNTWEfIRkfePmijLQnB9F8fJOxY9wXGUbphopHaA6iHzhAifqrOC6BkQ3r6FrYuk5hsRZHgR4SGVlDxDDxO+I8qIV0orwGAWauq0bJiqATHi1b75c5nhVU41XgmF6rvkg5QNE4tUUvKMNEo1JIJg9cmpuUJP0eZIaRlQ8yXnS9iQ4UWER9wiXhpesMsjiJYEYzCQpLwS8LoDG5JI0i4cJwGfN37jIkJxUg8v2aBeXolF8ogXGc5EEJGk1gNC1Jt6eUUMGV5EeFDEK3LE5skRphoCgJ2UDWXHcUKIOppsQEp28+en5QIGE+DzkAc5kUhd90KED23cE0s8I148L81rEuolktR6BglsRIxJ7gEQKiFeEa/K00JkwWiW5nWVAs9HL64R/BwHGV6yIS5CbcOrHTIYhQhA2VHhudF870TkVPgjXmR4JR4yvBKL5CJX/tfx1AjqlMlZ0rxunPF6vXC73XIPQ/G43W6YTCbU1NTA6w0jC6OiDEjNB7J6ADU14b1Ji17AqQOA42z4z1EpFosFBkPs8SoyvIjwECd8iTxtyS2EOiafWyiODze0rRYqzwBeFwAuumtGvbzkJ9zmycFktBMMr/LjAAbHZVhEHdjclEqGV8KhVMPEIrUD1GwDkrKA6nPC35HCDS+e51FcXIyysjK5h6IKeJ5Hbm4ujh8/Di4c52HWpcDQC4GkTODw4fDeJO8qIPMSwJIS/nNUisFgQMeOHWGxWGJ6HTK8iOapcQC1FcKxVF5lg0HwtpUfEyZ8rRleLOyemgOYovgjFWu8aEMjG5HmuwPURFkOnBTxkg2KeCUOnzcouiuRAxQQ1uHqc4JRl9NLuteNA8zoatWqFZKTk8MzJnSMz+dDRUUFUlNTw4vUnOUBb4qwjlnTwnuT6jKhFMWcDGR2iGW4isbn8+HkyZMoKipCu3btYrr3yPAimod5lKVq2siw+w0vLS7a0SoaMsSeUBTxkg0x1bBd+M/JIMMr4VCNl3yw+a3ytCAnHY4SGhEdFSUA7wU4A5DaSrrXtecBJXsUL3Tl9XpFo6tFixZyD0cV+Hw+1NbWwmazhWd4GTxCWn1yqhANDQcuFajmAIMXsIX5HJWSnZ2NkydPwuPxwGyOvjwmasPrhx9+wHvvvYdjx46htrY25Hdr166NekCEApE6r5yh5SbKsQhrAIEUHipYlQ8mCx9NxIu+t8TgrgFqyoRjMrwST1KmID7jqRHmcQ17vGVHTKnNEepJpUIU2CiW7jXjAKvpSk5OlnkkGsXnFQx7ADBGkKXD6vO9bqG2XcNRSJZi6PV6YzK8oqoSe+eddzBkyBDs3bsXH3zwAdxuN/bs2YNNmzYhPT096sEQCkXqvHKGlpsoi4ZXlCmUzJNcUwa4KiQZEhEhsSg8kbRuYmA9vIxWwJYh61B0SUgTZWVHTFRPvBygKvv+KL0wTnj9ARTOGJlhLxppvFCzr2GkuveiMryeeOIJPPvss/i///s/WCwWLFu2DL/++iuuv/56tGsXQVoOoQ6klpJnaFlSXjS8okw1tNkBq9+JQYXricfnC1z3qCJev5M8cyIIru+iDZk8iAIb6ti4qxappeQZ1ESZAAKGVyTRLkCYdw1BUS+iWaIyvA4dOoQ//vGPAITQW2VlJTiOwz333INXXnlF0gESCoAiXpHDDC97lKmGQJDABkVPEk7FKWEh4oyRORzS2wLgAE+1oGxJxBcW8aI0Q/kQIybkIIorFPEi4oloeEWRQscExDwu6cajYaIyvDIzM+F0OgEAbdq0we7duwEAZWVlqKqqkm50hDKI24TPIl4anPBjFdcIfi4pGyYeZuza2wDGCEphTdaAEVBOAhtxh9WlpObIOw49Qxv3xBA3ByhFvOJNQUEBOI4Dx3Ewm83IycnB5ZdfjhUrVsDn84Wcu2XLFlx55ZXIzMyEzWZDnz59sHTp0np9uDiOg81mw9GjR0MeHz9+PAoKCiIfJItWRaPCzKJkFPEKi6gMr2HDhqGwsBAAMGHCBMycORPTpk3DxIkTMXLkSEkHSCiARKQ4aCkty1Mb2BBGW+MFUC8vOSmLQliDkU51XgmD/Z1J7RQiwod6eSWGuKX8+1+v8rSwdhFxYezYsSgqKsKRI0fw6aef4rLLLsPMmTNx1VVXwePxAAA++OADXHrppWjbti02b96MX3/9FTNnzsRjjz2GG2+8EXydfRLHcZg/f740A/REmWoY/Bwv3T/hEJWq4fLly1Hj71A9b948mM1mbNmyBddeey3+9re/STpAQgHEO8XBXQXUlANJGdK+vlw4iwDwQsF/SsvoX4c2NPJRHoWwBiMjH/j9O0oRTQSi4UURL9mgiFdiiFfEK7mFUKPjcwupuxnqqdPneR7Vbm/zJ8aBJLMxIrEFq9WK3FwhG6JNmzbo378/Lr74YowcORIrV67ExIkTMW3aNFxzzTUhJTu33XYbcnJycM011+C9997DDTfcIP5u+vTpWLp0Ke6//3707t07tg8UbY1X8HPI8AqLqAyvrKxAd3ODwYA5c+ZINiBCYXg9QU0bJZ7wzUmCEllNmWCsaMXwCpaSj6Xgn6TJ5YMiXuqggiJeskOGV2IQHaASR7w4Tvj7KT8mGHcqMryq3V70nP+ZLO/9y9/HINkSWyvcESNGoF+/fli7di1atGiBs2fP4r777qt33tVXX42uXbvi7bffDjG8hg4div3792POnDn4+OOPYxoLGV6JI6pUw+3bt+Pnn38Wf/7www8xfvx4PPTQQ/V6ehEqp7IE4H2CyICUTRsZWly0pRDWAILENSjVMOFEIyXPyCCDOWFQjZf8sHnOWUw1HvHCVQG4HMKx1BGv4NdUeBNlLdK9e3ccOXIE+/fvBwD06NGj0fPYOcE8+eSTWL9+Pb766qvoB8H7AlLwsRpeWiobiRNRmet33HEH5syZgz59+uC3337DDTfcgD//+c9YvXo1qqqq8Nxzz0k8TEI24tW0kZGWC5T8oq3CXkeMPbwYweIaGm9MqDiY0RRVxMvvMS4jcY24QzVe8pPcMihV7VRsgkJEw7D73JIKWNOkf32VNFGuS5LZiF/+Pka295YCnudDUhbr1nEFwxr4BtOzZ09MnjwZc+bMwTfffBPdIESHCQcYojALmBIi7xOaMHOxRQK1TlRXZ//+/Tj//PMBAKtXr8all16KVatW4ZtvvsGNN95IhpeWiFdeOUOLkvKx9vBipLUGwAFelyBNnpod89CIMOD5oIhXFGk31EQ5MXhcQPU54Zjk5OXDYBDWh7JjQuYCGV7SIwprxGkdVmnmCcdxMaf7yc3evXvRsWNHdOnSRfx5yJAhDZ7H9t11WbhwIbp27Yp169ZFN4jgNMNoHLwGo2Cw+TzCa0VjvOmIqFINeZ4XJTA///xzXHnllQCA/Px8nDlDvWs0RbyENRhabKIcXOMVCyZLIIXKQemGCaO6FHBXCsfRbCJZpNNVLojGEPGB1Z4aLUBSprxj0TskBBRf4u4AJUl5Odi0aRN+/vlnXHvttRgzZgyysrKwZMmSeud99NFHOHDgQKMy8fn5+Zg+fToeeuiherLzYRFLfReDRb08lG7cHFEZXgMHDsRjjz2G//znP/jyyy/FZsqHDx9GTg7l2muKeEnJM7Q44ZdL0MOLQZLyiYelCKbmAGZb5M+3pgJJfgEiinrFD6ff8ErNpTRcuVFpxEQ1xEtKniF+fxpahxWGy+VCcXExTpw4ge3bt+OJJ57AuHHjcNVVV2Hy5MlISUnByy+/jA8//BC33347du3ahSNHjuDf//43CgoKMG3aNDHI0RBz587FyZMn8fnnn0c+OGZ4RdPDi0ECG2ETleH13HPPYfv27Zg+fTrmzZuHzp07AwDWrFnTYIiUUDFxj3hpcMEWxTWkMLxIYCPhxCIlzyCBjfgjzk3k7JMdLc7jSiJhES/6/uLF+vXrkZeXhw4dOmDs2LHYvHkznn/+eXz44YcwGoV6seuuuw6bN2/GsWPHcMkll6Bjx4647bbbMGfOnBCJ+YbIysrCgw8+KLZ6iggx4mWO/LkMMrzCJqpEzL59+4aoGjIWL14s3kCERqCIV2TUOIQUMyD2VEMgSFKeDK+EURaDsAYjPR8o+okiXvFEbHNB9V2yYycHUVyJe8QrKOWfhJwkZ+XKlVi5cmVY515yySVYv349AKCmpgbjxo3DypUrccsttyA7O1Dn3ZAIx9y5czF37tzIB+iJQdGQQYZX2EQV8WLU1tbi999/x7Fjx3Ds2DGUlJSgqEgjG2hCIFERr4oSbUgRsxoHW4Y06lOUaph4JIl4+UU5yknZMG6IiqtkeMkORbziS6IiXp5qoa8moQhsNhs+/PBDTJ48Gf/73//i90aS1HiR4RUuUasaTp06FVu2bAl5nMliRlXcRygTR5wNL61JEUulaMggT3LiEZsnx9BIlJooxx8nRbwUgyiuQYZXXIhX82SGOUlwFtaUCWs+idUoBpvNhjlz5sTvDXieDK8EE5Xhdcstt8BkMuHjjz9GXl5eSA8CQkO4nECtUziOl6fNYBA2TuXHhQmfDK9Q2OuQWljiYIYX1XgpG3EzSoaX7NiD2oL4vPHp+ahXfN5Af614rcOA8B3WlAlpjTk94/c+hLLweQD40xalqPHyeQCfT9jbEQ0SleG1c+dO/Pjjj+jevbvU4yGUhNi0MS0+TRsZaXnCBlULhb2SG17+DbyzGPDUxqY6RIRHLM2TGWLEi1IN4wbVeCmH1ByAMwrNUytK4msg6I3KM/6mtAYgpVX83ictDyj5RXVNlIkYYREqg1m4x6LFYBSez/v8vbyiUATWCVFd5Z49e1K/Lj0gCmvEeRFlGyctTPiioqEEwhoAkNISMFoB8NoRIFEyrgqhjxcgTY1X5WnAXR37uIj6UI2XcjAYA/M4pRtKC3NIprQCjHFsTKvFnppE80iRZggIgiyUbhgWURleixYtwgMPPIAvvvgCZ8+ehcPhCPlHaIR4C2swtFSYzVICY9m0B8NxJCmfSFi0y5YO2OzRv05SJmBO8b8mfW+S46kFqs4KxxTxUgbiPE5p0ZISb2ENBqsf00LmCRE+UhleQCBVkQyvJonKfTJq1CgAwMiRI0MeJ3ENjRFvKXmGliTlRUU8iSJegBA9O/cbbeATARPDSI9BWAMQDOaMfOD0r0K6YcsusY+NCFBZIvxvMAeaVRPyoiUHmpKIt5Q8gyJe+kSK5skMMeKlAYXqOBKV4bV582apx0EoEYp4RYbPF/gMUoqEsOiZgwyvuFMugaIhI6OdYHiRwIb0sLTk1Bwq4lYKorIhRbwkhSJeRDzxSNA8mUGphmERleF16aWXSj0OQolQxCsyKk8LEw5nkNZYpV5eiUOK5skMkpSPH8zwojRD5aAVB5rSSJgDlCJeuoRFp4zW2F+LDK+wiNpV+NVXX+Evf/kLhgwZghMnBA/Xf/7zH3z99deSDY6QmYRHvIqEnhJqhRlGqbnSeI8YVOOVOKRonswgSfn4QVLyyoMMr/iQMAeo//UrT1OqmMoZPnw4Zs2a1eQ5HTp0wHPPPRdU40URr0QRleH1/vvvY8yYMUhKSsL27dvhcrkAAOXl5XjiiSckHSAhI/Funsxgr++uBFwqFmdxSCwlzxAjXpTCE3fE5skU8VI0JCWvPCjVMD4kygGa3EKomQSvDYVhBVFQUACO48BxHMxmM3JycnD55ZdjxYoV8Pl88g2M9wmtCoCoxDXYZ+I4DikpKejS+3wUzHoEP+74qZ4T3ev14tlnn0WfPn1gs9mQmZmJK664At98803IeStXrgTHcRg7dmzI42VlZeA4Dl988UXE41QaURlejz32GF566SW8+uqrMJsDVvLQoUOxfft2yQZHyIjPG9jcxDu33JIsqMgB6k5zkLqHF4Nt4CniFX/KpIx4+evEKOIlPSQlrzxCmijLuJnUGmKNV5wjXgZDUGsXFa/DCmXs2LEoKirCkSNH8Omnn+Kyyy7DzJkzcdVVV8Hj8cgzKJ/f6OKMUTc9f+2111BUVIQ9e/bgH8v/gYrKKgy6ajLeWPmaeA7P87jxxhvx97//HTNnzsTevXvxxRdfID8/H8OHD8e6detCXtNkMuHzzz/XrJ5EVIbXvn37MGzYsHqPp6eno6ysLNYxEUqg8nRimjYytFDYyyJSUioaAgFPsqscqFFxRFDpeFxAhd/TK4W4hiiKchLwyrSwahUnRbwUR2ouAE5IM2JS/0Rs1FYK8z4Q/4hX8HuoxfDieeEayfEvwrIIq9WK3NxctGnTBv3798dDDz2EDz/8EJ9++ilWrlwpnnfs2DGMGzcOqampsNvtuP7663Hq1Cnx9wUFBRg/fnzIa8+aNQvDhw8Peczj8WDGjBlo164dWrVqhYcffhh83TH7/OuS0YKysjLcdtttyM7Oht1ux4gRI/DTTz81+7kyMjKQm5uLDh06YPSYMVizYhkm/ekKTJ85C6WlQk/M9957D2vWrMEbb7yB2267DR07dkS/fv3wyiuv4JprrsFtt92GyspK8TVTUlJw6623Ys6cOc1fWBUSlbhGbm4uDh48iA4dOoQ8/vXXX6NTp05SjIuQG5ZXnpoT36aNjLRc4PRedac4SFkfFIw1FbBlADVlQhpPLP2liMZhEUVTkpB2EyupOUL6hrdW+N4y28f+moQAiWsoD5MFSG0lZEo4TgCp2XKPSP2w+9ycAljT4v9+ahPYcFcBT8Q5EtgYD50ELCkxvcSIESPQr18/rF27Frfddht8Pp9odH355ZfweDy46667cMMNN0ScYvf666/j1ltvxcaNG/Hrr7/ir3/9K9q1a4dp06YFTmJphiYLJkyYgKSkJHz66adIT0/Hyy+/jJEjR2L//v3IyoqgZYfRgnumTcIbaz5GYWEhrr/+eqxatQpdu3bF1VdfXe/02bNnY+3atSgsLAwxKBcsWIDOnTtjzZo1uO666yL67EonqojXtGnTMHPmTGzbtg0cx+HkyZN46623cN999+HOO++UeoyEHCQqr5yhhcJstnG3SxzxAkjZMBGUBykaclzsr2cwBO4FSjeUloogOXlCOWhhHlcSorBGnjRzUnNoIfNEZXTv3h1HjhwBAGzcuBE///wzVq1ahQEDBmDQoEF444038OWXX+L777+P6HXz8/OxdOlSdOnSBZMmTcKMGTPw7LPPhp7k77n79Xc78d1332H16tUYOHAgunTpgmeeeQYZGRlYs2ZNZB/IaEH3zh0AQPxc+/fvR48ePRo8nT2+f//+kMdbt26NmTNnYt68efKlYsaJqEIZc+bMgc/nw8iRI1FVVYVhw4bBarXivvvuw4wZM6QeIyEHiVJSYqgtxaEhWFG51DVe7DVP7SbDK55IWd/FyMgHSg+TwIaUeN1A5RnhOFGOISI87G2AkztIYEMqEu4AVVnEy5wsRJ7kem8J4HkenN+o3rt3L/Lz85GfH1iDevbsiYyMDOzduxcXXnhh2K978cUXi68LAIMHD8aSJUvg9XphNPrruXjBoPnpl/2oqKhAixahmR7V1dU4dOhQZB/IaBZTGoPfv16aYx0slvriHg8++CBefvllrFixAtdff31k41AwURleHMdh3rx5uP/++3Hw4EFUVFSgZ8+eSE1NlXp8hFzQhB8ZHldAjETqVEOAIl6JoFzCHl6MdBLYkJyKEgA8YDBJkxJKSAdFvKQl4Q7QIIEUNcBxMaf7yc3evXvRsWPHsM83GAz1jBi3O0r5f7+4RkVVDfLy8hpMZ8zIyIjsNY0W7D14GADEz9WlSxfs3bu3wdPZ4127dm3wvefOnYuFCxfiqquuimwcCibqPl6AYKGmpaUhLy+PjC6tISopJcjwUnuKA/PwmmxAcgT50OFCUs3xR2yeLIGwBoO9FpOpJ2InOM3QENMSRkiNOE+pdB5XGrI5QOn7SwSbNm3Czz//jGuvvRaAkHZ3/PhxHD8ecNT98ssvKCsrQ8+ePQEA2dnZKCoKNYx37txZ77W3bdsW8vO3336LLl26BKJdgGh49e8/AMXFxTCZTOjcuXPIv5YtW0b2oYwWPPfqKtjTUjFq1CgAwMSJE3HgwAH83//9X73TlyxZgtatW+Pyyy9v8OVmzJgBg8GAZcuWRTYOBRPVquXxePDwww8jPT0dHTp0QIcOHZCeno6//e1v0VvehLJgBlBagjxtao94lQelGcYjF58k5eOPKI4ipeFFTZQlx0n1XYqFHETSImfKf4SqfUTTuFwuFBcX48SJE9i+fTueeOIJjBs3DldddRUmT54MABg1ahT69OmDSZMmYfv27fjuu+8wefJkXHrppRg4cCAAQZDjhx9+wBtvvIEDBw7gkUcewe7du+u937FjxzB79mwcOHAAb7/9Nl544QXMnDkz9CS/uMao0WMwePBgjB8/Hhs2bMCRI0ewZcsWzJs3Dz/88EOTn6usrAzFxcU4evQoCgsLcd1NU7Bq3Xq8+ORDyEgX2gTdeOONGD9+PKZMmYJ///vfOHLkCHbt2oU77rgDH3/8Md58882Q1lTB2Gw2LFy4EM8//3xE11vJRJVqOGPGDKxduxZPP/00Bg8eDADYunUrFixYgLNnz+LFF1+UdJCEDIjNkxOkGsYMvMoSQXo7EUqKUhJPYQ0gIFFPG/j4IWXzZAY1UZYeUdGQ6rsUB6UaSkuiI17sfdxVQE05kJSRmPfVAevXr0deXh5MJhMyMzPRr18/PP/885gyZQoM/sg9x3H48MMPMWPGDAwbNgwGgwFjx47FCy+8IL7OmDFj8PDDD+OBBx5ATU0Nbr31VkyePBk///xzyPtNnjwZ1dXVGDlyJEwmE2bOnInbb7896AxmWBvAGc345JNPMG/ePNxyyy04ffo0cnNzMWzYMOTkNO3guuWWWwAIBlKbNm3wh6FD8d1/30D/Pj2EiJrRBI7jsHr1ajz33HN49tln8f/+3/9DbW0tsrKysGPHDjGa1xhTpkzBkiVL8Msvv4R3sRVOVLvbVatW4Z133sEVV1whPta3b1/k5+dj4sSJZHhpAWeCmjYyUrKFmg2fR6iVkroXVrxxsObJcajvAgI1Xo6TQnNSSrGSFp83SBxFYnENQDDM6XuTBtHwooiX4gg2vHg+MUp8WiZRzZMZlmTAli4YXc4iMrwkYuXKlSG9upqiXbt2+PDDD5s8Z+HChVi4cGGjv2e1Wj6fD0899RTsdrto3DGO7NsNnD0IGM0AxyEtLQ3PP/98RJGlRgUzin8W9nLeWtGJbjKZcN999+G+++4DAGzfvh2jRo3Ca6+9hsWLF4tPLSgoQEFBQcjLGY1G7NmzJ+xxKZ2odgFWq7VeDy9AKKRrSJmEUBmuCsDlb9SbKE+bweBvwAn1FPYGwyJe8VA0BITvgTMIE1nl6fi8h55xFgkLhcEkbZTX3sb/vbnoe5OKCop4KRb2nXiqgepSeceidnw+ee51tQlsENHhrRX+N8Vhz260hL5HA/Tv3x8bN25ESkpK5MqJKicqw2v69Ol49NFH4XK5xMdcLhcef/xxTJ8+XbLBETLBPMqW1MQ262UbXjWmqYiGV5widUZzwDClOi/pYamA9jaAwdj0uZFgNAc2TZQmKg1U46VczDYg2V+Mr8Z5XElUnRGcQeCExtSJQu311kR4ePxGkVEewwsALrjgAixYsADnnXee9GNQMFGlGu7YsQMbN25E27Zt0a9fPwDATz/9hNraWowcORJ//vOfxXPXrl0rzUiJxCEKayTYo2zPA04gsLFSE+Vx7OHFSG8rfDeO3wEMiN/76JHyOCgaMtLzhTTGsqNA24HSv77eEFMNE1R/SkSGvbVgNDhOArm95R6NemGGa2orwYGTKNSuMEyEh1d+w0uvRGV4ZWRkiPKXjOCGb4TKSbSUPEOtEz7PBynixfHvIL0t8Pt3FPGKB0xYIx7fX0Y+cPxbEtiQCtYvjwwvZWJvAxTvImXDWEm0sAaDIl76gAwv2YjK8HrttdekHgehJBItJc9Q64RfUw7UVgjH8VI1BIKUDWlDIznxaJ7MSCdJecnwevwNlBFIvSWUBSkbSkOipeQZwZLyhHaJq+FlDn0PIgSS2CLqQxGvyGCe3aQsQRUqXtAGPn7Eo3kyQ2yiTN9bzFSeBsADnBFIibCxJ5EYyPCSBtkiXvT9aR6eB7z+nrvxSGMVI17U17choop4nT17FvPnz8fmzZtRUlICn88X8vtz585JMjhCJijiFRnxFtZgsGgapRpKTzxTRamJsnSwzWhqK2lFUAjpoCbK0iCbA1TF6sJEePjcEPt4xSPixZQSfR6hVQvN1SFEZXjdfPPNOHjwIKZOnYqcnBxw1KtDWyS6eTJDrTK25XHu4cUQe3nRhkZSeD4o4hWPVMOgiBf1NooNqu9SPhQxkQa5HKDs/SpKhIhFIoU9iMQgRrss8VmPOKPQRoX3Ce9FhlcIURleX331Fb7++mtR0ZDQGIlunsxgnr3aCqDGkVgp+1iIdw8vBjPsKk4BHhdgssb3/fRC5Rmh7xA4wB6H75DdF7VOoKYMSMqU/j30ghjxIsNLsYgRLzK8YkKuiFdKttDP0OcR1pp4r2uE5AwfPhznn38+li5dCgDo1KkTZs2ahVmzZgknePytoOJlVHOcYNR5aoQ6L7MtPu+jUqKq8erevTuqq6ulHguhBHzeILnmBE/4lhTAmi4cqynqxQyveAprAEByFmDyT2C0qZGOcr+iYVpufJpJWpIDvY2ozis2nBTxUjyiA80pONCI6BBrvBLsADUYAo4NNbZ2USAFBQXgOA4cx8FsNiMnJweXX345VqxYUa9UJx5s27YNt99+e+CB4IiXhHTo0EH8nEntL0CHQX/E9TfdjE2bNjV4/uuvv44LL7wQycnJSEtLw6WXXoqPP/445JwvvvgCHMehV69e8Hq9Ib/LyMjAypUrJf0MiSAqw+uf//wn5s2bhy+//BJnz56Fw+EI+UeomMozAO8VwsRyNChVYxNlRwJ6eAGCF4m9B9V5SUdZAloBUJ2XNFRQDy/FY0kBbBnCsZrmcSXhrhai44A897pYb03fn1SMHTsWRUVFOHLkCD799FNcdtllmDlzJq666ip4PJ64vnd2djaSk4OEv+KoaPj3v/8dRUVF2PfdF3hj2d+RYU/DqFGj8Pjjj4ecd9999+GOO+7ADTfcgF27duG7777DH/7wB4wbNw7Lly+v97q//fYb3njjDcnHKwdRGV4ZGRlwOBwYMWIEWrVqhczMTGRmZiIjIwOZmZRGo2pYXnlKK8AYVSZqbLAJX02etkT08GKQ4SU98ZSSZ7B7g/ULI6KDzQtyOIWI8CGBjdhgBo85GbClJ/79SVJecqxWK3Jzc9GmTRv0798fDz30ED788EN8+umnYtTmyJEj4DgOO3fuFJ9XVlYGjuPwxRdfiI/t3r0bV1xxBVJTU5GTk4Obb74ZZ86cafS9O3XqhOeee078mctsh3+t+gB/mjQNycnJ6NKlCz766KOQ53z00Ufo0qULbDYbLrvsMrz++uvgOA5lZWVNfs60tDTk5uaiXYeOGHbxALzy7GN4+OGHMX/+fOzbtw8A8O2332LJkiVYvHgx7rvvPnTu3Bk9evTA448/jlmzZuHee+/F8eOhTsoZM2bgkUcegcvlavL91UBUhtekSZNgNpuxatUqbNy4EZs2bcKmTZuwefPmRkOKhEqQK6+coTZJeZ83cM3irWoIBGqQHGR4SUZCIl4kKS8JcqVBE5EhCmyQ4RUVwVLycojxqEQghed5VLmrZPnH83zM4x8xYgT69euHtWvXhv2csrIyjBgxAhdccAF++OEHrF+/HqdOncL1118f0XsvXPoKrr/uWuzatQtXXnklJk2aJCqSHz58GNdddx3Gjx+Pn376CXfccQfmzZsX0esH9/KaOXMmeJ7Hhx9+CAB4++23kZqaijvuuKPe02bPng232433338/5PFZs2bB4/HghRdeiGwcCiSqkMbu3buxY8cOdOvWTerxEHIjl5ISQ22S8hUlgjQrZ0xMwT9FvKQnkRGvcop4xYRoeFHES9GoZOOuWBwyCVwxVBLxqvZUY9CqQbK897abtiHZHHvfzu7du2PXrl1hn798+XJccMEFeOKJJ8THVqxYgfz8fOzfvx9du3Zt+gX8BmPB9Vdj4k03AWYbnnjiCTz//PP47rvvMHbsWLz88svo1q0bFi9eDADo1q0bdu/eXS9dsEmY+JfXjayWWWjVqhWOHDkCANi/fz/OO+88WCz1Ux1bt24Nu92O/fv3hzyenJyMRx55BA899BCmTZuG9HQZIsESEVXEa+DAgfXCgIRGkD3ipY4JX4QZQGl5iUnNTKdeXpIjRrzi0DyZwYw6inhFj88LVJYIxxTxUjaUahgbogNUpvucDOeEwfN8RC2ZfvrpJ2zevBmpqaniv+7duwMADh06FMYbCgIVfXt0EaNSKSkpsNvtKCkR5td9+/bhwgsvDHnaRRddFPYYAYREvMDz9T5ncxHDhoyyqVOnokWLFli0aFFkY1EYUe0UZ8yYgZkzZ+L+++9Hnz59YDaHSlL27dtXksERMhCc4iAHapvwHQmSkmeIES/a0EgGi0JlxNPw8r82iWtET+VpoS8MZxAkrwnlorZ5XGnI7gBVRxPlJFMStt20Tbb3loK9e/eiY8eOAACDQYiFBBslbrc75PyKigpcffXVDRofeXlh3C9+YQ2zxRrSX4vjOGkVFg1mABwAHmdLinD69Gnxc3bp0gVff/01amtr6xlYJ0+ehMPhaDByZzKZ8Pjjj6OgoADTp0+XbqwJJirD64YbbgAA3HrrreJjHMeJFm1dyUdCRThk9rSpNeKVMMOLpaxRxEsSahxATblwnIhUw6qzQG2loPxGRAZLM0xpRQ05lQ4ZXrEhd8o/e19HkaKbvnMcJ0m6n1xs2rQJP//8M+655x4AgvogABQVFeGCCy4AgBChDQDo378/3n//fXTo0AEmUxRbeI/fkGtiDu3WrRs++eSTkMe+//77yN6H44Sol7cWy5Y9D4PBgPHjxwMAJk6ciBdeeAEvv/wyZsyYEfK0Z555BjabTbQz6jJhwgQsXrwYCxcujGw8CiIqw+vw4cNSj4NQCk6ZPW1swa44BXg98igrRoJoeCVAWAMIpPDUOgWDQQ7FKy3BIlBJWfE1hpIyAKsdcDmEeyab6mMjhuq71AOlGsaG3BEv9r7uSmHOonUmZlwuF4qLi+H1enHq1CmsX78eTz75JK666ipMnjwZAJCUlISLL74YTz31FDp27IiSkhL87W9/C3mdu+66C6+++iomTpyIBx54AFlZWTh48CDeeecd/Otf/4LR2IxTiknJGxrfW91xxx1YunQpHnzwQUydOhU7d+4UlRebS4t0Op0oLi6G2+3G4R0/4c331uBfq9bhySefROfOnQEAgwcPFrPmamtrMX78eLjdbrz55pt4/vnnsXLlSrRo0aLR93jqqacwZsyYpj+ngolqV9u+fXupx0EoBYdMTRsZKdmCUAXvr+eQq7g4XETDKwFS8oDQjDcpC6g+J7w3LYixweTd4xntYqTnAyV7hDovMrwih/XwSoSIDREbbN6uKQdcFYA1Vd7xqA25miczLCmANR1wlQt7AlpnYmb9+vXIy8uDyWRCZmYm+vXrh+effx5TpkwRUwwBQShj6tSpGDBgALp164ann34ao0ePFn/funVrfPPNN3jwwQcxevRouFwutG/fHmPHjg15nUYJw/Dq2LEj1qxZg9mzZ2PZsmUYPHgw5s2bhzvvvBNWq7XJl58/fz7mz58Pi8WC3FbZuPiCntj4f6tx2R+vDTnvueeeQ9++ffHPf/4Tf/vb31BTUwOLxYJNmzZh2LBhTb7HiBEjMGLECGzYsKH5z6tAog4nHDp0CM899xz27t0LAOjZsydmzpyJ8847T7LBBXPNNddg586dKCkpQWZmJkaNGoVFixahdevAxLRr1y7cdddd+P7775GdnY0ZM2bggQceCHmd1atX4+GHH8aRI0fQpUsXLFq0CFdeeWVcxqw6aiuFiRaQz9NmMAo9epwnhQlfLYaXPUERL0CIrjHDK6dX4t5XiyRCSp6R4Te8SNkwOpynhP+pebLysdkBS5oQmXcWAdYuco9IPfh8QdFdGe91ex5wulz4/lp1l28cGmDlypVixKg5evTogS1btoQ8VleIokuXLk1K0LOeX6xm67fffgsYZd5a8Ce219uz1O3Pdc011+Caa64Rf3788cfRtm1b2Gy2Rt+XqRaKOIoEh1lyw9GrW2+9VSxZOnLkCC699FL885//xNChQ8XI3fDhwxsU4vjss88aHYfSiUrV8LPPPkPPnj3x3XffoW/fvujbty+2bduGXr16obCwUOoxAgAuu+wyvPfee9i3bx/ef/99HDp0CNddd534e4fDgdGjR6N9+/b48ccfsXjxYixYsACvvPKKeM6WLVswceJETJ06FTt27MD48eMxfvx47N69Oy5jVh1ssjenCGlRcmFXUZ0XS6VJVI0XQHVeUpIIYQ1GOikbxoQYBSDDSxVQL6/oqDortCgBJ++9rrZ6ayI8WMTLWF81MJh//vOf+P777/Hbb7/hP//5DxYvXowpU6ZE9l7sPdh7NkGHDh3wxRdfoHv37vXq2rRGVBGvOXPm4J577sFTTz1V7/EHH3wQl19+uSSDC4YVHwJCquOcOXPEvFCz2Yy33noLtbW1WLFiBSwWC3r16oWdO3di6dKluP322wEAy5Ytw9ixY3H//fcDAB599FEUFhZi+fLleOmllyQfs+pghdB2mZo2MtQy4burBaU1IMGGF/XykoxER7yAQHojERkVFPFSFfbWwJl9JLARKUxYIyU7IMktBySQok3CNLwOHDiAxx57DOfOnUO7du0we/ZszJ07N7L3Mvnfw9O84QUIKY4LFiyI7D1USFSG1969e/Hee+/Ve/zWW2/Fc889F+uYmuXcuXN46623MGTIEFHKfuvWrRg2bFiINOWYMWOwaNEilJaWIjMzE1u3bsW9994b8lpjxozBunXrGn0vl8sFl8sl/uxwOAAIEp91ZT7lgI1BirFwZcdhAuBLzYVXxs9mSM2FEYC37Hf4FHCNgwm53s5jMAPgzcnwmFKBBI2VXR9f2XFZv6dEIeU9Xhdj2TEYAHhSW4OP87XkUlsLf19lxxT/vcXzmkeL0XFS+K6SWsb9u0o0SrzesWJMzYMBgLf0uOLmcUC515wrFdZhPi0XHjnX4ZQcYR0uPyHZ9xfLNXe73eB5Hj6fT1rZcw3DUvTYdQN4GHweAIDPYBbSWhthyZIlWLJkSb3HI7r2BjMMAHivG7wGvjOfzwee5+F2u+uJmERyT0dleGVnZ2Pnzp3o0iU0b3vnzp1o1apVNC8ZFg8++CCWL1+OqqoqXHzxxfj444/F3xUXF4s9Ahg5OTni7zIzM1FcXCw+FnxOcXFxo+/55JNPNihbuWHDBiQnK0fGVIoUz86nvkAvACccPmyvIyWaSLoUl6MngBO/fo8d1fKNoykKCwvR0vkLhgKoMKRj06efJuy925SewUAA547swjcyfk+JJh5pzGNKDsEG4OvdR1H+W3yvZWblcQwD4Dp1ABtU8r3FK3U8GkafOYokAN/sOoSyg+q4fpGipOsdK91LKtENwLE932KXQ7nfl9Kuefszm3A+gOIqI76TcZ7ocPos+gEoObhT8nFEc81NJhNyc3NRUVGB2trwIiiEgNPpBAAYfLWwA+DBweGsiH9mE+9DBgAOPjjKSsGrvA1IbW0tqqur8b///Q8ejyfkd1VVVWG/TlSG17Rp03D77bfjt99+w5AhQwAA33zzDRYtWlQvotQUc+bMabYD9d69e8Wu3Pfffz+mTp2Ko0ePYuHChZg8eTI+/vjjiLp+R8rcuXNDPpPD4UB+fj5Gjx4Nu13GOig/brcbhYWFuPzyy+s1so4Uw4avgZNA6+4DkTtCPsER7ucK4KPVaGs3Ik9hwifB19vySzlwEEhp0z2hAi3c7y2BI/9EC2O1LoRhpLzHQ/DUwLxDEJMZ+seJQFKmdK/dEBUlwP6FsLnLcOWYUc2meshJ3K55tPi8MO0Usg2GjL5WPvGfOKG46y0B3PbTwKcfon2GCW0VOE8p9ZobvvwJOA606twPV14h4zq8D8Ca15GT7JNsnYnlmtfU1OD48eNITU1tUuCBCMDzPJxOJ9LS0sBxHLjaCqAGgMkKe3pilCp51wlwPg/sKTbwZmmaTstFTU0NkpKSMGzYsHr3IMuGC4eoDK+HH34YaWlpWLJkiZjz2bp1ayxYsAB333132K8ze/ZsFBQUNHlOp06dxOOWLVuiZcuW6Nq1K3r06IH8/Hx8++23GDx4MHJzc3Hq1KmQ57Kfc3Nzxf8bOof9viGsVmuD8plms1lRk7Uk4/HLNRvT28Ao52fLEGqYDBXFMCjoGgdjNpthqhSulyG9bWLHmSW0c+CcRTAbDbppJiv531z5UeF/SyrMadnx9/5ltAZMNnCeGpirSoCsjs0/R2YUM89VlAotJsDBnNFa+f39okQx11sKMgXBGkNFkWLncUCB17xS2KMY09vKuw5nCjWpBqf063A019zr9YLjOBgMhvBk0wkxLZBdN1bfxRkt4BJ1DY0WwOcB53ODM8SxV2YCMBgM4Diuwfs3kvs5qtWL4zjcc889uOeee8QQZlpaWsSvk52dLXbqjhR2Q7H6K9ZngIltAEI4u1u3bsjMzBTP2bhxI2bNmiW+TmFhIQYPHhzVGDSH3M2TGWJRr8LFNcoTKMwQTGqu0OvM5xaiKHJ/X2qFKRqm5ydGTIbjBGGUsweFe0cFhpdiYHNTSrZmjS7NQeIM0SF382QG+/4qSwCvh/7utIDXX4eUSNEWowVwV4WlbKgXojJ5Dx8+jAMHDgAQDC5mdB04cKC+jr8EbNu2DcuXL8fOnTtx9OhRbNq0CRMnTsR5550nGk033XQTLBYLpk6dij179uDdd9/FsmXLQtIEZ86cifXr12PJkiX49ddfsWDBAvzwww+YPn265GNWJXI3T2Yw1bJaJ+ByyjuWphCbJyewhxcgLIBM+ZGUDaMnkc2TGSQpHx1iD6+cps8jlAPbuFedBdw18o5FTYhtE2Q2vFKyBQcf7wsoihLqJkxFQ0mJQFJeL0RleBUUFNRr8AYIBlJzqYPRkJycjLVr12LkyJHo1q0bpk6dir59++LLL78U0wDT09OxYcMGHD58GAMGDMDs2bMxf/58UUoeAIYMGYJVq1bhlVdeQb9+/bBmzRqsW7cOvXv3lnzMqsPnE1MNZfe0WdOE5puAsqNe5TL08GKw93SQ4RU1iZSSZzAjr5wMr4hgc5Pcm1EifJIyAZO/psNJUa+wUYrhZTAGnKDOxgXICBUhh+FlIsOrLlHFjnfs2IGhQ4fWe/ziiy+OS/SoT58+2LRpU7Pn9e3bF1999VWT50yYMAETJkyQamjaoeoM4PMA4IBUBXiV7XnAGaewCGV3lXs09eH5oIhXglMNAcHwOg6KeMUCM34SGvHyN2qmiFdksI2fEuYmIjw4TsgGOHtQSDfM6tT8c/SOuxqoLhWO5XaAAoLx5zjhN5wHyD0aIlbkjHiF2ctLD0QV8eI4TqztCqa8vBxerzfmQREywPLwU1vJ27SRofQmyjVlgLtSOLbLkJrJ0htZ1I2IHFkjXtREOSKcFPFSJVTnFRlsvTMlAbYMWYcCIGD8KTnzREcUFBRg/Pjx0T2Z54NqvCjVUE6iMryGDRuGJ598MsTI8nq9ePLJJ/GHP/xBssERCUQp6Q0MpS/YDr/Bk9wSkEMiNZ1S1mJGjHi1S9x7ijVeZHhFhGh4UcRLVdj9DiIHOYjCIlhYIxGCP83B6r0pVTQmCgoKBDl3/78WLVpg7Nix2LVrV0yvO3z48BCxuCbxugHwALgEi2v434v3Ar7IAzPDhw8Xr5vVakWbNm1w9dVXY+3atQ2e//HHH+PSSy9FWloakpOTceGFF2LlypUh5xw5cgQcx6FVq1b1gkjnn38+FixYEPE4IyEqw2vRokXYtGkTunXrhltuuQW33HILunXrhv/9739YvHix1GMkEgEzcOSI3jSEwiNenJhmKEN9V/D7UqphdHg9gXtelojXCaGukggPqvFSJ0p3oCkNp0IErhgU8ZKMsWPHoqioCEVFRdi4cSNMJhOuuuqqxA1ATDM0J9aoN5gEkZbgMUTItGnTUFRUhEOHDuH9999Hz549ceONN4ZoOADACy+8gHHjxmHo0KHYtm0bdu3ahRtvvBF//etfcd9999V7XafTiWeeeSaqMcVCVIZXz549sWvXLlx//fUoKSmB0+nE5MmT8euvv5JQhVqhiFdEcOKmXSbDizzJseE8KXjgjJbE1g2ltQ5qBUAF62Ej1ng13nORUCAKn8cVh+gAVcg6LDpAlfn98TwPX1WVLP94no9orFarFbm5ucjNzcX555+POXPm4Pjx4zh9+rR4zs8//4wRI0YgKSkJLVq0wO23346KiooGX6+goABffvklli1bJkaEjhw5EhJdMxqNyMzMhNFoxBebBZ2EDheOwWOPPYbJkycjNTUV7du3x0cffYTTp09j3LhxSE1NRd++ffHDDz+I73X27FlMnDgRbdq0QXJyMvr06YO33347ZDzDhw/H3XffjQceeABZWVnIzc0NRI786YacJRn/+te/8Kc//QnJycno0qULPvroo2avXXJyMnJzc9G2bVtcfPHFWLRoEV5++WW8+uqr+PzzzwEAx48fx+zZszFr1iw88cQT6NmzJzp37ozZs2dj8eLFWLJkCbZt2xbyujNmzMDSpUtRUlLS7BikJOrGDK1bt8YTTzwh5VgIOXEozPBSeMRLVBOUO+JVeVooyFZ5R/iEI9Z3tQUS2YzTaBKM5vJjwhiUEmFWMr4gOes0MrxUBTmIIkNpDtA0ZUe8+Opq7Osvj+hHt+0/gktOjuq5FRUVePPNN9G5c2e0aNECAFBZWYkxY8Zg8ODB+P7771FSUoLbbrsN06dPr5cqBwDLli3D/v370bt3b/z9738HIPTGXbZsGZ566ikAQr/bRx99FGvXrkX3zh0ACKl+zz77LJ544gk8/PDDePbZZ3HzzTdjyJAhuPXWW7F48WI8+OCDmDx5Mvbs2QOO41BTU4MBAwbgwQcfhN1ux3//+1/cfPPNOO+883DRRReJY3r99ddx7733Ytu2bdi6dSsKCgowdOhQXN6/E+CpBgAsXLgQTz/9NBYvXowXXngBkyZNwtGjR5GVlRXRNZwyZQpmz56NtWvXYtSoUVizZg3cbneDka077rgDDz30EN5++20MGjRIfHzixIkoLCzE3//+dyxfvjyi94+FqHccX331Ff7yl79gyJAhOHFCmFT/85//4Ouvv5ZscEQCcSrM06bwFAeOGV72BPfwYiRlAmb/pE/e5MiRq/k1QJLykVJ1NkhxtZXcoyEigSJekaG0lH82DqU6QFXExx9/jNTUVKSmpiItLQ0fffQR3n33XRj8jr9Vq1ahpqYGb7zxBnr37o0RI0Zg+fLl+M9//oNTp+r3UUtPT4fFYhGjQbm5uTAajUhPTxd/3rJlC1auXIk1a9Ygt2WG8ESOw5VXXok77rgDXbp0wfz58+FwOHDhhRdiwoQJ6Nq1Kx588EHs3btXfN82bdrgvvvuw/nnn49OnTphxowZGDt2LN57772QMfXt2xePPPIIunTpgsmTJ2PgwIHYuHFjQFIeQqRu4sSJ6Ny5M5544glUVFTgu+++i/h6GgwGdO3aVewdvH//fqSnpyMvr/4e1mKxoFOnTti/f3/I4xzH4amnnsIrr7yCQ4cORTyGaIkq4vX+++/j5ptvxqRJk7B9+3a4XC4AgqrhE088gU8++UTSQRIJQHERL/+EX3FKKMg0GOUdT13kTjXkOOG9z+wX6rxanCfPONSKHM2TGSSwERksJTO5hTIUV4nwYY6pihJBTtqUQDU1NaLUiFdtBVDjAGx2ecdTBy4pCd22/yjbe0fCZZddhhdffBEAUFpain/+85+44oor8N1336F9+/bYu3cv+vXrh5SUFPE5Q4cOhc/nw759+5CTE1lK/I4dOzBlyhQ8/fTTQvun0sNs5Ojbt694HnvdPn361HuspKQEubm58Hq9eOKJJ/Dee+/hxIkTqK2thcvlQnKdiF/w6wJAXl6ekMYXpKIYfE5KSgrsdnvUqX48z4OLoF7NYqk//4wZMwZ/+MMf8PDDD2PVqlVRjSNSoop4PfbYY3jppZfw6quvwmwOLIRDhw7F9u3bJRsckUDYhK8UT1tKNsAZhDqcisTm34YDJ2cPLwYJbEQPM3rSE6hoyKCIV2Q4WZqhQjajRPgkt/BvuniqaQwHpRle1lTA6je2FNhEmeM4GJKTZfkXyYYfEIyMzp07o3Pnzrjwwgvxr3/9C5WVlXj11Vclvy7FxcW45pprMHXqVNx8883Cg6KwBReyb2efo6HHfH4BqMWLF2PZsmV48MEHsXnzZuzcuRNjxoxBbW2oWEbwa7DX8fl8IYZXo+dEiNfrxYEDB9CxY0cAQJcuXVBeXo6TJ+tH12tra3Ho0CF07dpwT9innnoK7777Lnbs2BHxOKIhKsNr3759GDZsWL3H09PTUVZWFuuYiETjrhb6UgHKmfCNpoDogdLSHHhfYEzpMqUaAlQ/EQtyNE9miBEvMrzCQtyMkpS86uA4SjcMF54PGDdKSfkHFC+woVY4joPBYEB1tVD71KNHD/z000+orKwUz/nmm29gMBjQrVu3Bl/DYrHU651bU1ODcePGoXv37liyZEngF8zwikLQ8JtvvsG4cePwl7/8Bf369Wswba9J4tA37PXXX0dpaSmuvfZaAMB1110Hk8kU+pn9vPTSS6iqqsLkyZMbfK2LLroIf/7znzFnzhzJx9kQUaUa5ubm4uDBg+jQoUPI419//TU6daLu9KqDLYjmZMCWLu9YgknLEzZdCjO8bO4ycLxXkElNpCJeXaiXV/TI0TyZQRGvyBCl5ElYQ5XY2wClR8hB1BxVZwObYyWpd9rzgDP7FFtvrRZcLheKi4W5rLS0FMuXL0dFRQWuvvpqAMCkSZPwyCOPYMqUKViwYAFOnz6NGTNm4Oabb240zbBDhw7Ytm0bjhw5gtTUVGRlZeGOO+7A8ePHsXHjRpw+fRpOpxNVFU609LhgsZgRjeXVpUsXrFmzBlu2bEFmZiaWLl2KU6dOoWfPnuG9QLDhxUce3aqqqkJxcTE8Hg9+//13fPDBB3j22Wdx55134rLLLgMAtGvXDk8//TTuu+8+2Gw23HzzzTCbzfjwww/x0EMP4bHHHmtSdf3xxx9Hr169YDJFrTkYNlFFvKZNm4aZM2di27Zt4DgOJ0+exFtvvYXZs2fjzjvvlHqMRLwJTm9QQtNGhkI9pUm1Z4WDtNby1p6xaBulGkaGzxe4ZrJEvPzpjWXHBC830TQkJa9uFDqPKw52fVKylVULR02UJWH9+vXIy8tDXl4eBg0ahO+//x6rV6/G8OHDAQiS6Z999hnOnTuHCy+8ENdddx1GjhzZpNrefffdB6PRiJ49eyI7OxvHjh3Dl19+iaKiIvTs2RNt2rRB9+7d0Sa/Hbb88JPgLI6Cv/3tb+jfvz/GjBmD4cOHIzc3F+PHjw//BQwmiAZfFE2UX331VeTl5eG8887Dn//8Z/zyyy9499138c9//jPkvHvuuQdr167FV199hYEDB4py8itXrsRDDz3U5Ht07doVt956K2pqaiIeX6RE9S3MmTMHPp8PI0eORFVVFYYNGwar1Yr7778ft912m9RjJOKNQ2H1XQyFSsonuf2Gl1zCGgyxxos8yRFReRrwuoQaQjlUKdn35q4Cqs4BKS0SPwY14aSIl6ohwys8lFbfxVC4wrAaWLlyZYOS8HXp06cPNm3a1OTrBNO1a1ds3bo15DGm8gcINVoOhwPpFh5c2RHAaAn5PaNuT7IOHTqEPJaVlYV169Y1OfYvvvii3mMhzzFawJ/YDrToHHJOc+VJDb1uU4wbNw7jxo0DAJw7dw4jR47Eiy++iGuuuUYUA6n7+Rgvv/wyXn755YjeLxqiinhxHId58+bh3Llz2L17N7799lucPn0a6enpYqEboSKYJ4sm/LBIrlWK4cVS1n6nyEkksBS/tDx5VPLMNiDFL4teTsqGzUKGl7qhWtTwUJqUPEOhDlAiAnxu4f841FqFDVtrvbVNnychWVlZ+PzzzzFy5Mh6BqqcRGR4uVwuzJ07FwMHDsTQoUPxySefoGfPntizZw+6deuGZcuW4Z577onXWIl4IUrJK2xjo9AUBzHVUE5hDSCwQLsrA+IoRPOIioYyKlJmkMBG2FSQqqGqoYhXeCg14iU2UabvT7UwY0dOw4ulzybQ8AKAFi1aYP78+Rg5cmRC37cpIko1nD9/Pl5++WWMGjUKW7ZswYQJE3DLLbfg22+/xZIlSzBhwgQYjQrrt0Q0j1OhnjaFRryS3OeEA7kjXuYkILklUHVGiHolZco7HrUgKhrKICXPyGgHnPiRBDaaI1jpTU4hGyJ6yPAKD6VGvOwU8VI9HgUYXuy9vW75xqAQIjK8Vq9ejTfeeAPXXHMNdu/ejb59+8Lj8eCnn36KuKcBoSCU1jyZIUa8lDXhByJeMkZMGOltA4ZXbp/mzycCUSY5hDUYJCkfHlXnAmkyZHipE5Zq6CwCvB6hVQhRH8VGvPzrcMUp+v7UihIiXuy9PYmNeCmRiFINf//9dwwYMAAA0Lt3b1itVtxzzz1kdKkdsXeIQj1tLgfgqpB3LEGIES85hBnqQk2UI0cRqYb+aBtFvJqGScknt1CW0hsRPinZgqoZ7wukjRL1UWIPLwBIbQVwRuH7qzwt92gaFEUgmoE5r0wy1DQzjPKkGkqJVPdeRIaX1+uFxRJY/EwmE1JTUyUZCCETPp9yPW3WNMDiv7/YoiQ37ipYPU7hWO5Uw+AxkOEVPnI2T2aIES8S12gSNjeRlLx6MRipTigcHAoVuTIYA9FmGeutzWbBaKiqqpJtDKqE94HzeYRjJUS8vLWqFQOrrRWMxlhLqiKKGfM8j4KCAlitVgBCh+y//vWvSElJCTlv7dq1MQ2KSCBVZ/3eEE554hqAsAidPSBM+C07N39+vPErc/GWFHBKaDZtp15eEcHzQc2T5azxoibKYeFkwhoKnJuI8LG3Fu51xwkAF8o9GuXhrgGq/ZkUSjO8ACEK5zwplCXIlOhhNBqRkZGBkpISAELfK8q2ahqfzwePqxo1Hh6AAXC5Ac4jz2B4H+DhAfBAVYU8isIx4PP5cPr0aSQnJ8fcZDmiZ0+ZMiXk57/85S8xvTmhAJgHKyVbmX8Idr/hpRCBDU4sgG6rjGbTLOJFUs3hUVMG1CogYskiXtWlgMspRHeJ+jgVqrhKRAYJbDQNu89NNmWKJClEUj43V5gHmPFFNA3P86itLIfV4xAiTpVH5B2QoxTweYByoypTxw0GA9q1axezwR+R4fXaa6/F9GaEAhGbJyvQywYoT1LeH1ni7W2hALMrtJcX0Tws2pXcErAkyzcOmx2wpQM15cKYcnrKNxYlU0ERL01AvbyaJjjdXwkOvbooxHDmOA55eXlo1aoV3G5Sx2sOt9uNgx++g47Fa4H2fwCufk7eAa15DCj+CRjzJNDxcnnHEgUWiwUGQ1Ttj0MgeRq9IzZPVpiwBkNhkvKcw2/gKEWIhPUSc5wEfF4hH59oHCXUdzHS2wE1PwtjIsOrYajGSxsoZOOuWJQqJc9gjg+FKAwbjUZqXRQGRqMRKc7fYKs4DiSnADabvAOy2YCK44DjsPxjkZHYTTdC3Si1eTJDYREvlmrIK0FYAxCKng0mgPcqR4BEyYj1XQowvDJIYKNZxBovkpJXNWR4NY1SBa4YafT9qZVA+xsF7FmothkAGV6EUpsnMxQW8YKDpRoqQEoe8CuG+b87SjdsHiU0T2aQpHzzMDl5pW5IifCwB0XmifooPeWfmiirFsX1HQV037+SDC+9o9TmyQyFNVHmWI2CUgwvIEhggwyvZmHRJSUYXtREuWl4PhDFpebJ6sYelLng88k7FiWi9JR/MeKljHWYCJ/k2jPCgSLWPHI2AmR4EUpt2sgQPW3FQg2TnPA8UO6Xk1dC2J5BvbzCRwnNkxmUdtE01aWBZptKTYUmwiM1B+AMgqKZAprwKg61rMO1TkGFlVAHPg9s7lLhmNY8xUCGl95RuqctpZWwYPNe+RfsqnPgPNXCsZKuFxPYKCfFsGZRlLgGRbyahG1GkzIBk1XesRCxYTQHopakbFgfpTZPZljTAIu/5QXVEqsHZxEM8IE3WpSRNcDWvJpyoMYh71hkhAwvPeOuFrzKgHI9bUaTYHwB8qcb+lP5akzpytoIUsQrPGorhYbhgEK8f/60i4piwOOSdyxKhOq7tAUJbDRMcEqtku91sd6avj+1wDFHo70NIIEMesxYUwN96nQc9VLAN0HIhti0MQmwZcg6lCZRisCG37CptrSQdxx1SafwfVgww9RqB5IyZB0KACC5hfC3B5DR3BBU36UtyPBqmKpzgNfveFGy4aWQJspEBJQrTAwMIIENkOGlb4KVlJTYtJGhFEl5ZniZs+QdR12oOWl4KElKHhD+5ijnvXHUEAUgwofmqYZh61pyS8BkkXcsTUGGs+rgmENPKWseQAIbIMNL34i9QxRUr9QQFPFqGuZBqjoL1FbJOxYlU84UDZW0CFEvr0YRDS+KeGkC2rg3jNKl5BkKa6JMNA9LNVSUGBg5G8nw0jViQa/CFcOUkuIgGl4Ki3jZ0gFLqnBM3uTGUVrECwhqoqzfRahRWI1XqsLnJyI8qJdXwyhd4IpBTZTVB+s7qqQ1j0SlyPDSNU6VeNqU4in1GzVVZoVFvDiOBDbCQUmKhgyqz2sc5ynhf6U7hojwEOdxcg6FoJaIFzVRVh2iuAZFvBQFGV56xqEWT5tCJnylphoCZHiFA/OwKaGRJCOjvfC/jr1/jSKmQpPhpQmCI148L+9YlITqIl5keKmC4L6jdgUZXiSuQYaXrlF600aGXQETvtcjbgQVaXhR4XrziN4/JRlezPtHNV4h8DxQQREvTcEcaF6XoORHCKhmHfaPr+IU4PPKOxaiearOBvqOKkrVkNqokOGlZ1TjafNvvFzlQi8mOXAWAbwPvMEMl8kuzxiaglLWmsZTG4jwKjHV0HGSNjPB1JQBnhrhmGq8tIHJEujJSA6iAMyhqHT1zpRWAGcAeC9QeVru0RDN4RdsqjFlKKvvaErLQBsVnc4DZHjpleCmjUr3tFntgDlFOGZjTjQshc/eRlh8lAalGjaN4wQAHjDZgJRsuUcTIC0XMJgAn0f+VFolweq7bBmA2SbrUAgJUUq9rpIQHaAKX4eNpkBPPfr+lI/fCVultAyd4Jp0naYbKnAHSSSEqrOAt1Y4VrpHmeOCJOVlmvAdLFdaodHBdH8qQbk+PUjNElxkrKSedQZjIA1Ep4tQg1B9lzahlOhQPC5hLQYCRqmSUUq9NdE8ZczwainzQBpA5wIbZHjpFWbApGQru2kjQ+4JX4nqQMEER7yocL0+SpSSZ2RQQ8l6UH2XNqGIVyhsPTNagaRMeccSDvT9qQf/elKtRMNL55LyZHjpFadK8soZck/4/hQ+Pk1BRarBME+ypxqoLpV3LEpEiVLyDGqiXB82Pyk9Gk9EhtzzuNIIlpJXUiS+MaiJsnpQtAozRbwIPeJQSV45Q/aIlz81Jl2hhpfJGihc1+lk1iRlClQ0ZGSQ4VUPsYdXjrzjIKSFUg1DUYvAFYOtwyQpr3z864niarwA3a95ZHjpFbU0T2bI7SllES8l9cOoCwlsNA6Ta1dyxIsM5gAVfhEdtTiGiPCQex5XGmppnsxg35+Tvj/FU67gGi9xzdPnXoUML72ilubJDNkjXsIkxiupH0ZdSGCjcRRd46XvfPcGYeqlqRTx0hTBhhfVoqov5Z8iXurAVSGWHCiyxouteY4TgM8n71hkgAwvvaLaiJcME76rQugrBChXXAOgyElj+HwBz1qGElMNmbgGCaOIOCnipUnYPO6uBGrK5R2LEhDXYZU4QMWIl0xtXYjwYI5iWzo8xiSZB9MAaa2Ftjze2oCQko4gw0uviBsblUz4rKi3ojjxHhJWj2C1A9a0xL53JLBonE7D941SUQz43ABnVOZG3t4WACcIo1SekXs08hPcY5BqvLSFOQlIyhKOKd0wqHmySkRk2PzpKgdqK+UdC9E4Yt9RhTqKjabA3lOHjmIyvPQKW/TUEvFKzQHACY1mqxK8OWWTmJKjXUBgfFS4HgpL4bO3ESZ8pWGyBDZe5fosNg7B5RCMUIBUDbWIKLBBhpfqxDVsdsCSKhxTuqFy8YtW8Eres+hYYIMMLz3irgGqzwnHSowANITRDKT6VfsSvWCrxvDSd8FqoyhZSp6h874mIbBolzUdsCTLOxZCesS0cZ07iHhefeIaQFC9NRnOioWlGiqxppmh4/0KGV56hOWVm2zqaNrIkEtgQwzbK1hYAwiIaziLAK9H3rEoCeZRU/IilEH1eSJimiFFuzQJKRsKVJcCXpdwrBYHKBAwEinipVxEMSkFO4t1vOaR4aVHgpWU1NC0kSHXgs08s0qexAChj5fBDPA+anAZDEW81AXVd2kb6uUlwNax5BZCH0a1QBEv5aOmiJcO1zwyvPSI2ponM2SLeClYijwYgyFIUl5/4ftGUbKUPEPH+e71YD28qL5Lm1CqoYDoAFVJfReDJOWVj9LFNQCKeBE6Q21S8gy5UhxYX6x0hacaAoGJVu+bmmBUEfFikvL6W4Tq4fTLC1OqoTahVEMBtQlcMaiJsrLxusU9nqLFNYIjXjpro0KGlx5xqKxpIyNNhgmf59UjrgEExkgbeAGeD4p4KbCHF4OaKAdwqkxim4gMUjUUUFvzZAZFvJSN44RQbmC0AinZco+mcdhepdYZ6JOqE8jw0iPMcFFL00aGHBGvyjP+AmhOHSkhlGoYSnWp0KwVULbhzLx/rnJqLFtBES9Nw+ZxlwOoccg7FjlRW/NkBjVRVjbBwhqcgrf4lhShvhHQ3X5Fwd8KETfE4nWVetoSGfFy+CeE1Byh35LSESNelGoIACg7KvyfmgOYbfKOpSmsqYHGsnqPerENKdV4aRNrmtAqANC3CJDamicz2DpcUQz4fPKOhaiPGlLrGToV2CDDS484VBrxYhN+TTlQW5WY91RTmiGg694YDaIGYQ2GjouNRXiearz0AAlsqK95MiM1R4ik+DxA5Wm5R0PUhdY8xUOGl97gefVGvGzpgNnfUDVRnlLR8FKBsAYQVD9BhhcA8v6pDZczkBpKhpd2IYENdTZPBgCjSWhdApDAhhJRiwozELTm6UvNlwwvvVF1Lqhpo8o2NhyXeEl50fBSwSQGBCJz1aWAq0LesSgBVXn/mLKhvhahEFh9lyVNqAEgtIneDS+PC6g6IxyrLeIFUBNlJaNGZyNFvAhN41Rp00aGuGAn2vBSSaqhzR6on9BzGg9DXIQUrGjIoIgXKRrqBb03UWZZJ0YrkJwl71iigZooKxdVORv1WRpBhpfecKi0aSMj0RM+2xjYVZJqCJCyYTAshUFVi5CeDS+q79IFeo94BTsYOE7esUQDScorE58vsO6rKeKlM2cjGV56w6nSpo2MRKc4qC3iBQQpG5Lhpcq0C53lu4dQwepPyfDSNHrv5aVWgSuGPcEp/0R4VJ4OtL9Rg7OYZaJUlgDuGnnHkkDI8NIbam2ezEhkE2VPbSAlRE2Gl97TeBiuCqHWDVBJxIstQqcBd7W8Y5EL9veWmiPvOIj4ondVQ7U2T2ak6TxiqVSYszUtDzCa5R1LOCRlBgTTdOQoJsNLb6i1eTIjkREvZxEA3p+H3zL+7ycVFPESYNEuW7pQ+6Z0kjIBs19QQq/fnVoVV4nIYOtPdWniWoMoCYp4EfGACTOpIcMDENJsRYEN/WR6kOGlN9S+sUmkqmGwlLxBRX8qOlUKqkeZioQ1AGERYmPVa7qhk1INdYEtPeBk0OPmXe33uZh5osPvTsmoSViDkaG/Oi8V7SYJSRB7h6jU0yYaXsVCIWk8YYaXGnKlgxHFNXSaxsMoOyr8n64SwwsggQ2q8dIHHKfvdEO1pxqyiFdNuT4jlkpFTTXNjHT9KRuS4aU3WKqhWif8tFwAHOBzA1Vn4/teDpX18GIEpxryvLxjkRM1L0I68v6FINZ4keGlefSsbKj2VEOrXd8RS6Wi5oiXjpyNZHjpCY8rYKyodcI3moGUbOE43gIbwamGaiKtNQBOUDeqPCP3aOSDFiF14aoAav1Nv9NIXEPz6FUEiOfVH/HiuEBUWo+Gs1IpV6GzmGWk6MjZSIaXnmCTvdEqFPKrlUQJbLBUPTUpGgKAyRJQhXPoJ3xfD4p4qYsKfw8vSypgTZN3LET8SdeppHx1KeDxS2er1fACAs5bingpB7WJawBBGTr6qWsmw0tPOFTetJGRKEl5NfbwYpCyoUojXn7vnx4jXmwDR1Ly+kCvqYbsPk/KAsw2eccSC2ITZZ19f0qlxiHU3AEqW/P8Y3WcBHxeeceSIMjw0hNql5JnJCzixcQ11Gh46Vxgw+MKCDWoRdUQCCyYjpOA1yPvWBKN2hVXicjQa6qh2gWuGCQpryyYsy4pE7CmyjuWSEjLAwwmwOcJrAEahwwvPaH25smMRES8ahyAi3mPVFbjBZCkPDOaTUlAcgt5xxIJqTmA0QLwXv1tSEXDiyJeukC3ES+VC1wxqImyslBjhgcAGIyBuUAn+xUyvPSEU2OetnhGvNim15ahznoTvacaBtd3qSmt1mAIRAJ0sgiJVFDES1ew+7zytBCh1gtixEvl9zlFvJRFucr6VgajM4ENMrz0hNqVlBhMTSmeE75ahTUYek3jYaiteXIwGfpahEREKXmKeOmCpEzA5K9x0tPmXTPrMMs80Ud6mOJhhpca9yw6E9ggw0tPaMXTlogUBzVPYgBFvMr8E7ja0i4A/UrKU42XvghpoqyjdDWtGF7BES+fT96xEOpNNQQCa55OnI1keOkJMbdcI6mGNWWAuzo+7yEKa6iwvgsIGF7OYsDrlncscqBGKXmGmHahD++fCNV46Q+7DiXl1d48mZGaA4ATRBGqdNwvUimoes1jzkZ9OIrJ8NILPK+diJctQxBNAOKXouJQeaphckuhXxt4fW1qGKL3T42phjqNeLE+Xqm58o6DSBxixEtHKdFaiXgZzUBKtnCsxzVGaWgh4qWTNY8ML71QXQp4/QXMat/YcFz8BTbU2AE+GIMhSFJeH16kENTYSJKhxybKtZWAyyEcp6l8fiLCR2+php5aQUwEUH/ECyCBDaWg1vYpjGBxDZ6XdywJgAwvvcAWNrU3bWSIhb3xMryY90ilqYaAfgU2fN7A/a5GwzkjKO1CL7UTLM3QnKxOFVEiOvQ2R7HNsdGirjYXjUGS8sqA/f2orX0Kg+2z3JVCkEDjkOGlF7QiJc8QI15xmPB9vqCNu0pTDQH99vJyFgl1BwaTOqMn9jYAZxAi1Mw7rnVYmmFarrrk/4nY0FvES+ylqZH7nCJeyqAsSAxMjfeVOSmQtqqD2mYyvPSCQyNNGxlpcZzwK08D3lph86vm66VXZUO2CNnbCM0Z1YbRHLjvdLAIAQj8Has9DZqIDL0ZXloRuGKIES8yvGRFzcIaDB05isnw0gtOjQhrMOK5YDv8hkpqrrAJVitijZdO0ngYam4kyRAXIb0YXkERL0I/sFRDvaivakXgiiFGvHRiOCsVNQtrMDL0o2xIhpdeEJWUtOJpi2PESxTWUHGaIaDjiJeKe3gxdNbXJDA/keGlK5JbAgYzAD6QbqplNLsOUxNlWdFSxEsHax4ZXnpBa562tDiqGoqGl4qFNQDA7je8HDozvLQQ8WJj10HaBYDQGi9CPxgM8a3XVRpaczDoLVVUqZSruH0KQ1zztJ/lQYaXXtBabnlwUa/U8qPlKu/hxWCGY005UOOQdyyJpEzFUvIMHXn/AFCNl57Rk7KhQ2MiV8wBWlMGuKtlHYquCRbXUCts7DpY88jw0gtai3ixDZrPDVSdlfa1yzWQLw0Isty2DOFYD5sahqby3bW/CAEIqvHKkXccROJhRogealGdGhO5sqULEuYARb3kwucLrO9acDbqYM0jw0sPeFxA1RnhWCsTvskSkB+VesJnqYZ2lacaAkF1XjrY1ABC9JN9f6pehPTVUFLsb6SV+YkIH72kq/G89hygHEeS8nJTccqvwmxUd0YTW6+rzgK1VfKOJc6ozvByuVw4//zzwXEcdu7cGfK7Xbt24ZJLLoHNZkN+fj6efvrpes9fvXo1unfvDpvNhj59+uCTTz5J0MhlxKmxpo2MeAlsODSSaggEGV7a9yIBACrPAJ5qAFygxk2NsO+t1imk8WgZd7WQDgsAqRTx0h16STWsKfPPTdCWg4Ek5eWFre321oDRJO9YYsGWAVjShGONC4KpzvB64IEH0Lp1fave4XBg9OjRaN++PX788UcsXrwYCxYswCuvvCKes2XLFkycOBFTp07Fjh07MH78eIwfPx67d+9O5EdIPE6NNW1kxMNT6nEFCv21YHixTY3GJzIRVpiblitERdWKJVlQfAO0n/POHEOmJCF1idAXeol4McMkKVNoGKsVSFJeXrSg4gsIe9MMfbRRUZXh9emnn2LDhg145pln6v3urbfeQm1tLVasWIFevXrhxhtvxN13342lS5eK5yxbtgxjx47F/fffjx49euDRRx9F//79sXz58kR+jMTj0JiwBiMeES92rUw2bUQHmfGodW8yQwv1XQy91HkxwystR1uOISI8xIiXxjfuWhO4YsRTYZhoHq20vwF0I7ChmrjkqVOnMG3aNKxbtw7Jycn1fr9161YMGzYMFkvAyz1mzBgsWrQIpaWlyMzMxNatW3HvvfeGPG/MmDFYt25do+/rcrngcrnEnx0OQR3O7XbD7Za/4SMbQ1NjMZT9DiMAX2oOvAoYs1QYUnKEz1X2u2Sfizt3BCYAvL0NPB5Pvd+Hc72VBJeaBxMAX9kx1X73kVxzw7kjwj1hb6Paz8sw2tvCcHIHvGcPw5fgz5LI+5wrPyHco6m5qv/OokVt84qkJLWCGQDvLILHVQMYjAl520Rfc670d03e5+I6XH6i2c+l6/s8ThhKj8IIwJvWpt46obbrbUhrI3yWc0cTvubFSiTXWBWGF8/zKCgowF//+lcMHDgQR44cqXdOcXExOnbsGPJYTk6O+LvMzEwUFxeLjwWfU1zcePO/J598EgsXLqz3+IYNGxo0AOWisLCw0d/1PLEFXQD8dqYGezRU09bu7GlcAOD0bz/jW4k+V/7Zr9EfwJlaK7Y08ZpNXW8lkVVxHJcAqCo6gI0q/+7DueZ9fv8anQAcOlOLX1T+eXuddaMzgMM7/4c9Z+SJ4CXiPu9Usgl9ABQ5ffhB5d9ZrKhlXpEU3oerYYCB92LT/72DGnNmQt8+Ude8a/H/0APA8XIPdmroPs8rLcJFAEqP78XXYX4uXd7ncWLQoR+RC+Dn42U42sj1V8v17nyqEr0AnNy7Ddur1fU3UlUVviCIrIbXnDlzsGjRoibP2bt3LzZs2ACn04m5c+cmaGQB5s6dGxIlczgcyM/Px+jRo2G32xM+nrq43W4UFhbi8ssvh9lsbvAc47p1QAnQse8QtL/4ysQOMI5wh6zAsX+jVZIHV14pzecyfP0rcAxo0en8Bl8znOutKMr7AAceR4q3DFdeMRbgVJVdDCCya258bxVwGujU/1J0GKDue93w/e/AhvXolGVGe4nu73BJ5H1u2PQDcALI7XI+rhyt7u8sWlQ3r0gMdygXcJ7EyIE9wLfpn5D3TPQ1N3y6CSgC2vYYiNaXauc+537PBo4sR5apptl1WO/3eTwwvfIkAKD30CvQ67wRIb9T2/Xm9tQA695Dm1QfchO85sUKy4YLB1kNr9mzZ6OgoKDJczp16oRNmzZh69atsFqtIb8bOHAgJk2ahNdffx25ubk4depUyO/Zz7m5ueL/Dc+ciIsAADK/SURBVJ3Dft8QVqu13vsCgNlsVtSN3OR4/GIRxoy2MCpozDGTKUQBOGexdN9FhZCHb8hsB0MTr6m0779RMvMBzgDOWwuzq0zVfZLCuuYOId/d2KKT+u/1LCGCb3D83uS9GE8Scp9XlgAAjOmt1f+dxYhq5hWpSW8DOE/CVHUKSPDnT9g118M6bDQChuade7q9z6UmqH2KqUXHRv92VHO9W7A174Rsa160RHJ9ZTW8srOzkZ2d3ex5zz//PB577DHx55MnT2LMmDF49913MWjQIADA4MGDMW/ePLjdbvECFBYWolu3bsjMzBTP2bhxI2bNmiW+VmFhIQYPHizhp1IgrGjZrtGi3upzgLsGMNtif03W7ypdAz28AMBoFppNO08KRomKDa+wIHEN9cF6eKU27gAjNI4elA21Kq7BWkD43EIPptTm93SERNSUCy1HAG3sWdi67TgJeD3qlsdvAlXkHbVr1w69e/cW/3Xt2hUAcN5556FtW0EF5aabboLFYsHUqVOxZ88evPvuu1i2bFlImuDMmTOxfv16LFmyBL/++isWLFiAH374AdOnT5flcyUEng+Vk9cSSZmC+iAgnbKhlhSCGGIvL41LyteUAy5/Pyg1N09mpAc3lKyUdyzxxOnPQtDa/ESEjx56eWmteTLDZAFS/MYWSconFuaUS24BWFLkHYsUpOYABjPAezV9L6nC8AqH9PR0bNiwAYcPH8aAAQMwe/ZszJ8/H7fffrt4zpAhQ7Bq1Sq88sor6NevH9asWYN169ahd+/eMo48zlSXAp4a4VhLTRsBQXpaakl5ZpyoufluXZgnrFzDmxogEO1KytLGIpSUAVj9daRaNpq16hgiwkfrES+vG6g8LRxrLeIFkKS8XGgpwwMQ0lTZfkXDkvKqjON16NABPM/Xe7xv37746quvmnzuhAkTMGHChHgNTXk4Ndq0kWFvDZQelmbB1lrYnqGXiBfz/mkh2sVIzwdK9giLUHY3uUcjPe4aoKZMOCbDS79o3fByFgPgBW++FvpD1sXeGijepekohSLR6ppXekTT+xXNRLyIRmAeKC162QBpI17sD10rERNGuk5qhbTm/QOC6ryOyTuOeMHqu4xWwJYh61AIGdF6qqEY1c0LS3xCdVDESx7K/OtCejt5xyElGf7PotU1D2R4aR/mgdJaXjnDLuGEL9Z3aSjaBWh/U8NgE3WGhhYhZkRqNe0iuL6L4+QdCyEfLOLlLAJ8PnnHEg8cWl+H2fdHEa+EwvYsWot4Adpd80CGl/Zx+j3KWqvvYqRJOOGLhpeGJjFAP6mGWo54lWnU+0f1XQTgX584wFsriMloDc2vwyzzpFjecegNlsWiSTEwMrwItaJVKXkG27BJGvHS0CQGBAyRilOAxyXvWOKJVvPdAe0uQv7eRqIkNaFPjObAPaDFyLwoJa9Rw0vKzBMifDTtbNTomgcyvLRPcG65FglOUYkVtuDbNZZqmJwVkN3XavE6EIgKaSnVMKO98L9WFyGtz09E+GhZYEOrUvIMKTNPiPBw14jN5zW15onOxt+FdkgahAwvraP5iFdQikOsf6RajXhxnPbTDd3VAblmLXr/nEWAp1bescQDscaLIl66RzS8tBjx0rrIlT/zpLpUmIuJ+MPWcnOKoFqtFdhexVOtzbRjkOGlfbReQ8EML68LqDoX22tpMV+aoXWBDbYIWVK1tQilZPujlbw2v7sKjde+EOEjzlEajJpoXVwjKTOQVSFVT02iaUQxqXxtCROZrECqf7+q0dpmMry0jKdW200bAcBkAZJbCsexpDn4vIF0EC0aXlqvFRJldTW2CIVEKzX43bFifKrxIrSaasjz2k+p5TiSlE80Ws3QAYLaqGhwzQMZXtqGeZO12rSRIUVhb0UJ4HMDnDHgbdESWk811KKwBkPL8rpaV3sjwkerUfmacsBdJRxrNeUfkLbemmgeLQprMNh+RYtrHsjw0jYOjTdtZEhR2MsMkrQ8wGiKfUxKg/UmK9fYpoah5UVIq94/jwuo9qcHazUVmggfrUa8mCFiywDMSbIOJa6IES+NfX9KRQ/ORq2teX40vBsnNN88mSFFxMuh4bA9QBEvNZPuV6zSmvePSckbLdqqyyOiI9jw0pKamdYFrhhsHaaIV2IQnY0aUjRkMJVGje5XyPDSMnpJ45Ey4pWuMSl5ht1veGktjYehh4hX2VF5xyE1Yn1Xrrbq8ojoYOuUp1pQx9MKuluHyfBKCMHiGlpDTK8ncQ1CbejF0yZFE2WWgqfZiJffoHQ5hJoDrSFGvDTo/dNq2oW4ISVhDQKA2RYQStJSuprWmyczqIly4vB5A38jWnY2am3N80OGl5bRupISQyzqLY7+Nco1HDEBAEsKkJQlHGstfO/16GQROgH4fPKORUpYqiHVdxEMLdZ5ab15MoOaKCcOZzHg8wAGkzbnT+YAry4FXBXyjiUOkOGlZcQJX+sRL5ZbLkGqoV2jqYaAdgU2HCcA3ivUCmlRljyttaC26XMHlEq1AHMMaVFFlIgOLSob6sUBygwAZ7G2avSUCHMU21sDBqO8Y4kHtnTAmi4cazDqRYaXlhFTHDS+sWGGZdVZQSktGhwaTzUEtJuyFtz4WovqnUZTYEOqJYENJ0W8iDpoMuKll5R/v2HprRXWYiJ+aFlYgyFmemgsQwdkeGkXng+Vk9cySZmA0SocR1PY664JNJrWtOGlUWVDLQtrMLSY8y5GAsjwIvxo0fDSS8TLZNFmjZ4S0bKwBkPDAhtkeGmVmjJBHQrQvqeN42Ir7GXRLnOytmWttZjGA2hbSp6hxUWIaryIumhtjvK6gYoS4Vjr6zBAkvKJQlRh1vCap0Vnox8yvLSKQydNGxmxFPaWB/Xw0rKstWYjXn5jRBdpFxpahKjGi6iL1iJeFacA8IDBHIgGaZk0jX1/SqUsKL1eq7DPpqX0ej9keGkVp07yyhmxRLz0IKwBaNfw0lXESyOLkCeoDkTrKVhE+IgRL41s3B1B6bRarD+tC0W8EoOe1jwtORv96GAm0Cl6qe9ipMUw4etBWAMIfD7HSaEPiFagGi/1wdIMDWYgOUvesRDKgW3ca51AjUPesUiBXuq7GNREOf7wvE7ENfyfTSvOxiDI8NIqrKeV1nuHMNjCFo2nVOs9vBipuUGy5CVyj0YafL5ABE/T3j+2CB3ThlQzM7xSc7Sd3ktEhiVFSI8HtFHnpTcBGWqiHH+qSwF3pXCsZWcx2485i4RaSQ1BhpdWEaXkdZZqGI2nTazx0niqodEUZKBqYFMDCGqUXhfAGbSdKsoWWHcVUHVO3rFIAXMM6WVDSoSPlgQ29CIlz6CIV/xhNc0prQCzTd6xxJOUbL9aNa+NuSAIMry0itg8WS8Rrxgm/HKdpBoCQXVeGgnfs0UoLQ8wmuUdSzwx24SFFghICasZvUUCiPDRksCG7lIN/X/PWvjulEqwGJiWMRg0K7BBhpdWESNeOpnwg1McIknF4nl9SLMyRMNLIx4ksZ+JhnPdGVrKeScpeaIxtGR46S3ixT5n9TmhPyYhPXoQ1mBozVHshwwvraJXcQ2vS8iBDpeaskC+tB4WR5ZOqRVlQz0IazC0JLBBUvJEY2gp1VBvEa+kTH96GCjdMF7occ3TgrMxCDK8tIjXLdS+APowJgDAZAWSWwjHkXhKmQGS3FIf/c60JtGqK++fhhYhJ0W8iEbQSsSL54NS/nWyDnMcScrHGz1leTBRKa3sV/yQ4aVFnMXQVdNGRjR1XnoR1mBoyZsM6Mz7p6FFiMQ1iMbQiuHlcgSyKfQS8QKoiXK80dWapzFHsR8yvLSIU2dNGxn2KCTl9VTfBWivibIuI14aENeo8BteqTnyjoNQHlpxDrFoly0dsCTLO5ZEQhGv+KKH9ikMLWV5BKGjXbmOcOhMWIMRTRNlvSgEMdjnrDyt/uJnvTSSZGjF+xecCq23OYpoHhbxqikHXBXyjiUWRAeoTtIMGeI6XCzvOLRIbRVQdUY41sOeJdhR7PPJOxYJIcNLi+iteTIjmibKzKuq5R5QwSRlAma/91XtHuWaMqDWKRzrYhHyG17VpYDLKe9YYoE17zaYAnWZBMGw2QFLmnCs5qiJXlsmaCVVVIkwR7ElLdBoXMvY2wDgBNE05qzTAGR4aRG9NU9mRJPioLeIF8dpJ92QRbuSW+ojlcdmF9KWAHWnXgSnGeopFZoIH3HzrmLnkN6k5BnRZJ4Q4SEKa+QLa7nWMVkC95PaMz2CoFVPi+iteTIjJnENnRhegHZqKMqCFiG9oAWVJyfVdxHNoIWoid6k5BnRZJ4Q4aEnYQ2GVlLsgyDDS4vodcIPbqIcDj5vYHHQk+GllYiXKKyhg/ouhthEWcUCG6Kioc7mJyJ8tOAc0qsD1B5U48Xz8o5Fa+hJWIOhQYENMry0iG7FNfxe0qozgMfV/PnOYoD3CrUmevK+a6WXF3n/1IloeOnob46IDE1EvHSa8s/2HV4XUHVO3rFoDTbv69JRrOI1rw5keGkNng9EvPSWW56cBRitwnE4ikrMe5TWGjAY4zcupcF6lpWr2JsM6KuRJEML3r8KingRzaAFw0uvES+TNSCa41Tx96dE9OxsVPOaVwcyvLRGTTngrhKO9bax4biAglQ4dV4OHdZ3AdpJNdTzIqRm7x/VeBHNweYotaYaej1ApV+9U28RLyCoiTIJbEiKHtPrtVDXXAcyvLSGU6dNGxmReEr1KKwBBKUa/q7uHHw9NU9maCHiJaYa6kxmmwgftUe8Kk4BvE9IY0/Jlns0iUes81Lp96dEvJ6gmnQdrXlacDbWgQwvreHQaV45IxIpW5Zql66THl4MtqlxVwq9sNRIbSVQdVY41tUi5Pf+VRSHV8eoRMjwIpqDzVFVZ9XZ6F2M6ubqs2UCNVGWHudJf026WV/ZAmx9rykHahzyjkUidDgjaBy9Nk9mRCJlq9eIlzlJ6H0FqDfdkI3bageSMmQdSkJJbgGYkoRjNX53Xk+gEWYqGV5EI9gyAo3e1Rg1EYU1dHqPqz1iqUTE/UobfRnz1lQgKVM41kjUS0ffnk7Qq5ISI5ImyuyP2K4zwwtQv8CGHuu7AKGOUSw2VqGkfOVpADzAGYGUlnKPhlAqHKfuzbtehTUY1ERZevS65gEB57iaU+yDIMNLa9CEL/wfTooDK9zWW8QLUL+kfLkOmycz1PzdsY1Yait9KYkSkaNmw0vvDtC0CHtqEs2jRxVfhsYENsjw0hpi82RKcWiS2qqgGiE9Gl4qVzZk0R49ev/ULK9bcUr4X6/zExE+am6irHcHKIlrSI+eI14aE9ggw0trkLiG8L+zqGnFPraYW1IFBUi9oeZNDRBYhPTo/ctQsfdPjHiR4UU0A0W81EtakDiKWkWAlIYeVXwZWlDzDYIML63h1LmnjRlenhqgurTx84KFNTgu/uNSGmqPeOl6EfIbXmpchJwU8SLCRM2Gl94jXslZgNEqHFOdlzSIexYdrnkU8SIUi9cNVOi4aSMAmG1AUpZw3NSEr1dFQ4ZYJ6TyiFe6HiNe7LtTobiG3lOhifBRc1RevM91ug5zXOBvnOq8Yofng9Y8He5ZSFyDUCwVpwDw+m3ayBA9pU1M+Gwxt+ushxcjPWhT4/PKO5ZI8dQGNja6jHj5P7PjpPq+O1bjpac+NER0qDXiVeMAaiuEY71GvIDA90d1XrFTdRbwVAvHujS8NNC/MggyvLSEI6h+Qk99HuqSFkZhb7mOC1UBYeNrMAkNGdXW5NJxAgAPmGz6dDCk5Qrfnc+jvjQeMRKg4w0pER7MKVZRIjhb1AK7x63pgCVF3rHICSkbSgcTk0rNBUxWecciBykt1d2/sg463p1rEL3XdzHsYUz4wc0I9YjBGEiDUVsqT3lQyoUe6/MMxsCmVG2pF2KNF0W8iGZIbgEYLQB4wdOtFmgdFhAjXmR4xYyea5oBYZ1Xe116EGR4aQnyJguEFfHScQ8vhjiRqWzzrmdZXYYalQ19XqCS1aDqfI4imketTZQdVMcIgJooS4mehTUYGhLYIMNLS7DFya7Tgl5Gc02UeZ7ENQD1epD07v0DguR1j8o7jkioPA3wPoAz6DNFlIgcNQps6F1KnkHiGtKhZ2ENhoYk5cnw0hKkGCbQnJe0ujRQqKpXcQ0gkGapNmVDsXmyDhUNGWpsoswcISmthHRJgmgONUe8KNVQ+J/ENWKnXMd9KxnpFPEilIjemyczmktxYH+4Ka30WajKUGvEixleel6E1JhqyAwvqu8iwkWNhhel/AsEi2vwvLxjUTuis1HHWR6is1GFbVTqQIaXlqCiXgG2WFeeblgNS+/CGgy73/ByqMzwolRDdaZdMIEEvW9IifBRY6ohpfwLsL9zr0vIMiGih9a8oIiXyvYrDUCGl1bg+aCiXp1P+KIaFhpWwyJhDQE1Rrx8vqDvT8eLUEbQIqQWbzKLeFEPLyJcmPGipnRoingJmG1AUpZwrKaIpdJwVQQMV1rz/L1HffKOJUbI8NIKLifgrhSO9R7x4rimC3v13sOLwQyvqrNAbZW8YwmXimLA5wY4o743Nva2ADihVrHyjNyjCQ8x1VDnNahE+Kgt1dDrCTQJ13vECyBJeSlgjlFbOmCzyzsWOUlrLQgzeWsDf2MqhQwvrUBNG0NJa6Kwl01kehbWAISJ3JIqHKtlY8NS6+xtAKNJ3rHIickSMGDKVZLzToYXESliE+ViwahROpUlfuVOIyl3AkF1XipZX5QIOYoFjKbAvk5Ntc0NQIaXRuBYSp3eo12MppooOyjVEECdpoQqmcgo1z2A2uq82ByVSoYXESYp2YDBJBgzavByBysLk3JnYB1urLUL0TwkrBFAIwIbZHhpBcorD6WpJsrUjDAAM7zUUrxOi1AAtTWUdPo3zhTxIsLFYFRX1ISaJ4fS1DpMhAc5GwNoRFKeDC+NwImKhpRXDqDxJspeT8BI1buqIRBI5VGLwAYtQgHUFPHyeQMRC9qUEpEg1nmpwDlEDtBQ0prIPCHCo4xSDUUytKFsSIaXVqDmyaGIi3WdCd9ZJKStGMxCHy+9ozYPEi1CAdSUdlF1FuC9ADj6uyMiQ00CGyQlHwo1UY4dZmSQs1FdzsYmIMNLI3DkaQulsRQHUVijNWCg21+M+qlFrllsnkyLENJV1ESZzU8p2foWRSEiR029vGgdDoUiXrFD4hoB1OYobgTaeWoFSjUMJVhcI7jPkYN6QIWgpl5ePB+Uathe3rEogQy/4aUG759Y30U9vIgIoYiXemHXoeoM4HHJOxY14nUHlUbQniWQ5XFcPf0rG4AML41AEa86sOvgqQZqygKPi94jnSsaMoINL6VPZNXnALe/35jeWwEAgUXIVQ7UlMs7luag+YmIFjUZXnSfh5LcAjBahGNSNowcxwmhNMJopfYEQGC/UusM3depDDK8NADHe4HK08IP5GkTMCcBSZnCcXCag6hoSBt3AAEDxlMNVJfKO5bmYEZzag5gtsk7FiVgSQGSsoRjpUe9mLBGKkW8iAgRUw1VYHg5KPMkBI4L1J1TE+XIKQtyFFNphLDmJbcQjtWQpdMI9E1qAKu7HBw1baxPQ02Uy6mHVwgma0DsQOF50xy1AaiPWiTlxebJFAkgIiRYoMHnk3csTeFyCp54gO7zYNJUFLFUGiSsUR8NCGyQ4aUBbG5/pIKaNobSUBNl2rzXR0w3VHbxOkdS8vVRyyIkGl4U8SIiJDUH4AyAzxPI7FAi7B632gFrqrxjURLURDl6qDSiPmpxNjYBGV4aIEk0vMjLFkJDKQ4OpmpIqYYi6Srp5UVGc32YwEa5wiXlKyjiRUSJ0RxIUVWysiGL6FBLl1CoiXL0MBVfpmBLBDkbFb7mNQEZXhpAjHjZaVMTgphq6De8XBWBOibyIAVQiURrIOJFi5CI2iJeqbQpJaJADQIbJKzRMCQpHz2U5VEflexXmoIMLw1go4hXw9RNNWTeUqsdsNnlGZMSUUmfHI76mdRHDU2Ufb6AuAZFA4hoUIPhRVLyDWOv4wAlwqeM1rx6ZKjE2dgEZHhpgKRaMrwapK64hpiqRtGuENTSy4u8f/VRg/ev6qxQnwMOSG0l92gINWL3z1FKdg5RxKthxIiXgo1mJeLzkbhGQ4hrnsL3K01AhpcGCKQakqcthLoRLzK8GkacyJS7qTF5q8Gxvh3k/QvA0i4rTwPuannH0hisviu5hVCvQxCRQhEv9SKKaxQpv1ekkqg6A3hdADiqSQ9GXPNKAHeNvGOJEjK8NAClGjYCi3hVnhY6wDNvKU1ioTBxDedJwOuRdyyNkFR7VjiwZVCaaDBJmYDFr6CmVA8gSckTsaIGw4siXg3DroenRtVNbxMOS6VLyyOHVTBJmYA5WThW6prXDGR4aYAking1THILwGAGwAubP4p4NUxKK+E68T7F5uEn154RDijlIhSOU77KE0nJE7GihjpUsXkyGV4hmJOEzTKg2PVFkTClWlrzQgle85Su5tsIZHipHZcTJp8/3EqetlAMhiAp2yLqidEYBkOQR1mZG5skZniRrG59lN7XRJSSJ2ENIkqCI15KTFfzeYMEZMgBWg//NeHI8AofEtZoHJULbJDhpXb83mTemkZNGxvCHlTYy2qYyPCqj8ILVpNZqiF5/+qjdEl5kpInYoU50LwuoOqcvGNpiMrTAO8VGj2TgEx9qIly5JCYVOMofL/SHGR4qRyO8sqbhnnZHScp1bApRGVDZW7eAxEvWoTqofSIl5MiXkSMmCxCSjSgzKg8qz1LzQEMRnnHokT8f/sU8YoAcb9Ca149lL7mNQMZXmrHP5HxZHg1DEv7OLU7oBBEqSD1YQIbClU2pBqvJlBLxIsMLyIWlCywQQ7QpkmjXl4RQ6mGjcNKDpS65jUDGV4qR/QgURpPw7AUh9+/F/5PzRG8p0QoCu/lJaYa0iJUHyavq1Tvn1j7QptSIgaULLBBUvJN41+HKeIVASSu0TjifoXENQg5oIhX0zBP25n9wv+UZtgwzKBxKNDw8tTA5ikTjjNIXKMe4nd3QmiboCR4PqjGi1QNiRigiJd6IXGNyKhxADXlwjE5G+vDjFHHSUHYRmWQ4aVyqMarGepK+6ZTD68GYd5kJUa8/B5u3pQktAggQknNAYwWoR2A0jalVecAn98YJMOLiAUlG14kJd807LpUkLhGWLDshaRMEk1riLQ8wGACfB5VCraQ4aV2KOLVNHWvC3mPGoZFAqtLgdpKecdSB44Zgxn5Qg8PIhSDQbniKMwxlNyCUnyJ2FByqqHTbwxS/XDDsIhX5WlwPo/Mg1EBJKzRNAZjwBGjtDUvDMjwUjmBiBfVeDVI3Zx7O0W8GsRmB6x24VhpAhv+iZW30yLUKEoV2KggKXlCIijipV6SWwAGMwAE0saJxinz1y6R4dU4KhbYIMNLzXg9QGUJAIp4NYo5CbBlBH6mGq/GUWjUhGOGF313jaNUeV0nE9Ygw4uIESU3URYdoBTxahCDQcw+sdWWyjwYFUA9vJpHxQIbqjG8OnToAI7jQv499dRTIefs2rULl1xyCWw2G/Lz8/H000/Xe53Vq1eje/fusNls6NOnDz755JNEfQTpqSwBx/vggyHQ44SoT3DUizbvjaNQZUOO0i6aR/T+KWwRoog8IRVsHndXBoQHlICrAnA5hGOKeDWO/9rY3GR4NQtJyTdPhkKzPMJANYYXAPz9739HUVGR+G/GjBni7xwOB0aPHo327dvjxx9/xOLFi7FgwQK88sor4jlbtmzBxIkTMXXqVOzYsQPjx4/H+PHjsXv3bjk+Tuz4NzUuczo1bWyK4E0fGV6No9QaCop4NY9SI14VFPEiJMKcBCRlCcdKSjdkxf2WVMCaJu9YlIx/Dkgiw6t5KOLVPOkKXfPCQFWGV1paGnJzc8V/KSkp4u/eeust1NbWYsWKFejVqxduvPFG3H333Vi6dKl4zrJlyzB27Fjcf//96NGjBx599FH0798fy5cvl+PjxI4/r7zGnCnzQBQOS/8wWoHklvKORclQxEu9KLXGi/oMElIiOoeUZHgxYQ2KdjWJfx2miFcYUMSreURno7L2K+FgknsAkfDUU0/h0UcfRbt27XDTTTfhnnvugckkfIStW7di2LBhsFgCylljxozBokWLUFpaiszMTGzduhX33ntvyGuOGTMG69atS+THkA5nwPAiwdEmYOkf6W2EXHOiYdgkX7wL+OUjecciwosbG54WocYJXoQk+O44rxd5ZT+C+9UHGGOIpp/298+jiBchBfbWwKmfgf3rAXdVTC8l2T1+7Fv/2MjwahL/9cms+g3crx/Hds21DO8LZApQ38rGCRbX4HlVKR6rxvC6++670b9/f2RlZWHLli2YO3cuioqKxIhWcXExOnbsGPKcnJwc8XeZmZkoLi4WHws+p7i48T4ALpcLLpdL/NnhEHK53W433G55m5Uayn6HEUC1OVP2sSgZLiUXJgA+ext4Y7xO7Dpr8XpzqXnChFD8M/DezXIPR4QD4OOMcFuzAA1ed0lIagUTZwTndUny3ZkAXAQAh2N+KQCAJzkbPH13jaLleUVKDGl5MALA968K/2JA6nvcl5oX8/qiZdg63LLiV+D9ArmHo3h4czI8ZntMa56m55XkVjADgLsSbkcJkJwl63AiucayGl5z5szBokWLmjxn79696N69e0ikqm/fvrBYLLjjjjvw5JNPwmq1xm2MTz75JBYuXFjv8Q0bNiA5OTlu7xsOHU+fQ5uULnAmtcXPhYWyjkXJWNxW9M24CEeNF+O0RGIqhRq83hzvxflZlyDFpbyGhCczLsRvGzfJPQxFc17e9cgr/0HuYdSjwpqHnTuLgZ9ULGSUILQ4r0iJvbozeqX1htHnav7kBOI1WLC3tgfK1CzWFWdMXh7nZ1xEqYZhciJzMA5/+qkkr6XVeeUPKV3hMVixc8N/UWNpIetYqqrCj8DLanjNnj0bBQUFTZ7TqVOnBh8fNGgQPB4Pjhw5gm7duiE3NxenTp0KOYf9nJubK/7f0Dns9w0xd+7cEKPP4XAgPz8fo0ePht1ub3Ls8edKuN1uHCksxOWXXw6z2SzzeJTMjZBC99HtdqNQ09f7arkHUA+3243fNH3NpeJKyV5JyvvcDoBEtptG+/OKlPxVkleR+poPkWBMWsftvobu8zCxA+gR42tofl65UljzRsg8DCCQDRcOshpe2dnZyM7Ojuq5O3fuhMFgQKtWwnZ68ODBmDdvHtxut3iDFRYWolu3bsjMzBTP2bhxI2bNmiW+TmFhIQYPHtzo+1it1gYjamazWVE3stLGo3XoeiceuuaJh655YqHrnXjomiceuuaJha53/Ink+qpCaWDr1q147rnn8NNPP+G3337DW2+9hXvuuQd/+ctfRKPqpptugsViwdSpU7Fnzx68++67WLZsWUi0aubMmVi/fj2WLFmCX3/9FQsWLMAPP/yA6dOny/XRCIIgCIIgCILQAaoQ17BarXjnnXewYMECuFwudOzYEffcc0+IUZWeno4NGzbgrrvuwoABA9CyZUvMnz8ft99+u3jOkCFDsGrVKvztb3/DQw89hC5dumDdunXo3bu3HB+LIAiCIAiCIAidoArDq3///vj222+bPa9v37746quvmjxnwoQJmDBhglRDIwiCIAiCIAiCaBZVpBoSBEEQBEEQBEGoGTK8CIIgCIIgCIIg4gwZXgRBEARBEARBEHGGDC+CIAiCIAiCIIg4Q4YXQRAEQRAEQRBEnCHDiyAIgiAIgiAIIs6Q4UUQBEEQBEEQBBFnyPAiCIIgCIIgCIKIM2R4EQRBEARBEARBxBkyvAiCIAiCIAiCIOIMGV4EQRAEQRAEQRBxhgwvgiAIgiAIgiCIOEOGF0EQBEEQBEEQRJwhw4sgCIIgCIIgCCLOmOQegNrgeR4A4HA4ZB6JgNvtRlVVFRwOB8xms9zD0Tx0vRMPXfPEQ9c8sdD1Tjx0zRMPXfPEQtc7cTCbgNkITUGGV4Q4nU4AQH5+vswjIQiCIAiCIAhCCTidTqSnpzd5DseHY54RIj6fDydPnkRaWho4jpN7OHA4HMjPz8fx48dht9vlHo7moeudeOiaJx665omFrnfioWueeOiaJxa63omD53k4nU60bt0aBkPTVVwU8YoQg8GAtm3byj2MetjtdvrDSiB0vRMPXfPEQ9c8sdD1Tjx0zRMPXfPEQtc7MTQX6WKQuAZBEARBEARBEEScIcOLIAiCIAiCIAgizpDhpXKsViseeeQRWK1WuYeiC+h6Jx665omHrnlioeudeOiaJx665omFrrcyIXENgiAIgiAIgiCIOEMRL4IgCIIgCIIgiDhDhhdBEARBEARBEEScIcOLIAiCIAiCIAgizpDhRRAEQRAEQRAEEWfI8FI4//jHP9ChQwfYbDYMGjQI3333XZPnr169Gt27d4fNZkOfPn3wySefJGik6ufJJ5/EhRdeiLS0NLRq1Qrjx4/Hvn37mnzOypUrwXFcyD+bzZagEaufBQsW1Lt+3bt3b/I5dI/HRocOHepdc47jcNdddzV4Pt3jkfO///0PV199NVq3bg2O47Bu3bqQ3/M8j/nz5yMvLw9JSUkYNWoUDhw40OzrRroe6IWmrrfb7caDDz6IPn36ICUlBa1bt8bkyZNx8uTJJl8zmrlJTzR3jxcUFNS7fmPHjm32dekeb5zmrnlD8zrHcVi8eHGjr0n3eeIhw0vBvPvuu7j33nvxyCOPYPv27ejXrx/GjBmDkpKSBs/fsmULJk6ciKlTp2LHjh0YP348xo8fj927dyd45Orkyy+/xF133YVvv/0WhYWFcLvdGD16NCorK5t8nt1uR1FRkfjv6NGjCRqxNujVq1fI9fv6668bPZfu8dj5/vvvQ653YWEhAGDChAmNPofu8ciorKxEv3798I9//KPB3z/99NN4/vnn8dJLL2Hbtm1ISUnBmDFjUFNT0+hrRroe6ImmrndVVRW2b9+Ohx9+GNu3b8fatWuxb98+XHPNNc2+biRzk95o7h4HgLFjx4Zcv7fffrvJ16R7vGmau+bB17qoqAgrVqwAx3G49tprm3xdus8TDE8olosuuoi/6667xJ+9Xi/funVr/sknn2zw/Ouvv57/4x//GPLYoEGD+DvuuCOu49QqJSUlPAD+yy+/bPSc1157jU9PT0/coDTGI488wvfr1y/s8+kel56ZM2fy5513Hu/z+Rr8Pd3jsQGA/+CDD8SffT4fn5ubyy9evFh8rKysjLdarfzbb7/d6OtEuh7olbrXuyG+++47HgB/9OjRRs+JdG7SMw1d8ylTpvDjxo2L6HXoHg+fcO7zcePG8SNGjGjyHLrPEw9FvBRKbW0tfvzxR4waNUp8zGAwYNSoUdi6dWuDz9m6dWvI+QAwZsyYRs8nmqa8vBwAkJWV1eR5FRUVaN++PfLz8zFu3Djs2bMnEcPTDAcOHEDr1q3RqVMnTJo0CceOHWv0XLrHpaW2thZvvvkmbr31VnAc1+h5dI9Lx+HDh1FcXBxyH6enp2PQoEGN3sfRrAdE45SXl4PjOGRkZDR5XiRzE1GfL774Aq1atUK3bt1w55134uzZs42eS/e4tJw6dQr//e9/MXXq1GbPpfs8sZDhpVDOnDkDr9eLnJyckMdzcnJQXFzc4HOKi4sjOp9oHJ/Ph1mzZmHo0KHo3bt3o+d169YNK1aswIcffog333wTPp8PQ4YMwe+//57A0aqXQYMGYeXKlVi/fj1efPFFHD58GJdccgmcTmeD59M9Li3r1q1DWVkZCgoKGj2H7nFpYfdqJPdxNOsB0TA1NTV48MEHMXHiRNjt9kbPi3RuIkIZO3Ys3njjDWzcuBGLFi3Cl19+iSuuuAJer7fB8+kel5bXX38daWlp+POf/9zkeXSfJx6T3AMgCCVy1113Yffu3c3mOg8ePBiDBw8Wfx4yZAh69OiBl19+GY8++mi8h6l6rrjiCvG4b9++GDRoENq3b4/33nsvLE8dERv//ve/ccUVV6B169aNnkP3OKEV3G43rr/+evA8jxdffLHJc2luio0bb7xRPO7Tpw/69u2L8847D1988QVGjhwp48j0wYoVKzBp0qRmhZDoPk88FPFSKC1btoTRaMSpU6dCHj916hRyc3MbfE5ubm5E5xMNM336dHz88cfYvHkz2rZtG9FzzWYzLrjgAhw8eDBOo9M2GRkZ6Nq1a6PXj+5x6Th69Cg+//xz3HbbbRE9j+7x2GD3aiT3cTTrAREKM7qOHj2KwsLCJqNdDdHc3EQ0TadOndCyZctGrx/d49Lx1VdfYd++fRHP7QDd54mADC+FYrFYMGDAAGzcuFF8zOfzYePGjSHe52AGDx4ccj4AFBYWNno+EQrP85g+fTo++OADbNq0CR07doz4NbxeL37++Wfk5eXFYYTap6KiAocOHWr0+tE9Lh2vvfYaWrVqhT/+8Y8RPY/u8djo2LEjcnNzQ+5jh8OBbdu2NXofR7MeEAGY0XXgwAF8/vnnaNGiRcSv0dzcRDTN77//jrNnzzZ6/egel45///vfGDBgAPr16xfxc+k+TwByq3sQjfPOO+/wVquVX7lyJf/LL7/wt99+O5+RkcEXFxfzPM/zN998Mz9nzhzx/G+++YY3mUz8M888w+/du5d/5JFHeLPZzP/8889yfQRVceedd/Lp6en8F198wRcVFYn/qqqqxHPqXvOFCxfyn332GX/o0CH+xx9/5G+88UbeZrPxe/bskeMjqI7Zs2fzX3zxBX/48GH+m2++4UeNGsW3bNmSLykp4Xme7vF44fV6+Xbt2vEPPvhgvd/RPR47TqeT37FjB79jxw4eAL906VJ+x44dooreU089xWdkZPAffvghv2vXLn7cuHF8x44d+erqavE1RowYwb/wwgviz82tB3qmqetdW1vLX3PNNXzbtm35nTt3hsztLpdLfI2617u5uUnvNHXNnU4nf9999/Fbt27lDx8+zH/++ed8//79+S5duvA1NTXia9A9HhnNzSs8z/Pl5eV8cnIy/+KLLzb4GnSfyw8ZXgrnhRde4Nu1a8dbLBb+oosu4r/99lvxd5deeik/ZcqUkPPfe+89vmvXrrzFYuF79erF//e//03wiNULgAb/vfbaa+I5da/5rFmzxO8nJyeHv/LKK/nt27cnfvAq5YYbbuDz8vJ4i8XCt2nThr/hhhv4gwcPir+nezw+fPbZZzwAft++ffV+R/d47GzevLnBuYRdV5/Pxz/88MN8Tk4Ob7Va+ZEjR9b7Ltq3b88/8sgjIY81tR7omaau9+HDhxud2zdv3iy+Rt3r3dzcpHeauuZVVVX86NGj+ezsbN5sNvPt27fnp02bVs+Aons8MpqbV3ie519++WU+KSmJLysra/A16D6XH47neT6uITWCIAiCIAiCIAidQzVeBEEQBEEQBEEQcYYML4IgCIIgCIIgiDhDhhdBEARBEARBEEScIcOLIAiCIAiCIAgizpDhRRAEQRAEQRAEEWfI8CIIgiAIgiAIgogzZHgRBEEQBEEQBEHEGTK8CIIgCMLPkSNHwHEcdu7cGbf3KCgowPjx48Wfhw8fjlmzZsXt/QiCIAhlQIYXQRAEoRkKCgrAcVy9f2PHjg3r+fn5+SgqKkLv3r3jPNIAa9euxaOPPpqw9yMIgiDkwST3AAiCIAhCSsaOHYvXXnst5DGr1RrWc41GI3Jzc+MxrEbJyspK6PsRBEEQ8kARL4IgCEJTWK1W5ObmhvzLzMwEAHAchxdffBFXXHEFkpKS0KlTJ6xZs0Z8bt1Uw9LSUkyaNAnZ2dlISkpCly5dQoy6n3/+GSNGjEBSUhJatGiB22+/HRUVFeLvvV4v7r33XmRkZKBFixZ44IEHwPN8yHjrphqWlpZi8uTJyMzMRHJyMq644gocOHAgDleKIAiCSCRkeBEEQRC64uGHH8a1116Ln376CZMmTcKNN96IvXv3NnruL7/8gk8//RR79+7Fiy++iJYtWwIAKisrMWbMGGRmZuL777/H6tWr8fnnn2P69Oni85csWYKVK1dixYoV+Prrr3Hu3Dl88MEHTY6voKAAP/zwAz766CNs3boVPM/jyiuvhNvtlu4iEARBEAmHDC+CIAhCU3z88cdITU0N+ffEE0+Iv58wYQJuu+02dO3aFY8++igGDhyIF154ocHXOnbsGC644AIMHDgQHTp0wKhRo3D11VcDAFatWoWamhq88cYb6N27N0aMGIHly5fjP//5D06dOgUAeO655zB37lz8+c9/Ro8ePfDSSy8hPT290bEfOHAAH330Ef71r3/hkksuQb9+/fDWW2/hxIkTWLdunXQXiSAIgkg4VONFEARBaIrLLrsML774YshjwXVUgwcPDvnd4MGDG1UxvPPOO3Httddi+/btGD16NMaPH48hQ4YAAPbu3Yt+/fohJSVFPH/o0KHw+XzYt28fbDYbioqKMGjQIPH3JpMJAwcOrJduyNi7dy9MJlPIc1q0aIFu3bo1GpUjCIIg1AEZXgRBEISmSElJQefOnSV5rSuuuAJHjx7FJ598gsLCQowcORJ33XUXnnnmGUlenyAIgtAPlGpIEARB6Ipvv/223s89evRo9Pzs7GxMmTIFb775Jp577jm88sorAIAePXrgp59+QmVlpXjuN998A4PBgG7duiE9PR15eXnYtm2b+HuPx4Mff/yx0ffq0aMHPB5PyHPOnj2Lffv2oWfPnhF/VoIgCEI5UMSLIAiC0BQulwvFxcUhj5lMJlEUY/Xq1Rg4cCD+8Ic/4K233sJ3332Hf//73w2+1vz58zFgwAD06tULLpcLH3/8sWikTZo0CY888gimTJmCBQsW4PTp05gxYwZuvvlm5OTkAABmzpyJp556Cl26dEH37t2xdOlSlJWVNTr2Ll26YNy4cZg2bRpefvllpKWlYc6cOWjTpg3GjRsnwdUhCIIg5IIiXgRBEISmWL9+PfLy8kL+/eEPfxB/v3DhQrzzzjvo27cv3njjDbz99tuNRpMsFgvmzp2Lvn37YtiwYTAajXjnnXcAAMnJyfjss89w7tw5XHjhhbjuuuswcuRILF++XHz+7NmzcfPNN2PKlCkYPHgw0tLS8Kc//anJ8b/22msYMGAArrrqKgwePBg8z+OTTz6B2WyW4OoQBEEQcsHxjVX4EgRBEITG4DgOH3zwAcaPHy/3UAiCIAidQREvgiAIgiAIgiCIOEOGF0EQBEEQBEEQRJwhcQ2CIAji/7drxzQAADAMw/izHopoR20WUTvDux6ALxYvAACAmPACAACICS8AAICY8AIAAIgJLwAAgJjwAgAAiAkvAACAmPACAACICS8AAIDYAdY6dCLuZ8u5AAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Resumen estadístico por algoritmo:\n",
            "\n",
            "                Media  Desviación estándar\n",
            "DQN           -104.55            17.854603\n",
            "Double DQN    -358.35           178.699770\n",
            "Dueling DQN    -90.00            17.143358\n",
            "Boltzmann DQN  -92.90            14.171506\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se incrementó la capacidad de representación del modelo mediante el aumento del número de capas y neuronas, pasando de una arquitectura de 64/64/32 a una de 128/128/64. Esta mayor profundidad permite una abstracción más efectiva de las características del entorno, crucial para problemas de control complejos como Acrobot.\n",
        "\n",
        "Se incorporó regularización con Dropout, lo cual ayuda a prevenir el sobreajuste y favorece la generalización del modelo en nuevas situaciones.\n",
        "\n",
        "Se duplicó el número total de pasos de entrenamiento (nb_steps) de 100000 a 200000 para permitir una exploración y consolidación del aprendizaje más prolongadas.\n",
        "\n",
        "El factor de descuento gamma se incrementó de 0.98 a 0.99, dando mayor importancia a las recompensas futuras, lo que es coherente con el objetivo a largo plazo de equilibrar el péndulo.\n",
        "\n",
        "El tamaño del batch se amplió de 64 a 128 para obtener gradientes más estables durante el entrenamiento, y la tasa de aprendizaje learning_rate se redujo de 5e-4 a 1e-4 para que el ajuste de los pesos sea más fino y estable.\n",
        "\n",
        "La memoria de experiencias aumentó de 50000 a 100000 elementos, permitiendo al agente tener un mayor repertorio de experiencias para aprender.\n",
        "\n",
        "El periodo de exploración se extendió a 150000 pasos para permitir una exploración más rica antes de pasar a la explotación. Asimismo, el intervalo entre entrenamientos pasó de 1 a 4 para reducir la correlación temporal entre muestras y mejorar la eficiencia computacional.\n",
        "\n",
        "El número de pasos de calentamiento aumentó a 5000 para llenar adecuadamente la memoria antes de entrenar. Finalmente, el intervalo de actualización del modelo objetivo se duplicó (de 500 a 1000), lo que contribuye a una mayor estabilidad en la convergencia del aprendizaje."
      ],
      "metadata": {
        "id": "MyUoiGGWRVrk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Investigación y detalle teórico de variantes de DQN\n",
        "\n",
        "En esta sección analizamos y comparamos, a nivel teórico, las diferencias entre el agente DQN con política ε-greedy visto en baseline y tres políticas propuestas: política de Boltzmann, Double DQN y Dueling DQN.\n",
        "\n",
        "---\n",
        "\n",
        "### 2.1 Política de Boltzmann vs. ε-greedy\n",
        "\n",
        "- **ε-greedy**  \n",
        "  Explora con probabilidad ε escogiendo una acción al azar, y explota con probabilidad 1 – ε seleccionando la acción con mayor valor Q.  \n",
        "  - **Ventaja**: muy simple de implementar y entender.  \n",
        "  - **Desventaja**: la exploración es a ciegas (todas las acciones no óptimas tienen igual probabilidad).\n",
        "\n",
        "- **Boltzmann (softmax) policy**  \n",
        "  Define la probabilidad de elegir la acción \\(a\\) en el estado \\(s\\) como  \n",
        "  $$\n",
        "    P(a \\mid s)\n",
        "    = \\frac{\\exp\\bigl(Q(s,a)/\\tau\\bigr)}\n",
        "           {\\sum_{a'} \\exp\\bigl(Q(s,a')/\\tau\\bigr)},\n",
        "  $$\n",
        "  donde \\(\\tau\\) (“temperatura”) controla la exploración:  \n",
        "  - \\(\\tau\\) alto → distribución casi uniforme (más exploración).  \n",
        "  - \\(\\tau\\) bajo → mayor concentración en la acción de mayor Q (más explotación).  \n",
        "  - **Ventajas**:  \n",
        "    1. Exploración dirigida: las acciones se muestrean proporcionalmente a sus valores Q.  \n",
        "    2. Transición más suave exploración–explotación.  \n",
        "  - **Desventajas**:  \n",
        "    1. Selección de \\(\\tau\\) crítica.  \n",
        "    2. Cálculo de la softmax costoso si \\(\\lvert\\mathcal{A}\\rvert\\) es grande.\n",
        "\n",
        "| Política   | Exploración dirigida            | Parámetro | Complejidad  |\n",
        "|------------|---------------------------------|-----------|--------------|\n",
        "| ε-greedy   | No (equiprobable)               | ε         | Muy baja     |\n",
        "| Boltzmann  | Sí (proporcional a Q)           | τ         | Baja-media   |\n",
        "\n",
        "---\n",
        "\n",
        "### 2.2 Double DQN\n",
        "\n",
        "**Problema**  \n",
        "DQN estándar usa la misma red para seleccionar la acción \\(\\arg\\max\\) y evaluarla, lo que introduce **overestimation**.\n",
        "\n",
        "**Double DQN** (van Hasselt et al., 2016) propone:\n",
        "\n",
        "1. Acción seleccionada por la red principal \\(\\theta\\):  \n",
        "   \\[\n",
        "     a^* = \\arg\\max_{a'} Q(s',a';\\,\\theta).\n",
        "   \\]\n",
        "2. Valor estimado por la red objetivo \\(\\theta^-\\):  \n",
        "   \\[\n",
        "     Q(s',\\,a^*;\\,\\theta^-).\n",
        "   \\]\n",
        "3. TD target:\n",
        "   $$\n",
        "     y = r + \\gamma\\,Q\\bigl(s',\\,\\arg\\max_{a'}Q(s',a';\\theta)\\;;\\;\\theta^-\\bigr).\n",
        "   $$\n",
        "\n",
        "- **Ventaja**: reduce el sesgo de sobreestimación, mejora estabilidad y políticas finales.  \n",
        "- **Desventaja**: ligera complejidad extra (dos redes).\n",
        "\n",
        "---\n",
        "\n",
        "### 2.3 Dueling DQN\n",
        "\n",
        "**Motivación**  \n",
        "En muchos estados, el valor del estado \\(V(s)\\) domina, y la diferencia entre acciones es pequeña. Separar ambas estimaciones puede acelerar el aprendizaje.\n",
        "\n",
        "**Arquitectura** (Wang et al., 2016):\n",
        "\n",
        "1. Capas compartidas → dos “streams”:\n",
        "   - **Valor** \\(V(s)\\)\n",
        "   - **Ventaja** \\(A(s,a)\\)\n",
        "2. Combinación:\n",
        "   $$\n",
        "     Q(s,a)\n",
        "     = V(s)\n",
        "       + \\Bigl(A(s,a)\n",
        "         - \\tfrac{1}{|\\mathcal{A}|}\\sum_{a'}A(s,a')\\Bigr).\n",
        "   $$\n",
        "\n",
        "- **Ventaja**: aprendizaje más rápido en entornos con bajo impacto de la acción.  \n",
        "- **Desventaja**: arquitectura y ajuste de hiperparámetros algo más complejos.\n",
        "\n",
        "---\n",
        "\n",
        "### 2.4 Comparativa de variantes\n",
        "\n",
        "| Característica                    | ε-greedy DQN     | Boltzmann DQN      | Double DQN         | Dueling DQN        |\n",
        "|-----------------------------------|------------------|--------------------|--------------------|--------------------|\n",
        "| **Exploración**                   | ε-aleatoria      | softmax (τ)        | ε-aleatoria        | ε-aleatoria        |\n",
        "| **Overestimation bias**           | elevado          | elevado            | reducido           | reducido           |\n",
        "| **Estimación valor**              | directa \\(Q\\)    | directa \\(Q\\)      | directa \\(Q\\)      | separada \\(V/A\\)   |\n",
        "| **Complejidad arquitectural**     | baja             | baja-media         | media              | media-alta         |\n",
        "| **Estabilidad y convergencia**    | moderada         | moderada           | alta               | alta               |\n",
        "\n",
        "> **Nota**: es posible combinar Double y Dueling con exploración softmax en lugar de ε-greedy.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "rSDgjv9jYq6V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Una vez hayas entrenado y guardado los pesos (.h5f), puedes guardar todo el modelo en formato .h5 completo así:\n",
        "\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# 1) Reconstruye la arquitectura (o usa build_base_model / build_dueling_model)\n",
        "dqn_model = build_base_model()\n",
        "ddqn_model = build_base_model()\n",
        "dueling_model = build_dueling_model()\n",
        "\n",
        "# 2) Carga los pesos entrenados\n",
        "dqn_model.load_weights('dqn_weights.h5f')\n",
        "ddqn_model.load_weights('ddqn_weights.h5f')\n",
        "dueling_model.load_weights('dueling_dqn_weights.h5f')\n",
        "\n",
        "# 3) Guarda el modelo completo en .h5\n",
        "dqn_model.save('dqn_full_model.h5')\n",
        "ddqn_model.save('ddqn_full_model.h5')\n",
        "dueling_model.save('dueling_dqn_full_model.h5')\n",
        "\n",
        "print(\"Modelos guardados como .h5 en tu directorio de trabajo.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJVJNqQFALt5",
        "outputId": "8db16d8b-4ec7-432c-9428-46db74f27f9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modelos guardados como .h5 en tu directorio de trabajo.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Descargarlo\n",
        "files.download('dqn_full_model.h5')\n",
        "files.download('ddqn_full_model.h5')\n",
        "files.download('dueling_dqn_full_model.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "t3DeZO1hAy9M",
        "outputId": "df111a0a-7d41-4190-f48c-37ebf8030c79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'files' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-59cabf41c023>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Descargarlo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dqn_full_model.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ddqn_full_model.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dueling_dqn_full_model.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'files' is not defined"
          ]
        }
      ]
    }
  ]
}